title,url,text,cluster
Analysis: AI in health care could save lives and money — but not yet - PBS,https://news.google.com/rss/articles/CBMimAFBVV95cUxNUmZsb2dDUlZjdjN4T2xfOVZ2MXVic2h0TnM1MHNNT3gtejUyUHgyd2Z5Uk5KcUo1OTJ0QldfU21JVmVuY3ZhSDBOcS1oTEhpa0E0Q0htanVkc0ZuVE50aENEUnB1amxmb0dBQ3hhNFhMcUszcXBLTEJFUUg3cGhYOURQakdOcXRDRFhXQldhTnpFR3hfem5wZtIBngFBVV95cUxPM1hUdVVhTlNIQlJnaTI1M0s2TmtKSl9RZXNiY0FoS0dPWjRnUFI5Q3UyWk9lZnBXQnZ4NHowc3U3b19jVkl3SnZSTHJqMUVHYXY5SGVsUjNIUkRFWnFleWVLVWNDY3BuZ2s5cmt1ZUpDZWE5bGVXdlF5NzZ6X3VhN1FRUFhHQ2ZHOW50c3paQ2txT1JBTURmNmkwd080dw?oc=5&hl=en-US&gl=US&ceid=US:en,"helps your community explore new worlds and ideas through programs that educate, inform and inspire. Your tax-deductible donation helps make it all possible. Stand up for truly independent, trusted news that you can count on! Turgay Ayer, The Conversation Turgay Ayer, The Conversation Leave your feedback Imagine walking into your doctor s office feeling sick   and rather than flipping through pages of your medical history or running tests that take days, your doctor instantly pulls together data from your health records, genetic profile and wearable devices to help decipher what s wrong. This kind of rapid diagnosis is one of the big promises of artificial intelligence for use in health care. Proponents of the technology say that over the coming decades, AI has the potential to save hundreds of thousands, even millions of lives. What s more, a 2023 study found that if the health care industry significantly increased its use of AI, up to US$360 billion annually could be saved. WATCH: How artificial intelligence impacted our lives in 2024 and what s next But though artificial intelligence has become nearly ubiquitous, from smartphones to chatbots to self-driving cars, its impact on health care so far has been relatively low. A 2024 American Medical Association survey found that 66% of U.S. physicians had used AI tools in some capacity, up from 38% in 2023. But most of it was for administrative or low-risk support. And although 43% of U.S. health care organizations had added or expanded AI use in 2024, many implementations are still exploratory, particularly when it comes to medical decisions and diagnoses. I m a professor and researcher who studies AI and health care analytics. I ll try to explain why AI s growth will be gradual, and how technical limitations and ethical concerns stand in the way of AI s widespread adoption by the medical industry. Artificial intelligence excels at finding patterns in large sets of data. In medicine, these patterns could signal early signs of disease that a human physician might overlook   or indicate the best treatment option, based on how other patients with similar symptoms and backgrounds responded. Ultimately, this will lead to faster, more accurate diagnoses and more personalized care. AI can also help hospitals run more efficiently by analyzing workflows, predicting staffing needs and scheduling surgeries so that precious resources, such as operating rooms, are used most effectively. By streamlining tasks that take hours of human effort, AI can let health care professionals focus more on direct patient care. WATCH: What to know about an AI transcription tool that  hallucinates  medical interactions But for all its power, AI can make mistakes. Although these systems are trained on data from real patients, they can struggle when encountering something unusual, or when data doesn t perfectly match the patient in front of them. As a result, AI doesn t always give an accurate diagnosis. This problem is called algorithmic drift   when AI systems perform well in controlled settings but lose accuracy in real-world situations. Racial and ethnic bias is another issue. If data includes bias because it doesn t include enough patients of certain racial or ethnic groups, then AI might give inaccurate recommendations for them, leading to misdiagnoses. Some evidence suggests this has already happened. Humans and AI are beginning to work together at this Florida hospital. Health care systems are labyrinthian in their complexity. The prospect of integrating artificial intelligence into existing workflows is daunting; introducing a new technology like AI disrupts daily routines. Staff will need extra training to use AI tools effectively. Many hospitals, clinics and doctor s offices simply don t have the time, personnel, money or will to implement AI. Also, many cutting-edge AI systems operate as opaque  black boxes.  They churn out recommendations, but even its developers might struggle to fully explain how. This opacity clashes with the needs of medicine, where decisions demand justification. WATCH: As artificial intelligence rapidly advances, experts debate level of threat to humanity But developers are often reluctant to disclose their proprietary algorithms or data sources, both to protect intellectual property and because the complexity can be hard to distill. The lack of transparency feeds skepticism among practitioners, which then slows regulatory approval and erodes trust in AI outputs. Many experts argue that transparency is not just an ethical nicety but a practical necessity for adoption in health care settings. There are also privacy concerns; data sharing could threaten patient confidentiality. To train algorithms or make predictions, medical AI systems often require huge amounts of patient data. If not handled properly, AI could expose sensitive health information, whether through data breaches or unintended use of patient records. For instance, a clinician using a cloud-based AI assistant to draft a note must ensure no unauthorized party can access that patient s data. U.S. regulations such as the HIPAA law impose strict rules on health data sharing, which means AI developers need robust safeguards. WATCH: How Russia is using artificial intelligence to interfere in election | PBS News Privacy concerns also extend to patients  trust: If people fear their medical data might be misused by an algorithm, they may be less forthcoming or even refuse AI-guided care. The grand promise of AI is a formidable barrier in itself. Expectations are tremendous. AI is often portrayed as a magical solution that can diagnose any disease and revolutionize the health care industry overnight. Unrealistic assumptions like that often lead to disappointment. AI may not immediately deliver on its promises. Finally, developing an AI system that works well involves a lot of trial and error. AI systems must go through rigorous testing to make certain they re safe and effective. This takes years, and even after a system is approved, adjustments may be needed as it encounters new types of data and real-world situations. AI could rapidly accelerate the discovery of new medications. Today, hospitals are rapidly adopting AI scribes that listen during patient visits and automatically draft clinical notes, reducing paperwork and letting physicians spend more time with patients. Surveys show over 20% of physicians now use AI for writing progress notes or discharge summaries. AI is also becoming a quiet force in administrative work. Hospitals deploy AI chatbots to handle appointment scheduling, triage common patient questions and translate languages in real time. READ MORE: AI and  recession-proof  jobs: 4 tips for new job seekers Clinical uses of AI exist but are more limited. At some hospitals, AI is a second eye for radiologists looking for early signs of disease. But physicians are still reluctant to hand decisions over to machines; only about 12% of them currently rely on AI for diagnostic help. Suffice to say that health care s transition to AI will be incremental. Emerging technologies need time to mature, and the short-term needs of health care still outweigh long-term gains. In the meantime, AI s potential to treat millions and save trillions awaits. This article is republished from The Conversation under a Creative Commons license. Read the original article. Stand up for truly independent, trusted news that you can count on! Left: AI can help hospitals run more efficiently by analyzing workflows, predicting staffing needs and scheduling surgeries so that resources are used effectively. File photo via Getty Images By Hannah Grabenstein By John Yang, Kaisha Young By Jeffrey Brown By Simon Ostrovsky, Yegor Troyanovsky By Paul Solman, Ryan Connelly Holmes Turgay Ayer, The Conversation Turgay Ayer, The Conversation Turgay Ayer is a professor of industrial and systems engineering at the Georgia Institute of Technology. Support Provided By: Learn more Subscribe to Here s the Deal, our politics newsletter for analysis you won t find anywhere else. Thank you. Please check your inbox to confirm. Read Jul 27 Tom Lehrer, mathematician and singer-songwriter known for colorful satire, dies at 97 Watch Jul 25 Brooks and Capehart on the Epstein files fracturing Trump s base Read Jul 26 How germy is the public pool? An infectious disease expert weighs in Watch Jul 27 What to expect from the new U.S.-EU trade framework announced by Trump Read Jul 27 WATCH: Trump and von der Leyen announce U.S.-EU trade framework Nation Jul 27 By Jeff Martin, Associated Press World Jul 27 By Stefanie Dazio, Associated Press World Jul 27 By Associated Press Nation Jul 27 By Mike Householder, Ryan Sun, Michael Casey, Associated Press Politics Jul 27 By Will Weissert, Associated Press Nation Jul 27 By Karen Tan, The Conversation Arts Jul 27 By Gillian Flaccus, Associated Press World Jul 27 By Andrew Wilks, Associated Press   1996 - 2025 NewsHour Productions LLC. All Rights Reserved. PBS is a 501(c)(3) not-for-profit organization. Sections About Stay Connected Subscribe to Here's the Deal with Lisa Desjardins Thank you. Please check your inbox to confirm. Support for News Hour Provided By",2
HIMSSCast: Trends and strategies from the HIMSS AI in Healthcare Forum - Healthcare IT News,https://news.google.com/rss/articles/CBMinwFBVV95cUxQamZqTFV2YXNJNVpIaHRWWHpqN1dNTlB4cDBaaW55cWV4RTI0aEZMdTZJVzdWRk1WRlpfSkNwVS1kbUhfOHdROFRWemszR0M4b25oanh2M1RoanFnYWNMRFNCR1FpcDdRSzlSOHg1TnhRMTdiMHJLVHA4aFFEQ3RVS0lKMXRtaHRqekx1TXhKVzU0Vzh1M0lrbGlNcDcxVEk?oc=5&hl=en-US&gl=US&ceid=US:en,"At the HIMSS AI in Healthcare Forum earlier this month, more than 400 senior clinical, operational and information technology leaders from healthcare organizations of all shapes and sizes gathered in Brooklyn, New York, for two days of energetic discussion. The presentations, panel chats and networking breaks were all buzzing about the promise and potential   and the plenty of not-insignificant hurdles along the way   of deploying artificial intelligence and machine learning to help solve myriad health system challenges. Attendees were talking about big-picture imperatives like C-suite leadership and strategy and workforce training and trust, and also more fundamental needs, such as data governance, algorithmic transparency, workflow integration, change management, IT infrastructure, resource allocation, cybersecurity, patient safety and more. After a busy few years of pursuing AI projects and goals in earnest, healthcare leaders are still learning as they go and trying to keep pace as the technology evolves and its applications expand. To talk about those trends and others, we took some time at the show to speak with the forum's emcee, our colleague Rob Havasy, senior director of informatics strategy at HIMSS, about the trends, opportunities and challenges of AI in healthcare in 2025. Here's what he had to say. Like what you hear? Subscribe to the podcast on Apple Podcasts, Spotify or Amazon Music. Talking points: More about this episode: Building LLM-powered clinical apps that work for doctors AI-enabled workforce is essential for achieving IT investment ROI With AI, 'the only way to build trust is to earn trust' At NYU, new research into AI-enabled patient engagement What makes for a good health AI investment? One VC has some thoughts Implementing agentic AI in healthcare requires caution Building an in-house AI tool from scratch The role of AI in digital health People-centered care, the importance of trust in healthcare Underserved hospitals and the digital divide Newsletter Signup Thank you! Your submission has been received.   current_year Healthcare IT News is a publication of HIMSS Media",2
A doctor’s appointment that never comes: How AI can transform healthcare (and save lives) - EL PAÍS English,https://news.google.com/rss/articles/CBMi0AFBVV95cUxPUEUzY28zMms2cTBKNDg0OWZTX1JZOEpOYzdmS3RpVFdnRnZxX1JBLWxkb1cxZHllWkF0NUw2dkZkZTRmRFdpQVBrQWlTZUpQN0lzY25tSnNlY3pQcThleGtLa3NsZW1hUEdpUmRfdmZiTWU3emZnWV9xMUUyaWMyRDRUVWJIWHpPWHNpQkRqNzhOdFBfSmJvMFJkY0RLN1NVZUZ5SmZ5MnNuLWFBTlFiOWJwdlRSUnRYUWtwN1lFNVhhZ2R4ampoUG5RejJDVi1N0gHkAUFVX3lxTFBtX1dHc2JJUGtuRk1HZy1adUlsR1RWRGhZZFIxd2JuWUotS3NQbkc2SWRrVVpCLUFZQllwYjh0eXpwc196TGlzM014ZzdtbkhZYi13N2hneE40ZUhUMGJqSjJXQ3ZkeUR2ODU3YXU2dFpiMlc2RHFNTVFZVUhNWXdPQ2VOc3p1ZDU1TGRraDNKTDlfUzBDc1pWZUMwQ2JIdDdoMjZXak5uY25oa0JOQWVndEl4WnhsQTM1UkdyT0lLVi1EM0tPTXVYY09ORkYzNVBXajV1UXZJempxWGVDbDhxR2tadw?oc=5&hl=en-US&gl=US&ceid=US:en,"Andy Chang just turned 45. He lives in Chicago and holds a management position at the University of Chicago Medicine, one of America s leading academic hospitals. At his age, he is due a colonoscopy. A routine check, nothing extraordinary. The challenge is to get the appointment. You call the hospital, and they ask you to leave a message. You do. They call you back. Another message. Call again. He proposes to solve it by mail.  You can t,  he is told.  It has to be over the phone.  Six months on, he is still waiting. It sounds surreal, but the fact is, it is a health system bound by bureaucracy and overwhelmed by processes that have not kept pace with the needs of the population. Chang shared his experience with hundreds of people at CNX Chicago, one of the city s premier tech events. When the laughter died down, he said,  If AgentForce had existed, it would have got done.  According to the World Health Organization and the Centers for Disease Control and Prevention (CDC), men should start having a colonoscopy as early as age 45, even if they don t have symptoms. Colorectal cancer is among the most common globally, and catching it early can mean the difference between preventive intervention and a race against time. Dr. Montserrat Del Castillo from UNAM, Mexico s National Autonomous University, explains that a colonoscopy allows polyps to be identified and removed before they become malignant.  What good is the most advanced medical technology if a simple appointment takes half a year to come round?  she says. Chang was not talking about science fiction or magic shortcuts, but about a new way of conceiving technology: as an ally to transform the human experience in systems such as healthcare. Salesforce, the leading American company in customer relationship management, calls this vision the new agentic stack: a layered software architecture that unifies data, integrates applications, activates communication flows and allows AI agents to perform specific tasks without replacing the essential human bond. Beyond automating tasks or generating Ghibli-style images, AI can save lives. While patients continue to face considerable delays, the medical community is already debating how AI could prevent stories like Chang s from being the norm. This potential was the focus of the American Society of Clinical Oncology (ASCO) Annual Meeting, held recently in Chicago. More than 40,000 specialists discussed the future of cancer care under a clear motto:  Driving knowledge to action.   Medicine won t be the same in two years because of the widespread use of artificial intelligence,  said Dr. Debra Patt of the US Cancer Network. Her statement is not based on optimism, but on evidence: AI is already helping in cancer diagnosis, the design of personalized treatments, and access to clinical trials. All of this contributes to a fairer, more efficient and patient-centered system. Of course, the environmental impact of these technologies is real and must be addressed responsibly. But ignoring their potential also involves a risk: perpetuating inefficient systems and excluding those who need help most by refusing to use a tool that can make a difference. This is where technology such as Data Cloud, one of the pillars of Salesforce, comes into play. It allows patient information to be centralized without duplicating or fragmenting it. Integration is key to redesigning the clinical experience: anticipating, accompanying and responding in real time with precision and empathy. In Latin America, AI is also being considered to boost efficiency in health systems. Dr. Guadalupe Rodr guez Porcayo, former president of the Mexican Society of Public Health (2023 2024), believes that AI  would be very useful and would speed up medical appointments,  even in public institutions such as the IMSS (Mexican Institute of Social Security) or the ISSSTE (Institute of Security and Social Services of State Workers). However, she points out that its effectiveness in the healthcare system would be limited by a structural problem: the shortage of medical personnel.  More appointments can be scheduled with AI, but if there are not enough health professionals, each patient will continue to wait their turn. Technology does not replace the time needed for quality care,  she says. The real challenge, she says, is to reinforce the first level of care and promote a culture of prevention.  The underlying problem is the lack of health education in the population and timely detection. There is a lot of work to be done in health centers and family clinics.  AI alone will not solve all problems, but it can be a powerful ally in building a more agile, fair, and people-centered health system. If accompanied by political will, investment in medical personnel and a clear commitment to prevention, it is possible that stories like Andy Chang s will begin to become the exception, in the U.S., in Mexico and in the rest of Latin America. Sign up for our weekly newsletter to get more English-language news coverage from EL PA S USA Edition  Quieres a adir otro usuario a tu suscripci n? Si contin as leyendo en este dispositivo, no se podr  leer en el otro.  Por qu  est s viendo esto? Si quieres compartir tu cuenta, cambia tu suscripci n a la modalidad Premium, as  podr s a adir otro usuario. Cada uno acceder  con su propia cuenta de email, lo que os permitir  personalizar vuestra experiencia en EL PA S.  Tienes una suscripci n de empresa? Accede aqu  para contratar m s cuentas. En el caso de no saber qui n est  usando tu cuenta, te recomendamos cambiar tu contrase a aqu . Si decides continuar compartiendo tu cuenta, este mensaje se mostrar  en tu dispositivo y en el de la otra persona que est  usando tu cuenta de forma indefinida, afectando a tu experiencia de lectura. Puedes consultar aqu  los t rminos y condiciones de la suscripci n digital.",2
Beyond the Hype: Building AI Systems in Healthcare Where Hallucinations Are Not an Option - HIT Consultant,https://news.google.com/rss/articles/CBMixAFBVV95cUxPRkhXcHEycHQ4WWZJQm1TQ0ZfVkQwbmw2bGtNVndtLTBENzNuOElmVTZvcHZnTHRzWTdmNXFjdWpUU091VW9SQS1sUVJjUExDeXJwQTYzamdoUnNRbDh2SjU4WGZjZlQ5Rnh5RWxBYTZUNTB3Rjd0N2RXdDh1bFNwV3RfYnZsN2E0eU5VT1J2bjZCWWZmWElYb3RKSmoteXl5bGlxUng4YVVhQzdfbHkyMDNScVd4MTcycGdNYTZZS0FzR1Vy?oc=5&hl=en-US&gl=US&ceid=US:en,"by Dr Venkat Srinivasan Ph D Founder Chair at Gyan AI 07/25/2025 Leave a Comment As a technologist and entrepreneur who has spent decades architecting enterprise-grade AI systems across highly regulated industries, I ve seen firsthand the chasm between AI s promise and its practical risks, especially in domains like healthcare, where trust is not optional and the margin for error is razor-thin. Nowhere is the cost of a hallucinated answer higher than at a patient s bedside. When an AI system confidently presents false information whether in clinical decision support, documentation, or diagnostics the consequences can be immediate and irreversible. As AI becomes more embedded in care delivery, healthcare leaders must move beyond the hype and confront a difficult truth: not all AI is  fit for purpose . And unless we redesign these systems from the ground up with verifiability, traceability, and zero-hallucination as defaults we risk doing more harm than good. Hallucinations: A Hidden Threat in Plain Sight And yet, there is no doubt that Large language models (LLMs) have opened new frontiers for healthcare, enabling everything from patient triage to administrative automation. But they come with an underestimated flaw: hallucinations. These are fabricated outputs statements delivered with confidence, with no factual basis. The risks are not theoretical. In a widely cited study, ChatGPT produced convincing but entirely fictitious PubMed citations on genetic conditions. Stanford researchers found that even retrieval-augmented models like GPT-4 with internet access made unsupported clinical assertions in nearly one-third of cases. The consequences? Misdiagnoses, incorrect treatment recommendations, or flawed documentation. Healthcare, more than any other domain, cannot afford these failures. As ECRI recently noted in naming poor AI governance among its top patient safety concerns, unverified outputs in clinical contexts may lead to injury or death, not just inefficiency. Redefining the Architecture of Trustworthy AI Building AI systems for environments where human lives are at stake demands an architectural shift away from generalized, probabilistic models and toward systems engineered for precision, provenance, and accountability. This shift in my view rests on five foundational pillars: AI outputs in healthcare settings must be understandable not just to engineers but to clinicians and patients. When a model suggests a diagnosis, it must also explain how it reached that conclusion, highlighting the relevant clinical factors or reference materials. Without this, trust cannot exist. The FDA has repeatedly emphasized that explainability is essential to patient-centered AI. It s not just a compliance feature it s a safeguard. (b) Source Traceability and Grounding Every output in a clinical AI system should be traceable to a verified, high-integrity source: peer-reviewed literature, certified medical databases, or the patient s structured records. In systems we ve designed, answers are never generated in isolation; they are grounded in curated, auditable knowledge every claim backed by a source you can inspect. This kind of design is the most effective antidote to hallucinations. (c) Privacy by Design In healthcare, compliance is not an option, it is a necessity. Every component of an AI system must be HIPAA-aware, with end-to-end encryption, stringent access controls, and de-identification practices baked in. This is why leaders must demand more than just privacy policies they need provable, system-level safeguards that stand up to regulatory scrutiny. (d) Auditability and Continuous Validation AI models must log every input and output, every version change, and every downstream impact. Just as clinical labs are audited, so too should AI tools be monitored for accuracy drift, adverse events, or unexpected outcomes. This is not just about defending decisions it s also about improving them over time. (e) Human Oversight and Organizational Governance No AI should be deployed in a vacuum. Multidisciplinary oversight combining clinical, technical, legal, and operational leadership is essential. This isn t about bureaucracy; it s about responsible governance. Institutions should formalize approval workflows, set thresholds for human review, and continuously evaluate AI s real-world performance. An Executive Framework for Responsible AI Adoption For healthcare executives, the path forward with AI models should begin with questions. This can include, Is this model explainable, and to which practitioners or audience? Can every output be tied to a trusted, inspectable source? Does it meet HIPAA and broader ethical standards for data use? Can its behavior be audited, interrogated, and improved over time? Who is responsible for its decisions, and who is accountable when it fails? These questions should also be embedded into procurement frameworks, vendor assessments, and internal deployment protocols. Stakeholders in the healthcare ecosystem can start with low-risk applications, like administrative documentation or patient engagement, but design with future clinical use in mind. They should insist on solutions that are intentionally designed for zero hallucination, rather than retrofitted for it. And most importantly, any AI integration should involve investments in clinician education and involvement. AI that operates without clinical context is not only ineffective it is dangerous. From Possibility to Precision It s clear to me that the age of  speculative AI  in healthcare is ending. What comes next must be defined by rigor, restraint, and responsibility. We do not need more tools that impress we need responsible systems that can be trusted. Enterprises in healthcare should reject models that treat hallucination as an acceptable side effect. Instead, they should look to systems purpose-built for high-stakes environments, where every output is explainable, every answer traceable, and every design choice made with the patient in mind. In summary, if the cost of being wrong is high, as it certainly is in healthcare systems, your AI system should never be a cause or reason. About Dr. Venkat Srinivasan, Ph.D Dr. Venkat Srinivasan, PhD, is Founder & Chair of Gyan AI and a technologist with decades of experience in enterprise AI and healthcare. Gyan is a fundamentally new AI architecture built for Enterprises with low or zero tolerance for hallucinations, IP risks, or energy-hungry models. Where trust, precision, and accountability are important, Gyan ensures every insight is explainable, traceable to reliable sources, with full data privacy at its core. Tagged With: Artificial Intelligence Get in-depth healthcare technology analysis and commentary delivered straight to your email weekly Latest insightful articles delivered straight to your inbox weekly. Submit a Tip or Pitch Selecting the Right EMR: A Practical Guide to Streamlining Your Practice and Enhancing Patient Care Virta Health CEO: GLP-1s Didn t Kill Weight Watchers, Its Broken Model Did Latest insightful articles delivered straight to your inbox weekly Copyright   2025. HIT Consultant Media. All Rights Reserved. Privacy Policy |",2
Trump administration releases AI adoption plan - Healthcare Dive,https://news.google.com/rss/articles/CBMilAFBVV95cUxPeEYyUWdWaFowSjVHMDF3aE44Ulk3SUw0cTVVSk0xcElkTXRlU0dLY1hoOWJ6TF90T2xOZDhFYzdMaGEwV21YcjBudzRMdGxqd3JQOFdaaDRGNGNnVklRM19VaVRRRnhNYkdlUEV3MmswbFEyN2tuc3NIc0piTEZ5c3N3MHdjakFKZWN0U1g3OEM1SEs4?oc=5&hl=en-US&gl=US&ceid=US:en,"Let Healthcare Dive s free newsletter keep you informed, straight from your inbox. While the plan rarely mentions healthcare, it is one of the administration s first steps to set federal policies, which experts say is important to safely deploy the technology in the sector. The plan was created as a result of a January executive order issued by President Donald Trump that aims to ensure the U.S. continues to be a competitive player in AI development. A portion of the plan focuses on removing red tape for AI implementation, including asking federal agencies to consider limiting funding for states that have  burdensome  AI regulations. The plan, however, doesn t detail which state laws could be considered onerous. The administration also plans to set up AI Centers of Excellence across the country to deploy and test AI tools, enabled by agencies like the Food and Drug Administration. Additionally, the plan looks to create domain-specific groups of public, private and academic stakeholders, including in healthcare, to speed the adoption of national standards for AI and measure how much the technology increases productivity. Some healthcare industry groups praised the plan. In a statement, Soumi Saha, senior vice president of government affairs at group purchasing organization Premier, said it  sets a course towards secure, trustworthy artificial intelligence (AI) in healthcare.  Federal involvement in AI standards could be a positive move too, Leigh Burchell, chair of the EHR Association Executive Committee, said in a Thursday statement.  As we evaluate the implications of the AI Action Plan for our member companies, their healthcare provider clients, and, most importantly, patients, we reiterate our call for a uniform, risk-based regulatory model at the federal level,  she said.  Fragmented state mandates risk slowing innovation and complicating compliance, which could deter innovation and adoption.  Healthcare leaders are excited about the promise of the technology in the sector   hoping AI could ameliorate staffing challenges and providers  heavy burden of administration and work   but it comes with risks. Inaccurate or misleading responses, as well as bias embedded in AI tools, could have serious consequences for patients and providers. Additionally, AI products aren t easy to implement and can require plenty of human labor, like ongoing monitoring of algorithms to ensure they re still performing up to standards. Meanwhile, a federal framework for AI in healthcare has been nascent. Former President Joe Biden attempted to set the groundwork with a sweeping executive order, but Trump rescinded that order days into his term. Along with other policies, Biden s order asked the HHS to establish a task force and lay out a strategic plan for the technology in the sector, which the department had released shortly before Trump took office. Get the free daily newsletter read by industry experts Sen. Chuck Grassley, R-Iowa, suggested Congress could once again move to overhaul PBMs  controversial business practices after legislators pass President Donald Trump s conservative megabill this summer. The agency said it would increase the number of MA plan audits and complete its backlog of reviews by investing in technology and growing its medical coding workforce. Subscribe to Healthcare Dive for top news, trends & analysis Get the free daily newsletter read by industry experts Sen. Chuck Grassley, R-Iowa, suggested Congress could once again move to overhaul PBMs  controversial business practices after legislators pass President Donald Trump s conservative megabill this summer. The agency said it would increase the number of MA plan audits and complete its backlog of reviews by investing in technology and growing its medical coding workforce. The free newsletter covering the top industry headlines",2
AI Innovations Are Making Their Way to Healthcare - Oracle,https://news.google.com/rss/articles/CBMiV0FVX3lxTFBKT2tKblNsQ0xYODNUa25ITGJEdFphMWNhMW5qMHZOUWV3c1FyVjBSYjZGWTNPYW04ZnV1RnNFRVl3X0VZQ1NGaUZOSDZFZW1DWTNFZVJFcw?oc=5&hl=en-US&gl=US&ceid=US:en,"Aaron Ricadela | Senior Writer | July 2, 2025 Global healthcare systems are strained by aging populations, growing numbers of chronically ill patients, rising treatment and drug costs, and personnel shortfalls. Meanwhile, burdensome documentation requirements are contributing to physician and nurse burnout. Rapid advances in predictive and generative AI are already improving how medical professionals, clinical researchers, and administrators at hospitals and insurers work, and they re poised to deliver even more transformative changes in the coming years. These AI systems excel at spotting hidden patterns in large data sets, zeroing in on hard-to-discern details in medical images, supporting diagnoses in complex cases, and recommending operational improvements that may be applied to pare costs. These advances could lead to process reforms, productivity gains, and improved patient outcomes. Read on to learn about the benefits, challenges, and applications of AI in healthcare. AI uses complex statistical prediction models and large amounts of computing to help solve complex problems, understand and respond to natural language queries, create videos and other forms of online content, classify images, and more. Neural networks, including large language models, are trained on large amounts of historical data to construct AI models that can make predictions to help users anticipate and solve a range of problems. These models can also go back through their statistical parameters to correct errors and transfer their knowledge to draw inferences about new problems and domains. Large investments in the data centers and chips needed to train AI models and power their inferencing (the reasoning process they use to respond to user queries) have fueled the AI boom. Physicians, clinical researchers, pharmaceutical companies, and medical staff are using artificial intelligence technology to aid diagnoses, patient exams, drug development, and hospitals  efficiency. Electronic health records (EHRs) have come into widespread use in US hospitals and medical practices over the past 15 years, in large part because of billions of dollars in federal incentives. While they ve made recordkeeping more accurate and reduced medical errors, their unwieldy note-taking requirements, hard-to-navigate screens, and often superfluous alerts and inbox messages have also created extra work for healthcare professionals. AI agent enhanced EHRs can help save clinicians time and increase patient face time by requesting that they generate summaries of patients  conditions, medications, and lab results before exams, quickly jump to key functions, and speak or type natural language commands. In radiology, AI systems can help spot areas of scans with the highest probability of abnormal tissue growth or measure specific indicators, such as changes in kidney volume, that can help physicians predict function declines before they show up in blood tests. Many AI healthcare applications, however, are aimed at easing hospitals  and medical practices  administrative burdens for example, by automating billing and scheduling, helping write prior authorization letters to insurance companies, or reminding a patient that it s time for a mammogram. The healthcare IT sector is building GenAI systems that support diagnoses by analyzing patients  histories, exam findings, and lab test results alongside reviews of existing bodies of knowledge about diseases, and reaching conclusions that can assist physicians on complex cases. Key Takeaways AI is poised to deliver a range of benefits in medical research, drug development, clinical diagnoses and care, and healthcare administration. Applying AI to EHR data doesn t automatically result in improved insights, patient care, and hospital processes. Clinicians, administrators, and other staff need to trust the technology enough to use it regularly and be aware of the potential for mistakes. Financially strapped hospitals need to understand the high cost of cleaning up and anonymizing patient data so it s ready to train AI models. Read on for more on these and other challenges. Medical professionals are applying AI across a range of applications to improve diagnostic insights and clinical decisions, predict patient outcomes, and accomplish so much more. Here are 10 of the most common AI use cases in healthcare and life sciences. Further development and adoption of national and industry standards will help healthcare organizations and governments share more data, providing a stronger basis for AI-driven insights. But financially pressured hospitals will need to find ways to invest in the latest tools and prepare their data for AI analysis. Likely to come into wider use are hospital robots that nurses and other staff control from their phones to help ferry lab samples, medical equipment, and supplies to shorten delivery times and free up staff time. EHRs that use GenAI to quickly get pertinent information to physicians at the right time and cut down on complex screen navigation are also starting to come to market. Within the next decade, doctors will likely benefit from AI systems that help support medical decision-making during patient visits, suggesting diagnoses via a PC or tablet based on what the doctor has said, the existing literature, and data about similar past cases. The systems could also help recommend tests and medicines. Oracle Health products enhance various aspects of care, including through generative AI. They can help personalize workflows for staff, streamline managing patients, and provide relevant information before exams. Oracle Health Clinical AI Agent captures doctor-patient conversations to generate draft EHR notes, and it lets doctors call up data from patients  medical histories via voice commands. Oracle Health Data Intelligence lets providers and payers perform AI analyses on clinical and financial data. The services can prioritize high-risk patients, flag overdue screenings, and prompt patients to schedule appointments. How is AI used in healthcare? Artificial intelligence is transforming numerous aspects of patient care and healthcare administration, including diagnostic support, personalized treatment plans, documentation, clinical trials, and hospital planning. What is an example of AI in healthcare? AI-enhanced healthcare software can quickly call up information about patients  histories from electronic health records, help doctors more quickly document patient visits, assist pharma companies in designing clinical trials, and help hospitals plan staffing.",2
6 ways AI is transforming healthcare - The World Economic Forum,https://news.google.com/rss/articles/CBMiekFVX3lxTE1HYld1WGtIMC1zZVR5QUl5OU1IalVXUXdYUmdFRkxaZkxuRnNqYU12SXZJNUd5U1BaZHNBdlVrTWY5alB6SUYyUHBHcHA4eGN4QlE5X3pTN3NHSWE2MU5JeGQtV2szWEJEQzJSZW5mMUdXT0ZrYzVpbzFn?oc=5&hl=en-US&gl=US&ceid=US:en,"Some of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness. Some of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness. Some of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness. The World Economic Forum ( Forum ) uses necessary cookies to make our site work. We would also like to set optional  performance  cookies to gather anonymous site visitation data for internal use and we use ""marketing"" cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you ve provided to them or that they ve collected from your use of their services. By enabling these cookies, you can help the Forum provide a better website for users like yourself. For more information about the Forum cookies and third-party cookies, see our Cookie Declaration. Cookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies we need your permission. You can change your preferences at any time or withdraw your consent by clicking on  Change your consent  on the Cookie Declaration page. AI can help to assess ambulance needs. Image: REUTERS/Henry Nicholls This article has been updated. With 4.5 billion people currently without access to essential healthcare services and a health worker shortage of 11 million expected by 2030, AI has the potential to help bridge that gap and revolutionize global healthcare. It could even get us back on track to meet the United Nations' Sustainable Development Goal of achieving universal health coverage by 2030. But while the technology is rapidly developing, healthcare is ""below average"" in its adoption of AI compared to other industries, according to the World Economic Forum's white paper, The Future of AI-Enabled Health: Leading the Way. ""AI transformation goes beyond adopting new tools,"" it says. ""It involves rethinking the fundamentals of how health is delivered and accessed."" With the generative AI in the healthcare market expected to hit $2.7 billion this year - and reach close to $17 billion by 2034 - here are six ways AI is already transforming healthcare. A new AI software is ""twice as accurate"" as professionals at examining the brain scans of stroke patients. Two UK universities trained the software on a dataset of 800 brain scans of stroke patients and then trialled it on 2,000 patients. The results were impressive. Alongside the AI model's accuracy, the software was also able to identify the timescale within which the stroke happened - crucial information for professionals. As Dr Paul Bentley, consultant neurologist, told the Health Tech Newspaper:  For the majority of strokes caused by a blood clot, if a patient is within 4.5 hours of the stroke happening, he or she is eligible for both medical and surgical treatments. Up to 6 hours, the patient is also eligible for surgical treatment, but after this time point, deciding whether these treatments might be beneficial becomes tricky, as more cases become irreversible. So it s essential for doctors to know both the initial onset time, as well as whether a stroke could be reversed.  Surprisingly, urgent care doctors miss broken bones in up to 10% of cases. What s more, X-ray technicians are both in short supply and overloaded. So using AI to do the initial scan could potentially avoid both unnecessary X-rays and missed fractures. The UK s National Institute for Health and Care Excellence (NICE) says the technology is safe, reliable and could reduce the need for follow-up appointments. Accept our marketing cookies to access this content. These cookies are currently disabled in your browser. But there are concerns around the fast rollout of AI in healthcare. ""It is important that people using these tools are properly trained in doing so, meaning they understand and know how to mitigate risks from technological limitations ... such as the possibility for wrong information being given,"" Dr Caroline Green of the Institute for Ethics in AI at the University of Oxford told the BBC. How is the World Economic Forum creating guardrails for Artificial Intelligence? In response to the uncertainties surrounding generative AI and the need for robust AI governance frameworks to ensure responsible and beneficial outcomes for all, the Forum s Centre for the Fourth Industrial Revolution (C4IR) has launched the AI Governance Alliance. The Alliance unites industry leaders, governments, academic institutions, and civil society organizations to champion responsible global design and release of transparent and inclusive AI systems. This includes the workstreams part of the AI Transformation of Industries initiative, in collaboration with the Centre for Energy and Materials, the Centre for Advanced Manufacturing and Supply Chains, the Centre for Cybersecurity, the Centre for Nature and Climate, and the Global Industries team. In the UK, around 350,000 people are taken by ambulance to hospital each month. It s down to paramedics to decide who does or doesn t need to go, and always with an awareness of how few beds are available. A study in Yorkshire in the north of England found that in 80% of cases AI could correctly predict the patients that needed to be transferred to hospital. The AI model was trained on factors such as a patient s mobility, pulse and blood oxygen levels and chest pain   it also proved to respond without bias. NICE did caution, however, that before being put into more widespread use, more training was needed. A new AI machine learning model can detect the presence of certain diseases before the patient is even aware of any symptoms, according to its maker AstraZeneca. Using medical data from 500,000 people who are part of a UK health data repository, the machine could  predict with high confidence a disease diagnosis many years later . Slav  Petrovski, who led the research, told Sky News: ""For many of these diseases, by the time they manifest clinically and the individual goes to the doctor because of an ailment or visible observation, that is far down the line from when the disease process began.  We can pick up signatures in an individual that are highly predictive of developing diseases like Alzheimer's, chronic obstructive pulmonary disease, kidney disease and many others,"" he said. Another UK study has found that an AI tool can successfully detect 64% of epilepsy brain lesions previously missed by radiologists. Trained on the MRI scans of over 1,100 adults and children globally, the AI tool was able to spot lesions more quickly than a doctor, but also discover tiny or obscured ones that had evaded the human eye. ""It's like finding one character on five pages of solid black text,"" lead researcher Dr Konrad Wagstyl told the BBC. ""AI can find about two-thirds that doctors miss - but a third are still really difficult to find."" Combining AI's findings with human oversight and expertise has the potential to speed up both diagnosis and cure, the researchers say. Doctors must make informed, swift medical decisions: AI could potentially speed these up, but it could also provide unreliable or biased information. A US study found that standard large language models (LLMs) like ChatGPT, Claude or Gemini were unable to provide clinicians with sufficiently relevant or evidence-based answers to their medical questions. But ChatRWD, a retrieval-augmented generation (RAG) system   which essentially combines LLMs with retrieval systems to improve output   produced useful answers to 58% of the questions (compared with 2%-10% for the LLMs). Digital interfaces are increasingly being rolled out to help triage patients, too. In an insight report from 2024, part of the World Economic Forum s Digital Healthcare Transformation Initiative, a case study on digital patient platform Huma, revealed it could reduce readmission rates by 30%, time spent reviewing patients by up to 40% and  alleviated the workload of healthcare providers . The report anticipates a future in which technologies like these could  dramatically transform the patient experience. People who are generally healthy can use self-monitoring devices to optimize their mental and physical health, while those with health issues will have access to a wide range of digital solutions . Administrative tasks are both inevitable and time-consuming in the medical field. Using AI co-pilots could free up clinicians to focus more of their time on patients. Microsoft has recently announced its Dragon Copilot, an AI healthcare tool that can listen to, and create notes on, clinical consultations. And Google already has a suite of AI models specifically tailored to alleviate some of the administrative burdens in healthcare. In Germany, an AI platform called Elea has cut testing and diagnosis times from weeks to hours and its founders are determined to show that the technology ""can be an ally, not an obstacle"". Co-founder Dr Sebastian Casu told EU-Startups: ""No one joins the healthcare sector to spend hours on admin"". Of course, having an AI tool listening in and taking notes on your doctor's appointment will be a big mental leap for many. A recent study in the UK found that just 29% of people would trust AI to provide basic health advice (although over two-thirds are comfortable with the technology being used to free up professionals' time). Then there is the question of accuracy. A report last year found that OpenAI's Whisper, used by many hospitals to summarize patient meetings, was hallucinating some of the transcriptions. This is why regulation of AI tools is so vital. In the UK, AI-powered medical devices are strictly regulated by the Medicines and Healthcare products Regulatory Agency. In the US, the Food and Drug Administration (FDA) last year examined the regulation of AI in healthcare and concluded that, while the FDA will  continue to play a central role in ensuring safe, effective, and trustworthy AI tools  it was also essential that  all involved entities   attend to AI with the rigour this transformative technology merits . Accept our marketing cookies to access this content. These cookies are currently disabled in your browser. Create a free account and access your personalized content collection with our latest publications and analyses. License and Republishing World Economic Forum articles may be republished in accordance with the Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International Public License, and in accordance with our Terms of Use. The views expressed in this article are those of the author alone and not the World Economic Forum. Bringing you weekly curated insights and analysis on the global issues that matter. How Brunei s BruHealth journey charts the future of digital health Mohammad Isham Jaafar and Gong Yingying July 22, 2025 Heightened heatwave risks for the elderly, and other health stories Shyam Bishen July 17, 2025 How one global health leader prepares teams for the unexpected: Gavi CEO How we can harness AI for a healthier, more equitable world Michael Johnson July 14, 2025 Improving access to innovative medicines in Africa starts with clinical trials Jayasree K. Iyer July 9, 2025 Why we should be redefining the private sector's role in global health Sharmishta Sivaramakrishnan July 9, 2025 About us More from the Forum Engage with us Quick links Language editions Privacy Policy & Terms of Service Sitemap   2025 World Economic Forum Remove menu",2
"Health-e Law Episode 18: AI in Clinical Research: Opportunities, Risks, and the Road Ahead with Bill Kish and Dr. Brad Pruitt of Kenosha AI [Podcast] - The National Law Review",https://news.google.com/rss/articles/CBMisgFBVV95cUxPb1ZmcXRlbHU1QmtHMGxOQzZOa25EbGg3a0FKVmpxQWM2c1g0YzFUd2Y1Q3Jua3lFZDRmWTRLSU4wX3IwOUJ1UWlMVWo5aGYtckhmWEtPVEZiMUNFMUdsY0w0Y0x0STZpdkZIUmlwbE5IeHgxLXQxemdfbVpUQjk0YkdVT3ltaldxc3p2NHNRRTJfbzduSkpEZHJWMjlTQlZmTEpvb05EenIxaFJEOFBhNXJ3?oc=5&hl=en-US&gl=US&ceid=US:en,"We use cookies on this site to enhance your user experience. 12 Welcome to Health-e Law, Sheppard Mullin's podcast exploring the fascinating health tech topics and trends of the day. In this episode, Sara Shanti welcomes Bill Kish and Dr. Brad Pruitt of Kenosha AI to explore how artificial intelligence can enhance efficiency and compliance in healthcare research. CLICK HERE TO READ TRANSCRIPT What We Discussed in This Episode: About Bill Kish Bill Kish is the CEO and Co-Founder of Kenosha AI, bringing over 30 years of dynamic experience as a technologist, entrepreneur, and leader across five successful startups. His expertise has led burgeoning companies to flourish into multi-billion-dollar enterprises, solidifying his position as an industry innovator. A graduate with honors in Computer Engineering from Carnegie Mellon University, Bill s career has been defined by groundbreaking advancements in AI and machine learning applications. He co-founded Ruckus Wireless, serving as CTO and Board Director, where his contributions shaped the company into a $400M/year business and a leader in the wireless technology industry, culminating in a $1.5 billion acquisition by Brocade. At Cogniac Corporation, Bill enabled industries to leverage AI-powered visual inspection, serving as the CEO and CTO to drive operational innovation. He also founded Jiggy AI, a boutique AI consulting firm specializing in large language model applications. Additionally, as the organizer of the Silicon Valley Machine Learning Meetup, Bill has fostered a thriving global community of over 10,000 members passionate about machine learning. About Dr. Brad Pruitt Dr. Brad Pruitt is the President and Co-Founder of Kenosha AI. With over 25 years of experience in clinical research and healthcare, including 13 years in the Contract Research Organization (CRO) industry, he specializes in revolutionizing clinical trials through advanced AI-powered tools like copilots and GPTs. Dr. Pruitt is a seasoned executive and entrepreneur with a proven track record of leading ventures to success. He has held executive roles at top-tier CROs, served as the Founding CEO of an acquired startup, and contributed to three successful acquisitions in the past eight years. His prior leadership roles include Chief Medical Officer at Alethium Health Systems, where he developed go-to-market strategies for clinical trial innovation, and Senior Vice President of Medical Affairs at Safe Health, where he drove business expansion into connected diagnostics. In addition to his role with Kenosha AI, Dr. Pruitt is a Principal at Prucor and serves as a mentor and advisor for healthcare and clinical trial technology companies participating in the EvoNexus incubator program. Dr. Pruitt earned his MD from Michigan State University College of Human Medicine and his MBA from UC San Diego s Rady School of Management. His academic foundation, combined with his professional achievements, positions him as a visionary leader at the intersection of technology, healthcare, and clinical research. Contact Information Bill Kish Dr. Brad Pruitt Additional Resources Kenosha AI - Kenosha AI is currently offering a free trial of its RegChatTM, which is an AI-powered Clinical Regulatory Guidance Assistant that provides a simple chat interface for answering questions about global regulatory guidance using AI and official regulatory guidance documents with referenced summarizations and multi-agency comparisons. Thank you for listening! Don't forget to SUBSCRIBE to the show to receive new episodes delivered straight to your podcast player every month. If you enjoyed this episode, please help us get the word out about this podcast. Rate and Review this show on Apple Podcasts, Amazon Music, or Spotify. It helps other listeners find this show. Find Your Next Job ! More Upcoming Events Sign Up for any (or all) of our 25+ Newsletters You are responsible for reading, understanding, and agreeing to the National Law Review's (NLR s) and the National Law Forum LLC's Terms of Use and Privacy Policy before using the National Law Review website. The National Law Review is a free-to-use, no-log-in database of legal and business articles. The content and links on www.NatLawReview.com are intended for general information purposes only. Any legal analysis, legislative updates, or other content and links should not be construed as legal or professional advice or a substitute for such advice. No attorney-client or confidential relationship is formed by the transmission of information between you and the National Law Review website or any of the law firms, attorneys, or other professionals or organizations who include content on the National Law Review website. If you require legal or professional advice, kindly contact an attorney or other suitable professional advisor. Some states have laws and ethical rules regarding solicitation and advertisement practices by attorneys and/or other professionals. The National Law Review is not a law firm nor is www.NatLawReview.com intended to be a referral service for attorneys and/or other professionals. The NLR does not wish, nor does it intend, to solicit the business of anyone or to refer anyone to an attorney or other professional. NLR does not answer legal questions nor will we refer you to an attorney or other professional if you request such information from us. Under certain state laws, the following statements may be required on this website and we have included them in order to be in full compliance with these rules. The choice of a lawyer or other professional is an important decision and should not be based solely upon advertisements. Attorney Advertising Notice: Prior results do not guarantee a similar outcome. Statement in compliance with Texas Rules of Professional Conduct. Unless otherwise noted, attorneys are not certified by the Texas Board of Legal Specialization, nor can NLR attest to the accuracy of any notation of Legal Specialization or other Professional Credentials. The National Law Review - National Law Forum LLC 2070 Green Bay Rd., Suite 178, Highland Park, IL 60035 Telephone (708) 357-3317 or toll-free (877) 357-3317. If you would like to contact us via email please click here. Copyright  2025 National Law Forum, LLC",0
How AI is transforming medicine - Harvard Gazette,https://news.google.com/rss/articles/CBMilAFBVV95cUxOaktibzQxczJqMUlKOWxpQTUwNjJhclQzdE8wYTN5dWZFbVNPTElCcFFKeTdQcFhyR3pwZEptalU5Tzg4Rnd4S1N0UkVNQzIydjNGUGRlbG11YzhYWW1Gam5JeF9JVno5MEJaMFRUSE1MYTFYVVJmYUdyT3FGaTY3MWFpUUNRZlJFRTd1UFlXRDByazlE?oc=5&hl=en-US&gl=US&ceid=US:en,"A series of random questions answered by Harvard experts. Photo illustrations by Judy Blomquist/Harvard Staff Alvin Powell Harvard Staff Writer When Adam Rodman was a second-year medical student in the 2000s, he visited the library for a patient whose illness had left doctors stumped. Rodman searched the catalog, copied research papers, and shared them with the team.  It made a big difference in that patient s care,  Rodman said.  Everyone said,  This is so great. This is evidence-based medicine.  But it took two hours. I can do that today in 15 seconds.  Rodman, now an assistant professor at Harvard Medical School and a doctor at Beth Israel Deaconess Medical Center, these days carries a medical library in his pocket   a smartphone app created after the release of the large language model ChatGPT in 2022. OpenEvidence   developed in part by Medical School faculty   allows him to query specific diseases and symptoms. It searches the medical literature, drafts a summary of findings, and lists the most important sources for further reading, providing answers while Rodman is still face-to-face with his patient.  We say,  Wow, the technology is really powerful.  But what do we do with it to actually change things?  Artificial intelligence in various forms has been used in medicine for decades   but not like this. Experts predict that the adoption of large language models will reshape medicine. Some compare the potential impact with the decoding of the human genome, even the rise of the internet. The impact is expected to show up in doctor-patient interactions, physicians  paperwork load, hospital and physician practice administration, medical research, and medical education. Most of these effects are likely to be positive, increasing efficiency, reducing mistakes, easing the nationwide crunch in primary care, bringing data to bear more fully on decision-making, reducing administrative burdens, and creating space for longer, deeper person-to-person interactions. Adam Rodman, assistant professor at Harvard Medical School and physician at Beth Israel Deaconess Medical Center ADAM RODMAN: I am obsessed with metacognition, with thinking about thinking. So what excites me most about AI and medicine? Well, the optimist in me hopes that AI and medicine can make us doctors better versions of ourselves to better care for our patients. I think the best case scenario for me is a world in which an artificial intelligence is communicating with me and my patients, looking for signs of implicit bias, looking for signs that I might be making the wrong decision, and more importantly, feeding back that information to me so that I can improve over time, so that I can become a better human. My worry is actually directly related to this. These are very powerful reasoning technologies, and really what is medical education other than a way to frame and shape the medical mind? So part of my worry is that because these technologies are so powerful, they ll shortcut many of the ways that we know that doctors learn and get better, and we may end up with generations of physicians who don t know how to think the best. I don t think that this is the foregone conclusion, but it really is my worry about the way that things are going. But there are serious concerns, too. Current data sets too often reflect societal biases that reinforce gaps in access and quality of care for disadvantaged groups. Without correction, these data have the potential to cement existing biases into ever-more-powerful AI that will increasingly influence how healthcare operates. Another important issue, experts say, is that AIs remain prone to  hallucination,  making up  facts  and presenting them as if they are real. Then there s the danger that medicine won t be bold enough. The latest AI has the potential to remake healthcare top to bottom, but only if given a chance. The wrong priorities   too much deference to entrenched interests, a focus on money instead of health   could easily reduce the AI  revolution  to an underwhelming exercise in tinkering around the edges.  I think we re in this weird space,  Rodman said.  We say,  Wow, the technology is really powerful.  But what do we do with it to actually change things? My worry, as both a clinician and a researcher, is that if we don t think big, if we don t try to rethink how we ve organized medicine, things might not change that much.  Five years ago, when asked about AI in healthcare, Isaac Kohane responded with frustration. Teenagers tapping away on social media apps were better equipped than many doctors. The situation today couldn t be more different, he says. Kohane, chair of the Medical School s Department of Biomedical Informatics and editor-in-chief of the New England Journal of Medicine s new AI initiative, describes the abilities of the latest models as  mind boggling.  To illustrate the point, he recalled getting an early look at OpenAI s GPT-4. He tested it with a complex case   a child born with ambiguous genitalia   that might have stymied even an experienced endocrinologist. Kohane asked GPT-4 about genetic causes, biochemical pathways, next steps in the workup, even what to tell the child s parents. It aced the test.  This large language model was not trained to be a doctor; it s just trained to predict the next word,  Kohane said.  It could speak as coherently about wine pairings with a vegetarian menu as diagnose a complex patient. It was truly a quantum leap from anything that anybody in computer science who was honest with themselves would have predicted in the next 10 years.  Isaac Kohane, chairman of Harvard Medical School s Department of Biomedical Informatics and editor-in-chief of the New England Journal of Medicine s new AI journal ISAAC KOHANE: I am most excited that AI is going to transform the patient experience. Just merely having an instant second opinion after any interaction with a clinician will change to the better the nature of the doctor-patient relationship. Also, with regard to what things I fear could go wrong, it s that parties that do not have the patient s best interest will be the ones steering the tendencies/biases or prejudices of our new AI companions. And none too soon. The U.S. healthcare system, long criticized as costly, inefficient, and inordinately focused on treatment over prevention, has been showing cracks. Kohane, recalling a faculty member new to the department who couldn t find a primary care physician, is tired of seeing them up close.  The medical system, which I have long said is broken, is broken in extremely obvious ways in Boston,  he said.  People worry about equity problems with AI. I m here to say we have a huge equity problem today. Unless you re well connected and are willing to pay literally thousands of extra dollars for concierge care, you re going to have trouble finding a timely primary care visit.  Early worries that AI would replace physicians have yielded to the realization that the system needs both AI and its human workforce, Kohane said. Teaming nurse practitioners and physician assistants with AI is one among several promising scenarios.  It is no longer a conversation about,  Will AI replace doctors,  so much as,  Will AI, with a set of clinicians who may not look like the clinicians that we re used to, firm up the tottering edifice that is organized medicine?  How LLMs were rolled out   to everyone at once   accelerated their adoption, Kohane says. Doctors immediately experimented with eye-glazing yet essential tasks, like writing prior authorization requests to insurers explaining the necessity of specific, usually expensive, treatments.  People just did it,  Kohane said.  Doctors were tweeting back and forth about all the time they were saving.  Patients did it too, seeking virtual second opinions, like the child whose recurring pain was misdiagnosed by 17 doctors over three years. In the widely publicized case, the boy s mother entered his medical notes into ChatGPT, which suggested a condition no doctor had mentioned: tethered cord syndrome, in which the spinal cord binds inside of the backbone. When the patient moves, rather than sliding smoothly, the spinal cord stretches, causing pain. The diagnosis was confirmed by a neurosurgeon, who then corrected the anatomic anomaly. One of the perceived benefits of employing AI in the clinic, of course, is to make doctors better the first time around. Greater, faster access to case histories, suggested diagnoses, and other data is expected to improve physician performance. But plenty of work remains, a recent study shows. Research published in JAMA Network Open in October compared diagnoses delivered by an individual doctor, a doctor using an LLM diagnostic tool, and an LLM alone. The results were surprising, showing an insignificant improvement in accuracy for the physicians using the LLM   76 percent versus 74 percent for the solitary physician. More surprisingly, the LLM by itself did best, scoring 16 percentage points higher than physicians alone. Rodman, one of the paper s senior authors, said it s tempting to conclude that LLMs aren t that helpful for doctors, but he insisted that it s important to look deeper at the findings. Only 10 percent of the physicians, he said, were experienced LLM users before the study   which took place in 2023  and the rest received only basic training. Consequently, when Rodman later looked at the transcripts, most used the LLMs for basic fact retrieval.  The best way a doctor could use it now is for a second opinion, to second-guess themselves when they have a tricky case,  he said.  How could I be wrong? What am I missing? What other questions should I ask? Those are the ways, we know from psychological literature, that complement how humans think.  Among the other potential benefits of AI is the chance to make medicine safer, according to David Bates, co-director of the Center for Artificial Intelligence and Bioinformatics Learning Systems at Mass General Brigham. A recent study by Bates and colleagues showed that as many as one in four visits to Massachusetts hospitals results in some kind of patient harm. Many of those incidents trace back to adverse drug events.  AI should be able to look for medication-related issues and identify them much more accurately than we re able to do right now,  said Bates, who is also a professor of medicine at the Medical School and of health policy and management at the Harvard T.H. Chan School of Public Health. David Bates, co-director of the Center for Artificial Intelligence and Bioinformatics Learning Systems at Mass General Brigham DAVID BATES: AI has a great deal of promise. Burnout is rampant in many parts of medicine, especially, for example, primary care, and artificial intelligence will make many routine tasks like documentation much faster. Ambient scribes in particular are already doing that. There are also concerns about things going wrong. There are many ways that any time gains could be used, for example, just to increase physician workloads. It s also very important that medical records be correct, and AI has a tendency to hallucinate, and that is a worry, because we don t want things in people s records that are not really there. Another opportunity stems from AI s growing competence in a mundane area: notetaking and summarization, according to Bernard Chang, dean for medical education at the Medical School. Systems for  ambient documentation  will soon be able to listen in on patient visits, record everything that is said and done, and generate an organized clinical note in real time. When symptoms are discussed, the AI can suggest diagnoses and courses of treatment. Later, the physician can review the summary for accuracy. Automation of notes and summaries would benefit healthcare workers in more than one way, Chang said. It would ease doctors  paperwork load, often cited as a cause of burnout, and it would reset the doctor-patient relationship. One of patients  biggest complaints about office visits is the physician sitting at the computer, asking questions and recording the answers. Freed from the note-taking process, doctors could sit face-to-face with patients, opening a path to stronger connections.  It s not the most magical use of AI,  Chang said.  We ve all seen AI do something and said,  Wow, that s amazing.  This is not one of those things. But this program is being piloted at different ambulatory practices across the country and the early results are very promising. Physicians who feel overburdened and burnt out are starting to say,  You know what, this tool is going to help me.  Bernard Chang, Harvard Medical School Dean for Medical Education BERNARD CHANG: What most excites me about AI s promise in medicine is that these technological tools will allow physicians to spend more time on the human aspects of the profession, which is sorely needed, while facilitating the ability to access information quickly, analyze large amounts of important data, and make the difficult connections necessary to consider the rare diagnoses, the less obvious treatment paradigms, and ultimately the optimal care for patients. In medical education, students can use AI tools to accelerate their learning and move more quickly beyond rote practice to higher levels of cognitive analysis on their way to becoming the most outstanding doctors of the future. Whether things might go long lies in our hands. We need to be cautious about hallucinations and misinformation, bias, an erosion of fundamentals in learning, and an over-reliance on machines. As a society, we need to be mindful of the environmental impacts of the high energy costs involved. On the whole, I see AI as a transformative tool on par with the availability of the internet in terms of its effect on medicine and medical education. For all their power, LLMs are not ready to be left alone.  The technology is not good enough to have that safety level where you don t need a knowledgeable human,  Rodman said.  I can understand where it might have gone aground. I can take a step further with the diagnosis. I can do that because I learned the hard way. In residency you make a ton of mistakes, but you learn from those mistakes. Our current system is incredibly suboptimal but it does train your brain. When people in medical school interact with things that can automate those processes   even if they re, on average, better than humans   how are they going to learn?  Doctors and scientists also worry about bad information. Pervasive data bias stems from biomedicine s roots in wealthy Western nations whose science was shaped by white men studying white men, says Leo Celi, an associate professor of medicine and a physician in the Division of Pulmonary, Critical Care and Sleep Medicine at Beth Israel Deaconess Medical Center. Leo Celi, associate professor of medicine and a physician in Beth Israel Deaconess Medical Center s Division of Pulmonary, Critical Care and Sleep Medicine LEO CELI: AI could be the Trojan horse we ve been waiting for to redesign systems from a clean slate. I am talking about systems for knowledge creation, health care delivery, and eduction, which are all quite broken. The legacy of AI is to make us better critical thinkers, by putting data at the front and center, and making the breadth and the depth of the problems crystal clear. But we need to design human AI systems, rather than build algorithms. We have to be able to predict how humans will mess up. The designs should be similar those of systems for aviation, road safety, space, nuclear power generation. We need psychologists, cognitive scientists, behavioral economists, anthropologists to design human AI systems.   You need to understand the data before you can build artificial intelligence,  Celi said.  That gives us a new perspective of the design flaws of legacy systems for healthcare delivery, legacy systems for medical education. It becomes clear that the status quo is so bad   we knew it was bad and we ve come to accept that it is a broken system   that all the promises of AI are going bust unless we recode the world itself.  Celi cited research on disparities in care between English-speaking and non-English speaking patients hospitalized with diabetes. Non-English speakers are woken up less frequently for blood sugar checks, raising the likelihood that changes will be missed. That impact is hidden, however, because the data isn t obviously biased, only incomplete, even though it still contributes to a disparity in care.  They have one or two blood-sugar checks compared to 10 if you speak English well,  he said.  If you average it, the computers don t see that this is a data imbalance. There s so much missing context that experts may not be aware of what we call  data artifacts.  This arises from a social patterning of the data generation process.  Bates offered additional examples, including a skin cancer device that does a poor job detecting cancer on highly pigmented skin and a scheduling algorithm that wrongly predicted Black patients would have higher no-show rates, leading to overbooking and longer wait times.  Most clinicians are not aware that every medical device that we have is, to a certain degree, biased,  Celi said.  They don t work well across all groups because we prototype them and we optimize them on, typically, college-age, white, male students. They were not optimized for an ICU patient who is 80 years old and has all these comorbidities, so why is there an expectation that the numbers they represent are objective ground truths?  The exposure of deep biases in legacy systems presents an opportunity to get things right, Celi said. Accordingly, more researchers are pushing to ensure that clinical trials enroll diverse populations from geographically diverse locations. One example is Beth Israel s MIMIC database, which reflects the hospital s diverse patient population. The tool, overseen by Celi, offers investigators de-identified electronic medical records   notes, images, test results   in an open-source format. It has been used in 10,000 studies by researchers all around the world and is set to expand to 14 additional hospitals, he said. As in the clinic, AI models used in the lab aren t perfect, but they are opening pathways that hold promise to greatly accelerate scientific progress.  They provide instant insights at the atomic scale for some molecules that are still not accessible experimentally or that would take a tremendous amount of time and effort to generate,  said Marinka Zitnik, an associate professor of biomedical informatics at the Medical School.  These models provide in-silico predictions that are accurate, that scientists can then build upon and leverage in their scientific work. That, to me, just hints at this incredible moment that we are in.   What is becoming increasingly important is to develop reliable, faithful benchmarks or techniques that allow us to evaluate how well the outputs of AI models behave in the real world.  Zitnik s lab recently introduced Procyon, an AI model aimed at closing knowledge gaps around protein structures and their biological roles. Until recently, it has been difficult for scientists to understand a protein s shape   how the long molecules fold and twist onto themselves in three dimensions. This is important because the twists and turns expose portions of the molecule and hide others, making those sites easier or harder for other molecules to interact with, which affects the molecule s chemical properties. Marinka Zitnik, assistant professor of biomedical informatics MARINKA ZITNIK: I am most excited about AI s ability to learn and innovate on its own, instead of just analyzing existing knowledge. AI can generate new ideas, uncover hidden patterns, and propose solutions that humans might not consider. In biomedical research and drug development, this means AI could design new molecules, predict how these molecules interact with biological systems, and match treatments to patients with greater accuracy. By integrating information across genetics, proteins, all the way to clinical outcomes, AI can speed up discoveries in ways that was previously not possible. A major challenge, however, is that AI models tend to focus on problems that have already been extensively studied, while other important areas receive less attention. If we are not careful, medical advances may become concentrated in familiar areas, while other conditions remain under-explored, not because they are less important, but because there is less existing knowledge to guide AI systems. Another issue is that AI-driven drug design and treatment recommendations often rely on experimental findings generated in research labs that might not fully capture the complexity of real patients. Insights from research labs don t always translate into effective treatments, and AI could amplify this gap if it s not designed to bridge it. The opportunity is to build AI that makes discoveries and ensure that those discoveries lead to meaningful advances, bringing innovation to areas where it s needed most. Today, predicting a protein s shape   down to nearly every atom   from its known sequence of amino acids is feasible, Zitnik said. The major challenge is linking those structures to their functions and phenotypes across various biological settings and diseases. About 20 percent of human proteins have poorly defined functions, and an overwhelming share of research   95 percent   is devoted to just 5,000 well-studied proteins.  We are addressing this gap by connecting molecular sequences and structures with functional annotations to predict protein phenotypes, helping move the field closer to being able to in-silico predict functions for each protein,  Zitnik said. A long-term goal for AI in the lab is the development of  AI scientists  that function as research assistants, with access to the entire body of scientific literature, the ability to integrate that knowledge with experimental results, and the capacity to suggest next steps. These systems could evolve into true collaborators, Zitnik said, noting that some models have already generated simple hypotheses. Her lab used Procyon, for example, to identify domains in the maltase glucoamylase protein that bind miglitol, a drug used to treat Type 2 diabetes. In another project, the team showed that Procyon could functionally annotate poorly characterized proteins implicated in Parkinson s disease. The tool s broad range of capabilities is possible because it was trained on massive experimental data sets and the entire scientific literature, resources far exceeding what humans can read and analyze, Zitnik said. The classroom comes before the lab, and the AI dynamic of flexibility, innovation, and constant learning is also being applied to education. The Medical School has introduced a course dealing with AI in healthcare; added a Ph.D. track on AI in medicine; is planning a  tutor bot  to provide supplemental material beyond lectures; and is developing a virtual patient on which students can practice before their first nerve-wracking encounter with the real thing. Meanwhile, Rodman is leading a steering group on the use of generative AI in medical education. These initiatives are a good start, he said. Still, the rapid evolution of AI technology makes it difficult to prepare students for careers that will span 30 years.  The Harvard view, which is my view as well, is that we can give people the basics, but we just have to encourage agility and prepare people for a future that changes rapidly,  Rodman said.  Probably the best thing we can do is prepare people to expect the unexpected.  New study finds link between sleep curfew, higher levels of moderate-to-vigorous physical activity Researchers rush to get hands around multiple serious health risks as blazes mount   and get bigger The recent development of cancer immunotherapies marks a turning point in the centuries-old quest to fight cancer by harnessing the power of patients  own immune systems. Parent emerged over 4,000 years ago in Siberia, farther east than many thought, then rapidly spread west 6 min read Experiment with synthetic self-assembling materials suggests how it all might have begun 6 min read New study finds link between sleep curfew, higher levels of moderate-to-vigorous physical activity 6 min read A series of random questions answered by Harvard experts. A series focused on the personal side of Harvard research and teaching.",2
How Can Rural Healthcare Organizations Benefit From AI? - HealthTech Magazine,https://news.google.com/rss/articles/CBMinAFBVV95cUxPSS1uWFZEQ1E1bG1PTWFqWWpYWXRfS0JmOC1rUWxHRnZESmxBTDhyWHROMWVLSGZreEFTMENYcDNtY1k0WE9ieEV0dFZYLThjdlhYMmtYOWJwYVVIb1JEelVYbzc0Vmc1Wi1fUVlGQVpESXVvYk9JNExLd2hoN292UXY5MklWeGdSQW9teG4wUGdGTlM2VnZGZTMzaUM?oc=5&hl=en-US&gl=US&ceid=US:en,"These health IT influencers are change-makers, innovators and compassionate leaders who use technology to make a difference in provider experiences and patient outcomes. See how IT leaders are tackling AI opportunities and challenges. Brian Eastwood is a freelance writer with more than 15 years of experience covering healthcare IT, healthcare delivery, enterprise IT, consumer technology, IT leadership and higher education. Across the industry, health systems are laying the foundation for artificial intelligence success, ensuring they have the infrastructure, governance and technical expertise to make the most of the technology. Organizations that get this right are well positioned to increase efficiency, enhance care quality and take advantage of predictive models. For rural, independent and community health systems, the conversation about AI is a bit more nuanced. Though the benefits are there, unique challenges exist as well. As a 2025 paper from researchers at Texas State University notes, there are practical obstacles (such as the limitations of technology infrastructure and the cost of upgrading it) as well as  hypothesized barriers  (including beliefs that AI offers little benefit to the practice of medicine and that its use will hurt a health system s reputation in the community). With that in mind, rural health systems would be best served to focus on use cases that are both accessible and acceptable to providers, patients and policymakers, says Mei Wa Kwong, executive director of the Center for Connected Health Policy.  AI can be beneficial on the administrative end, where there are tasks that otherwise need a lot of resources,  she says.  Where there are tools to help humans work more efficiently or effectively, people see that as a good use of AI.  Click the banner below to read the new CDW Artificial Intelligence Research Report. National Rural Health Association CEO Alan Morgan says three common use cases for AI come up in his conversations with hospitals and health systems. The first is deploying ambient AI to document patient appointments. This can help practitioners pay more attention to patients  needs while alleviating the burden of note taking.  It s amazing how much time this is freeing up,  Morgan says.  I think this is potentially the greatest benefit we may see coming from AI.  Using AI to take notes improves the patient experience, Kwong explains, as practitioners no longer focus exclusively on their computer keyboard. Beyond the appointment, AI models can assess a patient s records and flag issues worth a follow-up. For example, if a patient mentions in many visits that they re having difficulty falling asleep, an AI model trained to detect patterns may flag that issue and prompt the health system to follow up.  AI can help identify patterns that a doctor may not see at first, or that they may initially think is an offhand comment,  Kwong says. This scenario tends to come with little resistance, as it provides additional information to providers without explicitly telling them what to do. The second is AI-based second opinions, which can help to reduce diagnostic errors. While hallucinations in AI models and biases in training data sets remain issues, the potential to access consultations in a matter of seconds has a clear benefit, Morgan notes. This is especially true in rural settings, where specialists may be hundreds of miles away or unavailable outside of normal business hours. RELATED: Here are 13 ways AI enhances healthcare operations, patient care and treatments. The third is streamlining billing and coding   a use case critical for the survival of rural organizations. Roughly one-third of rural hospitals are at risk of closure due to financial problems that stem from the cost of care delivery, the limitations of federal assistance and low financial reserves, according to a report from the Center for Healthcare Quality and Payment Reform. Morgan adds that rural hospital leaders see AI in the revenue cycle as a response to insurers  use of AI to assess claims   a practice facing class-action lawsuits. In a podcast with the Rural Health Information Hub, Jordan Berg, director of the National Telehealth Technology Assessment Resource Center indicates applying AI to the revenue cycle is more than a matter of automating routine tasks. With the right AI tools, he says, organizations can ensure services are billed at the appropriate level, notify vendors and patients when bills are due, and identify opportunities for further revenue cycle optimization,  all with very minimal input from users and stakeholders.  Additional use cases for AI in rural settings include optimizing workflows in the electronic health record (EHR), augmenting diagnosis and decision support, deploying mobile clinics with practitioners supported by AI agents, and improving scheduling and follow-up messaging. These examples work well because they don t cause much friction.  Patients are fine with getting an automated appointment reminder call,  Kwong says.  For organizations that have limited resources, a tool like that can be a good investment.  READ MORE: Revolutionize prior authorizations with AI. One wrinkle for rural, independent and community health systems looking for guidance on where and how to best use AI is the lack of direction from Capitol Hill. Recent regulations have closely defined the implementation of EHR systems, the acceptable use cases for telehealth, and the standards and infrastructure necessary to exchange health information, among other things. Meanwhile, no such framework exists for AI in healthcare. Morgan says this isn t surprising; given the typical long on-ramp for technology adoption in healthcare, policy moves at a slow pace. Right now, he adds,  it seems very hands-off.  The Trump Administration s proposed spending bill includes a 10-year pause on state or local AI laws in lieu of overarching federal regulation with few compliance hurdles.  It s a very volatile time, and policymakers are still feeling their way around that,  Kwong says. Even without the guardrails of federal policy   or a body of empirical research into how rural healthcare organizations are using AI   adoption appears to be taking off, Morgan says.  I ve seen a lot of fads,  such as rolling out robots in hospitals,  but AI has such amazing potential, and we re basically talking about utilization picking up in just the last year.  Security Across Healthcare, Collaboration Is a Critical Ingredient of Data Security Security Data Risk Management Best Practices for Healthcare Visit Some Of Our Other Technology Websites: Tap into practical IT advice from CDW experts Copyright   2025 CDW LLC 200 N. Milwaukee Avenue, Vernon Hills, IL 60061Do Not Sell My Personal Information These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly. These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant ads on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information. These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.",2
AI-backed medical debt firm portrays payment plans as healthcare solution amid consumer group’s warnings - The Guardian,https://news.google.com/rss/articles/CBMikwFBVV95cUxOampob3hBSVFhZXhhTjZsc0ZTZDVFM0ZyOW5ad3hCY1JMa09VQ0hMRkZGRVJoSjNzMjJjSnVuT0dTSnNOYTdQSjBnOUxiY0xSZWRiMVFMLU1MejJkNkhVVEVfZlpSeWtOTVB2dEh6T3NhZHRGUzRGd3paSHFienZBRjl6UVR3Ym5MNUVqeG8zSkhub3M?oc=5&hl=en-US&gl=US&ceid=US:en,"PayZen s model relies on purchasing hospital bills at discount as Republican cuts set to leave millions without insurance The CEO of the artificial intelligence-backed medical bills purchasing company PayZen believes payment plans can be part of the solution to America s high-priced healthcare, even as consumer rights advocates warn third-party financial agreements lack transparency. The company is just one in a sea of healthcare financing companies, whose executives see  acceleration  in conversations with cash-strapped hospitals facing historic Republican-led healthcare cuts. Signed into law by Donald Trump, the cuts are expected to leave 17 million people without insurance through 2034. As those uninsured people struggle to pay for healthcare, the change is effectively a cut to hospital revenue, and threatens some cash-strapped facilities with closure.  We believe most people want to pay their bills   they re decent people trying to be responsible,  said Itzik Cohen, PayZen CEO.  It s not a collections problem   it s an affordability problem.  PayZen s solution is to provide payment plans up to 60 months with 0% interest.  If you extend the payment plan to three, four, five years   Then more people will pay their bills and successfully,  said Cohen.  What we re trying to do is make it affordable.  PayZen s business model relies on purchasing medical bills from hospitals at a discount, and is backed by venture capital from groups such as New Enterprise Associates, a New York-based firm with big-name partners such as Dr Scott Gottlieb, the president s first-term Food and Drug Administration (FDA) commissioner. NEA and Gottlieb deferred requests for an interview. PayZen may pay as little as 10% and as much as 90% of the value of the bill depending on an AI-backed prediction of whether the patient will pay, according to a 2022 contract with the University of Texas Medical Branch Health (UTMB) at Galveston obtained by the Guardian. The company then collects the full face value of the bill from patients. That same contract shows that PayZen also charges hospitals a transaction-based  platform fee .  PayZen charges a 5% platform fee to support outreach, enrollment, underwriting and serving all payment plans,  it reads. Cohen declined to comment on platform fees and said the 5% figure  is not accurate and is not reflective of how our pricing works . PayZen is part of an industry of companies, some of which provide interest-bearing financing, that help cash-strapped hospitals with a growing a liquidity problem.  This is not a new business. It is based on an old model,  said Ge Bai, a healthcare finance professor at Johns Hopkins University s Carey School of Business.  A hospital takes the unpaid bill to a financial institution, sells these bills to the financial institutions, then the financial institution will give them [the hospital] money immediately   It changes ownership.  Chief among hospitals facing liquidity problems are rural facilities   153 of which have closed or lost key hospital services since 2010. For these facilities, government cuts, expected to result in an $87bn drop in revenue are only the latest blow. Over the last decade, insurers have increasingly pushed costs on to patients. From 2006 to 2025, the average deductible   an upfront payment that must be made before insurance kicks in   for a single person has grown from $303 to $1,562, outpacing inflation by more than 352%. Those payments represent a hardship for many Americans, more than one-third of whom can t afford an unexpected $400 expense. Unpaid, they also turn into bad debt on a hospital balance sheet. In 2022, people with health insurance became the largest group of patients in debt to hospitals   a sea change in the industry. And those debts, known as  patient responsibility  or  self-pay  are very hard for providers to collect. Companies like PayZen come in and pay hospitals up front for bills that might otherwise languish on the hospital balance sheet and become bad debt.  Because of the growth in high deductible health plans, many people have $2,500, $10,000 [deductibles] for families   so they re really financing so much of their care,  said Richard Gundling, chief mission impact officer at the Healthcare Financial Management Association (HFMA). Consumer advocates question the transparency of such deals for patients.  I don t think there s any transparency to the patient that PayZen has just acquired this account at a fraction of its face value,  said April Kuehnhoff, senior attorney at the National Consumer Law Center. UTMB Health confirmed that it does not tell patients that PayZen bought their debt at a discount.  If the hospital was willing to accept this reduced amount, was there a discount that the patient could have accessed by directly paying the hospital instead of paying the full amount to this third-party company?  Kuehnhoff asked. Advocates also argue there is a risk that low-income patients, who are often eligible for federally required discounted care, are caught up in payment plans. UTMB Health confirmed that PayZen does not screen patients for what is commonly called  charity care , despite performing a  soft  credit pull and information on their debt and incomes.  UTMB directs all patients to PayZen to discuss the terms and conditions of the specific agreements with PayZen,  said a spokesperson for the hospital system.  We provide basic FAQ information, but the relationship is between the patient and PayZen.  Although PayZen relies on purchasing debt, Cohen objects to the label  debt buyer , which he said refers to companies buying bundles of debt in default. Such companies were highlighted in a segment on John Oliver s Last Week Tonight.  Calling it debt buying is insulting to patients quite frankly,  said Cohen.  When you purchase something with a [buy now, pay later] approach, is it debt buying? You re being offered a way to pay for your purchase in a convenient, integrated way that extends payments to you because now you can afford it.  Cohen said his company did not use  extraordinary collection practices , such as filing debt law suits and objects to describing PayZen as  buy now, pay later .  We never actually called ourselves  buy now, pay later  for healthcare or  care now, pay later .  In fact, Cohen authored a 2021 blogpost on the company website headlined:  PayZen s  Care Now, Pay Later  Mission.  He later clarified that his company had moved beyond that description. Cohen said PayZen is running a  pilot  to pre-qualify accounts for charity care, but that only  two to three  of the roughly 100 healthcare providers it works with participate. Some states require hospitals to screen patients for charity care. If hospitals continue to struggle to collect money from patients, Bai noted that  hospitals will engage in even more aggressive mechanisms .  For example, all upfront payments   no payment, no service   this will happen,  Bai added. UTMB Health instituted one such policy, which was presented in a PayZen-sponsored report as  masterclass in revenue optimization . The hospital required patients to pay before seeing a doctor as early as 2019. However, the implementation reportedly led to loud exchanges in waiting rooms, as patients argued they could not afford to pay before seeing the doctor, according to local news outlets. In 2023, UTMB publicly affirmed its payment-first policy, and contracted with PayZen to provide patients with long-term pay plans through its AI-backedmedical bill purchasing model.  When thoughtfully implemented, pre-service payment policies can significantly increase collections without driving care avoidance,  the PayZen-sponsored report said. This article was amended on 27 July 2025 to clarify the nature of the payment obligations purchased by PayZen",2
The damage AI hallucinations can do – and how to avoid them - Healthcare IT News,https://news.google.com/rss/articles/CBMikgFBVV95cUxNZVBEWUlsUENoVFdISUFMLXFLME5UQ3ctUUpNUDAxQXdDTEViQVFYVlc4MmtmQmxkQWFxWF9DbkExT2tGendrbVpiMnZTRXIwaEg5dFZwUkJYMTBlcVpDSFljeEdRTEJPUW5OOG4tTXo5QkQ5QlNuS3lFMFB4UHJRenQxQndQSkdUa09tRHNLMV9mdw?oc=5&hl=en-US&gl=US&ceid=US:en,"Health systems are embracing artificial intelligence tools that help their clinicians simplify the creation of chart notes and care plans, saving them precious time every day. But what's the impact on patient safety if AI gets the facts wrong? Even the most casual users of ChatGPT and other large language model-based generative AI tools have experienced errors   often called ""hallucinations."" An AI hallucination occurs when an LLM cannot find an appropriate answer and simply makes something up. Essentially, when an LLM doesn't know the correct answer or can't locate appropriate information, it fabricates a response, rather than admitting uncertainty. These fabricated responses are particularly problematic because they're often very convincing. The hallucinations can be very difficult to distinguish from factual information, depending on what's being asked. If an LLM can't find the right medical code for a particular condition or procedure, for example, it might invent a number. The core issue is that LLMs are designed to predict the next word and provide responses, not to acknowledge when they don't have sufficient information. That creates a fundamental tension between the technology's drive to be helpful and its tendency to generate plausible sounding but inaccurate content when faced with uncertainty. For some further perspective on AI hallucinations and their potential impact on healthcare, we spoke recently with Dr. Jay Anders, chief medical officer at Medicomp Systems, a vendor of evidence-based, clinical AI-powered systems designed to make data usable for connected care and enhanced decision-making. He plays a key role in product development and acts as a liaison to the healthcare community. Q. What does AI's ability to generate hallucinations mean for clinical and administrative staff in healthcare wanting to use AI? A. The implications are significantly different for clinical versus administrative applications. In clinical medicine, hallucinations create serious problems because accuracy isn't negotiable. I recently read a study showing AI summarization gets things right about 80% of the time. That might earn you a B-minus in college, but B-minus doesn't work in healthcare. Nobody wants B-minus healthcare   they want A-level care. Let me give you specific examples from clinical record summarization, which many healthcare IT companies are rushing to implement. When AI summarizes clinical records, it can make two critical errors. First, it may fabricate information that simply isn't there. Second, it can misattribute diseases   taking a family member's condition and assigning it to the patient. So, if I mention ""my mother has diabetes,"" the AI might document that I have diabetes instead. AI also struggles with context. If I'm discussing a physical exam, it might introduce elements that have nothing to do with physical examinations. It loses track of what we're actually talking about. For administrative tasks, the risks are generally lower. If AI makes errors in equipment inventory, pharmacy supplies or scheduling, while problematic, these mistakes won't directly harm patients. The stakes are fundamentally different when we're dealing with clinical documentation versus operational logistics. Q. What are potential negative outcomes of hallucinations in healthcare AI, and how can they propagate through processes and systems? A. The negative outcomes operate on multiple levels and create cascading effects that are extremely difficult to reverse. When AI assigns incorrect diseases, lab results or medications to a patient's record, these errors become nearly impossible to correct and can have devastating long-term consequences. Consider this scenario. If AI incorrectly documents that I have leukemia based on my mother's medical history, how will I get life insurance? Will employers want to hire someone they believe has active leukemia? These errors create immediate and long-term impacts that extend far beyond the healthcare setting. The propagation problem is particularly insidious. Once incorrect information enters a medical record, it gets copied and shared across multiple systems and providers. Even if I, as a physician, catch the error and document a correction, that original record already has been sent to numerous other healthcare providers who won't receive my correction. It becomes like a dangerous game of telephone   the error spreads throughout the healthcare network, and each iteration makes it more difficult to trace and correct. This creates two types of propagation: The spread of actual errors and the erosion of trust in the system. I've seen AI-generated summaries that can't even maintain consistency about a patient's gender within a single document   calling someone ""he,"" then ""she,"" then ""he"" again. When lawyers encounter this kind of inconsistency in legal proceedings, they'll question everything: ""If it can't determine whether someone is male or female, how can we trust any of the information?"" The trust issue is crucial because once confidence in AI-generated content erodes, even accurate information may be dismissed as unreliable. Q. What are actions hospitals and health systems can take when using AI tools to avoid negative consequences of hallucinations? A. Healthcare organizations need to approach AI implementation strategically rather than throwing technology at every problem like ""mud on a wall."" The key is focused, purposeful deployment with strong human oversight. First, clearly define what problem you're trying to solve with AI. Are you addressing clinical diagnostics, or are you managing pharmacy inventory? Don't jump straight into high-risk clinical applications without understanding what the technology can and cannot do reliably. I know of a vendor that implemented an AI sepsis detection system that was wrong 50% of the time. The hospital CEO, who's a friend of mine, simply turned it off because they realized they didn't even have a significant sepsis problem to begin with. Second, choose your AI tools carefully. Different models excel at different tasks. What GPT-4 does well, Claude might not, and vice versa. Validate the technology with your own data and patient populations. Vendors should provide confidence levels for their systems, whether they're accurate 90%, 95% or only 20% of the time for your specific use case. Most important, maintain human oversight at all times. AI should augment human processes, not replace them. Always keep a human in the loop to validate AI outputs before they're implemented or documented. This applies whether you're dealing with billing, coding or clinical decisions. When humans catch AI mistakes, that feedback can help improve the system over time. The current environment feels like Dodge City. Everyone is using AI for everything without proper validation or safeguards. This ""AI for AI's sake"" mentality is dangerous. Not every process requires artificial intelligence. If a patient comes to my office with a low-grade fever, sore throat and runny nose, I don't need AI to tell me it's likely viral. Some things are straightforward enough that adding AI complexity only increases cost and potential for error. Q. What should healthcare CIOs, CAIOs and other IT leaders ask vendors with AI in their tools about how they are protecting against hallucinations? A. IT leaders need to ask direct, specific questions about validation and performance. Start with the fundamentals: What confidence levels can you provide for your system's accuracy? Can you demonstrate how your AI performs with real healthcare data similar to ours? Don't accept vague promises   demand concrete evidence of performance metrics. Ask about the training data and validation process. How was the AI model trained, and with what type of medical information? Has the system been tested specifically for the clinical scenarios you plan to implement? Different AI models have varying strengths, so ensure the vendor's system aligns with your intended use cases. Inquire about human oversight mechanisms. How does the vendor recommend integrating human validation into their workflow? What safeguards are built into the system to flag potentially problematic outputs? The vendor should have clear recommendations for maintaining human oversight rather than encouraging full automation. Request information about error detection and correction processes. When hallucinations occur   and they will   how quickly can they be identified and corrected? What mechanisms are in place to prevent the propagation of errors across systems? How does the vendor handle feedback to improve their models over time? Finally, be wary of vendors promising revolutionary capabilities that seem too good to be true. Some companies are developing ""doctor replacement"" chatbots or complex multi-LLM systems that claim to outperform clinicians. Even if these systems are right 80% of the time, that still means they're wrong 20% of the time. Would you be comfortable being in that 20%? The goal isn't to avoid AI entirely. The technology offers genuine benefits when used appropriately. But we need to implement it thoughtfully, with proper safeguards, and always with human oversight. The stakes in healthcare are simply too high for anything less than the most careful, validated approach to AI deployment. Follow Bill's HIT coverage on LinkedIn: Bill SiwickiEmail him: bsiwicki@himss.orgHealthcare IT News is a HIMSS Media publication. WATCH NOW: Seattle Children's chief AI officer talks better outcomes through the technology Newsletter Signup Thank you! Your submission has been received.   current_year Healthcare IT News is a publication of HIMSS Media",2
Aidoc raises $150M for AI foundation model - Healthcare Dive,https://news.google.com/rss/articles/CBMiigFBVV95cUxOUnlfRmVoTFpjejhid3pzWWp3eWFXZVh5RXphNEdDeUhFT21zYVRnTXhHcUpiS3pWYm9hTEpETDF2NjJYNHloN1FtMkluX3Fxck04eU1EX3ZyOWtyMmRoa091a0tpRGhFb3NPTHd3YzVCamdmbTRaMjVNTEV4LVU4Z3hORHBZcXprV3c?oc=5&hl=en-US&gl=US&ceid=US:en,"Let Healthcare Dive s free newsletter keep you informed, straight from your inbox. The technology, called CARE, is expected to allow for faster development of AI models that can cover more health conditions. First published on When Aidoc was founded in 2016, it started with building point solutions for radiology, such as AI to help detect and triage intracranial hemorrhages and pulmonary embolisms. Since then, the company has looked to develop broader foundation models, which can be used to detect multiple features in an image. Aidoc developed its CARE foundation model, which the company says will allow for development of new indications faster. The company plans to transition all of its AI models to CARE, and expects to cover more clinically relevant diseases in the future, such as cancer and cardiovascular conditions. Chief Technology Officer Michael Braginsky said in a statement that he expects  foundation models will soon be as ubiquitous in healthcare as ChatGPT is in general use.  Aidoc said it plans to invest more than $150 million in the coming years through strategic initiatives with Nvidia and Amazon Web Services to bring its CARE model to market. Aidoc also plans to use some of the funding to expand a separate solution, called aiOS, an operating system to help health systems deploy and manage multiple AI solutions. The technology is designed to support Aidoc and third-party models. The funding round includes a $40 million revolving credit facility. Aidoc said it has raised $370 million to date. The company said it currently supports more than 150 health systems. Get the free daily newsletter read by industry experts Sen. Chuck Grassley, R-Iowa, suggested Congress could once again move to overhaul PBMs  controversial business practices after legislators pass President Donald Trump s conservative megabill this summer. The agency said it would increase the number of MA plan audits and complete its backlog of reviews by investing in technology and growing its medical coding workforce. Subscribe to Healthcare Dive for top news, trends & analysis Get the free daily newsletter read by industry experts Sen. Chuck Grassley, R-Iowa, suggested Congress could once again move to overhaul PBMs  controversial business practices after legislators pass President Donald Trump s conservative megabill this summer. The agency said it would increase the number of MA plan audits and complete its backlog of reviews by investing in technology and growing its medical coding workforce. The free newsletter covering the top industry headlines",2
Can AI finally crack the code to cost-effective healthcare? - WIRED,https://news.google.com/rss/articles/CBMilAFBVV95cUxPLUlYMFhfbmUtUmR4X1o0amdvbnU1UmpaRERvMmVmcEdGa0tSRTlHb0NDVFotdnJkdjJqTWxYM2MzekRMakN1OW9YcHMxeHd6dHhmYXlXelhUN2FQOHpqTnRnMzZLVVQtWnlpYWplX0oxRUt5eFhTRUNsU2JfUlhqeFNfWnpYcDdyOERoYU9MX1cwNzd1?oc=5&hl=en-US&gl=US&ceid=US:en,"Currently, only residents from certain countries and US states can opt out of certain Tracking Technologies through our Consent Management Platform. Additional options regarding these technologies may be available on your device, browser, or through industry options like AdChoices. Please see our Privacy Policy for more information. Global healthcare systems face challenges today that impose an enormous burden. It s a familiar litany of problems: Populations are aging; chronic diseases are rising; and as we know from recent history, infectious diseases can still have a devastating impact around the globe. All this piles incredible pressure onto nurses and doctors, with healthcare systems struggling to cope given the widespread shortage of trained staff. Add to this that healthcare costs are rising, and it s little wonder that countries around the globe are finding it tough to deliver the quality healthcare that the public demands. Policy makers know that the solution will involve the smart application of artificial intelligence (AI). It has a proven ability to improve diagnostic accuracy and enable the early detection of disease, while treatment plans for patients can be personalized and administrative tasks streamlined. AI can be used to improve diagnosis and treatment of people that have felt overlooked in the past, including women who make up most of the population. Educational institutions that teach medicine are looking at how AI can be integrated into the training of future nurses and doctors. The potential of AI to deliver impactful solutions and improved patient care is therefore recognized, but can it be the cost solution for healthcare systems? Philips, which for many years has been at the forefront of developing medical products and services in areas including diagnostic imaging, ultrasound, and image-guided therapy has been exploring the use of AI to address these challenges. As Shez Partovi, Chief Innovation & Strategy Officer at Philips explains:  With aging populations and longer waiting times for patients, we need to develop credible solutions. Philips has been harnessing AI to increase scan detection rates, lower the burden on staff, and improve diagnosis. Key to the future success of health systems is delivering value and creating partnerships for change.  Philips has identified the potential for AI to make cost effective changes both on the clinical and operational side of healthcare systems. Examples include streamlining workflows and operations, providing clinical insights, expanding access to patient care, and supporting personalized healthcare. The result enables clinicians to see more patients, diagnose disease earlier, and deliver higher quality of care. This leads to patients recovering faster and spending less time in hospital, making treatment cheaper, and lowering the cost of healthcare per patient. Yet AI on its own is not enough Philips believes that the key lies in combining AI with the very human skills and experience that one will find in hospitals and clinics:  The aim is to create solutions that integrate into the workflows of healthcare providers and people s daily health routines. Something must be done or we risk being the first generation facing a healthcare system worse than previous generations.  Another advantage we are seeing with the AI solutions that Philips can deliver is the ability to provide quality care from hospitals into homes, increasing access to vital care in a range of settings. This includes the ability to detect different types of heart issues early on, putting care in the hands of patients whilst clinicians can monitor issues and manage diagnoses more effectively.  In our view, AI is about supporting healthcare providers in their daily decision making, improving operational efficiency so there can be more focus on patient care, and empowering the public to take more care of their own health and wellbeing.  AI makes it so much easier for healthcare professionals to access large sets of data, enabling quicker assessment and more personalized treatment. This results in follow-up actions that have more impact, enhancing the quality of care a patient is getting, and a better outcome for that patient. Healthcare leaders are turning to virtual care and AI-enabled innovation to address pressures in the sector such as workforce shortages, financial constraints, and growing demand. By using AI, they have turned an overload of information into meaningful insights that allow care providers to provide better care to more people. Going forward, Philips is helping to build the strategic partnerships that will enable AI to be rolled out in such a way that healthcare systems can truly deliver to patients, while preventing burnout among staff. The company is also fostering a culture among healthcare stakeholders where the benefits of AI are fully understood and the will to implement grows ever stronger. The genie is out of the bottle. Artificial Intelligence is poised to change health systems in countries around the world for the better by delivering cost effective solutions. But it needs buy-in and commitment from all healthcare stakeholders. They are invited to work with Philips to create a healthcare system fit for purpose in the 21st century. More From WIRED Reviews and Guides   2025 Cond  Nast. All rights reserved. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond  Nast. Ad Choices",2
"White House AI plan could help boost transparency, oversight - American Medical Association",https://news.google.com/rss/articles/CBMiuwFBVV95cUxPdlZ4ZGxmVEQ1MUc2RUo1R2ZyaE5KcFQ2U200bmNoNGVES1NHeG41QWEwRUNJX3JOSGJ1eGR6YS1zV05Tb2VPSnpnbU1ELVhYNlZlR0Z5TFNIb01wQnFSN3Q0Y3dlSkZTVmpTR185dF84UmJsTktDa2lNNVlWcmpzSE9PbGRrZUVUeWpVdk9ZQ0kwWENfMmd4M0c0OHBvc2VwZmRCWDVZYnFiUnJoLUhicjNIR2hLZVlGN3hv?oc=5&hl=en-US&gl=US&ceid=US:en,"Caffeine can be part of a healthy diet for most people, but too much may pose a danger to your health. Four physicians share what to keep in mind. The AMA Update covers a range of health care topics affecting the lives of physicians and patients. Learn more about cervical cancer and at-home screening tests. Strong physician representation is needed to ensure that health AI is transparent and has the right oversight so it works for patients and doctors. Get real answers from the AMA to common myths about documenting the time spent on each specific task associated with an outpatient visit. International medical graduates (IMGs) play a critical role in U.S. health care. Learn how the AMA works to help IMGs meet the nation s health needs. Residents share the responsibility to create an effective and respectful learning environment. The AMA has advice on how to make that happen. When writing your personal statement, veteran residency program directors said that authenticity will trump AI every time. ChatGPT agrees. Medical student research experience among residency applicants is common, but not always decisive. Learn about the physician specialties where it matters most. Most physicians practice where they completed residency, but not all. Learn which specialties and states are most likely to keep you local. After gaining footing as interns, second-year residents take on a new role and increased responsibility. These tips help new PGY-2s excel. Proposed 2026 Medicare physician payment rule would redistribute pay across specialties and practice types and more in the latest Medicare Payment Reform Advocacy Update. AMA expresses strong concern over proposed rulemaking on Medicaid provider tax reforms and more in the latest National Advocacy Update. This two-day boot camp, Sept. 17-18, 2025, will equip attendees with the time-saving tools and strategies to reform their organizations and enhance professional satisfaction. ChangeMedEd  is a national conference that brings together leaders and innovators to accelerate change in medical education across the continuum. Learn more. Review the agenda and schedule of events for the 2025 HOD Interim Meeting at the Gaylord National Resort and Convention Center in National Harbor, Maryland. Read the House of Delegates (HOD) speakers' updates for the 2025 HOD Interim Meeting. Download PDFs of reports organized by year for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA Interim and Annual Meetings. Download PDFs of reports on this topic for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA interim and annual meetings. Read current and past issues of WPS Members & News Highlights for the latest information on the work being done by WPS members and its leadership. An examination of ""kinkeeping"" and gender equity, and an introduction to the 2025-26 goals, the ""5 As:"" Action, Activism, Advancement, Advocacy and Achievement. In the news: FDA panel discusses risk of SSRIs during pregnancy, how the pandemic aged our brains, eating eggs may protect against Alzheimer s and more. AMA participates in health care conferences and events held throughout the U.S.A. as well as internationally. Strong physician representation is needed to ensure that health AI is transparent and has the right oversight so it works for patients and doctors. Jul 25, 2025 What s the news: The White House this week unveiled its action plan on AI, identifying more than 90 recommended policy changes in federal government policy that the Trump administration will pursue in the coming months. According to the White House, the plan is built on three pillars:  accelerating innovation, building American AI infrastructure and leading in international diplomacy and security.  See our real-world impact on issues critical to patients and physicians. The plan is an encouraging step forward, as the administration is prioritizing the responsible development and deployment of augmented intelligence (AI) often called artificial intelligence to improve health outcomes. The AMA applauds the plan s commitments to: The AMA also welcomes to the opportunity to work with the administration to address key areas that deserve more attention. These include: Why it s important: From AI implementation to EHR adoption and usability, the AMA is fighting to make technology work for physicians, ensuring that it is an asset to doctors not a burden. The AMA is committed to ensuring that AI can meet its full potential to advance clinical care and improve clinician well-being. As the number of AI-enabled health care tools continue to grow, it is critical that they are designed, developed and deployed in a manner that is ethical, equitable and responsible. The use of AI in health care must be transparent to both physicians and patients. In addition to medical devices, AI is increasingly used in health care administration or to reduce physician burden, and policy and guidance for both device and non-device use of health care AI is necessary. Recognizing this, the AMA has developed policy (PDF) that addresses the development, deployment and use of health care AI, with particular emphasis on: In June, the AMA House of Delegates adopted new policy to help ensure that health AI is  explainable,  validated, well defined and is not used to conduct medical research fraud. AMA survey research from 2024 shows (PDF) that physicians are largely enthusiastic about the potential of AI in health care, with 68% seeing at least some advantage to the use of AI in their practice. Meanwhile, 66% reported using some type of AI tool in practice. Learn more: The AMA STEPS Forward   Governance for Augmented Intelligence  toolkit, developed in collaboration with Manatt Health, is a comprehensive eight-step guide for health care systems to establish a governance framework to implement, manage and scale AI solutions. Catch up with this AMA report on emerging landscape of health care AI. Also, explore how to apply AI to transform health care with the  AMA ChangeMedEd  Artificial Intelligence in Health Care Series.  In addition, check out JAMA+ AI, which offers scientific content, educational reviews and commentary on AI and medicine published across JAMA , JAMA Network Open and the JAMA specialty journals. Follow the latest news on AI and its applications and effects for health care delivered to your inbox. The AMA promotes the art and science of medicine and the betterment of public health. The best in medicine, delivered to your mailbox We use third party technologies for analytics, personalization and marketing purposes. These technologies may be used by us or our partners to personalize the site or deliver relevant marketing to you on third party sites. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.",0
"Aidoc Secures $150M for CARE, its Healthcare Foundation Model, to Transform Clinical Decision-Making for 100 Million Patients - PR Newswire",https://news.google.com/rss/articles/CBMiiwJBVV95cUxNQlR6M3R3bkN2ZXFOYTBuQmxXSGZsUXVNeWlTOWx5ZkFTTklhTzRlazlPUmJxaWcxaHl3ZDI3TXpCMndvMXdVNFF0ZGo1dFZfQlZXZ2xEaXc4SkVETkN1Si1SLTBPMC1YVlhJLUcyWHNYTlNUQXdyRUsxT1JicFpKNnVvaV85ckxJcDFWa0wxZUVfWVVaM0Fra1NJMmtWcDYzaGpyb0k1cE5HdFJwTVV6VWpyTERBalkwMVQya21Qd3VLQlNhdmdfMzVvbEtwZmlHNk85TEx6Rlc5S096amZXQ3k3eEJZT25wNkdlemNyZkJIVHNTbjZtb01sWHpUUENxUFRiTDhPa1ZOMTg?oc=5&hl=en-US&gl=US&ceid=US:en,"Searching for your content... In-Language News Contact Us 888-776-0942 from 8 AM - 10 PM ET Jul 23, 2025, 08:15 ET Share this article The Era of Clinical AI Has Arrived: Trusted by Leading Health Systems, Aidoc's Platform Brings AI to the Heart of Patient Care NEW YORK, July 23, 2025 /PRNewswire/ -- Aidoc, the leading clinical AI platform with the highest number of FDA-cleared solutions in its category, today announced a $150 million financing round to accelerate the development of CARE , the company's clinical-grade foundation model that assists physicians in clinical decisions. The round was led by General Catalyst and Square Peg, with participation from NVentures (NVIDIA's venture capital arm) and four major U.S. health systems, including Hartford HealthCare, Mercy, Sutter Health and WellSpan Health. This round also includes a $40 million revolving credit facility, bringing the company's total funding to $370 million. The investment builds on a wave of momentum: two CARE-powered solutions have earned FDA clearance, and new enterprise-wide rollouts were announced in the past month at Advocate Health and Sutter Health. Aidoc currently supports care for more than 45 million patients annually across 150+ health systems, growing to 100 million in three years. Leading health system partners also include Mount Sinai Health System, Yale New Haven Health System, Northwell Health, University of Miami Health System and Temple Health. ""Our mission is to reduce diagnostic errors and improve patient outcomes,"" said Elad Walach, Chief Executive Officer of Aidoc. ""CARE compresses decades of roadmap into years,bringing forward a future where AI supports every patient encounter, helping physicians provide the care they believe their patients deserve."" The funding will also support the expansion of aiOS , Aidoc's enterprise-grade platform for deploying and governing clinical AI at scale. With a growing focus on enabling an open ecosystem, the investment will accelerate efforts to support seamless integration of both Aidoc and third-party AI solutions. ""At Hartford HealthCare, we have seen the significant impact that AI has had on outcomes, and we believe the future of healthcare must be grounded in trust, innovation and compassion. Our Center for AI Innovation in Healthcare and our collaboration with Aidoc reflect our commitment to advancing AI in a safe, scalable and thoughtful way, supporting clinicians and improving care for those we serve,"" said Jeffrey A. Flaks, President and Chief Executive Officer of Hartford HealthCare. Clinical AI: Infrastructure for Modern Care Healthcare faces a perfect storm: 371,000 annual deaths from diagnostic errors, a growing physician shortage and an ever-expanding body of clinical knowledge that sets ever-evolving standards no health system can realistically meet. Clinical AI addresses this by improving clinical decision-making at the point of care. It flags suspected high-risk findings, automates follow-ups and gets patients to the right care faster, supporting real-time, high-stakes decisions. The promise of AI for efficiency is real, yet clinical AI goes further. It drives better outcomes, alleviates cognitive overload and helps reduce clinician burnout. ""Aidoc is an important partner as we seek to fulfill our system-wide commitments to improve patient outcomes, enable our healthcare workers to be at their best and stay at the forefront of tech-powered healthcare transformation,"" said Roxanna Gapstur, PhD, RN, President and Chief Executive of WellSpan Health. ""In one year, Aidoc has helped our radiologists to analyze more than 200,000 cases, making efficient the work of scanning what can sometimes be thousands of images per case, leading to a significant reduction of critical diagnosis delays."" CARE: Rewriting the Rules of Clinical AI CARE, a clinical-grade foundation model trained on multimodal data, already powers FDA-cleared applications and enables development of new indications up to 20x faster. With unmatched clinical accuracy, all Aidoc models will transition to CARE. Within three years, CARE is expected to massively expand and cover 90% of clinically relevant diseases, including cancer and cardiovascular conditions. ""Model accuracy is paramount when touching the core of a physician's work,"" said Michael Braginsky, Co-founder and Chief Technology Officer of Aidoc. ""Foundation models will soon be as ubiquitous in healthcare as ChatGPT is in general use. Scaling clinical AI is an enormous lift   it requires top-tier talent, powerful infrastructure, deep real-world insight and sustained funding. Success isn't guaranteed, but we believe we're in a unique position to bring this vision to life, and we feel a deep responsibility to do so."" Through strategic initiatives with NVIDIA and AWS, Aidoc will invest over $150 million in the coming years to bring CARE to market, combining high-performance compute and AI development platforms to redefine model performance, efficiency and real-time inference. aiOS: The Catalyst for a Clinical AI Ecosystem As clinical AI scale grows exponentially, Aidoc is doubling down on aiOS, an enterprise operating system built to run clinical AI. It enables health systems to deploy and govern multiple AI solutions responsibly, with real-time performance monitoring, seamless integrations and, most critically, outcome measurement. Supporting both Aidoc and third-party models, aiOS is a true platform for system-wide governance. ""We're not doing this alone,"" said Walach. ""aiOS is an ecosystem-driven platform, built to host both Aidoc and external solutions. Today, 69% of our customers are running non-Aidoc models on aiOS, and we're committed to growing that ecosystem, ensuring each solution we onboard will lead to significant clinical impact."" With this latest investment, Aidoc is ushering in a new era for clinical AI, one where foundational models, enterprise-grade infrastructure, and an open ecosystem converge to deliver measurable impact at scale. As CARE and aiOS continue to gain traction across leading health systems, Aidoc is setting a new standard for how clinical AI can safely and effectively support clinicians, improve outcomes and transform the delivery of care for millions of patients. About Aidoc Aidoc assists physicians in clinical decisions for over 45 million patients a year, helping health systems deliver smarter and faster care when it matters most. Its mission is to transform patient outcomes through ""always on"" clinical AI, eliminating preventable care gaps that lead to loss of lives and disabilities. Built on the proprietary aiOS  platform, Aidoc seamlessly integrates real-time intelligence into provider workflows at the point of care. With the most FDA-cleared AI solutions in its category and deployments across 150+ health systems globally, Aidoc elevates the physician and patient experience. Learn more at aidoc.com. SOURCE Aidoc Sutter Health, a leading, not-for-profit, integrated health system serving more than 3.5 million Californians, today announced a strategic... At HLTH Europe, Aidoc announced the release of BRIDGE, a new open-source framework developed to help healthcare organizations deploy AI safely,... Artificial Intelligence Computer Software Computer Software Health Care & Hospitals Do not sell or share my personal information:",2
"AI Chat With Roland Rott, President & CEO of Imaging at GE HealthCare - The Motley Fool",https://news.google.com/rss/articles/CBMilwFBVV95cUxPY1g4TE1pZUR1dnNCaHowZFl6akxBRWlVSGtRcThEOXVPOWl5cF85NmU1SVRXQW1jSUU1YTFRb0xxVGg1Q242MXFneFVyS0U1Vi1RYzE0QUM1V1VIWEU0ZTZfSi1zWXRGYkk4RWlCY185V0h3NHNCOG9kTXo1QWs2elZhQXQ1MS15NEZkUmN0amY1VWJESnJv?oc=5&hl=en-US&gl=US&ceid=US:en,"Founded in 1993, The Motley Fool is a financial services company dedicated to making the world smarter, happier, and richer. The Motley Fool reaches millions of people every month through our premium investing solutions, free guidance and market analysis on Fool.com, top-rated podcasts, and non-profit The Motley Fool Foundation. Founded in 1993, The Motley Fool is a financial services company dedicated to making the world smarter, happier, and richer. The Motley Fool reaches millions of people every month through our premium investing solutions, free guidance and market analysis on Fool.com, personal finance education, top-rated podcasts, and non-profit The Motley Fool Foundation. Founded in 1993, The Motley Fool is a financial services company dedicated to making the world smarter, happier, and richer. The Motley Fool reaches millions of people every month through our premium investing solutions, free guidance and market analysis on Fool.com, top-rated podcasts, and non-profit The Motley Fool Foundation. Founded in 1993, The Motley Fool is a financial services company dedicated to making the world smarter, happier, and richer. The Motley Fool reaches millions of people every month through our premium investing solutions, free guidance and market analysis on Fool.com, personal finance education, top-rated podcasts, and non-profit The Motley Fool Foundation. You're reading a free article with opinions that may differ from The Motley Fool's Premium Investing Services. Become a Motley Fool member today to get instant access to our top analyst recommendations, in-depth research, investing resources, and more. Learn More A set of AI use cases within the medical space. In this podcast Motley Fool analysts David Meier and Asit Sharma and GE HealthCare's CEO of Imaging, Roland Rott, discuss: To catch full episodes of all The Motley Fool's free podcasts, check out our podcast center. When you're ready to invest, check out this top 10 list of stocks to buy. A full transcript is below. This is a modal window. Beginning of dialog window. Escape will cancel and close the window. End of dialog window. This is a modal window. This modal can be closed by pressing the Escape key or activating the close button. This podcast was recorded on July 20, 2025. Roland Rott: We were very focused on using AI to create solutions which make an impact on patients. Ricky Mulvey: That was Roland Rott, CEO of GE Healthcare Imaging, segment within GE Healthcare. David Meier and Asit Sharma talked with him about everything from GE Healthcare overall to a bunch of examples of how AI is used in healthcare to both enhance efficiency and to boost patient outcomes. David Meier: Hello, everyone, and welcome to this installment of the CEO interview. I'm your host David Meier with my Foolish colleague, Asit Sharma. Asit, how are you? Asit Sharma: Doing very well, David. Excited for this. David Meier: Me too, because we have an incredible guest. We have the CEO of GE Healthcare Imaging, which is a nine billion dollar segment within GE Healthcare, Roland Rott. Hello, Roland. How are you? Roland Rott: Hello. Hi, David. Hi, Asit. Everyone. Thanks for having me. Looking forward to this conversation. David Meier: We are too, and we're very glad to have you. Let's kick off and start a little bit broad and talk about GE Healthcare, the overall business, what its business model is and what its mission is. Roland Rott: Yeah, David. GE Healthcare, I'm sure many of you will know, has been part of General Electric for the first 123 years, if you will. General Electric was a very iconic American company highly successful in many fields, healthcare being one of them. We have been essentially over 100 years in healthcare and have been at the forefront of innovation in all these generations of medical devices and medical imaging. Now what is very exciting is that a couple of years ago, beginning of 2023, we actually spun out of General Electric and we became an independent public company, traded at NASDAQ, now being an independent freestanding public company with approximately $19.6 billion of revenues and serving ultimately more than a billion patients worldwide across 160 countries. It's a very significant impact this company has, a very strong legacy but a very exciting future ahead also in this new phase of being a public company ourselves. David Meier: A long time ago, I used to work at GE in what was known as the Power Systems segment, and I have to say, GE Healthcare back between 1998-2005, was always held up within the company as a great model. Maybe let's talk a little bit about its business model and that is, how do hardware sales, software sales, service agreements, how do those all tie together to basically be the operating engine for GE Healthcare? Roland Rott: Great question. And if you think about medical imaging and healthcare overall, what we provide essentially is solutions in order to detect diseases early, to diagnose disease, to ultimately support treatments and monitor these treatments, monitor the health of patients. As GE Healthcare, we are active in all this spectrum. We are doing that with a strategy which is what we call the D3 strategy. We want to provide smart devices, devices which are smart, which are intelligent. We will talk about artificial intelligence, so they are substantially AI-enabled. But also smart drugs, and we align those smart devices and drugs on certain disease states, for example, cancer, or cardiovascular disease. Then we also provide digital solution. We leverage the data which these devices are generating in these specific disease areas as physicians use it, and putting all that together provides solutions which can really improve and impact patient outcomes. That is, in essence, what we provide. Again, relevant hardware, smart devices. Think about systems like CT or MR or ultrasound devices. These are technologies which allow physicians to take a look at patients' conditions and then using the relevant software to get to a good diagnosis and to ultimately make meaning of what these devices actually are producing. From a business model standpoint, once we are offering these devices, they are obviously in use for an extended period of time. We also provide services in order to keep everything not only up and running, but also up to date. We also keep customers vital with newer possibilities such as new versions of AI, etc. Asit Sharma: Roland, let's stick with product for a moment. Could you break down for our members what are the major product areas within the imaging segment? Roland Rott: You can almost define it by the generation it was created. When imaging was starting 100 years ago, we only had X-ray. X-ray was the first modality, and it was a foundational one for many further on technologies like mammography is a piece of it, which we use in breast cancer screening. We then had the rise of CT, which is, again, technology-wise, X-ray-based. Then came MRI, a very revolutionary way to look inside the human body without ionization and with very powerful capabilities. I would say in the last phase, you have this field of molecular imaging, which essentially combines some of the traditional capability like CT and MR with additional sensors, with additional detectors, which can actually allow physicians to look inside the body, find cancers through radioisotopes, slight radioactive drugs which are injected and ultimately can visualize and target specific cancers as an example. Very, very advanced technologies from a standpoint of imaging. As you see in this range, all of these modalities have their particular areas of use. They have their designation. They have obviously their different reach. It's easier to deploy a mobile X-ray device than a big RON, if you will, MR device or a PET CT system. But they have a significant impact on patients. Asit Sharma: If I were to ask you out of these, which products maybe are driving the most value for the imaging segment, what would those be? I have an idea that part of it might be related to the PET type products, so these nuclear tracing products. But I would love to hear from you what is looking into the future the biggest driver value going forward? Roland Rott: If we look at it, we can look at it from a lens of patients and obviously from a financial and from a business standpoint. Really starting on the patients side, we always pull patients first. We would say at least in mature markets, there's good coverage on some of these earlier imaging technologies, but there's yet a lot of potential to provide patients more access to contemporary MRI or to PET CT and PET MR, so this molecular imaging space. These are areas which keep growing substantially because, a, they don't have the visibility, and on the other hand, molecular imaging or theranostics, which combines therapy and diagnostic actually is still growing in its clinical applications. There are more types of disease which can actually be handled with those technologies. We do expect from a business standpoint, significant growth over the next years in these areas of molecular imaging, advanced therapies. But still, also in these traditional technologies, you could say, which help reach more patients or make physicians more efficient in order to handle the patient volume, which we simply have to deal with. Asit Sharma: From an investment standpoint, I'm curious, where are you focusing most of your R&D? Roland Rott: I think in general, when it comes to R&D, we work in the life cycle approach vis-a-vis all of these technologies. We have opportunities, for example, in CT to work on some more advanced next generation capabilities. As we announced, we're working on a deep silicon-based photon counting architecture, which we believe will take the possibility of CT another step forward. That after CT has been around for such a long time, when we think about molecular imaging, it's a big area of investment because there is so much new capability with new radioisotopes available. In that sense, it makes sense to invest further in creating more applications and putting these technologies in the hand of more physicians. Ultimately, we do invest today as we are a stand-alone public company, factually more than ever before from a nominal standpoint, and we have a rich pipeline, which definitely fuels also further growth based on that investment. David Meier: I think this is a good segue to talk a little bit more broadly about innovation, especially since you're at all time highs for R&D budgets. Innovation is definitely within the lifeblood of GE. When I was there, it was extremely important and became even more important under Jeff Immelt when he became CEO and that's when I left. But I'm sure it's still extremely important to the culture. Maybe can you give some examples of how innovation is working within healthcare or imaging if you wanted to go specifically there. It can be anything. It can be maybe a product upgrade or even a major breakthrough that gets you into a new market. Roland Rott: I would say one of the biggest areas of innovation and also going back to investments is, of course, artificial intelligence, AI, deep learning in the context of healthcare. AI has been around for some time. AI principle has been around for several decades. However, with the rise of possibilities NVIDIA provided us, for example, to have very powerful capabilities within a computer, we are now able to process large amounts of data, and that ultimately can help to make these systems and smart devices even smarter. We invested significantly in AI. Today, actually, we are a leading company in the field of AI. We have more than 85 FDA cleared medic devices today in the market. They are cleared, they are commercially available, and they have physicians to treat patients more efficiently and on the other hand deal with this large amount of patient volume and get to better insights. It's really important for us to have physicians and see AI as a partner. Often it's used as a Copilot to augment the possibilities of physicians and helping them get to the result with confidence as efficient as possible, and that way also help to improve the outcomes. If I give you a few examples, we have been able with AI to streamline the reconstruction time, first of all, and the processing time in MR by more than 70%, and in cardiology, even 83%. We are able to slash these exam times. That means it's more comfortable for a patient. You don't need to lay in such a device for an extended period of time. Think about many patients which are in the queue. If you can be faster, you can handle more patients in the same time frame. Ultimately, we have also been able to improve that image quality. Make this image quality more robust, take certain artifacts away, etc, give the physicians a cleaner image, in that sense, ability to confidently screen or diagnose. This is just one example where AI already makes a significant impact, and with the technology I described, we have already handled more than 30 million patients, actually. This is quite proven. This is not in the infancy stage. David Meier: Maybe I'll follow on with an AI question, and we'll start internally and work our way out. It's very clear that the creation of data from your machines is very important. Maybe internally, how are your teams using AI to maybe get a little bit more marginal in return on the R&D budget, things like that? Roland Rott: I would say it's very interesting your question because we can also use AI in the process of creating these solutions. My early example, and that's really our evolution, we started with customers first. We started actually to use AI first to create solutions which make an impact. Maybe it also related to the timing because we were in COVID. Many of our customers and physicians had challenges to deal with the load of patients, etc. We were very focused on using AI to create solutions which make an impact on patients. While doing though, we then in recent years, spend also quite some efforts to look at the process. As you will know, there's a lot of documentation required in medical device generation. There's a strong quality management framework, which we are adhering to regulatory requirements. Today, we actually find a lot of opportunity to use AI to augment our engineers in doing exactly that work and also be more productive that way, get more agile, shorten some of the creation time, or if you will, get more output in the same period of time. That last piece still has a lot of potential. We're just at the beginning really of unlocking that. I think we're going to keep learning, and we're going to keep evolving, obviously, as we also get more possibilities with AI. David Meier: Maybe before Asit asks this question, I'll just have one comment. I used to be a Black Belt in the old Six Sigma realm. Is AI basically Six Sigma on steroids now? It's like the next 10 levels higher type of a thing. Roland Rott: Maybe to translate, Six Sigma is one approach which also General Electric has used early on and also relates to Lean. Lean is very much a culture, and it's also a set of tools of continuous improvement and to take waste out, for example, of processes. In that sense, you could say, AI is a close cousin. It's a tool which allows us to do exactly that. Ironically, as you mentioned this point, we actually reimplemented Lean very substantially over the last years in parallel to AI. We deployed Lean consequently. Larry Culp is the CEO of GE and is our chairman today, with his vast experience inspire that. Today, actually, we both deploy Lean and use AI to get processes more efficient, to take waste out to actually speed up and be productive all in the spirit of serving customers faster, but also obviously as precise as needed. Asit Sharma: I want to go back to something that you mentioned earlier because I think many of our members will have an interest in the competitive edge that imaging solutions has. You talked about the clarity of images that have been enabled by AI. Basically, we have a scan, and in any number of outputs, you have a visualization, which is then I would call it as a layperson, almost recreated by AI. Some of the noise gets removed, and you have more signal, the image has more clarity. But at the end of the day, it's an algorithmic type of improvement. We're curious what kind of edge is this vis-a-vis competitors? For example, someone using these images, a physician maybe has a higher confidence level in his or her diagnostic capability, if the image is better. As you already mentioned it cuts down on the time it takes to run the test and get all the way to a diagnosis. Is this something that a competitor could also working in an AI kitchen come up with or do you have some type of clear edge versus those who offer similar products? Roland Rott: I think in principle, and that's always true, all these capabilities are in theory, available to many. We see a lot of innovation, generally speaking, when it comes to AI in healthcare. Let me also say that we are cultivating a pretty open ecosystem. We are not only creating our own AI, we are partnering very closely with customers, which can be very large healthcare systems generating a lot of data, applying that, having their own models, and then ultimately that can lead to some start-up which ultimately offers that and we integrate that. We are really using the broader ecosystem lens here. We have also acquired a few companies over the last years in the space of AI, such as Caption Health in ultrasound or MIM in the space of molecular imaging software, as I mentioned before. They all use AI, and they all are enhancing so to say, what we organically do. But really to your question of competitiveness, we do believe and based on the fact that we started earlier and we have a lead in FDA cleared medical devices today, a lot of customers look at that and understand that we invested into this space, we created meaningful impactful solutions, and that gives us credibility to further charge ahead and creating further such solutions. We have just started, if you will, with these first 85, but some of those AI applications have been very narrowly focused on improving a certain image area and so forth. But we have now extended the field quite broadly to also create solutions which combine such exams across modalities. Think about a care pathway where a patient first gets diagnosed with an ultrasound system or gets screened with an ultrasound system in mammography, you use mammography, you then use MRI, so you go through these different technologies, and as more data is generated, how can we use AI also to give physicians a comprehensive summary and comprehensive insight about the patient's condition? Those applications are actually now really interesting based on the possibilities we have found. It's really innovating the specific individual smart devices is one, but it's creating solutions across the care pathway, which have a lot of even more impact. Ricky Mulvey: As always, people on the program may have interest in the stocks they talk about, and the Motley Fool may have formal recommendations for or against, so don't buy or sell stocks based solely on what you hear. While personal finance content follows Motley Fool editorial standards, is not approved by advertisers. Advertisements are sponsored content and provided for informational purposes only. If they are Fool advertising disclosure, please check out our show notes. That's all for today. We'll see you tomorrow. Asit Sharma has positions in GE Aerospace and Nvidia. David Meier has no position in any of the stocks mentioned. The Motley Fool has positions in and recommends GE HealthCare Technologies and Nvidia. The Motley Fool recommends GE Aerospace. The Motley Fool has a disclosure policy. Stocks Mentioned *Average returns of all recommendations since inception. Cost basis and return based on previous market day close. Invest better with The Motley Fool. Get stock recommendations, portfolio guidance, and more from The Motley Fool's premium services. Making the world smarter, happier, and richer.   1995 - 2025 The Motley Fool. All rights reserved. Market data powered by Xignite and Polygon.io. About The Motley Fool Our Services Around the Globe Free Tools Affiliates & Friends",2
Agentic AI's greatest potential benefit? Changing how a health system functions - Healthcare IT News,https://news.google.com/rss/articles/CBMisgFBVV95cUxOaFJsU3owZUxacDd5NGZSOTZ2bTVCa3Zkb1R1d3g3VlpvOHRvMHM2VXk3RzJoQm14WS14cEI1VnU0YkdWVHE2Wi1YTzBvVEM2aXpSRlZZWUVjZ0RMSm5PS1VuNXZzRUVMZkdha2x0MmFQWTUxaVFXQ2xBVHFkMTZtaGNjUXE0N3p2T1l6NmdYZnhrU2F5NmRwRDN0YTVwYmxHdGZoUjZ5UWlMeElLSlRMa29B?oc=5&hl=en-US&gl=US&ceid=US:en,"The artificial intelligence applications most people have grown familiar with in recent years usually take the form of assistants: Chatbots answer questions and respond to prompts, but a human instigates each step. An AI agent, on the other hand, can take action on behalf of a user. Models with agency have existed for a few years, but recent advances in natural language processing and memory structure have made them much more powerful. An AI agent can conduct an entire task   starting an action and completing it. An agentic AI system is a network of agents that can work together to complete an entire workflow, not just an individual task. In the healthcare context, this could mean reviewing and processing a complex insurance claim. Using natural language, an agentic AI system can plan, collaborate and connect. For individual AI agents and for systems, people intervene only to take care of problems as they crop up and to ensure oversight. AI agents, in a sense, are virtual co-workers   and very efficient ones. ""There are use cases across the healthcare value chain and from the beginning to end of many individual tasks,"" said Jessica Lamb, a partner in the social, healthcare and public entities practice in the New York office of McKinsey & Company, a research and consulting firm. ""Think of a hospital, for example, that has to order a wide range of supplies, from gowns and gloves to high-end medical equipment. ""AI agents can be deployed to flag when items are running short, source the best supplier, initiate the purchase order and make the payment,"" she explained. ""Agents also can be used to support patients, helping them prepare for upcoming appointments, streamlining discharges and coordinating case management."" Agentic AI is a good fit for healthcare for several reasons, she added. ""For starters, healthcare already is heavily dependent on information technology, and agentic AI can work with existing software tools and platforms,"" Lamb said. ""That can make the transition somewhat easier. ""Another example is how the healthcare industry continues to rely on many manual processes, based on legacy technology and practices,"" she continued. ""As the examples I used indicate, AI agents can perform a wide range of complex but repetitive tasks that, for a variety of reasons, have not yet been automated."" And the benefits could be substantial. The American Hospital Association estimates administration accounts for 40% of hospital expenses. For hospitals and health systems looking to deploy agentic AI, getting it right can be challenging. So where is the best place to start? Lamb said, at the beginning, of course. But what is the beginning, according to her? Follow the need. ""Figure out where the highest potential for implementation is, such as the complex processes and workflows that involve a lot of touchpoints and handoffs and that are more manual than they have to be,"" she suggested. ""Once that is done, set actionable goals   agentic AI is not just a shiny new technology, it needs to be results-driven. ""It's important not to get stuck in pilot purgatory   an application in one department, an experiment in another,"" she continued. ""Unfortunately, that is where many organizations are, and it can be difficult to escape. The better approach is to pick a whole domain or work area, rather than a specific task."" The greatest benefit of agentic AI comes when it is scaled and changes how an organization functions, not when it is used simply to cut costs here and there, Lamb believes. ""Think of it this way: A use case is generating an appeal letter   a domain change is reimagining revenue cycle management,"" she said. Finally, embed change management from the beginning   if agentic AI is implemented well, it will transform the way the whole organization functions, and the way people work, she added. ""It's important to remember the human element,"" she said. ""Agentic AI is not just about tech. It is   or should be   about improving the job experience, creating higher-value work and democratizing technology in a way that fosters human connection."" Managing risks and ensuring governance certainly are essential for building trust in agentic AI systems. Hospitals and health systems implementing the technology must plan carefully. ""For ethical, security and management reasons, trust is essential,"" Lamb stated. ""And that means organizations need to take the principles of responsible AI to heart. Here is something that may not be well known: Investing in trust pays off. As my colleague Roger Roberts recently put it, 'When implemented well, responsible AI leads to real ROI.' The reason is simple: Without trust in agentic AI, people will not use it. ""It is up to people to define the parameters of agentic AI autonomy,"" she continued. ""That means structuring the workflow of each agent so it is deliberate about what it can and cannot do."" Another concern, bolstered by well-publicized instances of AI glitches, is the possibility of biases in inputs and outputs. That requires ensuring data accuracy and security. ""Keeping humans in the loop is a critical safeguard,"" Lamb advised. ""From the start, people need to be at the center of agentic AI development. That means education, in the form of a clear communications strategy that informs everyone about how and why the organization is planning to use agentic AI. To ease concerns about the possibilities of job losses, there also should be a clear strategy to re-skill existing employees. ""A transformation of this breadth and depth is a general responsibility,"" she continued. ""Good governance therefore requires mechanisms to report problems and cross-functional collaboration to ensure all perspectives are included."" The principle is simple: Just do it   but do it thoughtfully, she added. So, what does the future of agentic AI look like in healthcare? What is next for hospitals and health systems? Lamb puts it simply. ""The future of agentic AI is readily expressed: There will be more and more of it,"" she predicted. ""Older, manual and slow systems are on their way out   and that is a good thing. The use of AI agents will ease some of healthcare's most annoying pain points while allowing people to focus on more interesting and novel work. ""The process will not be straightforward,"" she cautioned. ""Some hospitals and health systems will be faster and defter than others. But the trend is clear: Agentic AI is the future."" Follow Bill's HIT coverage on LinkedIn: Bill SiwickiEmail him: bsiwicki@himss.orgHealthcare IT News is a HIMSS Media publication. WATCH NOW: Helpful tips on smart rooms from OhioHealth Newsletter Signup Thank you! Your submission has been received.   current_year Healthcare IT News is a publication of HIMSS Media",2
Charta Health raises $22M for AI healthcare operations platform - Mobi Health News,https://news.google.com/rss/articles/CBMimgFBVV95cUxNcFhNUHJqbHF3TmljTFFxSDhueHF0MmRWX2Q1dGdwZTVwN0RFOUhWbGlDZ0ptb3lUa20yOS1QNTBGdGNSS2VGSWxtVU1RTFA1OFJPelB0Nk8zU010dmxQQ09zSVYwOVdpdVdkOWxkT21qYm8xb3JNWmk0NUUxeldFZi1qTlA5ZFRIb1M5Ui1JQVJBU256SURTSTlB?oc=5&hl=en-US&gl=US&ceid=US:en,"Charta Health, an AI-enabled platform that automates billing and coding workflows, has secured $22 million in Series A funding. Bain Capital Ventures led the round, with participation from SV Angel, South Park Commons, Madrona Venture Group and Refract Ventures. WHAT IT DOES The San Francisco-based company's platform utilizes AI to review patient charts before submission, flagging missed codes in an effort to reduce administrative burden and prevent denials before they occur. The company will use the funds to expand its engineering and go-to-market teams, accelerate product development and speed up enterprise integrations with payers and providers. ""Historically, the complexity and fragmentation of healthcare infrastructure made it nearly impossible to apply AI meaningfully at scale,"" Justin Liu, cofounder and CEO of Charta Health, said in a statement. ""Today, we can layer advanced AI directly onto any EMR, surfacing patient-level insights: clinical, financial and operational   in real time, and unlocking automation without disrupting workflows or overburdening clinicians."" MARKET SNAPSHOT Earlier this year, Charta Health raised $8.1 million in seed funding. Charta Health was founded by Liu and Scott Morris, who formerly worked as engineers at Rockset, a company that offered tools for real-time search and data analytics. Rockset was acquired by OpenAI last year. MobiHealthNews Thank you! Your submission has been received. Add MobiHealthNews to your network.   current_year MobiHealthNews is a publication of HIMSS Media",2
How Rwanda is using AI to transform healthcare - The World Economic Forum,https://news.google.com/rss/articles/CBMiVkFVX3lxTE8xLU5qWnlMTU90MnlOS2k0d2EyM1dhQVpYTjVudlFZcElLOE1yUGUzSWpZTjBYT0RXVTBSYjJIQWhEYURHaUNBQVI5VjFpdi1PZ0NoLWhR?oc=5&hl=en-US&gl=US&ceid=US:en,"Some of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness. Some of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness. Some of the data collected by this provider is for the purposes of personalization and measuring advertising effectiveness. The World Economic Forum ( Forum ) uses necessary cookies to make our site work. We would also like to set optional  performance  cookies to gather anonymous site visitation data for internal use and we use ""marketing"" cookies to personalise content and ads, to provide social media features and to analyse our traffic. We also share information about your use of our site with our social media, advertising and analytics partners who may combine it with other information that you ve provided to them or that they ve collected from your use of their services. By enabling these cookies, you can help the Forum provide a better website for users like yourself. For more information about the Forum cookies and third-party cookies, see our Cookie Declaration. Cookies are small text files that can be used by websites to make a user's experience more efficient. The law states that we can store cookies on your device if they are strictly necessary for the operation of this site. For all other types of cookies we need your permission. You can change your preferences at any time or withdraw your consent by clicking on  Change your consent  on the Cookie Declaration page. C4IR | Impact On The Ground Thirty years ago, Rwanda faced a devastated healthcare system with minimal infrastructure and a shortage of medical professionals. Today, through strategic investments and innovation, the country has achieved universal health coverage and strengthened its healthcare workforce, particularly through its community health worker model. But challenges remain, particularly in equipping frontline workers with advanced clinical expertise. Thirty years ago, Rwanda's healthcare system was in crisis, lacking infrastructure and medical professionals. Through strategic investments, the country has achieved universal health coverage and strengthened its workforce, particularly with community health workers. Despite progress, a key challenge remains: equipping frontline health workers with advanced clinical expertise. Rwanda is addressing this by integrating AI-powered decision support tools. In partnership with Digital Umuganda, locally trained language models in Kinyarwanda assist in diagnosing diseases, increasing accuracy from 8% to 71%. The country is now extending its generative AI capabilities to sectors like agriculture and e-governance. This approach demonstrates that AI-driven solutions can be inclusive, scalable, and effective in tackling global challenges. Crystal Rugege Managing Director, Centre for the Fourth Industrial Revolution, Rwanda C4IR | Impact On The Ground | Azerbaijan is transforming and upskilling its workforce with free online education C4IR | Impact On The Ground | How is Serbia diagnosing rare diseases with AI C4IR | Impact On The Ground | How India is supporting smallholder farmers using AI C4IR | Impact on the Ground | How smart manufacturing is helping small businesses in Saudi Arabia AI geopolitics and data centres in the age of technological rivalry Mark Esposito July 24, 2025 How orbiting  gas stations  could tackle the problem of space debris How Brunei s BruHealth journey charts the future of digital health Mohammad Isham Jaafar and Gong Yingying July 22, 2025 Quantum computing will soon crack today s encryption methods. Here are 3 ways businesses can prepare 3 ways quantum technologies are poised to transform the world of finance Rethinking Media Literacy: A New Ecosystem Model for Information Integrity About us More from the Forum Engage with us Quick links Language editions Privacy Policy & Terms of Service Sitemap   2025 World Economic Forum Remove menu",2
The future of artificial intelligence in health care - Deloitte,https://news.google.com/rss/articles/CBMiygFBVV95cUxNZXljVC1GdTk0R1lCTUxKVkVtX1F5STNxRlBsM3MyRFJoc0lfVnhDMTBGY2dHWnQycDFfMnZNLUVXRkh4TVBmazNjUzlvTWhTNDNWS05oVzhxUHRhcmdnWFNCLW1XcWtCellJQWQ3cjlHN0l4bTJuRjRVOGEzSzlIYmp6X1lwT1JlT0ZRQWxaUm5CV28tZ1pyUXRreXRLcWRqTk43Y203Y09DTlJNNUdJSjZBSVdVclFiNDRCWWItZENmcWRoSXAwZFpn?oc=5&hl=en-US&gl=US&ceid=US:en,"Select your location No results found If we have selected the wrong experience for you, please change it above. Artificial intelligence (AI) is transforming the way we interact, consume information, and obtain goods and services across industries. In health care, AI is already changing the patient experience, how clinicians practice medicine, and how the pharmaceutical industry operates. The journey has just begun. As AI finds its way into everything from our smartphones to the supply chain, applications in health care fall into three broad groupings1: The future of AI in health care could include tasks that range from simple to complex everything from answering the phone to medical record review, population health trending and analytics, therapeutic drug and device design, reading radiology images, making clinical diagnoses and treatment plans, and even talking with patients. The future of artificial intelligence in health care presents: 1 Laura Craft, Emerging Applications of Ai for Healthcare Providers, GARTNER, June 30 2017, accessed June 24, 2019 From patient self-service to chat bots, computer-aided detection (CAD) systems for diagnosis, and image data analysis to identify candidate molecules in drug discovery, AI is already at work increasing convenience and efficiency, reducing costs and errors, and generally making it easier for more patients to receive the health care they need. While NLP and ML are already being used in health care, they will become increasingly important for their potential to: While each AI technology can contribute significant value alone, the larger potential lies in the synergies generated by using them together across the entire patient journey, from diagnoses, to treatment, to ongoing health maintenance. Based on our work with clients on applications of AI in health care, we offer these insights: Health care providers can prepare for the inevitable changes related to the future of AI in health care with the following key considerations. Create an innovation steering committee that meets regularly to identify processes that could be automated or enhanced by AI. Develop analytics teams to leverage as much information from patients, providers, and other populations to enhance provider care and operations. Since many decisions business and clinical will be based on this data, create the culture and processes to promote clean, complete, and timely data. Be open to new ideas and look to other industries for inspiration. Invest in adequate data center and IT infrastructure solutions to support the technologies you implement. Opens in new window Did you find this useful? To tell us what you think, please update your settings to accept analytics and performance cookies.   2025. See Terms of Use for more information. Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (""DTTL""), its network of member firms, and their related entities. DTTL and each of its member firms are legally separate and independent entities. DTTL (also referred to as ""Deloitte Global"") does not provide services to clients. In the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the ""Deloitte"" name in the United States and their respective affiliates. Certain services may not be available to attest clients under the rules and regulations of public accounting. Please see www.deloitte.com/about to learn more about our global network of member firms.",0
From Diagnosis to Surgery: How AI Tech Is Quietly Reshaping Healthcare Profits - PR Newswire,https://news.google.com/rss/articles/CBMizgFBVV95cUxNUUtKRjVlRmN0SDBFY0JoZ3l6WTJtSVo2OWxCV2Q3OGhxYjVyTUU1dTJGRkFwbk9zWjVsVzU5MmljYXY2TXpBaGtyYzJOM3o0SjI1X0tIMFZfTkxpZXR5OUx3Zy1pNERCRVBWQkpqM1JoajcwN0RjaGtLXzNzYWphemd5LW9VTjhOcy1RUlYySV9yMXVqUmEzakd1OVA5bVBDQXlCbHo5MkpFR0NBcmpBcHlRNXJwWEluRi1mRTJOVEhaaVU0VHhlcUlJYWw0dw?oc=5&hl=en-US&gl=US&ceid=US:en,"Searching for your content... In-Language News Contact Us 888-776-0942 from 8 AM - 10 PM ET Jul 22, 2025, 09:15 ET Share this article USA News Group News CommentaryIssued on behalf of Avant Technologies Inc. VANCOUVER, BC, July 22, 2025 /PRNewswire/ -- USA News Group News Commentary   The AI revolution in healthcare is fully underway, with the potential to save lives and money, but there's still more to come. Healthcare startups are already figuring out how to implement AI to multiply their patients, and chase profits, to the benefit of investors. More Americans than ever are utilizing AI to diagnose their health issues, as the adoption of AI tools is becoming the norm among medical professionals. Now the developers of these game-changing technologies are overcoming regulatory barriers and moving closer to implementation of their solutions in the real world. Among the players that are moving closer to major milestones include Avant Technologies, Inc. (OTCQB: AVAI), Agenus Inc. (NASDAQ: AGEN), Clover Health Investments, Corp. (NASDAQ: CLOV), Zimmer Biomet Holdings, Inc. (NYSE: ZBH), and Monogram Technologies Inc. (NASDAQ: MGRM). Currently, the AI in healthcare sector projected to surpass $110 billion by 2030. However, according to Accenture, artificial intelligence could unlock an additional $461 billion in value across healthcare by 2035 on top of a sector already projected to surpass $2.26 trillion. Avant Technologies, Inc. (OTCQB: AVAI) and joint-venture partner Ainnova Tech are continuing to raise visibility for their AI-powered diagnostic platform, with Ainnova CEO Vinicio Vargas recently featured as a speaker at Roche's ""Macular Spectacular"" Ophthalmology Conference in Cartagena, Colombia. The event brought together leading voices in visual health from across Latin America, with discussions focused on advancing patient outcomes through AI innovation. Vargas, who also sits on the board of Ai-nova Acquisition Corp. (AAC), presented Vision AI as a transformative tool to expand access to early diabetic retinopathy screening and reduce preventable blindness throughout the region. Vargas also highlighted a Q4 2024 alliance with Roche and prepaid health plan provider Salud 360 that is piloting Vision AI among high risk diabetic patients; if successful, the program will be rolled out in the United States, Canada, and Europe through Ai nova Acquisition Corp. (AAC), which holds worldwide rights to Ainnova's technology portfolio . This momentum builds on a recent regulatory breakthrough in the U.S. market. Avant and Ainnova recently completed a key pre-submission meeting with the FDA for Vision AI, the companies' flagship diagnostic platform for diabetic retinopathy and other retinal diseases. ""We're truly excited about this next phase,"" said Vinicio Vargas, CEO at Ainnova and a member of the Board of Directors of Ai-nova Acquisition Corp. (AAC), the company formed by the partnership between Avant and Ainnova to advance and commercialize Ainnova's technology portfolio. ""We're getting ready to begin data collection across primary care clinics in the U.S. with a study that is simple, yet rigorous comparing our AI-based retinal screening to the readings of three retinologists. The July 15 meeting marked a critical step toward securing 510(k) clearance for Vision AI. The FDA session offered clear feedback on study design, site selection, and execution strategy enabling Ainnova and its clinical trial partner, Fortrea, to finalize preparations for U.S.-based trials. ""This milestone not only brings us closer to validating our platform in the world's largest healthcare market, but it also paves the way for the upcoming approval of our new automated retinal camera,"" added Vargas. ""We believe will [it] be a game changer making diabetic retinal screenings faster, more accessible, and available from virtually any point of care."" While regulatory efforts advance in the U.S., the company has already launched a groundbreaking chronic care model across Latin America. The program, now live through Grupo D kka's Fischel and La Bomba pharmacy chains, offers free, walk-in retinal risk assessments delivering real-time AI results and connecting at-risk patients directly with clinics and specialists. Over 30% of diabetics develop diabetic retinopathy, a leading cause of preventable blindness. Vision AI offers early, low-cost screening without the need for an ophthalmologist onsite. The model has gained support from pharmacies, insurers, and pharmaceutical partners demonstrating real traction across the healthcare value chain. Avant's role in these efforts continues to grow. Through Ai-nova Acquisition Corp., which it co-founded and structured, Avant maintains global licensing rights to Ainnova's platform and stands to benefit from both U.S. and international commercialization. The Latin American pilot programs are already generating momentum, with U.S. trials positioned to unlock a vastly larger market. The company is also preparing a standalone venture to house a potential therapeutic candidate for diabetes bringing leadership, data, and IP into one streamlined structure for diagnostics and treatment alike. Looking ahead, Vision AI could serve as a frontline tool for broader disease detection. Ainnova's future roadmap includes a cloud-connected retinal camera for rural clinics and new modules aimed at identifying Alzheimer's, cardiovascular conditions, and other chronic diseases through retinal or blood biomarkers. A structural simplification may be on the horizon as well. A previously announced non-binding LOI remains active for Avant to acquire 100% of Ainnova Tech potentially consolidating all IP, leadership, and commercial rights under one public entity. Such a move would give investors pure-play exposure to this fast-evolving tech stack, while deepening operational alignment between the firms. CONTINUED  Read this and more news for Avant Technologies Inc. https://usanewsgroup.com/2023/10/26/unlocking-the-trillion-dollar-ai-market-what-investors-need-to-know/ Agenus Inc. (NASDAQ: AGEN) has partnered with AI biotech firm Noetik to develop predictive biomarkers for its cancer immunotherapy combo BOT/BAL. ""At Agenus, we are committed to transforming cancer care through scientific innovation and next-generation immunotherapies,"" said Dr. Garo Armen, Chairman and CEO of Agenus. ""This collaboration with Noetik enables us to harness cutting-edge AI to better understand patient biology and tailor treatments more precisely."" By using Noetik's advanced ""virtual cell"" models trained on massive tumor datasets, the collaboration aims to identify which patients are most likely to benefit from treatment. This could improve clinical trial outcomes and accelerate personalized cancer care using AI-driven insights Clover Health Investments, Corp. (NASDAQ: CLOV) recently launched a new pharmacy pilot program across New Jersey in partnership with IPC's iCare+ network. The initiative enables local pharmacies to use AI tools and real-time data to help seniors manage medications and reduce hospital visits. ""This is more than a new program it's a new model for how we support our New Jersey members,"" said Jamie Reynoso, CEO of Medicare Advantage at Clover Health. ""By working with the pharmacists who are already embedded in our communities, we can deliver better care, deliver amazing customer experiences and support the independent businesses that keep New Jersey strong."" It's part of Clover's strategy to deliver more personal, community-based healthcare while supporting independent pharmacies Zimmer Biomet Holdings, Inc. (NYSE: ZBH) has entered a definitive agreement to acquire Monogram Technologies Inc. (NASDAQ: MGRM) for approximately US$177 million in cash, with additional milestone-based payments up to US$12.37 per share. ""Monogram's technology is a major leap forward, demonstrating our commitment to becoming the boldest and broadest innovator in surgical robotics and navigation,"" said Ivan Tornos, Chairman, President and CEO of Zimmer Biomet. ""Upon closing, our customer-centric portfolio will consist of the most comprehensive and flexible technology ecosystem to support the varying preferences of a vast array of surgeons   now and into the future. With Monogram's proprietary technology, Zimmer Biomet has the potential to become the first company to deliver fully autonomous capabilities and redefine both the standard of care and the future of orthopedic surgery."" The acquisition adds Monogram's semi- and fully autonomous orthopedic robotics to Zimmer Biomet's ROSA  platform, aiming to create the most comprehensive robotics suite in the orthopedic space. Monogram's FDA-cleared knee arthroplasty robot is expected to commercialize in 2027, with a fully autonomous version in development. ""Since our inception, we have been singularly focused on advancing orthopedic robotics with technology designed to safely, efficiently and accurately support surgeons with total knee arthroplasty,"" said Benjamin Sexson, CEO of Monogram. ""We are thrilled by the opportunity to add our technology to Zimmer Biomet's leading portfolio of surgical robotics, navigation solutions and trusted implants and to benefit from their deep industry expertise and global scale."" Zimmer Biomet expects the deal to be accretive by 2028, supporting long-term growth in the high-demand surgical robotics market. Source: https://usanewsgroup.com/2023/10/26/unlocking-the-trillion-dollar-ai-market-what-investors-need-to-know/ CONTACT:USA NEWS GROUPinfo@usanewsgroup.com(604) 265-2873 DISCLAIMER: Nothing in this publication should be considered as personalized financial advice. We are not licensed under securities laws to address your particular financial situation. No communication by our employees to you should be deemed as personalized financial advice. Please consult a licensed financial advisor before making any investment decision. This is a paid advertisement and is neither an offer nor recommendation to buy or sell any security. We hold no investment licenses and are thus neither licensed nor qualified to provide investment advice. The content in this report or email is not provided to any individual with a view toward their individual circumstances. USA News Group is a wholly-owned subsidiary of Market IQ Media Group, Inc. (""MIQ""). MIQ has been paid a fee for Avant Technologies Inc. advertising and digital media from the company directly. There may be 3rd parties who may have shares Avant Technologies Inc., and may liquidate their shares which could have a negative effect on the price of the stock. This compensation constitutes a conflict of interest as to our ability to remain objective in our communication regarding the profiled company. Because of this conflict, individuals are strongly encouraged to not use this publication as the basis for any investment decision. The owner/operator of MIQ own shares of Avant Technologies Inc. which were purchased in the open market. MIQ reserves the right to buy and sell, and will buy and sell shares of Avant Technologies Inc. at any time thereafter without any further notice. We also expect further compensation as an ongoing digital media effort to increase visibility for the company, no further notice will be given, but let this disclaimer serve as notice that all material disseminated by MIQ has been approved by the above mentioned company; this is a paid advertisement, and we own shares of the mentioned company that we will sell, and we also reserve the right to buy shares of the company in the open market, or through other investment vehicles. While all information is believed to be reliable, it is not guaranteed by us to be accurate. Individuals should assume that all information contained in our newsletter is not trustworthy unless verified by their own independent research. Also, because events and circumstances frequently do not occur as expected, there will likely be differences between any predictions and actual results. Always consult a licensed investment professional before making any investment decision. Be extremely careful, investing in securities carries a high degree of risk; you may likely lose some or all of the investment. Logo - https://mma.prnewswire.com/media/2603685/5425599/USA_News_Group_Logo.jpg SOURCE USA News Group USA News Group News Commentary   Within the last week, silver crossed the $39 barrier, hitting its highest level since 2011. Now analysts are... USA News Group News Commentary   After a brief rise, gold's price leveled off as the market responded to US President Donald Trump shooting down... Health Care & Hospitals Medical Pharmaceuticals Artificial Intelligence Medical Equipment Do not sell or share my personal information:",2
Is Your AI Scribe HIPAA-Compliant? - HealthLeaders Media,https://news.google.com/rss/articles/CBMiggFBVV95cUxOXzdTVUI1bWtLTHVTQ0pDemhSQU41TXVqN0cyVEtKQzJGMDEyZFhCcjRWTUJPaUFqeFk4NHdQWW1TdklPVnBUcU83WlFweVRIU0dGamtrbk5rOEJDTVJ5Z1VxMU1qVTRZTHhETW5CNlZkUkFmSVRYWG9LRWMtZ1dRVlZB?oc=5&hl=en-US&gl=US&ceid=US:en,"Analysis | By Eric Wicklund | July 23, 2025 AI scribes are one of the hottest tools on the market, as healthcare executives look for technology that can reduce the administrative burden on providers and give them a complete and codable transcription of the doctor-patient encounter. Healthcare executives need to treat these tools as third parties in the healthcare setting, rigorously testing for security flaws and ensuring that protected health information captured during the conversation is used properly and safely. Because AI tools get better by continuously updating and accessing better data, there will come a time when healthcare leaders need to re-evaluate their data use priorities and develop new strategies for using PHI. Healthcare executives who are letting their doctors use AI scribes to capture and code patient encounters need to be careful they aren't exposing or misusing protected health information. Ambient AI technology, described as the ""digital sidekick of modern healthcare,"" is quickly gaining favor among health systems, hospitals and payers looking to reduce the administrative burden on providers and accurately capture the doctor-patient visit. But those tools, which can easily be downloaded onto a smartphone, could be running afoul of HIPAA. ""Technically, it's a third party listening into the conversation,"" says Aaron Maguregui, a partner with the Foley & Lardner law firm who specializes in AI and healthcare technology. An AI scribe, he says, is essentially a service provider, so the healthcare provider using that app, as a covered entity, would need a Business Associate Agreement (BAA). The challenge is particularly acute at this point in the AI cycle, when vendors are flooding the market with their own products   some from companies new to the healthcare industry and unfamiliar with or ignorant of the regulatory requirements. In many cases these apps can be downloaded by providers and put to use almost immediately. That's a nightmare for healthcare leaders trying to keep track of what their doctors are using. There are plenty of stories of CIOs and CEOs learning that a doctor in one of their hospitals or clinics is using ChatGPT or some other product on their own. ""There might be some (doctors) that are already enjoying the scribe and you just don't know about it,"" Maguregui, who also chairs the American Telemedicine Association's Artificial Intelligence Committee, points out. ""Technically you could have an unauthorized disclosure of PHI."" Maguregui recently authored a blog on the Foley & Lardner website with Jennifer Hennessy, a data privacy and security attorney with the law firm, on the most common mistakes that healthcare executives make in managing AI scribes. Those pitfalls, he says, can be grouped into two basic issues: Data use rights and patient consent. Issues around data use are particularly critical, and point to an intriguing catch-22 in the healthcare space. AI needs access to better data to learn and improve, and vendors often will ask for more data in which to train and improve their products. But healthcare leaders are notoriously stingy in granting access to that data. ""It's always interesting to me that the knee-jerk reaction is we don't want you to train AI on our data. And that to me is backwards thinking,"" Maguregui says. ""You want the technology to be efficient and accurate, but you don't want it to use your data. Without that you don't get the full value of AI."" That's especially tchellenging, he says, with tech companies that are new to healthcare, bringing in ideas from other industries. ""There are some really cool stuff out there, but maybe this is their first foray into healthcare and they don't understand that, yes, they can very much ingest all the data that they believe they're allowed to ingest, and then?"" he asks. ""The output would somehow not be theirs to be able to use, to train their products. That's a very foreign concept to the tech world.   [Those] data use cases, data use rights, those end up being a pretty sticky subject."" On the other hand, Maguregui points out, using AI in clinical care means training the technology on the best data available   including protected health information. ""Specificity counts, and specificity is what we're looking to get to with respect to AI,"" he says. ""We want AI to give specific answers. We want it to be nuanced. Those nuances are going to have to at some point start to take into account identifiable information in order to glean cohorts and cultural differences and social determinants of health, things that we probably want to learn. We want to understand these concepts. But we also need to make sure that we're being cautious with people's privacy rights."" And that's where the second hang-up with AI scribes comes into play. By using a third-party app to record their patient encounters, providers need to secure the patient's permission to be recorded, a requirement included in federal wiretapping laws. Maguregui says providers need to understand that getting a patient's permission to record their encounter has to be a part of the workflow. And that may be fine in the doctor's office, but what happens when AI captures conversations in the Emergency Department, ICU or even the operating room? Whatever the case, healthcare executives need to make sure their ambient AI tools are HIPAA-compliant   and they need to make sure their AI strategies take into account the potential for using PHI in future programs. To that end, in their blog, Maguregui and Hennessy offer five steps that healthcare leaders should take when dealing with AI scribes: Eric Wicklund is the associate content manager and senior editor for Innovation at HealthLeaders. Physicians are in short supply. They are costly. Is the APP the answer to the CMO's workforce and budget challenges? ... In a social media landscape shaped by hashtags, algorithms, and viral posts, nurse leaders must decide: Will they let the narrative spiral, or can they adapt and join the conversation? ... This site uses cookies in order to give you the best experience. We and our third-party partners may use cookies and similar technologies, for example, to analyze usage and optimize our sites and services, personalize content, tailor and measure our marketing and to keep the site secure. Please visit our privacy policy for more information. Privacy Policy This site uses cookies in order to give you the best experience. We and our third-party partners may use cookies and similar technologies, for example, to analyze usage and optimize our sites and services, personalize content, tailor and measure our marketing and to keep the site secure. Please visit our privacy policy for more information. Privacy Policy",2
Reimagining Healthcare Through Artificial Intelligence Innovations - Healthcare Tech Outlook,https://news.google.com/rss/articles/CBMiwAFBVV95cUxOTC1Ob0RWazF4R3Z6RFNrRVl0VThuQUhBdXFmMzBUaUV3bVhEN3U3bHZuT3Q4TjFfeUpYdzRCT2IwOXJibXlDR3lyaE5yenRlcGsxYWVRejRRbHRKWUZXWDgtaEhUVUlnWnl6RnhzWHczSVhDZjZndkp3RThub0dwWUZTcl9ldjZTVGgzbno1Q2pQUjBtZ1Z5WlJrY3E3TmQyVDNTNVZlTDdkNUtBRWY2Rm5TNF96bzRMQjZuc0V0OUo?oc=5&hl=en-US&gl=US&ceid=US:en,"Advertise with us Weekly Brief Be first to read the latest tech news, Industry Leader's Insights, and CIO interviews of medium and large enterprises exclusively from Healthcare Tech Outlook THANK YOU FOR SUBSCRIBING I agree We use cookies on this website to enhance your user experience. By clicking any link on this page you are giving your consent for us to set cookies. More info Welcome to Healthcare Tech Outlook! We use cookies and similar technologies to enhance your experience on our website and to provide personalized content and advertisements. By clicking ""Accept,"" you agree to the use of these technologies as described below. You can manage your choices or withdraw consent at any time by visiting our Privacy Policy page. Essential Cookies: We use essential cookies to make our site work. You may disable these by changing your browser settings, but this may affect how the website functions. Types of Personal Data Processed: We process personal data including unique identifiers and browsing behavior. For more information, please refer to our Cookie and Privacy Policies. We Use Your Data For: Google Services: Please note that we use Google services such as Ad Manager, Search Console, and Google Analytics GA4 to improve our website's functionality and performance. Information about cookies privacy policy! What Are Cookies? Cookies are small text files that are stored on your device (computer, tablet, or mobile phone) when you visit a website. They allow the website to recognize your device and store certain information about your preferences or actions. How We Use Cookies: We use cookies and similar technologies for the following purposes: Your Choices Regarding Cookies: You have the option to accept or reject non-essential cookies when you first visit our website. You can also manage your cookie preferences at any time by adjusting your browser settings or visiting our cookies consent form. Managing Cookies: Most web browsers allow you to control cookies through their settings. You can typically find these settings in the ""Options"" or ""Preferences"" menu of your browser. Please note that disabling certain cookies may affect the functionality of our website. By Healthcare Tech Outlook | Friday, July 25, 2025 Stay ahead of the industry with exclusive feature stories on the top companies, expert insights and the latest news delivered straight to your inbox. Subscribe today. Artificial intelligence (AI) is transforming healthcare, fundamentally changing how providers deliver care, how patients receive it, and how healthcare systems operate. AI technologies improve diagnostic accuracy and enhance operational efficiency, allowing healthcare organizations to develop unprecedentedly. Additionally, AI helps match patients with the most suitable therapies based on their profiles. With advancements in AI, personalized medicine enables physicians to create treatment plans tailored to a patient's genetic makeup, lifestyle choices, and existing health conditions. Administrative functions have benefited immensely from AI. Robotic process automation tools streamline repetitive tasks like claims processing and appointment scheduling, freeing time for healthcare workers to focus on patient care. Clinical decision support systems provide real-time alerts and recommendations to help doctors make informed choices, improving patient safety and reducing medical errors. Trends Driving Future Applications Several critical factors have catalyzed the rapid adoption of AI in healthcare. Chief among them is the exponential growth in medical data. Electronic health records (EHRs), wearable devices, genomics, and medical imaging have generated structured and unstructured data. Healthcare providers need AI tools to analyze this information, extract insights, and support decision-making processes. The global push toward value-based care demands more accurate, cost-effective, personalized treatment approaches an area where AI excels. Another driving factor is the shortage of medical professionals in rural or underserved regions. AI can assist physicians by automating routine tasks, offering diagnostic support, and monitoring patient vitals remotely through wearable tech and Internet of Medical Things (IoMT) devices. Predictive analytics is another major trend, helping providers anticipate patient deterioration, hospital readmissions, and disease progression. AI algorithms are now routinely used in radiology to detect abnormalities in X-rays, MRIs, and CT scans, sometimes outperforming human radiologists in specific tasks. AI's applications extend across every aspect of healthcare. In diagnostics, it can analyze images, pathology slides, and even genetic data to pinpoint diseases like cancer, cardiovascular conditions, and rare genetic disorders. While data, ethics, and infrastructure challenges persist, AI's growing momentum in healthcare is undeniable. By embracing AI with a strategic, patient-centric approach, the healthcare industry stands poised to achieve unprecedented advancements, offering smarter, faster, and more equitable care. Overcoming Hurdles with Solutions Ensuring compliance with HIPAA while enabling innovation is a delicate balance. Encryption, secure cloud storage, and federated learning, a technique that trains AI models across decentralized data sources without moving the data, are promising solutions to mitigate these concerns. EHRs often contain inconsistent or incomplete information, making it difficult for AI models to learn accurately. Public-private partnerships, government incentives, and international cooperation can support these foundational improvements. Efforts to create interoperability standards are helping to improve data consistency across systems. Continuous collaboration between healthcare providers, data scientists, and technology vendors is essential to refine algorithms and ensure meaningful outcomes. Bias in AI algorithms is also a pressing concern. If the data used to train an AI system does not represent diverse populations, it may lead to disparities in diagnosis and treatment. Developers must prioritize inclusive datasets and conduct bias audits regularly. Regulatory bodies and ethical oversight committees can further guide the responsible deployment of AI solutions. There's a cultural and organizational resistance to adopting AI tools. Many healthcare professionals fear being replaced or mistrust the reliability of automated systems. Addressing this challenge requires education, change management strategies, and clinicians' inclusion in designing and deploying AI tools. By positioning AI as a tool to augment rather than replace human expertise, healthcare organizations can foster greater acceptance. Infrastructure limitations hinder AI adoption. Investments in digital health infrastructure, including cloud computing, high-speed internet, and mobile health platforms, are crucial to ensuring equitable access to AI-powered solutions. Navigating Future Market Trends The impact of AI on the healthcare market is both profound and far-reaching. The increasing demand for efficient clinical workflows, early disease detection, remote patient monitoring, and drug discovery fuels this explosive growth. Pharmaceutical companies are leveraging AI to expedite research and development. AI algorithms analyze molecular structures, predict drug interactions, and simulate clinical trials, significantly reducing time-to-market for new medications. In public health, AI assists in tracking disease outbreaks, modeling infection spread, and optimizing resource allocation during emergencies. Hospitals and healthcare systems adopting AI technologies report improvements in operational efficiency, patient satisfaction, and clinical outcomes. Predictive maintenance of medical equipment, automated billing, and enhanced diagnostic support reduce downtime, lower costs, and minimize human error. AI-powered mobile apps can offer basic diagnostic capabilities and health advice in remote or underserved areas. Virtual health assistants, supported by natural language processing, guide patients through symptom checkers, medication reminders, and mental health support, empowering individuals to manage their health proactively. AI will continue to evolve from a support tool to a strategic partner in care delivery, playing an integral role in precision medicine, population health management, and predictive care models. It includes transparent algorithms, continuous evaluation, inclusive design, and robust data governance frameworks. AI-powered healthcare transformation is not just a technological evolution; it's a paradigm shift that redefines the delivery, accessibility, and quality of care. More in News Cancer Treatment Reimagined: Insights into New Developments Future Trends in Healthcare Technology Confronting Challenges in Medical Imaging with AI Innovations Promising Technologies in Cancer Research Today However, if you would like to share the information in this article, you may use the link below: www.healthcaretechoutlookeurope.com/news/reimagining-healthcare-through-artificial-intelligence-innovations-nid-4610.html",2
The AI-powered future of health: Insights from Microsoft leaders - Microsoft,https://news.google.com/rss/articles/CBMizAFBVV95cUxNWUNXRnUwUm5lUWdYel9hQ2MwLW9qTFdMU1YyMW9LSThwejZYLWJQOFhtV19Kc2pTanFWSjdIS0xMZm04M2VCWGprWHFHbE91TU9hWWFrRjNJZUlxSS11bHkwb2doUk92bG0wUERabHpPT1k5YV9CVXlMcUhXREItMGJpU0huU2dmTllmVGZTMWUwM0hDc04ydFlWNDVxTklTTF9OVExFQ05xVk9jcXR3dnVSY3RQcDBnemxpUi1IM2FtUnpMWGhnWTQ4ZzE?oc=5&hl=en-US&gl=US&ceid=US:en,"Over the last few years, healthcare and life sciences organizations have made great strides in harnessing AI to accelerate scientific breakthroughs, enhance clinician productivity and wellbeing, and improve patient experiences and outcomes. It s remarkable to think how far we ve come since Microsoft was founded 50 years ago. But what s truly astonishing is the pace of progress we re now seeing, as rapid advancements in AI create opportunities to solve industry problems that once seemed intractable. Microsoft has been at the frontier of AI research and development for decades, and we re committed to sharing our learnings and insights with stakeholders throughout healthcare and life sciences. That s why we ve created the 2025 AI in Healthcare Decision Brief. This in-depth industry analysis is split into two parts: Part 1: Insights on navigating the AI platform shift, and Part 2: Perspectives on the role of AI in shaping the future of healthcare. Each part features expert perspectives from Microsoft leaders, inspirational examples of AI successes in healthcare and life sciences, and practical advice for accelerating AI adoption in your organization. Here s an overview of what you ll find in the report. Over half (57%) of life sciences organizations and 45% of healthcare organizations see generative AI as the most important technology to adopt, and 79% are currently using some form of AI.1 While early use cases for generative AI typically focus on boosting productivity, as trust and adoption continue to grow, new use cases will emerge that have a transformational impact on the entire sector and on patients  health. Realizing this AI-powered future of health will require organizations to: 2025 AI Decision Brief AI innovators are already delivering meaningful impact in healthcare and life sciences from creating synthetic data to accelerate drug development to supporting physicians with real-time clinical insights at the point of care. New technology advances will allow innovators to create solutions that will have an even greater, industry-wide impact, dramatically improving health equity and care outcomes for patients worldwide. Maximizing AI s potentiaL: As we celebrate the accomplishments of Microsoft employees, alumni, partners, and customers over the last half-century, we re also looking ahead to what the next 50 years could bring, as we continue our mission to empower every person and every organization on the planet to achieve more. Wherever your organization is on its AI journey, we re here to make the path smoother and help you achieve the right outcomes. Get The 2025 AI in Healthcare Decision Brief, Part 1: Insights on navigating the AI platform shift now for Microsoft AI leadership perspectives on: Read The 2025 AI in Healthcare Decision Brief, Part 2: Perspectives on the role of AI in shaping the future of healthcare for more leadership perspectives on: 1McKinsey, Market perspective: AI and GenAI in Life Sciences Transcend boundaries with tailored industry solutions. Accelerate time to value, speed up innovation, and drive benefits for your customers, employees, and organization. Notifications",2
[Webinar] AI Governance in Healthcare: How Compliance Teams Can Manage Risk and Stay Ahead - JD Supra,https://news.google.com/rss/articles/CBMiigFBVV95cUxOLVpWWXhEYk5vZkQ1dzJfaHk0Sm9ROFhiVlE1Y2RkYzFrdGJDQkRNT0FzUHoyV29NbWJPY05tUkxudEg5dlZEUTVpY3kzYTdxMUo1RXU0cXYtRkx2ZUNWdld1RXA1d3ZUdFF4Nk9ETEN3LUxtbFM2czNJU2MzZXVScXJZNjJTd2tOUGc?oc=5&hl=en-US&gl=US&ceid=US:en,"As AI adoption accelerates in healthcare, compliance, privacy, and risk teams are under pressure to adapt. Join experts from NAVEX and Granite GRC to learn how a proactive AI governance strategy can help you stay compliant, mitigate operational risk, and protect patient safety. Why AI Governance Can t Be an Afterthought in Healthcare As AI continues to shape everything from diagnostics to care delivery, the associated risks, including data privacy, bias, and compliance blind spots, are growing just as fast. Without oversight, even beneficial tools can introduce reputational, regulatory, and safety consequences. Join this timely conversation to learn how risk and compliance teams can lead secure AI adoption by embedding oversight into broader GRC strategies. What you ll learn: Who should attend? This webinar is ideal for: Sound like you? Meet the speakers: Senior Director and the Healthcare Vertical Leader NAVEX Director-in-Charge Granite GRC Consulting Director, Compliance & Privacy Services Granite GRC Consulting See more     NAVEX 2025 Refine your interests   Back to Top Explore 2025 Readers' Choice Awards Copyright   2025 JD Supra, LLC",2
Augmented intelligence in medicine - American Medical Association,https://news.google.com/rss/articles/CBMilgFBVV95cUxPajkxSnUyTzFoTVNOdGdtNFZqQlA3bW5SWDJiT18ybWEzR3dZMUJiS1N3dzNSclU2NFkwNC0tcUVxRTFCVFlpVFl4TWctSVdrZ25DaUh5MnR1VTdwUXA4NW9LOWc3T3Y1SzFDR0ZreFl6Y2M2WnJwTU9OcGFWMXpWZ2tXS2VBcTZSaGd2SzdPQmhjaUU5alE?oc=5&hl=en-US&gl=US&ceid=US:en,"Caffeine can be part of a healthy diet for most people, but too much may pose a danger to your health. Four physicians share what to keep in mind. The AMA Update covers a range of health care topics affecting the lives of physicians and patients. Learn more about cervical cancer and at-home screening tests. Strong physician representation is needed to ensure that health AI is transparent and has the right oversight so it works for patients and doctors. Get real answers from the AMA to common myths about documenting the time spent on each specific task associated with an outpatient visit. International medical graduates (IMGs) play a critical role in U.S. health care. Learn how the AMA works to help IMGs meet the nation s health needs. Residents share the responsibility to create an effective and respectful learning environment. The AMA has advice on how to make that happen. When writing your personal statement, veteran residency program directors said that authenticity will trump AI every time. ChatGPT agrees. Medical student research experience among residency applicants is common, but not always decisive. Learn about the physician specialties where it matters most. Most physicians practice where they completed residency, but not all. Learn which specialties and states are most likely to keep you local. After gaining footing as interns, second-year residents take on a new role and increased responsibility. These tips help new PGY-2s excel. Proposed 2026 Medicare physician payment rule would redistribute pay across specialties and practice types and more in the latest Medicare Payment Reform Advocacy Update. AMA expresses strong concern over proposed rulemaking on Medicaid provider tax reforms and more in the latest National Advocacy Update. This two-day boot camp, Sept. 17-18, 2025, will equip attendees with the time-saving tools and strategies to reform their organizations and enhance professional satisfaction. ChangeMedEd  is a national conference that brings together leaders and innovators to accelerate change in medical education across the continuum. Learn more. Review the agenda and schedule of events for the 2025 HOD Interim Meeting at the Gaylord National Resort and Convention Center in National Harbor, Maryland. Read the House of Delegates (HOD) speakers' updates for the 2025 HOD Interim Meeting. Download PDFs of reports organized by year for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA Interim and Annual Meetings. Download PDFs of reports on this topic for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA interim and annual meetings. Read current and past issues of WPS Members & News Highlights for the latest information on the work being done by WPS members and its leadership. An examination of ""kinkeeping"" and gender equity, and an introduction to the 2025-26 goals, the ""5 As:"" Action, Activism, Advancement, Advocacy and Achievement. In the news: FDA panel discusses risk of SSRIs during pregnancy, how the pandemic aged our brains, eating eggs may protect against Alzheimer s and more. AMA participates in health care conferences and events held throughout the U.S.A. as well as internationally. The AMA House of Delegates uses the term augmented intelligence (AI) as a conceptualization of artificial intelligence that focuses on AI s assistive role, emphasizing that its design enhances human intelligence rather than replaces it. The AMA is committed to ensuring that AI can meet its full potential to advance clinical care and improve clinician well-being. As the number of AI-enabled health care tools continue to grow, it is critical they are designed, developed and deployed in a manner that is ethical, equitable and responsible. The use of AI in health care must be transparent to both physicians and patients. In addition to medical devices, AI is increasingly used in health care administration or to reduce physician burden, and policy and guidance for both device and non-device use of health care AI is necessary. Recognizing this, the AMA has developed new policy (PDF) that addresses the development, deployment and use of health care AI, with particular emphasis on: In 2023, the AMA conducted a comprehensive study of over 1,000 physicians  sentiments towards the use of AI in health care including current use and future motivations for use, key concerns, areas of greatest opportunity and requirements for adoption.  Given the rapidly evolving AI landscape across health care, the AMA repeated the study in late 2024 (PDF). The objectives of this research remain: Physicians largely remain enthusiastic about the potential of AI in health care, with 68% seeing at least some advantage to the use of AI in their practice, up from 65% in 2023. We also saw use of AI increase from 38% in 2023 to 66% of physicians reporting they use some type of AI tool in practice in 2024. However, there are still key concerns as physicians continue to explore how these tools will impact their practices. Implementation guidance and research, including clinical evidence, remain critical to helping physicians adopt AI tools. AMA's latest study on physician sentiments around the use of AI in heath care: motivations, opportunities, risks and use cases. AI is playing an increasingly important role at all stages of the medical education continuum, both as a tool for educators and learners and as a subject of study in and of itself. AI has the potential to transform the educational experience as a part of precision education and transform patient care as a part of precision health. Learn more about how AI can impact medical education. AMA partners with technology and health care leaders to bring physicians critical insights on AI's potential applications and ensure that physicians have a voice in shaping AI's role in medicine. The current CPT  code set drives communication across health care by enabling the seamless processing and advanced analytics for medical procedures and services. AMA offers several resources to provide guidance on the updated CPT  code set for classifying various AI applications as well as advisory expertise through the Digital Medicine Payment Advisory Group (DMPAG). DMPAG identifies barriers to digital medicine adoption and proposes comprehensive solutions on coding, payment, coverage and more. Stay up-to-date on the criteria for CPT  codes, access applications and read frequently asked questions. Learn how AI is being used in health care as the medical community s understanding of the application grows. AMA articles focus on ways coding content advances to reflect the emergence of digital health and diagnostics, and how AI should be incorporated into physician training. Stay up-to-date on information about health care AI, including the latest news, trends and AMA statements. The technological capacity exists for AI algorithms and tools to transform health care, but real challenges remain in ensuring that tools are developed, implemented and maintained responsibly in your practice. Learn more with the AMA. Explore the current landscape of AI in medicine from terminology to current and future use cases to addressing risks. Download the report now. AMA Board of Trustees is responsible for implementing AMA policy. Given the number of stakeholders and policymakers involved in the evolution of AI in health care, it is important that AMA not only adopt a base level of policy to guide engagement but equally continue to refine policy as this technology develops. AMA Board reports included here summarize the need for additional AMA policy on AI. Through the AMA PolicyFinder, users can search for current AI policy initiatives. Explore the components of AI in health care and delve into the potential challenges and opportunities for physicians. An AMA Ed Hub  article explores how AI, used ethically, has the power to serve as a transformative and powerful tool for physicians. The AMA promotes the art and science of medicine and the betterment of public health. The best in medicine, delivered to your mailbox We use third party technologies for analytics, personalization and marketing purposes. These technologies may be used by us or our partners to personalize the site or deliver relevant marketing to you on third party sites. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.",0
"Waystar to Acquire Iodine Software, Accelerating the AI-Powered Transformation of Healthcare Payments - PR Newswire",https://news.google.com/rss/articles/CBMi7AFBVV95cUxPVU0ydHhpWWNzOGV2U1AwTUdaQWdGZ1BIeHVDUDFpaUpOWXFYdUNWY0tUR0J2SkVzYkZYLUREQ2Nfdm5taGJVRVhGbW5iNHh6NDVaYWNXc3Jsald5UXJ3VUZCbUxXa19wN1lFdFM2cU5ZVFNPMllKMkhfY0UzZ0RGUW0yVFFBX1BaX2JfbEFDQ0FGUVhOTUp4OFpSeElCNDV2ZVhGQTh0OTB6Y05uczAtX29lU0NRY1hDN3Q5UHk1NWV2Z3YyZUwyMlpEejhQd1lxM0JLWWdzSS1sdmVlaXhxVDFwQktSTl82SnBnSg?oc=5&hl=en-US&gl=US&ceid=US:en,"Searching for your content... In-Language News Contact Us 888-776-0942 from 8 AM - 10 PM ET Jul 23, 2025, 16:31 ET Share this article Extends Waystar's AI leadership into clinical intelligence software, unlocking greater value for clients and shareholders Highly recurring subscription-based business projected to be accretive to Waystar's financial profile Expected to expand Waystar's total addressable market by more than 15% Conference call to be held Wednesday, July 23, 2025, at 5:30 p.m. ET LEHI, Utah and LOUISVILLE, Ky., and AUSTIN, Texas, July 23, 2025 /PRNewswire/ -- Waystar (Nasdaq: WAY), a provider of leading healthcare payment software, today announced a definitive agreement to acquire 100% of Iodine Software (""Iodine"") from shareholders led by Advent International, a leading global private equity investor, for a total enterprise value of $1.25 billion. The proposed transaction is expected to accelerate Waystar's ability to transform healthcare payments through its leading cloud-based software platform, empowering more than one million providers with advanced AI capabilities to prevent denials, reduce manual work, and improve financial performance. Building on Waystar's track record of successful M&A execution and synergy realization, Waystar expects the acquisition of Iodine to be immediately accretive to gross margin and adjusted EBITDA margin, and accretive to revenue growth and non-GAAP net income per diluted share in 2027. Iodine is trusted by many of the nation's premier health systems for its AI-powered clinical intelligence software. Up to 60 million claims are denied each year due to administrative errors in the critical stage between care delivery and submission, costing providers billions in lost revenue. This highlights the essential role of accurate clinical documentation and coding in preventing revenue leakage and underpayments. Together, Waystar and Iodine will be better positioned to help decrease the estimated $440 billion in annual administrative costs1 burdening providers. Waystar brings a decade-long track record of applying AI pervasively across its software platform to simplify healthcare payments. Iodine extends that leadership into clinical intelligence software, leveraging proprietary AI models trained on one of the industry's largest clinical datasets, representing more than a third of all U.S. inpatient discharges. ""Our mission is to simplify healthcare payments by eradicating unnecessary denied claims, automating manual work, and increasing transparency for providers and patients,"" said Matt Hawkins, Chief Executive Officer of Waystar. ""We are committed to transforming healthcare through harnessing the power of AI to tackle the most critical challenges in healthcare payments. Welcoming Iodine's talented team and clinical intelligence platform to Waystar is a terrific next step in achieving our mission."" ""We are proud to have built a market-leading AI software company in partnership with Advent, Bain Capital Ventures, and Silversmith Capital Partners, and are thrilled to join Waystar, an organization that shares our deep commitment to modernizing the revenue cycle for providers,"" said William Chan, Co-Founder and Chief Executive Officer of Iodine Software. ""From day one, our focus has been helping hospitals and health systems capture the full value of care through transformational AI. As part of Waystar, we are excited to accelerate that mission and amplify the value delivered to healthcare providers."" ""Our success has been driven by strong partnerships, continuous innovation, and meaningful outcomes,"" added Mike Kadyan, Co-Founder and Chairman of Iodine Software. ""We look forward to delivering even greater outcomes for providers as part of Waystar's market-leading platform."" ""It has been a privilege to partner alongside the Iodine team as they have built a category-defining AI-powered revenue cycle platform consistently delivering exceptional ROI to its clients,"" said Lauren Young and Carmine Petrone, Managing Directors at Advent. ""We are excited to build on that foundation together with Waystar to drive even greater impact across healthcare, empowering organizations to optimize their financial performance."" Strategic and Financial Benefits Transaction DetailsThe transaction will be funded with a 50/50 mix of cash and stock consideration. Upon closing of the transaction, current Waystar shareholders will own approximately 92% of the combined company on a fully diluted, pro forma basis and Iodine equity holders will own approximately 8%. Advent, Iodine's largest shareholder, is expected to only receive Waystar shares in connection with the transaction and will agree to be locked up for 18 months after closing. Following the transaction, Waystar expects to maintain a strong balance sheet with an estimated adjusted net leverage ratio at transaction close of approximately 3.5x. The transaction is anticipated to close by year-end 2025, subject to customary closing conditions and applicable regulatory approvals. Preliminary Second Quarter 2025 ResultsWaystar expects second quarter 2025 revenue to be approximately $271 million, representing approximately 15% year-over-year growth. The foregoing estimates are preliminary and unaudited and based on management's initial analysis of operations for the quarter. Waystar looks forward to sharing additional information regarding the company's second quarter 2025 results as previously scheduled on July 30, 2025. AdvisorsBarclays is serving as exclusive financial advisor, and Simpson Thacher & Bartlett LLP is serving as legal advisor to Waystar. J.P. Morgan Securities is serving as exclusive financial advisor, and Weil, Gotshal & Manges LLP and Queen Saenz + Schultz PLLC are serving as legal advisors to Iodine. Conference CallWaystar will discuss the transaction on a conference call today, Wednesday, July 23, 2025, at 5:30 p.m. Eastern Time. The conference call can be accessed by dialing (800) 715-9871 from the United States and Canada or (646) 307-1963 internationally and using conference code 8810133. A live audio webcast of the conference call will be available on Waystar's investor relations website at investors.waystar.com/news-events/events. Following the call, an audio replay will be archived on the site. About WaystarWaystar's mission-critical software is purpose-built to simplify healthcare payments so providers can prioritize patient care and optimize their financial performance. Waystar serves approximately 30,000 clients, representing over 1 million distinct providers, including 16 of 20 institutions on the U.S. News Best Hospitals list. Waystar's enterprise-grade platform annually processes over 6 billion healthcare payment transactions, including over $1.8 trillion in annual gross claims and spanning approximately 50% of U.S. patients. Waystar strives to transform healthcare payments so providers can focus on what matters most: their patients and communities. Discover the way forward at waystar.com. About Iodine Software Iodine Software is the leader in AI-powered clinical intelligence, built to eliminate revenue leakage, lower administrative burden, and ensure accurate reimbursement. Trusted by more than 1,000 hospitals and health systems, Iodine delivers real-time insight and automation across the mid-revenue cycle: connecting clinical documentation, utilization management, and prebill workflows from admission through claim submission. For over a decade, health systems have trusted Iodine to apply the right AI   from machine learning, deep learning, large language models, GenAI, to Agentic AI   to the right use case, consistently delivering reliable, high-impact financial results. At the core of the platform is IodineIQ, our proprietary Clinical Reasoning Knowledge Engine, featuring a robust clinical condition library and a dataset of millions of patient encounters and billions of clinical data points. IodineIQ mirrors clinical reasoning to surface opportunities, predict outcomes, and guide decisions; ensuring the patient's clinical picture is fully and accurately reflected in status, documentation, and final codes. Discover more at www.iodinesoftware.com. About AdventAdvent is a leading global private equity investor committed to working in partnership with management teams, entrepreneurs, and founders to help transform businesses. With 16 offices across five continents, we oversee more than USD $94 billion in assets under management* and have made over 430 investments across 44 countries. Since our founding in 1984, we have developed specialist market expertise across our five core sectors: business & financial services, consumer, healthcare, industrial, and technology. This approach is bolstered by our deep sub-sector knowledge, which informs every aspect of our investment strategy, from sourcing opportunities to working in partnership with management to execute value creation plans. We bring hands-on operational expertise to enhance and accelerate businesses. As one of the largest privately-owned partnerships, our 660+ colleagues leverage the full ecosystem of Advent's global resources, including our Portfolio Support Group, insights provided by industry expert Operating Partners and Operations Advisors, as well as bespoke tools to support and guide our portfolio companies as they seek to achieve their strategic goals. To learn more, visit our website or connect with us on LinkedIn. *Assets under management (AUM) as of March 31, 2025. AUM includes assets attributable to Advent advisory clients as well as employee and third-party co-investment vehicles. Forward-Looking StatementsThis press release contains forward-looking statements, within the meaning of the Private Securities Litigation Reform Act of 1995, that reflect our current views with respect to, among other things, statements regarding Waystar's expectations relating to future operating results and financial position, including full year 2025, and future periods; anticipated future investments; our industry, business strategy, goals, and deployment of artificial intelligence in our solutions, our market position, offerings, future operations, margins, and profitability. Forward-looking statements include all statements that are not historical facts. These statements may include words such as ""anticipate,"" ""assume,"" ""believe,"" ""continue,"" ""could,"" ""estimate,"" ""expect,"" ""intend,"" ""may,"" ""plan,"" ""potential,"" ""predict,"" ""project,"" ""future,"" ""will,"" ""seek,"" ""foreseeable,"" ""outlook,"" the negative version of these words or similar terms and phrases to identify forward-looking statements in this press release, including any discussion of our guidance for full fiscal year 2025. The forward-looking statements contained in this press release are based on management's current expectations and are not guarantees of future performance. The forward-looking statements are subject to various risks, uncertainties, assumptions, or changes in circumstances that are difficult to predict or quantify. Our expectations, beliefs, and projections are expressed in good faith, and we believe there is a reasonable basis for them. However, there can be no assurance that management's expectations, beliefs, and projections will result or be achieved. The following factors are among those that may cause actual results to differ materially from the forward-looking statements: our operation in a highly competitive industry; our ability to retain our existing clients and attract new clients; our ability to successfully execute on our business strategies in order to grow; our ability to accurately assess the risks related to acquisitions and successfully integrate acquired businesses (including our proposed acquisition of Iodine); our ability to establish and maintain strategic relationships; the growth and success of our clients and overall healthcare transaction volumes; consolidation in the healthcare industry; our selling cycle of variable length to secure new client agreements; our implementation cycle that is dependent on our clients' timing and resources; our dependence on our senior management team and certain key employees, and our ability to attract and retain highly skilled employees; the accuracy of the estimates and assumptions we use to determine the size of our total addressable market; our ability to develop and market new solutions, or enhance our existing solutions, to respond to technological changes, or evolving industry standards; the interoperability, connectivity, and integration of our solutions with our clients' and their vendors' networks and infrastructures; the performance and reliability of internet, mobile, and other infrastructure; the consequences if we cannot obtain, process, use, disclose, or distribute the highly regulated data we require to provide our solutions; our reliance on certain third-party vendors and providers; any errors or malfunctions in our products and solutions; failure by our clients to obtain proper permissions or provide us with accurate and appropriate information; the potential for embezzlement, identity theft, or other similar illegal behavior by our employees or vendors, and a failure of our employees or vendors to observe quality standards or adhere to environmental, social, and governance standards; our compliance with the applicable rules of the National Automated Clearing House Association and the applicable requirements of card networks; increases in card network fees and other changes to fee arrangements; the effect of payer and provider conduct which we cannot control; privacy concerns and security breaches or incidents relating to our platform; the complex and evolving laws and regulations regarding privacy, data protection, and cybersecurity; our ability to adequately protect and enforce our intellectual property rights; our ability to use or license data and integrate third-party technologies; our use of ""open source"" software; legal proceedings initiated by third parties alleging that we are infringing or otherwise violating their intellectual property rights; claims that our employees, consultants, or independent contractors have wrongfully used or disclosed confidential information of third parties; the heavily regulated industry in which we conduct business; the uncertain and evolving healthcare regulatory and political framework; health care laws and data privacy and security laws and regulations governing our processing of personal information; reduced revenues in response to changes to the healthcare regulatory landscape; legal, regulatory, and other proceedings that could result in adverse outcomes; consumer protection laws and regulations; contractual obligations requiring compliance with certain provisions of the Bank Secrecy Act and anti-money laundering laws and regulations; existing laws that regulate our ability to engage in certain marketing activities; our full compliance with website accessibility standards; any changes in our tax rates, the adoption of new tax legislation, or exposure to additional tax liabilities; limitations on our ability to use our net operating losses to offset future taxable income ; losses due to asset impairment charges; restrictive covenants in the agreements governing our credit facilities; interest rate fluctuations; unavailability of additional capital on acceptable terms or at all; the impact of general macroeconomic conditions; our history of net losses and our ability to achieve or maintain profitability; actions of certain of our significant investors, who may have different interests than the interests of other holders of our securities; and each of the other factors discussed under the heading of ""Risk Factors"" in the Company's 10-K filed with the Securities and Exchange Commission (the ""SEC"") on February 18, 2025, and in other reports filed with the SEC, all of which are available on the Investor Relations page of our website at investors.waystar.com. Any forward-looking statements made by us in this press release speak only as of the date of this press release and are expressly qualified in their entirety by the cautionary statements included in this press release. Factors or events that could cause our actual results to differ may emerge from time to time, and it is not possible for us to predict all of them. You should not place undue reliance on our forward-looking statements. We undertake no obligation to publicly update or review any forward-looking statement, whether as a result of new information, future developments, or otherwise, except as may be required by any applicable securities laws. Waystar Media ContactKristin Leekristin.lee@waystar.com Daniel Yunger / Nick Capuano / Mark FallatiKekst CNCkekst-waystar@kekstcnc.com Waystar Investor Contactinvestors@waystar.com Iodine Software Media ContactMichelle Whitemjwhite@iodinesoftware.com Isabella MorrealeSolCommsisabella@solcomms.com 1 CAQH Index Report 2024 SOURCE Waystar Waystar Holding Corp. (Nasdaq: WAY), a provider of leading healthcare payment software, announced today that it will report financial results for the ... Waystar (Nasdaq: WAY), a provider of leading healthcare payment software, today announced the results of a commissioned study conducted by Forrester... Financial Technology Financial Technology Banking & Financial Services Artificial Intelligence Do not sell or share my personal information:",2
AI in Healthcare: Trust and Accountability - Hastings Center,https://news.google.com/rss/articles/CBMinwFBVV95cUxQVlBrNkVqQXJQS0FtaG1kdmpQZmw2MWNtWm0xUC0xS0ZpSlNINTFWVlhUYm5CY1RBLU5EZERVb0huZExTXzFqY2s3REhRQk5wTWMyVmJOVFRjdFpBaFlkay1MNG54UnV3bi12STlaaGF0bEl3azVWOThRblR5ckg4SkJCZTcyUHNXNEtLLUZhTi1PdGRQeW9UdWRvT29mQ1U?oc=5&hl=en-US&gl=US&ceid=US:en,"All Events Cedars-Sinai and The Hastings Center are proud to jointly present this timely conference on AI in healthcare. Through discussions with leading experts in the field, the event will address evolving ethical, societal, and legal issues raised by artificial intelligence in medicine and biomedical research. For more information, see the Agenda page. This is an in-person only event. Venue Directions Cedars- Sinai Medical Center 8700 Beverly Blvd., Los Angeles, CA 90048 The conference will be held at Cedars- Sinai Medical Center in the Harvey Morse Auditorium, located in the South Tower. Valet Parking is available curbside at the South Tower. Self Parking is available in the Pavilion (P4) lot located on Sherbourne Drive. After checking in with security at the South Tower entrance, take the elevator to the Plaza Level, where you will find the Harvey Morse Auditorium. Registration is required. Your email address will not be published. Required fields are marked * Comment * Name * Email * Website Save my name, email, and website in this browser for the next time I comment.   Registered 501(c)(3). EIN: 13-2662222This site is protected by reCAPTCHA and the Google Privacy Policy and Terms of Service apply.",2
A critical look into artificial intelligence and healthcare disparities - Frontiers,https://news.google.com/rss/articles/CBMiogFBVV95cUxOQTVZT2hlWXZnX0Nvak11RUJhelFOTm93Wl9BZVFoNUlDM1Z0V19JNnVQWV9KRnlVWEllSU8tV0tzUkhaUW13ZkRwOXNFMDZpUEV4VFdyMm9pRDNkdkl1ak5ZWGVpdG9PcVMzcHE1MEpsY3NnOTNhaTA3aUZBQTMzLVdnNWtpY1FROXhUN0daaXFFTzVGSWVCYWNTVC1HOG5BaEE?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better OPINION article Front. Artif. Intell., 06 March 2025 Sec. Medicine and Public Health Volume 8 - 2025 | https://doi.org/10.3389/frai.2025.1545869 Artificial intelligence (AI) has permeated many aspects of daily life, including medicine, in recent years. As of 2021, 343 AI-enabled medical devices had been approved by the United States (US) Food and Drug Administration, with many more in development (Badal et al., 2023). Most notable thus far has been AI's ability to assist with every step of radiology workflow: it can determine the appropriateness of imaging, recommend the most appropriate imaging exam, predict wait times or appointment delays, and interpret imaging, with much more potential utilizations (Syed and Zoga, 2018). The World Health Organization proposed that AI tools be integrated into healthcare to improve efficiency and achieve sustainable health-related development (World Health Organization, 2021). AI in healthcare can reduce costs and administrative burdens, reduce waiting times for patients to receive care, improve diagnostic abilities and patient care, facilitate data management, and expedite discovery (Botha et al., 2024a,b). However, the advancement of AI in healthcare comes with unique drawbacks. For example, data security and privacy are at risk and must be improved, as patients may more readily and unknowingly provide consent for covert data collection methods (Khan et al., 2023; He et al., 2019). Use of AI must be seriously reconsidered if it poses a risk to patient confidentiality, a non-negotiable in healthcare. With the ability of AI to rapidly gather and analyze large amounts of patient data, controlling the scope of its use becomes a challenge: these tools may progress to collect and disclose data without patient consent or direct investigator oversight (Botha et al., 2024a). In addition, as most healthcare-based AI research has been conducted in non-clinical settings, rolling out AI in certain clinical settings may result in non-evidence-based practice (Khan et al., 2023). For example, clinicians may feel tempted to use AI for tasks beyond their validation, and training data may not adequately represent the scenarios clinicians encounter (Nilsen et al., 2024). In fact, many studies on AI in healthcare have been administered in non-clinical settings (Botha et al., 2024a). That is not to say AI should not be used in healthcare. It does, however, require immense consideration in how it is designed and why it is utilized. Some have contended that a goal of developing AI for healthcare should be to minimize health disparities and make the healthcare system more equitable (Badal et al., 2023; Campbell et al., 2021). Yet, many characteristics of AI make this goal difficult to achieve. As such, there is a growing body of literature that discusses AI's role in both closing and perpetuating these inequalities (Celi et al., 2022; d'Elia et al., 2022; Ali et al., 2023). As the ability of AI is directly proportional to the quality of the training sets used, authors have addressed concerns regarding bias in training datasets and lack of diversity in development teams ultimately resulting in AI-driven disparities in care (Botha et al., 2024a; Green et al., 2024; Ferrara, 2024; Haider et al., 2024). This article draws from existing literature to add to the ongoing conversation about the implications of AI in healthcare disparities. Specifically, we discuss economic implications, the explainability of AI systems, and the importance of compassionate care. Ultimately, while AI may indeed confer benefits to the healthcare system, it remains far from the goal of closing healthcare disparities and may, instead, backfire. One essential consideration in any kind of social disparity is economics. The US is notorious for having the highest healthcare expenditure globally, with healthcare costing $3.5 trillion, or 17.9% of the Gross Domestic Product (Khanna et al., 2022). Any measure to decrease this economic burden either in the US or internationally may be attractive. AI has the potential to save billions in annual healthcare costs (Zhu et al., 2024). AI may greatly streamline workflow, even in non-clinical tasks. An automated system may alleviate administrative burdens such as scheduling patients, estimating wait times, and billing insurance companies (Syed and Zoga, 2018; Zhu et al., 2024; Knight et al., 2023). Such workflow optimization may reduce the cost of healthcare delivery by cutting out intermediaries that typically handle these mundane tasks. In turn, patients' financial responsibility related to their care may be reduced. On the clinical side, AI may be used to screen for and diagnose conditions, stratify disease risk, and devise treatment plans (Khanna et al., 2022). It may significantly reduce medical errors and factors that are associated with adverse outcomes (Botha et al., 2024a). Eventually, as technology advances, it may even perform procedures, given that it is deemed ethical, safe and evidence-based. While these benefits may seem like simply a perk to those practicing in physician-rich areas, they could become indispensable to those in areas affected by shortages of medical professionals (Lamem et al., 2025). Urban and rural communities bear the brunt of this inequity, with many struggling to access both primary and specialty care (Kirch and Petelle, 2017). It has been estimated that by 2030, there may be a shortage of up to 104,900 physicians in the US (Kirch and Petelle, 2017). As such, AI implementation in these underserved populations may help to alleviate these challenges and improve disparities regarding access to care (Lamem et al., 2025). Furthermore, AI assistants may help decrease physician burnout and therefore improve quality of care (Lin et al., 2019). These advantages of AI are conferred only with the proper development, installation and maintenance of these systems. AI requires immense investment. One model for an AI glaucoma screening tool in the Changjiang county in China estimated that the fifteen-year accumulated incremental cost of using this tool was $434,903.20 for ~2,000 patients (Xiao et al., 2021). While the costs of this screening tool are arguably worth early detection and reduced disease progression, it may be impractical to roll out to larger populations. Health institutions in wealthy countries may easily make this investment. But what about institutions in developing countries? Community hospitals with limited government funding? Practices in rural areas with less purchasing power? Even if analyses demonstrate that costs are saved in the long run, the upfront investment may be too large an obstacle (Khanna et al., 2022). Once a system is developed, purchased, and installed, maintenance becomes another issue. Software updates, advanced computing technologies, and ever-increasing cloud storage requirements add to costs (Badal et al., 2023). The evolving cybersecurity needs to protect patient health information may create further barriers to widespread application of AI (Shah et al., 2023). These all-around cost barriers are more nuanced than the mere ability to implement AI in practice. Inevitably, there are AI algorithms with higher and lower levels of sophistication, infrastructures that are more and less robust, and security measures that are stronger and weaker. The AI system that institutions choose will be closely tied to their financial status. Of course, AI development will then leave behind under-resourced communities. Currently, there is a lack of  explainable  AI regarding algorithms and data sets that play a role in decision making (Amann et al., 2020). In other words exactly how do these technologies work? How are they making these decisions? These are questions that even developers themselves cannot answer; we know they work, yet nobody can fully explain how. This  black box  of AI holds important implications to healthcare disparities worldwide. Machine learning (ML) is a component of AI which involves automated decision making based on datasets (Tack, 2019). Detecting and correcting biases based on limited training sets is an ethical prerequisite of justice in AI- and ML-based clinical decision-making (Kirch and Petelle, 2017). In other words, explainable AI enables developers to identify and correct training set-based biases that currently skew algorithms (Celi et al., 2022; Green et al., 2024). The discussion of justice behind explainable AI requires additional considerations. Explainable AI models keep model developers accountable for their work, as lack of accountability precedes error (Amann et al., 2020). This concern is compounded by the fact that patients who are less literate are less likely to ask questions or seek more information about their care (Katz et al., 2007). Since these patients may be less prepared to participate in shared decision making, they may not challenge questionable decisions (Keij et al., 2021). AI should be treated as a tool to support decision-making, not one to make decisions independently. For example, AI prescription systems have been developed to aid physician workflow and prevent human error (Tantray et al., 2024; Tully, 2012). Inevitably, physicians will encounter scenarios in which the AI recommendation conflicts with their clinical judgement. Some of these scenarios may arise if AI systems are not trained on datasets that adequately represent the populations they treat, thereby generating recommendations poorly aligned with the realities of patients' needs (Botha et al., 2024a). This challenge is particularly relevant to minority communities that have historically been under-studied (Haider et al., 2024). Healthcare providers should critically assess the AI recommendation in the context of their clinical experience and patient preferences. Institutions should establish clear policies on how to accept or reject AI suggestions to maintain quality patient care. Justice in explainable AI systems is important also in that more transparent technologies will foster patient trust in providers and the healthcare system. Unexplainable, opaque models, on the other hand, may exacerbate the mistrust that already pervades the healthcare system. This mistrust is particularly prevalent in socially and economically marginalized communities (Jaiswal and Halkitis, 2019). A key component of trust in underprivileged populations is the patient's comfort with the physician and physicians' personal involvement in patient care (Gopichandran and Chetlapalli, 2013). As such, we may see that the unexplainable black box of AI and ML   if not handled correctly   would certainly exacerbate these concerns. Lack of explanation for these impersonal, automated algorithms may further alienate this vulnerable population and widen health disparities. Even if we are to elucidate the black box, can AI ever replace the physician-patient relationship in delivering empathic care? Currently, it seems unlikely   one recent study demonstrated that healthcare chatbots delivering both empathetic and sympathetic responses to patients in fact lowered patients' perception of their authenticity (Seitz, 2024). In contrast, empathy and sympathy expressed by human physicians did not induce this negative effect (Seitz, 2024). This lack of perceived authenticity may not only undermine patients' subjective satisfaction with their AI providers but may also objectively worsen patient outcomes. While some AI tools provided sound biomedical recommendations for diabetes management, they overlooked psychosocial components that are also necessary for glycemic control (Romero-Brufau et al., 2020). Algorithms that determine A1c goals, calculate medication dosages, and send prescriptions may certainly help optimize patient care. However, recommendations poorly tailored to psychosocial challenges disproportionately affect those with greater social barriers. Continuing the stand-alone case of diabetes, for example, significant social barriers to care include having the ability to afford healthy food, the free time for follow-up visits and the literacy to understand health information (Paduch et al., 2017). Now combine this diabetes with a slew of other health conditions, medications, unemployment concerns and an ailing family member. Surely, physicians can manage this patient in countless different ways. There is no one correct path. Regardless, it is imperative that health providers human or AI-based address these concerns with compassion. Palliative care, which emphasizes relieving suffering and optimizing quality of life in end-of-life care, is a field in which compassion is key (Adegbesan et al., 2024). While AI may help assist in decision-making, it risks depersonalizing cases and lacking empathy when patients and their families need it the most. Death and dying are often rooted in culture, personal beliefs, and spirituality. The experience is deeply personal and unique to each family (Adegbesan et al., 2024). Whereas some encourage open communication about death, others feel uncomfortable with it; whereas some value life-prolonging measures regardless of prognosis, others less so (Ohr et al., 2017). Palliative care AI models risk imposing a  one-size-fits-all  model of care based on a Western training dataset (Adegbesan et al., 2024). Once again, understudied populations and cultural minorities fall behind in AI's  understanding or lack thereof of their values. Society at large, including regulators, policy makers, insurance companies, healthcare professionals and patients should carefully consider incorporating AI practices into the practice and business of medicine. Regulators have raised concerns over the need for regulation of clinical AI as well as generalizability to different populations (Hogg et al., 2023). Another area of concern relevant to several stakeholders including healthcare providers and regulators is legal responsibility for AI clinical decision making (Hogg et al., 2023). This fear exists for physicians in the scenario of a medical error made by AI and conversely for accusations of negligent care for not using AI. Physicians were neither prepared for nor agreed with assuming responsibility for errors made by AI, while AI developers believed they should not be liable since they do not practice medicine (Hogg et al., 2023). Each side felt they only understood  part of the whole  when it comes to AI, further highlighting the need for explainable AI. Appropriate oversight by policy makers and regulators is needed to ensure accountability and promote development of explainable AI. These risks may be further mitigated by informing patients that AI was involved in decision making (Lorenzini et al., 2023). Certain narratives have pitted AI as a rival to the skills and education of physicians, with claims that AI will one day replace physicians (Lorenzini et al., 2023). AI remains solely a tool to assist in clinical setting with the final decision being made by a human. Rhetoric that continues to pit AI against physicians will only hinder the incorporation of AI into clinical practice (Lorenzini et al., 2023). Patients will not benefit from AI replacing their physicians, but they also will not benefit from avoiding AI altogether. In discussing the role of AI in closing healthcare disparities, we must consider the role of AI in low- and middle-income countries (LMICs). In areas where medical resources and personnel are scarce, AI can reduce the workload on healthcare personnel (Schwalbe and Wahl, 2020). AI can also improve access to medical care, especially for areas where specialty care is not available (L pez et al., 2022). Disease outbreaks can be predicted earlier and allow for mobilization of treatment to affected areas. ML has been used to assess disease severity and predict treatment failure for illnesses such as malaria, tuberculosis and dengue fever (Schwalbe and Wahl, 2020). However, LMICs face significant challenges in implementing AI. The lack of electronic health records and health data is a limiting factor since this data is the primary input used in AI algorithms (L pez et al., 2022). Most AI systems are developed in high income countries (HICs) and the ML models reflect datasets from those populations. When applying these technologies to LMICs, models must be updated to reflect the population it is being applied to. Failure to do so can reinforce and exacerbate existing health disparities (L pez et al., 2022). Gaining both physician and patient trust in the integration of AI in healthcare remains a problem that needs to be addressed in the future. In a small study interviewing patients on their perspectives on AI in general medical practice, subjects had mixed feelings on implementation of AI (Mikkelsen et al., 2023). A common concern amongst participants related to sharing of and access to their medical information (Mikkelsen et al., 2023). The patients wanted assurance that appropriate consent would be obtained prior to sharing of their data and that anonymization would be used. A survey of 203 participants on public opinion of AI in medicine also yielded mixed results, with a near 50/50 split when asked if they trust AI as a physician's tool for diagnosing medical conditions (Rojahn et al., 2023). In the same study, a majority of participants trusted a human physician over AI in making a culturally biased decision. There was a more positive outlook toward the future as over 25% of respondents believe AI will improve medical treatment in the next 10 years and nearly half of respondents for the next 50 years (Rojahn et al., 2023). Similarly, what AI lacks that physicians have is not intelligence, but rather wisdom the sense of intuition that a human being can accumulate only over time (Powell, 2019). Can AI develop this intuition over time? Can an AI model mimic the human brain in synthesizing decades' worth of information to analyze a unique case and provide appropriate medical decision making? For simple cases, it likely can. But complex cases are a different story risks and benefits of intervention must be weighed and complications must be predicted, all while delivering this information to the patient in an easily-digestible manner. Yet another layer of nuance is added when shared-decision making is introduced now, AI must understand patients' desires and uncertainties on a human level and incorporate that into its recommendations. Moreover, patients believe their physician should remain the primary decision maker, with AI can be used as a support tool (Mikkelsen et al., 2023). The use of AI in this setting may decrease time physicians spend on mundane tasks and leave more time for physicians to have meaningful conversations with their patients, facilitating the delivery of compassionate care (Hogg et al., 2023; Sauerbrei et al., 2023). Once again, compassion and trust are key components to patient care for all patients alike. While AI has the potential to increase access to care for vulnerable populations and help bridge gaps in healthcare, we must ensure that data and algorithms are inclusive of patients to avoid worsening existing disparities. The healthcare system hinges on trust to maintain patient confidentiality, recommend the optimal course of action, and execute the plan appropriately. Particularly in marginalized communities, the critical process of building and maintaining this trust has proved difficult even in the absence of AI and continues to pose a significant obstacle in the success of AI to improve healthcare delivery. Both physicians and patients alike do not wish to see AI replace the standard physician-patient interaction. Instead, AI can serve as an adjunct to improve quality of care by reducing the chance of human error. Collaboration among patients, physicians, and AI developers is essential to achieve this goal in an equitable manner. DL: Conceptualization, Investigation, Methodology, Writing   original draft, Writing   review & editing. SP: Conceptualization, Supervision, Writing   review & editing. AC: Conceptualization, Supervision, Writing   review & editing. The author(s) declare that no financial support was received for the research, authorship, and/or publication of this article. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The author(s) declare that no Gen AI was used in the creation of this manuscript. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. Adegbesan, A., Akingbola, A., Ojo, O., Jessica, O. U., Alao, U. H., Shagaya, U., et al. (2024). Ethical challenges in the integration of artificial intelligence in palliative care. J. Med. Surg. Public Health 4:100158. doi: 10.1016/j.glmedi.2024.100158 Crossref Full Text | Google Scholar Ali, M. R., Lawson, C. A., Wood, A. M., and Khunti, K. (2023). Addressing ethnic and global health inequalities in the era of artificial intelligence healthcare models: a call for responsible implementation. J. R. Soc. Med. 116, 260 262. doi: 10.1177/01410768231187734 PubMed Abstract | Crossref Full Text | Google Scholar Amann, J., Blasimme, A., Vayena, E., Frey, D., and Madai, V. I. (2020). The precise Qc. Explainability for artificial intelligence in healthcare: a multidisciplinary perspective. BMC Med. Inform. Decis. Mak. 20:310. doi: 10.1186/s12911-020-01332-6 PubMed Abstract | Crossref Full Text | Google Scholar Badal, K., Lee, C. M., and Esserman, L. J. (2023). Guiding principles for the responsible development of artificial intelligence tools for healthcare. Commun. Med. 3:47. doi: 10.1038/s43856-023-00279-9 PubMed Abstract | Crossref Full Text | Google Scholar Botha, N. N., Ansah, E. W., Segbedzi, C. E., Dumahasi, V. K., Maneen, S., Kodom, R. V., et al. (2024a). Artificial intelligent tools: evidence-mapping on the perceived positive effects on patient-care and confidentiality. BMC Digit. Health 2:33. doi: 10.1186/s44247-024-00091-y Crossref Full Text | Google Scholar Botha, N. N., Segbedzi, C. E., Dumahasi, V. K., Maneen, S., Kodom, R. V., Tsedze, I. S., et al. (2024b). Artificial intelligence in healthcare: a scoping review of perceived threats to patient rights and safety. Arch. Public Health 82:188. doi: 10.1186/s13690-024-01414-1 PubMed Abstract | Crossref Full Text | Google Scholar Campbell, J. P., Mathenge, C., Cherwek, H., Balaskas, K., Pasquale, L. R., Keane, P. A., et al. (2021). Artificial intelligence to reduce ocular health disparities: moving from concept to implementation. Transl. Vis. Sci. Technol. 10 :19. doi: 10.1167/tvst.10.3.19 PubMed Abstract | Crossref Full Text | Google Scholar Celi, L. A., Cellini, J., Charpignon, M-. L., Dee, E. C., Dernoncourt, F., Eber, R., et al. (2022). Sources of bias in artificial intelligence that perpetuate healthcare disparities A global review. PLOS Digit. Health 1:e0000022. doi: 10.1371/journal.pdig.0000022 PubMed Abstract | Crossref Full Text | Google Scholar d'Elia, A., Gabbay, M., Rodgers, S., Kierans, C., Jones, E., Durrani, I., et al. (2022). Artificial intelligence and health inequities in primary care: a systematic scoping review and framework. Fam. Med. Community Health. 10:1670. doi: 10.1136/fmch-2022-001670 PubMed Abstract | Crossref Full Text | Google Scholar Ferrara, E. (2024). Fairness and bias in artificial intelligence: a brief survey of sources, impacts, and mitigation strategies. Sci. 6:3. doi: 10.3390/sci6010003 Crossref Full Text | Google Scholar Gopichandran, V., and Chetlapalli, S. K. (2013). Dimensions and determinants of trust in health care in resource poor settings a qualitative exploration. PLoS ONE 8:e69170. doi: 10.1371/journal.pone.0069170 PubMed Abstract | Crossref Full Text | Google Scholar Green, B. L., Murphy, A., and Robinson, E. (2024). Accelerating health disparities research with artificial intelligence. Front. Digit. Health 6:1330160. doi: 10.3389/fdgth.2024.1330160 PubMed Abstract | Crossref Full Text | Google Scholar Haider, S. A., Borna, S., Gomez-Cabello, C. A., Pressman, S. M., Haider, C. R., Forte, A. J., et al. (2024). The algorithmic divide: a systematic review on ai-driven racial disparities in healthcare. J. Racial Ethn. Health Disparit. doi: 10.1007/s40615-024-02237-0. [Epub ahead of print]. PubMed Abstract | Crossref Full Text | Google Scholar He, J., Baxter, S. L., Xu, J., Xu, J., Zhou, X., Zhang, K., et al. (2019). The practical implementation of artificial intelligence technologies in medicine. Nat. Med. 25, 30 36. doi: 10.1038/s41591-018-0307-0 PubMed Abstract | Crossref Full Text | Google Scholar Hogg, H. D. J., Al-Zubaidy, M., Talks, J., Denniston, A. K., Kelly, C. J., Malawana, J., et al. (2023). Stakeholder perspectives of clinical artificial intelligence implementation: systematic review of qualitative evidence. J. Med. Internet Res. 25:e39742. doi: 10.2196/39742 PubMed Abstract | Crossref Full Text | Google Scholar Jaiswal, J., and Halkitis, P. N. (2019). Towards a more inclusive and dynamic understanding of medical mistrust informed by science. Behav. Med. 45, 79 85. doi: 10.1080/08964289.2019.1619511 PubMed Abstract | Crossref Full Text | Google Scholar Katz, M. G., Jacobson, T. A., Veledar, E., and Kripalani, S. (2007). Patient literacy and question-asking behavior during the medical encounter: a mixed-methods analysis. J. Gen. Intern. Med. 22, 782 786. doi: 10.1007/s11606-007-0184-6 PubMed Abstract | Crossref Full Text | Google Scholar Keij, S. M., van Duijn-Bakker, N., Stiggelbout, A. M., and Pieterse, A. H. (2021). What makes a patient ready for Shared Decision Making? A qualitative study. Patient Educ. Couns. 104, 571 577. doi: 10.1016/j.pec.2020.08.031 PubMed Abstract | Crossref Full Text | Google Scholar Khan, B., Fatima, H., Qureshi, A., Kumar, S., Hanan, A., Hussain, J., et al. (2023). Drawbacks of artificial intelligence and their potential solutions in the healthcare sector. Biomed. Mater. Dev. 1, 731 738. doi: 10.1007/s44174-023-00063-2 PubMed Abstract | Crossref Full Text | Google Scholar Khanna, N. N., Maindarkar, M. A., Viswanathan, V., Fernandes, J. F. E., Paul, S., Bhagawati, M., et al. (2022). Economics of artificial intelligence in healthcare: diagnosis vs. treatment. Healthcare 10:2493. doi: 10.3390/healthcare10122493 PubMed Abstract | Crossref Full Text | Google Scholar Kirch, D. G., and Petelle, K. (2017). Addressing the physician shortage: the peril of ignoring demography. JAMA. 317, 1947 1948. doi: 10.1001/jama.2017.2714 PubMed Abstract | Crossref Full Text | Google Scholar Knight, D. R. T., Aakre, C. A., Anstine, C. V., Munipalli, B., Biazar, P., Mitri, G., et al. (2023). Artificial intelligence for patient scheduling in the real-world health care setting: a metanarrative review. Health Policy Technol. 12:100824. doi: 10.1016/j.hlpt.2023.100824 Crossref Full Text | Google Scholar Lamem, M. F. H., Sahid, M. I., and Ahmed, A. (2025). Artificial intelligence for access to primary healthcare in rural settings. J. Med. Surg. Public Health 5:100173. doi: 10.1016/j.glmedi.2024.100173 Crossref Full Text | Google Scholar Lin, S. Y., Mahoney, M. R., and Sinsky, C. A. (2019). Ten ways artificial intelligence will transform primary care. J. Gen. Intern. Med. 34, 1626 1630. doi: 10.1007/s11606-019-05035-1 PubMed Abstract | Crossref Full Text | Google Scholar L pez, D. M., Rico-Olarte, C., Blobel, B., and Hullin, C. (2022). Challenges and solutions for transforming health ecosystems in low- and middle-income countries through artificial intelligence. Front. Med. 9:958097. doi: 10.3389/fmed.2022.958097 PubMed Abstract | Crossref Full Text | Google Scholar Lorenzini, G., Arbelaez Ossa, L., Shaw, D. M., and Elger, B. S. (2023). Artificial intelligence and the doctor-patient relationship expanding the paradigm of shared decision making. Bioethics 37, 424 429. doi: 10.1111/bioe.13158 PubMed Abstract | Crossref Full Text | Google Scholar Mikkelsen, J. G., S rensen, N. L., Merrild, C. H., Jensen, M. B., and Thomsen, J. L. (2023). Patient perspectives on data sharing regarding implementing and using artificial intelligence in general practice - a qualitative study. BMC Health Serv. Res. 23:335. doi: 10.1186/s12913-023-09324-8 PubMed Abstract | Crossref Full Text | Google Scholar Nilsen, P., Sundemo, D., Heintz, F., Neher, M., Nygren, J., Svedberg, P., et al. (2024). Towards evidence-based practice 2.0: leveraging artificial intelligence in healthcare. Front. Health Serv. 4:1368030. doi: 10.3389/frhs.2024.1368030 PubMed Abstract | Crossref Full Text | Google Scholar Ohr, S., Jeong, S., and Saul, P. (2017). Cultural and religious beliefs and values, and their impact on preferences for end-of-life care among four ethnic groups of community-dwelling older persons. J. Clin. Nurs. 26, 1681 1689. doi: 10.1111/jocn.13572 PubMed Abstract | Crossref Full Text | Google Scholar Paduch, A., Kuske, S., Schiereck, T., Droste, S., Loerbroks, A., S rensen, M., et al. (2017). Psychosocial barriers to healthcare use among individuals with diabetes mellitus: a systematic review. Prim. Care Diabetes. 11, 495 514. doi: 10.1016/j.pcd.2017.07.009 PubMed Abstract | Crossref Full Text | Google Scholar Powell, J. (2019). Trust Me, I'm a Chatbot: How Artificial Intelligence in Health Care Fails the Turing Test. J. Med. Internet Res. 21, e16222. doi: 10.2196/16222 PubMed Abstract | Crossref Full Text | Google Scholar Rojahn, J., Palu, A., Skiena, S., and Jones, J. J. (2023). American public opinion on artificial intelligence in healthcare. PLoS ONE 18:e0294028. doi: 10.1371/journal.pone.0294028 PubMed Abstract | Crossref Full Text | Google Scholar Romero-Brufau, S., Wyatt, K. D., Boyum, P., Mickelson, M., Moore, M., Cognetta-Rieke, C. A., et al. (2020). lesson in implementation: a pre-post study of providers' experience with artificial intelligence-based clinical decision support. Int. J. Med. Inform. 137:104072. doi: 10.1016/j.ijmedinf.2019.104072 PubMed Abstract | Crossref Full Text | Google Scholar Sauerbrei, A., Kerasidou, A., Lucivero, F., and Hallowell, N. (2023). The impact of artificial intelligence on the person-centred, doctor-patient relationship: some problems and solutions. BMC Med. Inform. Decis. Mak. 23:73. doi: 10.1186/s12911-023-02162-y PubMed Abstract | Crossref Full Text | Google Scholar Schwalbe, N., and Wahl, B. (2020). Artificial intelligence and the future of global health. Lancet 395, 1579 1586. doi: 10.1016/S0140-6736(20)30226-9 PubMed Abstract | Crossref Full Text | Google Scholar Seitz, L. (2024). Artificial empathy in healthcare chatbots: Does it feel authentic? Comp. Human Behav. 2:100067. doi: 10.1016/j.chbah.2024.100067 Crossref Full Text | Google Scholar Shah, C., Nachand, D., Wald, C., and Chen, P-. H. (2023). Keeping patient data secure in the age of radiology artificial intelligence: cybersecurity considerations and future directions. J. Am. College Radiol. 20, 828 35. doi: 10.1016/j.jacr.2023.06.023 PubMed Abstract | Crossref Full Text | Google Scholar Syed, A. B., and Zoga, A. C. (2018). Artificial intelligence in radiology: current technology and future directions. Semin. Musculoskelet. Radiol. 22, 540 545. doi: 10.1055/s-0038-1673383 PubMed Abstract | Crossref Full Text | Google Scholar Tack, C. (2019). Artificial intelligence and machine learning | applications in musculoskeletal physiotherapy. Musculoskelet Sci Pract. 39, 164 169. doi: 10.1016/j.msksp.2018.11.012 PubMed Abstract | Crossref Full Text | Google Scholar Tantray, J., Patel, A., Wani, S. N., Kosey, S., and Prajapati, B. G. (2024). Prescription precision: a comprehensive review of intelligent prescription systems. Curr. Pharm. Des. 30, 2671 2684. doi: 10.2174/0113816128321623240719104337 PubMed Abstract | Crossref Full Text | Google Scholar Tully, M. P. (2012). Prescribing errors in hospital practice. Br. J. Clin. Pharmacol. 74, 668 675. doi: 10.1111/j.1365-2125.2012.04313.x PubMed Abstract | Crossref Full Text | Google Scholar World Health Organization (2021). The Importance of Ethics in Artificial Intelligence. Geneva: World Health Organization. Google Scholar Xiao, X., Xue, L., Ye, L., Li, H., and He, Y. (2021). Health care cost and benefits of artificial intelligence-assisted population-based glaucoma screening for the elderly in remote areas of China: a cost-offset analysis. BMC Public Health 21:1065. doi: 10.1186/s12889-021-11097-w PubMed Abstract | Crossref Full Text | Google Scholar Zhu, C., Attaluri, P. K., Wirth, P. J., Shaffrey, E. C., Friedrich, J. B., Rao, V. K., et al. (2024). Current applications of artificial intelligence in billing practices and clinical plastic surgery. Plast Reconstr. Surg. Glob. Open. 12:e5939. doi: 10.1097/GOX.0000000000005939 PubMed Abstract | Crossref Full Text | Google Scholar Keywords: artificial intelligence, ethics, social determinants of health, SDH, health inequalities, technology in healthcare, social disparities in health Citation: Li DM, Parikh S and Costa A (2025) A critical look into artificial intelligence and healthcare disparities. Front. Artif. Intell. 8:1545869. doi: 10.3389/frai.2025.1545869 Received: 16 December 2024; Accepted: 21 February 2025; Published: 06 March 2025. Edited by: Reviewed by: Copyright   2025 Li, Parikh and Costa. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Ana Costa, ana.costa@stonybrookmedicine.edu Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on",2
"To implement health AI, first decide who’s accountable - American Medical Association",https://news.google.com/rss/articles/CBMirwFBVV95cUxNdUNQNEd3REFWNVRZRVlxNzNzdVN4bFpOR2ZZWWlFclpGZENVY2RSUkdnR1VNTk50VTM3MmhPNnotM3ZBTmFCMzU0Y0lYT0xpdGR2ZUEyX3NPQ0p5bzYxVWpLNzlhQVVYSDhZNTF2T3dZLTExbDdfT3U1dzFMeGhYc19JTUl5ZXJLdW5pa1B3Vkh3c1p6RVdMcmNZak9KaDJwZXppMVVZejhSSTFYNEVr?oc=5&hl=en-US&gl=US&ceid=US:en,"Caffeine can be part of a healthy diet for most people, but too much may pose a danger to your health. Four physicians share what to keep in mind. The AMA Update covers a range of health care topics affecting the lives of physicians and patients. Learn more about cervical cancer and at-home screening tests. Strong physician representation is needed to ensure that health AI is transparent and has the right oversight so it works for patients and doctors. Get real answers from the AMA to common myths about documenting the time spent on each specific task associated with an outpatient visit. International medical graduates (IMGs) play a critical role in U.S. health care. Learn how the AMA works to help IMGs meet the nation s health needs. Residents share the responsibility to create an effective and respectful learning environment. The AMA has advice on how to make that happen. When writing your personal statement, veteran residency program directors said that authenticity will trump AI every time. ChatGPT agrees. Medical student research experience among residency applicants is common, but not always decisive. Learn about the physician specialties where it matters most. Most physicians practice where they completed residency, but not all. Learn which specialties and states are most likely to keep you local. After gaining footing as interns, second-year residents take on a new role and increased responsibility. These tips help new PGY-2s excel. Proposed 2026 Medicare physician payment rule would redistribute pay across specialties and practice types and more in the latest Medicare Payment Reform Advocacy Update. AMA expresses strong concern over proposed rulemaking on Medicaid provider tax reforms and more in the latest National Advocacy Update. This two-day boot camp, Sept. 17-18, 2025, will equip attendees with the time-saving tools and strategies to reform their organizations and enhance professional satisfaction. ChangeMedEd  is a national conference that brings together leaders and innovators to accelerate change in medical education across the continuum. Learn more. Review the agenda and schedule of events for the 2025 HOD Interim Meeting at the Gaylord National Resort and Convention Center in National Harbor, Maryland. Read the House of Delegates (HOD) speakers' updates for the 2025 HOD Interim Meeting. Download PDFs of reports organized by year for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA Interim and Annual Meetings. Download PDFs of reports on this topic for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA interim and annual meetings. Read current and past issues of WPS Members & News Highlights for the latest information on the work being done by WPS members and its leadership. An examination of ""kinkeeping"" and gender equity, and an introduction to the 2025-26 goals, the ""5 As:"" Action, Activism, Advancement, Advocacy and Achievement. In the news: FDA panel discusses risk of SSRIs during pregnancy, how the pandemic aged our brains, eating eggs may protect against Alzheimer s and more. AMA participates in health care conferences and events held throughout the U.S.A. as well as internationally. Health AI is not risk-free, which makes health system governance on powerful tech critical. Find out where to start. Jul 24, 2025 Augmented intelligence (AI) commonly called artificial intelligence is rapidly affecting health care. In 2024, the number of physicians using AI tools nearly doubled, from 38% to 68%, and as their use grows so does the imperative to use AI responsibly, safely and effectively. The AMA defines AI as augmented intelligence to emphasize that AI s role is to help health care professionals, not replace them. See our real-world impact on issues critical to patients and physicians.  Clinical decision-making must still lie with clinicians,  according to Margaret Lozovatsky, MD, who is chief medical information officer and vice president of digital health innovations at the AMA.  AI simply enhances their ability to make those decisions.  That distinction is vital. AI directly affects patient care and outcomes, and as it becomes more embedded in daily operations and workflows it poses new challenges and responsibilities.  There are genuine risks in implementing these technologies,  Dr. Lozovatsky said during a recent AMA webinar (now available on demand).  That makes it important to understand the critical need for governance.  The foundational pillars of responsible AI adoption are: The AMA STEPS Forward   Governance for Augmented Intelligence  toolkit, developed in collaboration with Manatt Health, is a comprehensive eight-step guide for health care systems to establish a governance framework to implement, manage and scale AI solutions. From AI implementation to EHR adoption and usability, the AMA is fighting to make technology work for physicians, ensuring that it is an asset to doctors not a burden. Establishing accountability is the first and most essential step in safe, scalable and meaningful AI integration. Executive leadership determines the vision, provides oversight and ensures that AI and its implementation align with system-wide priorities.  Engaging the C-suite is critical,  said Dr. Lozovatsky, a pediatric hospitalist and a nationally recognized leader in digital health and health care informatics.  All of their areas will be impacted, so buy-in from those leaders is imperative.  The executive leadership often includes the: Such a multidisciplinary governance structure drives implementation based on feedback from those delivering care.  It is critical to have people representing all of those spaces because they understand the pertinent considerations best,  Dr. Lozovatsky explained. From there, each leader should delegate responsibilities to trusted stakeholders within their domain to shape workflows, policies, and project evaluations. Find out how participants in the AMA Health System Member Program are using AI to make meaningful change. The AMA recommends a three-tiered model to organize AI governance. This approach ensures that AI supports broader institutional goals and that resources are allocated properly.  Organizations likely already have processes for evaluating technology,  said Dr. Lozovatsky.  They will need to consider unique aspects of AI and how those will be addressed with existing models and which additional governance bodies will be necessary to create  the goal being to ensure consistent oversight and alignment with institutional priorities. Clinical informatics encompasses clinical, technical and operational considerations, making this expertise integral to decision-making.  Governance of any clinical technology relies on a deep understanding of what both technology and people can and should do,  said Dr. Lozovatsky. Including these experts early in the governance conversation is essential. Organizations must answer several strategic questions before designing a structure: Establishing a governance framework starts with treating AI as a tool to advance institutional goals. Evaluate existing internal capabilities to determine their readiness for assessing and implementing AI. After that, health care organizations can make fully informed decisions about incorporating AI into existing structures or create new ones. Follow the latest news on AI and its applications and effects for health care delivered to your inbox. Perhaps the most important function of AI governance is building trust among physicians and other health professionals that these tools are safe, among patients that their data is secure, and among leaders that AI adoption will advance their mission of care.  Health care organizations must ensure that AI is implemented in a safe, thoughtful manner,  said Dr. Lozovatsky.  We must prove that we re supporting care for our patients and our clinicians in their ability to deliver that care.  Clarity of purpose and strong structures for implementation empower health systems to move forward with confidence. Establishing executive accountability within a viable governance framework supports collaboration that drives meaningful AI integration. AI governance isn t just oversight. It s about creating a culture of innovation in a structured framework that positions AI as a tool to improve care.  Doing this in a safe, thoughtful manner,  as Dr. Lozovatsky put it,  supports care for our patients and our clinicians in delivering it.  In addition to fighting on the legislative front to help ensure that technology is an asset to physicians and not a burden, the AMA has developed advocacy principles (PDF) that address the development, deployment and use of health care AI, with particular emphasis on: Learn more with the AMA about the emerging landscape of health care AI. Also, explore how to apply AI to transform health care with the  AMA ChangeMedEd  Artificial Intelligence in Health Care Series.  Making technology work for physicians CME: Improving patient-clinician telehealth communication Podcast: The future of telehealth services Video: Uses of artificial intelligence in mental health Webinar: State models of licensure flexibility Playbook: Telehealth implementation The AMA promotes the art and science of medicine and the betterment of public health. The best in medicine, delivered to your mailbox We use third party technologies for analytics, personalization and marketing purposes. These technologies may be used by us or our partners to personalize the site or deliver relevant marketing to you on third party sites. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.",0
AI Agents Show Promise in Transforming Healthcare - Oracle,https://news.google.com/rss/articles/CBMidEFVX3lxTFBHOXpFMFgta2dFa1FyVmxabzNqcmkwV0NyQXBQMWMtZlhfX0cxZDhTX0QyQ2F4YVh4eEdGd1JMTzVKN18tT2pCNXZmbE5tMUtRdEdEYUkyY2hSRGRpYXNwcFRRNHMzbm5ET09ULWVUZW1lcVla?oc=5&hl=en-US&gl=US&ceid=US:en,"Would you like to visit an Oracle country site closer to you? Margaret Lindquist | Senior Writer | April 16, 2025 In This Article AI agents, which are digital assistants designed to help organizations automate certain kinds of workloads and improve decision-making, have the potential to help change the way healthcare is delivered and how people manage their health. Although healthcare organizations are beginning to adopt AI for isolated use cases, the real value will be realized with the adoption of AI agents that are each responsible for different tasks but work together seamlessly. In a clinical setting, one AI agent might be trained to record and interpret verbal instructions or conversations between a physician and patient, another to understand lab results, and a third to code a treatment plan for proper reimbursement. Working together, AI agents can help create a big-picture view of patients that physicians can use to make better-informed care decisions. Broadly, AI agents, combined with a variety of data types, use large language models (LLMs machine learning models that can perform natural language processing tasks) and retrieval-augmented generation (RAG which provides a way to optimize the output of an LLM without modifying the model itself). AI agents can be assigned tasks, examine their environments, take actions based on their roles, and fine-tune their behavior based on their experiences and user feedback. These tasks can range from answering simple questions and analyzing language context and tone to resolving complex operational challenges in industries such as healthcare, retail, and hospitality. In healthcare settings, using chat, text, or voice interfaces, AI agents can summarize spoken words, uncover signals that require human attention, and scan internal and external data to provide users patients and clinicians with real-time results and continually improve performance and accuracy. AI agents work by taking in human language requests, encoding them, and then sending them to the enterprise data store. The agent uses an LLM to understand the query, then searches the knowledgebase for relevant data, re-ranks the content for semantic relevance, combines the most relevant content and the query into a coherent response, and then sends the response and the content used to create it to the requester. At a basic level, by automating routine tasks, AI agents in healthcare use artificial intelligence to help reduce the workloads of healthcare professionals, including administrators, allowing them to focus more on patient interactions, higher-level decision-making, and operational improvements. At a more advanced, clinical level, AI agents can analyze vast amounts of data from EHRs, medical research repositories, government regulation libraries, and other sources to assist in diagnoses and help personalize treatment plans based on patient histories and other characteristics. They use predictive analytics to process and interpret large data sets, both historical and current, to help providers make more-informed choices and improve patient outcomes. For example, an agent might take historical data about disease outbreaks and interpret it against current laboratory value patterns. If it identifies a cluster of certain values in a community, it can provide clinicians with insights and predictions pertaining to a potential outbreak. The agent tasked with parsing current data sets may call upon another agent for regional or national data and would need to know the values present at the baseline and what the break point would be for an outbreak. Another set of agents might be directed to pull together data from a repository of mammograms, which would show the typical progression of disease associated with a particular type of mammogram, compare that data set with a single patient s mammogram, and in doing so help determine how a condition may progress and what options the physician has for treatment. Key Takeaways AI agents for healthcare are digital assistants that help improve interactions between patients and healthcare providers by combining healthcare intelligence with voice, chat, and text messaging interfaces. Rather than acting as standalone applications, healthcare AI agents are embedded into administrative and clinical workflows, such as patient registration, during which an agent can automate the filling out of lengthy, repetitive forms. Physicians can call on agents before appointments and request a  pre-briefing  on a patient through their device to acquaint themselves with the patient s medical history, most recent test results, and reason for their visit while walking to the exam room. (That s an example of a lead agent pulling together information from other, specialized agents to create a unified report.) Physicians can also call on AI agents during appointments, when, with the patient s permission, an AI agent can listen in on patient-physician interactions and help automatically create a summary of what was discussed and decided on. Healthcare AI agents can also be trained on data sets that are specific to a condition or disease so physicians can benefit from the broadest range of clinical knowledge when treating their patients. In practice, the biggest value of healthcare AI agents, at least initially, will likely come from their ability to take manual data entry out of the hands of clinicians so they can focus on the patient, applying their medical knowledge and human intuition. Although physician burnout is on the decline since the pandemic, according to the American Medical Association, almost half of physicians still report at least one symptom. One stressor is the volume of administrative work they re required to perform. Another benefit of AI agents is that they can help align treatment codes with payer guidelines so practitioners are reimbursed for the care they provide. It s an important consideration given that US healthcare organizations operate with an average profit margin of just 4.5%, according to the Kaufman Hall National Hospital Flash Report published in November 2024. AI agents require intensive computing power, far more than any healthcare organization would have onsite, so running them in the cloud is a necessity. The cloud also gives healthcare organizations the benefits of large language models trained on medical data sets. Alternatively, that training can take place on private data sets in a private cloud so organizations don t lose control of their own data. The healthcare industry is still in the early stages of adopting AI agents due to the industry s complexity and regulations that govern how they can be used. For example, a task such as renewing a prescription might seem ripe for automation, but the agent would first need to factor in whether it s safe to provide additional doses of the medication without a recent in-clinic examination or telemedicine appointment. However, when implemented correctly, AI agents have the potential to free clinicians from data entry tasks, help improve care reimbursement rates and accuracy, and help improve healthcare decision-making. (More on these and other benefits later.) Healthcare organizations can use a combination of agents, rather than one single agent. Each agent is designed to perform a specific task, such as setting up appointments, preregistering patients, prepping clinicians, recording and summarizing details of an examination, and managing patient follow-up. They work by tapping into a vast store of knowledge from internal sources (including patient EHRs) and external sources to recognize patterns and understand user needs. Here s the standard planning process for creating a set of AI agents: The short answer: They re not yet. As stated earlier, the industry is still in the early stages of adopting AI agents. And although patients will perceive AI agents as a single source of help and knowledge, in actuality there are many different agents, all tasked with handling different parts of the healthcare journey, and these agents will likely come from multiple vendors, all with expertise in specific areas. That said, AI agents do have the potential to help transform the healthcare industry, as they can relieve clinicians of many of their manual data entry tasks and help them gain a more informed and focused view of patients while providing patients with a trusted  assistant  to help them navigate the complex healthcare system and achieve better health outcomes. AI agents stand to benefit healthcare organizations and their patients in two main ways: by helping to improve clinical decision-making and treatment and by reducing the cost and burden of administrative tasks. For more on these and other benefits, read on. AI agents use a variety of inputs to do their work, depending on the agent s specific role in the healthcare environment. A patient-interaction agent will converse with patients, tapping different sources of patient and other data to respond to queries and provide lifestyle support. Another agent will listen in on an examination and pull out the specific information needed to update the patient record. Some agents respond only to requests from other agents for example, an agent responsible for summarizing a treatment plan would connect with an agent that understands laboratory values and another that can interpret radiology images. Most healthcare AI agents require a complex mix of the following components: Learn how patients, clinicians, and health organizations can use new technologies to improve health outcomes and reduce costs and staff burden. The best healthcare use cases for AI agents take advantage of AI s ability to analyze vast amounts of data to help improve patient care and reduce administrative overhead. The best use cases are ones that enable the agent to learn over time. The following rise to the top: Most of the early technologies used in healthcare were cumbersome, time-consuming, and frustrating for clinicians at every level. Clinicians were responsible for knowing where different sets of data were stored and for pulling together that data to obtain a complete, accurate view of the patient. Relieving that administrative burden has been the highest priority for healthcare technology companies, since that burden has contributed to burnout, early retirement, and physicians simply leaving the profession for other, less stressful positions. AI agents can promise to help relieve that burden, as well as reduce diagnostic errors, letting physicians engage more with patients, improving health outcomes, and helping ensure physicians are paid in a timely manner for the services they provide. With the acquisition of EHR developer Cerner, Oracle has expanded its already substantial portfolio of healthcare technology products and services, which is augmented by its deep expertise in data management, cloud applications and infrastructure, and AI. Oracle Health Clinical AI Agent can help assist at every point in the patient lifecycle, from initial intake to clinical follow-up. By automating the entire documentation process and synchronizing data with the patient s EHR, the AI agent can contribute to a better patient experience and help improve diagnoses and treatments. How will AI be used in healthcare?Although there s no substitute for the knowledge, experience, and intuition of a talented healthcare practitioner, AI can become a trusted assistant by automating scheduling, check-in, and other administrative processes, summarizing the details of examinations, helping inform diagnoses and treatments, and helping manage follow-ups with patients. Which AI tool is used in healthcare?The most common AI tools so far are GenAI-based assistants that can help improve the accuracy and speed of documentation and reduce clinician administrative burdens. What are the types of AI agents?Types of AI agents include simple reflex agents, which react to input without considering the broader context; model-based reflex agents, which use a model of the environment related to their function to assess the effects of actions before making a recommendation; goal-based agents, which consider long-term objectives and make recommendations based on that information; utility-based agents, which perform a single function; and learning agents, which adjust their performance over time based on interactions with users. What is the most common AI in healthcare?Although AI is beginning to take on many functions in healthcare organizations, some of the most common are analyzing lab results, summarizing appointments, and interpreting paper forms and scanned images such as X-rays and CT scans.",2
The Healthcare AI Adoption Index - Bain & Company,https://news.google.com/rss/articles/CBMicEFVX3lxTFBtTDU0QVo2X1BBWU1DX0I0bW9nWjlrZjc0dUFqM1pXY2RWZVp5aFFYb2hXazNyejBEZGtDbWdWc2VTTlpoTUVvNW1XWUVCUkhNQXBoLTdVLTNrZWxfc00tNUY1YmYzWWtBeVJFMGNfMTI?oc=5&hl=en-US&gl=US&ceid=US:en,"Bookmark content that interests you and it will be saved here for you to read or share later. Bookmark content that interests you and it will be saved here for you to read or share later. Content added to saved items Removed from saved items Article A survey of more than 400 healthcare buyers reveals what s fueling their AI experimentation and pinpoints opportunities for innovation partners. By Eric Berger, Caitlin Dowling, Sofia Guerra, and Steve Kraus Article For the full research report from Bessemer Venture Partners, go to  The Healthcare AI Adoption Index  on bvp.com. The healthcare industry is undergoing a once-in-a-generation shift at warp speed. The transition is being propelled not by regulation as was the case with the gradual migration to electronic health records (EHRs) but by a shared sense of urgency and strategic conviction among payers, providers, and pharmaceutical companies. Just two and a half years after the launch of ChatGPT, artificial intelligence (AI) and especially generative AI has gone from experimental curiosity to boardroom priority. It s dominating executive agendas as leaders explore how the technology will radically reshape healthcare diagnosis (Dx), delivery, and management, as well as drug discovery and development. Bain & Company, Bessemer Venture Partners, and Amazon Web Services (AWS) surveyed more than 400 senior leaders across payers, providers, and pharmaceutical companies. The goal was to unpack how healthcare buyers are adopting AI, what  jobs to be done  they are prioritizing, and where the greatest opportunities and challenges lie. We provide our assessment of the current state with the AI Dx Index, which can help healthcare companies and investors accurately assess the industry s AI transformation. Healthcare leaders are confident in AI, and they re backing it up with real investment. Of healthcare executives surveyed, 95% say generative AI will transform the industry. A slight majority (54%) of respondents are already seeing meaningful return on investment (ROI) at their organization after their first year of generative AI implementations. But it s still early days. To determine where AI is making inroads and where it s headed, we identified jobs to be done across payers, providers, and pharmaceutical companies. We found only 45% of AI applications have moved beyond the ideation or proof-of-concept (POC) stages. In addition, the path to scale is complex: Only 30% of completed POC projects have made it into production. Providers appear to be the furthest along, with 35% of their POCs in production, while pharmaceutical companies lag with only 24% of POCs in production. One illustration of a fast-moving provider use case is AI-powered ambient scribes for clinical documentation support. Of providers surveyed, 30% are deploying them systemwide, and another 60% are piloting or implementing them. So, why are so many organizations stuck in the experimentation stage? Even as enthusiasm runs high, key roadblocks are slowing progress from pilot to scale: Budget, however, is not a primary constraint. According to those we surveyed, 65% of AI projects are funded centrally, and most respondents say they have sufficient budget to scale. What will the AI development landscape look like in the future? Unlike the EHR wave, where vendors drove much of the change, today s AI innovation is happening in-house. In fact, more than half of all AI development is internal, and only 15% of solutions are currently being built by start-ups. Thus, this era is defined by codevelopment, where internal teams and external partners collaborate to embed AI deeply into existing workflows. Given the speed of evolution in the market, we developed the AI Dx Index, a data-driven framework that evaluates the current adoption of and prospective opportunities related to AI use cases. The goal is to help healthcare companies prioritize focus areas when buying. Our index considers three dimensions: Based on our survey data, we position AI use cases on an index where the y-axis is the opportunity score and the x-axis is the adoption score. As AI adoption accelerates, these use cases will move along the maturity curve. With that, the opportunity often narrows, as organizations solve problems and the amount of manual work decreases. Notes: Opportunity score is average percentage of respondents claiming a job is a significant pain point and percentage of respondents describing job as a mostly manual process; adoption score is average of development stage where ""not yet started"" is 0% and implementation or full rollout is 80% Notes: Opportunity score is average percentage of respondents claiming a job is a significant pain point and percentage of respondents describing job as a mostly manual process; adoption score is average of development stage where ""not yet started"" is 0% and implementation or full rollout is 80% Notes: Opportunity score is average percentage of respondents claiming a job is a significant pain point and percentage of respondents describing job as a mostly manual process; adoption score is average of development stage where ""not yet started"" is 0% and implementation or full rollout is 80% Buyers can use this assessment as a navigation tool. They can benchmark progress to assess how their AI adoption compares with peers  and to identify mature, scalable solutions. It can also help healthcare IT (HCIT) companies identify high-pain-point, low-automation use cases where they can gain traction early. From the assessment and our broader experience, we ve identified some key lessons for both parties. For buyers, the deluge of AI pitches and management of dozens of AI initiatives can feel overwhelming. Three key principles can help. In the current environment, HCIT companies need to stand out from the full ecosystem that includes buyers  internal teams. To break through, leading HCIT companies will take five steps: AI in healthcare is no longer theoretical. It is growing and transforming the way organizations diagnose, deliver, and manage care. But success won t come from simply buying tools or launching isolated pilots. Many of those early-stage projects, of course, will fall short. Instead, the winners in this next wave will be the organizations that: Our AI Dx Index is designed to help healthcare buyers and HCITs on this path to success. They can identify the most promising use cases, targeting the greatest pain points, most nascent solutions, and highest-potential transformation. Over time, the gap between hype and real value will narrow, as the industry moves beyond the inflection point and into its future. Read the full report The Healthcare AI Adoption Index Bessemer Venture Partners helps entrepreneurs lay strong foundations to build and forge long-standing companies. With more than 145 IPOs and 300 portfolio companies in the enterprise, consumer and healthcare spaces, Bessemer supports founders and CEOs from their early days through every stage of growth. Bessemer s global portfolio has included Pinterest, Shopify, Twilio, Yelp, LinkedIn, PagerDuty, DocuSign, Wix, Fiverr, and Toast and has more than $18 billion of assets under management. Bessemer has teams of investors and partners located in Tel Aviv, Silicon Valley, San Francisco, New York, London, Hong Kong, Boston, and Bangalore. Born from innovations in steel more than a century ago, Bessemer s storied history has afforded its partners the opportunity to celebrate and scrutinize its best investment decisions and also learn from its mistakes. Since 2006, Amazon Web Services has been the world s most comprehensive and broadly adopted cloud. AWS has been continually expanding its services to support virtually any workload, and it now has more than 200 fully featured services for compute, storage, databases, networking, analytics, machine learning and artificial intelligence (AI), Internet of Things (IoT), mobile, security, hybrid, virtual and augmented reality (VR and AR), media, and application development, deployment, and management from 99 Availability Zones within 31 geographic regions, with announced plans for 15 more Availability Zones and five more AWS Regions in Canada, Israel, Malaysia, New Zealand, and Thailand. Millions of customers including the fastest-growing startups, largest enterprises, and leading government agencies trust AWS to power their infrastructure, become more agile, and lower costs. To learn more about AWS, visit aws.amazon.com. Bain s research identifies the most useful digital applications, including those that offer the best opportunities for competitive advantage. Disruption is mandatory. Obsolescence is optional. Why companies that adapt to the new realities will come out ahead. Generative AI and intelligent agents are transforming finance. Security, privacy, and institutional trust are the battlegrounds for AI-led payments. We work with ambitious leaders who want to define the future, not hide from it. Together, we achieve extraordinary outcomes. Stay ahead in a rapidly changing world. Subscribe to Bain Insights, our monthly look at the critical issues facing global businesses. *I have read the Privacy Policy and agree to its terms.   1996-2025 Bain & Company, Inc. Contact Bain",2
Generative AI to Reshape the Future of Health Care - Deloitte,https://news.google.com/rss/articles/CBMirgFBVV95cUxPQUlKUWtmR3gtU29FSjZNcjhtT293OVgwbEltTDZsa3J2TlFVUVZMT1FEZlZFTFNLSkJtSjViSUVfZWFuckprWnRkc2tXSXJqOU1OTEJXa003b2FYS3BiTEtxaEdMTkRWcWVuUVBLTVVVeF9UQmdLU2RVN2I1OS0zamZ1RE5UaGdHRkF0VTVRMUlSTXQxaUJuRDg3NHVYUF9NU3lJMy1yRGlUWVc3ZkE?oc=5&hl=en-US&gl=US&ceid=US:en,"Select your location No results found If we have selected the wrong experience for you, please change it above. Generative artificial intelligence (AI) has begun to unleash digital waves. It s enhancing commercial, product, and operations functions across industries, heralding a new era for health care transformation. Discover how this powerful technology can help bring unprecedented efficiency, effectiveness, and innovation to the health care sector and explore a framework that can help your organization capitalize on these transformations. The health care industry is facing a complex set of challenges, from labor shortages and clinician burnout to declining profitability and worsening health outcomes, particularly among underserved communities. Addressing these challenges will likely require targeted transformation of core operational, commercial, and product strategies. Generative AI offers revolutionary approaches that can help secure a competitive advantage in the health care sector and re-establish genuine care and trust in health care practices. In recent years, natural language processing and machine learning have found applications in various health care use cases, but new Generative AI models are taking health care technology to new heights. These models demonstrate unprecedented capabilities in natural language generation, summarization, translation, insight retrieval, reasoning, and managing unstructured, unlabeled data. Generative AI technology has the potential to democratize knowledge, increase interoperability, accelerate discovery, and enable true personalization in health care. Perhaps most importantly, Generative AI can either deepen and restore trust or exacerbate mistrust and introduce new skepticism among consumers and health care stakeholders alike. To successfully address the challenges facing health care organizations, Generative AI must be designed, deployed, and scaled using a transformational approach that incorporates organizational change, ethics, and trust. Generative AI in health care also holds promise across industries to streamline operations, from discovery through commercialization enhancing efficiency, compliance, and consumer-centricity. By harnessing Generative AI, companies can create a competitive advantage, accelerate innovation, and ensure more agile and informed decision-making across their value chain. According to the Deloitte Center for Health Solutions, surveys indicate that health care organizations are already recognizing the potential of Generative AI: Organizations can begin a Generative AI activation strategy from multiple starting points. The key is to rethink how to frame the challenge and desired outcome, taking a problem-first, technology-second approach, rather than a technology-first, problem-second approach. We propose a five-part approach to actualizing Generative AI with a set of Trustworthy AI  considerations within your business, where organizations can begin in any of the five areas and pursue parallel progress. Download our full report to gain additional insights into each of these parts. Part 1: Infuse AI into your organization Part 2: Establish an AI operational foundation Part 3: Develop an AI infrastructure plan Part 4: Initiate experimentation and pilots Part 5: Develop production-ready solutions and operational systems Successful integration of Generative AI into health care hinges on effectively balancing transformative improvements in core business areas against the inherent risks. Leaders should conduct a rigorous, case-by-case assessment of each potential Generative AI application, aligning it with strategic transformations in commercial, product, and operations domains to maximize industry advantage and mitigate hazards. Given the rapidly accelerating pace of this technology coupled with its complexity Generative AI is challenging for organizations to implement effectively. That s where the role of a trusted third party becomes invaluable. By working with specialists who have an in-depth understanding of the technology, health care leaders can navigate the complexities, maximize their return on investment, and ensure the technology is tailored to meet their unique needs. If you d like to learn more about the benefits of AI in health care or how your organization can better leverage the technology to sustain growth and innovation, let s set up a conversation. Insights and solutions that can help your organization build a competitive advantage. From code to cure, how Generative AI can reshape the health frontier Download the full report Did you find this useful? To tell us what you think, please update your settings to accept analytics and performance cookies.   2025. See Terms of Use for more information. Deloitte refers to one or more of Deloitte Touche Tohmatsu Limited, a UK private company limited by guarantee (""DTTL""), its network of member firms, and their related entities. DTTL and each of its member firms are legally separate and independent entities. DTTL (also referred to as ""Deloitte Global"") does not provide services to clients. In the United States, Deloitte refers to one or more of the US member firms of DTTL, their related entities that operate using the ""Deloitte"" name in the United States and their respective affiliates. Certain services may not be available to attest clients under the rules and regulations of public accounting. Please see www.deloitte.com/about to learn more about our global network of member firms.",0
What Is Agentic AI and How Can It Be Used in Healthcare? - HealthTech Magazine,https://news.google.com/rss/articles/CBMikAFBVV95cUxPcWhrcDF6bFBSVGVLSG5PanBqVThOREFfaFJ1MEdoTlh0MHAwdnNmSzM4U1I5NlJHSEFHVXhFVFJrVFVGQ1RXbHljMVBxd0F1dUVzbl83RVFOZTJXWkxFREpSYUhhX1hKN0g0YmNkS1NFeVRGbVIyeS1IMWJlTmN2Q283Ulp6ckpsTlpiVzVJTHc?oc=5&hl=en-US&gl=US&ceid=US:en,"These health IT influencers are change-makers, innovators and compassionate leaders who use technology to make a difference in provider experiences and patient outcomes. See how IT leaders are tackling AI opportunities and challenges. Erin Laviola is a freelance writer who specializes in the healthcare industry. Agentic artificial intelligence is emerging as the next big thing in healthcare technology. Intelligent agents within an agentic AI framework are designed to autonomously reason, solve multistep medical challenges and make decisions about what to do next with limited oversight. Use of the technology is still in its earliest stages, but interest and adoption of AI agents is expected to escalate quickly. While less than 1% of enterprise software applications included agentic AI in 2024, Gartner predicts usage will surge to 33% by 2028. A recent report from Market.us also estimates the global agentic AI market will reach nearly $200 billion by 2034.  Agentic AI will change the way we work in ways that parallel how different work became with the arrival of the internet,  says Amanda Saunders, director of generative AI software marketing at NVIDIA. Click the banner below to read the new CDW Artificial Intelligence Research Report. Like other forms of artificial intelligence, agentic AI is only as accurate as the data that fuels it. It relies on  a digital ecosystem of large language models (LLMs), machine learning (ML), and natural language processing (NLP) to perform autonomous tasks on behalf of the user or another system,  according to IBM. Agentic AI is the term used to describe the overall concept. AI agents are the individual components within the model that are created to handle specific tasks and processes. Agents within an agentic AI system have the  agency  to analyze data and then make decisions about what to do with the results. While a significant step forward, both Saunders and Jason Warrelmann, vice president of healthcare strategy at UiPath, caution that agentic AI is still considered artificial narrow intelligence. Artificial general intelligence, which would allow machines to think like humans, does not yet exist.  Right now, the best we can do is provide context so that the agent understands how to answer. There s still a large language model behind it, so the agentic AI isn t acting completely on its own,  Warrelmann says.  The computing required for that is still beyond us.   While agents and reasoning are powerful capabilities, they re still no match for the incredible complexity of human intelligence,  Saunders agrees. EXPLORE: How can data governance and LLMs help healthcare organizations avoid bias and inaccuracy? Generative AI applications use data from large language models to craft responses. The quality of the output relies largely on the specificity and guidance provided by the user, a process known as prompt engineering. Agentic AI is more proactive. It can pull information from multiple sources, use sophisticated reasoning and then automatically complete the next task.  Agentic AI builds on generative AI, taking simple responses further with the ability to consider options, go back and redo steps,  says Saunders.  It works much more like we do when we solve problems and work out how to consider new information.  In healthcare, agentic and generative AI can work together to increase efficiencies and boost productivity. For example, after a surgery, generative AI can use the patient s record and the surgeon s notes to write post-op instructions for medication use, activity limitations and follow-up care. Agentic AI can then share the generated instructions, monitor if the patient has accessed the document within the patient portal and send reminders about future appointments. If the patient reports a serious symptom, the healthcare AI agent could automatically alert a nurse or schedule a virtual consult with the provider. Amanda Saunders Director of Generative AI Software Marketing, NVIDIA Healthcare AI agents have many applications across the medical industry, including: According to a report released by the American Hospital Association, more than 40% of total hospital expenses are administrative costs. Warrelmann says he expects that in the near future, healthcare organizations will adopt AI agents on a wider scale to enhance their back-office operations.  Hospitals have to figure out staffing, salaries, bed utilization rates, inventory management and quality protocols,  he says, explaining that AI agents can quickly analyze all of that data and provide recommendations for how the hospital could be more efficient. DISCOVER: Here are 13 ways AI enhances healthcare operations, patient care and treatments. Because AI agents can operate autonomously, data quality and oversight are even more critical. Warrelmann stresses that healthcare and IT leaders who introduce agentic AI into their operations need to keep a close eye on where the AI is pulling information. For example, while healthcare AI agents may be allowed to ingest data from an electronic medical record, they should be blocked from accessing private email exchanges.  You start to get into segmentation of data and making sure confidential information doesn t end up in the wrong place,  says Warrelmann. Saunders adds that heath systems may be able to rely on their existing partnerships with third-party vendors to implement agentic AI.  Many technology providers are now building agents into their own platforms, which provides a simple way for healthcare IT leaders to use agents in the systems they re already working with to run their operations.  These technology platforms have integrated agentic AI into their systems:  Agentic AI is already transforming enterprises and is likely to be a multitrillion-dollar opportunity,  says Saunders.  This means that healthcare IT leaders should lean in to learn how AI agents can help transform work across drug discovery, patient care, operations and so much more.  Security Across Healthcare, Collaboration Is a Critical Ingredient of Data Security Security Data Risk Management Best Practices for Healthcare Visit Some Of Our Other Technology Websites: Tap into practical IT advice from CDW experts Copyright   2025 CDW LLC 200 N. Milwaukee Avenue, Vernon Hills, IL 60061Do Not Sell My Personal Information These cookies enable the website to provide enhanced functionality and personalisation. They may be set by us or by third party providers whose services we have added to our pages. If you do not allow these cookies then some or all of these services may not function properly. These cookies may be set through our site by our advertising partners. They may be used by those companies to build a profile of your interests and show you relevant ads on other sites. They do not store directly personal information, but are based on uniquely identifying your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies do not store any personally identifiable information. These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. All information these cookies collect is aggregated and therefore anonymous. If you do not allow these cookies we will not know when you have visited our site, and will not be able to monitor its performance.",2
Health Care Artificial Intelligence Code of Conduct - National Academy of Medicine,https://news.google.com/rss/articles/CBMirAFBVV95cUxOSUdzZVl4d0U2elI0bjZRVkE0d1B5aEx0U0xneEM4cl92c1JuV00yVGR6UzhCWTV0Z2RPaWpuV0NYM0xjWTdaX3RnVEdiaTdDalV1dEdOY0k1UUhjZFVfWXVuSGZtVzdJczY1ZWlPdE55Y193ZzhmNkRPb1UxS1ctZW9xdHZQcVVVRFFpS0NxSTgwcTFrMWZoVlIyQ0pobEVKOVFadVMzWkhzODZX?oc=5&hl=en-US&gl=US&ceid=US:en,"The Artificial Intelligence Code of Conduct (AICC) project is a pivotal initiative of the NAM, aimed at providing a guiding framework to ensure that AI algorithms and their application in health, health care, and biomedical science perform accurately, safely, reliably, and ethically in the service of better health for all. In May 2025, the NAM released the special publication, An Artificial Intelligence Code of Conduct for Health and Medicine: Essential Guidance for Aligned Action, which presents a unifying AI Code of Conduct framework developed to align the field around responsible development and application of AI and to catalyze collective action to ensure that the transformative potential of AI in health and medicine is realized. People are scared of dying, they re scared of losing their mom, they re scared of not being able to parent and walk their child down the aisle. How can we start using the power of these tools, not through a lens of fear and reluctance, but to create a culture change from  doctor knows best  or  patient knows best  to  person powered by AI knows best ? Historically in times of technology advancements, health care disparity gaps have widened. AI runs the same risk, but it has a much greater opportunity to avoid further exacerbating the disparities among populations. We have a chance to introduce culturally competent care and to understand the determinants that affect the outcomes. The AICC Steering Committee s primary responsibility is providing NAM staff with strategic guidance, so project activities and deliverables achieve their intended aims. Steering Committee members provide thought leadership on issues such as governance, policy development, environmental awareness, risk analysis, and adoption of the Code throughout the industry. *(Digital Health Action Collaborative Co-chair) The Universal Declaration of Human Rights from 1948 includes the right to enjoy scientific advancement and its benefits. This has been a dormant right   we have failed to operationalize it and use it to promote certain policy approaches, and that is a great shame. Learn More The AICC vision is to align and catalyze collective action to realize the potential of AI to revolutionize health care delivery, generate groundbreaking advances in health research, and contribute to robust health for all, adhering to the highest standards of ethics, equity, privacy, security, and accountability. The AICC activities are to: 1) harmonize the many sets of AI principles/frameworks/blueprints for health care and biomedical science and identify and fill the gaps to create a best practice AI Code of Conduct with  guideline interoperability ; 2) align the field in advancing broad adoption and embedding of the harmonized AI Code of Conduct; 3) identify the roles and responsibilities of each stakeholder at each stage of the AI lifecycle; 4) describe the national architecture needed to support responsible health care AI; and 5) identify ways to increase the speed of learning about how to govern health care AI in service of a learning health system. Aligning the field means bringing together a broad stakeholder group to ensure all perspectives are understood throughout the process of developing the Code of Conduct to: 1) assure, that to the extent possible, the Code reflects the needs and priorities of all parties; and 2) earn stakeholder support for and commitment to the Code, with the ultimate goal of having the Code woven throughout the fabric of health, health care, and biomedical science. The systems view of AI in health care and biomedical science considers the aspects of the AI lifecycle (e.g., data acquisition, validation, and representativeness, data modeling and testing, systems procurement and implementation, post-implementation vigilance, etc.), the stakeholders  roles in each phase, and identifies the ecosystem needed to support trustworthy AI for health, health care, and biomedical science. This project will address both predictive AI (e.g., models that help to identify patients who are at risk of developing certain conditions, recommending treatment plans, and predicting outcomes) as well as LLM AI (e.g., ChatGPT, Bard, OPT-IML, etc.), as both have significant implications for health, health care, and biomedical science and are becoming integrated in their use. The AICC outputs are: 1) a harmonized and broadly supported Code of Conduct; 2) a comprehensive landscape assessment that includes a systematic review of the literature; a review of the guidelines/frameworks/blueprints from federal agencies; and the guidelines issued by medical specialty societies; 3) a description of the roles and responsibilities of each stakeholder at each stage of the AI lifecycle; 4) a description of the national architecture needed to support responsible health care AI; and 5) identification of priority actions going forward. The work products that contain the above include: 1) summaries from the AICC Steering Committee meetings; 2) a NAM Commentary paper outlining the landscape assessment and key components of the Code; and 3) a final NAM special publication containing a proposed Code of Conduct framework to be deployed for testing, validation, learning, and improvement. The current AICC project will last for 3 years and will conclude in December 2025. We anticipate that additional projects may spawn from this work. The work will draw primarily from the U.S. experience but will be informed by international efforts to ensure the inclusion of best practices, and to support global AI guideline harmonization efforts for health, health care, and biomedical science. The stakeholder groups involved in the AICC project and developing the Code include patients and families; providers; privacy, security and ethics experts; equity experts; health systems; tech companies, government agencies; biomedical scientists and researchers; health plans; pharma and health care product manufacturers; professional societies; medical societies; and health care AI collaborations. In addition, the NAM will work with these stakeholder groups to embed the Code into their own sets of guidelines and principles and develop their specific implementation guides. The project will be guided by a Steering Committee of national thought leaders representing all key stakeholders. The AICC Steering Committee provides strategic guidance for the project as a whole and overarching leadership for the development of a Code of Conduct fully informed by stakeholders, assuring that process and outcomes warrant and achieve the desired broad support for and implementation of the Code. The project team identified candidates for steering committee membership to ensure broad stakeholder engagement and includes individuals with expertise in ethics, patient advocacy, communications, health systems, technology, and research. The criteria for selection also included: 1) national recognized thought leadership; 2) capacity to influence the adoption and embedding of the AI Code of Conduct through the industry; and 3) ability to provide thought leadership and strategic guidance on issues such as governance, policy development, environmental awareness, and risk analysis. A foundational principle in the development of the AICC project was the importance of honoring and building upon the work that has already been done to promote trustworthy AI in health, health care, and biomedical science. To that end, an early activity in the project plan is a comprehensive landscape assessment that includes a systematic review of the literature; a review of the guidelines/frameworks/blueprints from federal agencies; and the guidelines issued by medical specialty societies. This environmental scan is serving as the foundation upon which the Code is being developed. However, the AICC will not provide the specific implementation guidance on topics already covered by federal agencies or other coalitions, such as those presented in the NIST Risk Management Framework or the Blueprint for Trustworthy AI in Healthcare produced by the Coalition for Health AI (CHAI). Throughout the course of the project, the NAM effort will inform the efforts of CHAI, which is providing robust best practice technical guidance, including assurance labs and implementation guides to enable clinical systems to apply the Code of Conduct. Similarly, the efforts of the CHAI and other groups addressing responsible AI will inform and clarify areas that will need to be addressed in the NAM Code of Conduct. The work and final deliverables of these projects are mutually reinforcing and coordinated to ensure broad adoption and active implementation of the AICC in the field. The NAM Learning Health System Shared Commitments provided the foundation for the Code Principles, seamlessly integrating the constructs. Just as the LHS Shared Commitments establish expectations for all participants in the health system, the Code Principles provide analogous guidance specifically tailored to the development and implementation of health AI. They represent a natural evolution of the LHS framework to address emerging technologies. The Code Principles reflect the values and norms to be applied in the context of health AI governance to promote trust while ensuring the benefits and mitigating the risks associated with Al in health, health care, and biomedical science. This project is supported by The Gordon and Betty Moore Foundation, The Patrick J. McGovern Foundation, The California Health Care Foundation, Epic, and the National Institutes of Health. For more information on the project, please email Laura Adams, NAM Senior Advisor, at LAdams@nas.edu and Sunita Krishnan, Program Officer, at SKrishnan@nas.edu. Sign up for NAM email updates ""*"" indicates required fields   Copyright 2025   National Academy of Sciences. All rights reserved. Powered by Social Driver. Copyright 2025   National Academy of Sciences. All rights reserved. Powered by Social Driver. Terms of Use | Privacy Policy Notifications",0
AI in Healthcare in Australia: Transforming Patient Care & Operations - appinventiv.com,https://news.google.com/rss/articles/CBMia0FVX3lxTE16NUpFMk41WW8tdEU5eVQ3eXNSRnF3VmN1RTFDUkJCMmJfMUdSWmxNSklXSEtXQ0RLWWE4NzhzQnVBcFN5c2JtT25fLUlYclhnYWlqX2dmWE5wRU5QRGIwakJGS1poSUtwR2hZ?oc=5&hl=en-US&gl=US&ceid=US:en,"We believe in change driven by technology and innovation. Meet the brains behind our smooth running and powerful machine. Join our team of experts to make a difference in the real world. Learn about Appinventiv's product lifecycle development process. Our software development services are built to evolve your business idea into a successful growth story that deploy customized solutions in a wide range of industries to steadfast success for top globally leading brands A leading digital platform to offer engaging shopping experience to users A transforming ERP Solution for the world's largest furniture retailer A mobile app to digitalize & expand KFC's digital footprint A refined UX strategy for Domino's to increase their conversion rate by 23% The MIT Innovation award-winning app with $52 Million funding reshaping the employment landscape A SaaS-based financial literacy and smart money management platform for kids We believe in change driven by technology and innovation. Join our team of experts to make a difference in the real world. Learn about Appinventiv's product lifecycle development process. Meet the brains behind our smooth running and powerful machine. that deploy customized solutions in a wide range of industries Our software development services are built to evolve your business idea into a successful growth story A leading digital platform to offer engaging shopping experience to users A transforming ERP Solution for the world's largest furniture retailer A mobile app to digitalize & expand KFC's digital footprint A refined UX strategy for Domino's to increase their conversion rate by 23% The MIT Innovation award-winning app with $52 Million funding reshaping the employment landscape A SaaS-based financial literacy and smart money management platform for kids to steadfast success for top globally leading brands Key takeaways: Healthcare has always carried a heavy load of complexity dealing with intricate patient needs, managing countless different treatments, and facing non-stop demand for services. Because of this, healthcare providers constantly find themselves juggling the challenge of delivering quality care efficiently while keeping costs under control, patients happy, and outcomes strong. Now, fast forward to 2025, artificial intelligence is stepping in to tackle some of the biggest headaches in healthcare, making the whole system work better around the globe. AI s expanding presence in healthcare is already reshaping how doctors diagnose problems, treat patients, and handle ongoing care. This isn t some far-off sci-fi dream anymore it s happening right now, and it s completely changing what healthcare looks like. Whether it s making diagnoses more accurate or making hospital paperwork run smoother, AI has the power to take some serious pressure off the entire system. In Australia, where you ve got huge distances between communities and a population that keeps growing fast, these complications make healthcare delivery even trickier. That s where AI in healthcare has stepped up as an essential tool. With all the innovations happening in AI technology, the Australian healthcare system is sitting on the edge of some really exciting changes. AI applications in Australian healthcare are already making things run more efficiently, helping patients get better results, and making healthcare more accessible, especially for people living in remote areas or places that don t get enough medical attention. Simply put, the role of AI in healthcare in Australia is changing fast, giving businesses and healthcare providers the tools they need to handle the increasing pressure on the system. Australia s healthcare industry is starting to welcome these shifts, and businesses are definitely paying attention. As AI keeps reshaping this sector, it s becoming pretty obvious that the future of healthcare in Australia is going to be deeply connected to these new technologies. Let s dig into how AI is already making real, measurable changes and what companies can do to get ahead of the curve. Ready to invest in the future of care? We can help you lead the AI healthcare revolution! Australia s healthcare system finds itself at a crucial crossroads right now. Back in 2022 23, the country put a massive $252.5 billion toward health goods and services, getting back to the spending levels we saw before the pandemic hit. This kind of investment shows just how serious Australia is about keeping its healthcare infrastructure strong, even when facing all sorts of new challenges. But the sector is definitely facing some major obstacles. With a global shortage of healthcare workers, the pressure on an already stretched system is growing, and this could impact both the quality of patient care and people s ability to access timely treatment. To tackle this, the Australian government has put $7.9 billion on the table in the coming months to boost public hospitals and health services. This money is supposed to help cut down those long hospital waiting lists, make emergency room waits shorter, and deal with the whole ambulance ramping mess. With all these challenges piling up, AI in healthcare in Australia is starting to look like a real game-changer. When you bring artificial intelligence integration in healthcare industry processes into the mix, it can handle a lot of the boring administrative stuff, which frees up medical professionals to spend more time actually taking care of patients. Additionally, AI applications in Australian healthcare, such as predictive analytics and diagnostic tools, are already demonstrating significant promise in enhancing operational efficiency and improving patient outcomes. According to a Microsoft News Report, Generative AI has the potential to add $13 billion annually to Australia s healthcare sector by 2030. When you combine that kind of cost savings with better care delivery, AI becomes a pretty essential piece of solving the problems the sector is facing right now. At the HIC 2024 seminar that Deloitte and AWS put together, 66% of Australian healthcare leaders admitted that while there s some readiness for AI out there, there are still some big challenges like training the workforce and figuring out how to implement things properly that need to be worked out. On top of that, a report from Deloitte Access Economics and the Deloitte AI Institute found that 47% of Australian employees now think generative AI will actually make social outcomes in healthcare better. As the healthcare world continues to evolve, adopting AI technologies will be crucial for Australian healthcare providers to successfully navigate the complexities of delivering modern care. [Also Read: AI in Australia: A Practical Guide for Implementation, Opportunities, and Challenges] AI is quickly establishing itself as a fundamental part of modern healthcare, making operations smoother, helping with better decision-making, and improving how patients get taken care of. Through automating boring routine tasks, crunching through massive amounts of data, and making workflows run better, AI is giving healthcare providers in Australia the tools they need to manage their resources smartly and get better results for everyone. Here are 10 key applications of AI in healthcare in Australia: AI-powered systems are taking over those mind-numbing administrative jobs like scheduling, billing, and entering patient data. This takes a huge load off healthcare staff, giving them more time to actually spend with patients. AI also handles patient records, making sure the information stays accurate, current, and easy to find when you need it. Real-World Example: St. Vincent s Health Australia uses AI tools to predict patient no-shows, optimize appointment scheduling, and streamline patient flow, reducing wait times and improving operational efficiency. AI in healthcare uses predictive analytics to figure out what patients might need next, predict how diseases will progress, and spot potential problems before they get serious. By looking at past data, AI helps healthcare providers use their resources more wisely and step in before small issues turn into big ones. [Also Read: AI Analytics for Businesses   Benefits, Use Cases, and Real Examples] Real-World Example: Monash Health uses AI to predict patient deterioration and identify high-risk individuals, ensuring timely interventions and reducing the likelihood of readmissions. AI is completely changing how diagnostics work by examining medical images like MRIs, CT scans, and X-rays. Machine learning algorithms can spot weird stuff in images with incredible accuracy, often catching early signs of things like cancer, broken bones, or heart problems that human eyes might miss. Real-World Example: At The Royal Melbourne Hospital, AI-powered diagnostic tools help radiologists detect early-stage cancers, reducing diagnostic errors and improving patient outcomes. Keeping track of medical supplies efficiently is absolutely crucial for healthcare operations. AI helps healthcare facilities monitor inventory, predict what they ll need, and automatically reorder supplies, ensuring critical supplies are always available while reducing costs associated with excessive or insufficient stock. Real-World Example: NSW Health leverages AI to predict supply shortages and monitor inventory levels across its hospitals, reducing waste and ensuring essential supplies are consistently available. AI algorithms dig into patient information like genetics, lifestyle choices, and medical history to create personalized treatment plans. This lets healthcare providers offer customized therapies that have a much better chance of working, which means better results for patients and happier people overall. Real-World Example: HealthEngine, an AI-powered platform in Australia, provides personalized health recommendations and guides patients toward the most appropriate treatments based on their unique profiles. Virtual health assistants that run on AI help patients take care of their health by giving them 24/7 access to information, letting them schedule appointments, and even answering health questions. These assistants make healthcare way more accessible, especially for people living in remote areas or those who have trouble getting around. Real-World Example: Telstra Health s AI-driven virtual assistant helps patients navigate their health journey, from managing symptoms to scheduling doctor visits, and improving overall healthcare access. [Also Read: The Cost and Benefits of Developing an AI-Powered Smart Personal Assistant App] AI is shaking up the whole drug discovery process by analyzing enormous datasets to find potential new drugs much more efficiently. Machine learning algorithms can predict how molecules will act in the human body, making the discovery of new treatments faster and cutting down the time it takes to get drugs to people who need them. Real-World Example: CSL Behring, a global biotechnology company based in Australia, uses AI to accelerate the development of life-saving drugs by predicting molecular interactions and optimizing drug formulations. AI tools are helping doctors make better clinical decisions. By analyzing patient data and medical research, AI can provide insights, suggest different treatment options, and point out potential issues that might have been missed, giving doctors better support for making important decisions. Real-World Example: The Australian National University (ANU) has developed AI systems to assist clinicians with diagnosing and treating rare diseases, helping doctors access critical knowledge quickly and improving patient outcomes. Remote patient monitoring is one of the major application of AI in healthcare in Australia. AI-powered wearable devices and sensors make real-time health monitoring possible, especially for patients dealing with chronic conditions. These devices can track vital signs, alert both patients and doctors when something seems off, and provide useful insights that help catch problems early. Real-World Example: AliveCor, in collaboration with Australian healthcare providers, offers wearable ECG monitors that use AI to detect irregular heart rhythms and alert healthcare providers, enabling timely intervention. AI can make hospital operations run much better by predicting patient admissions, figuring out when the busy periods will be, and making sure staff and resources get used effectively. This helps cut down wait times, prevents overcrowding, and makes healthcare facilities run more smoothly overall. Real-World Example: The Queensland Health System uses AI-powered tools to optimize resource allocation in its hospitals, improving patient care and reducing operational bottlenecks. Also Read: Digital Transformation Strategy for Australian Enterprises: A Practical Roadmap While AI brings incredible potential for changing healthcare in Australia, getting to widespread adoption isn t exactly a walk in the park. From worries about data privacy to dealing with regulatory red tape, healthcare providers have to work through a bunch of obstacles before they can fully tap into what AI in healthcare in Australia has to offer. Let s dig into some of the main challenges the sector is facing and look at solutions that can help get past them. AI in healthcare needs tons of patient data to work properly. While this data-heavy approach can make patient outcomes better, it also creates some serious worries about keeping data private and secure. Making sure sensitive health information stays protected from breaches or misuse is absolutely crucial, especially with Australia s tough privacy laws. Solution: To handle these risks, healthcare organizations in Australia need to focus on artificial intelligence integration in healthcare industry solutions that follow data protection rules like the Australian Privacy Principles (APPs). Encrypting patient data and using AI systems that have security features built right in can help make sure sensitive information stays safe while still allowing AI-driven insights to work their magic. Getting AI in Australian healthcare systems to work with older infrastructure can be complicated and take forever. Lots of healthcare facilities are still using outdated software and manual systems, which makes it really tough to bring in AI solutions without messing up daily operations. Solution: Healthcare organizations need to focus on bringing in AI solutions that actually work with what they already have, or they need to invest in upgrading their systems bit by bit. Working with technology partners who know their way around AI applications in Australian healthcare can make this transition smoother and ensure integration happens with minimal disruption. Even though AI can save money and improve things in the long run, the upfront costs of putting AI technology into healthcare can be way too expensive for many Australian healthcare providers, especially smaller clinics and rural hospitals. The price of software, infrastructure, training, and maintenance can become a huge roadblock. [Also Read: AI Integration Examples: How Leading Enterprises Are Transforming Operations with AI] Solution: To tackle this problem, healthcare providers can look for scalable healthcare AI solutions in Australia that offer flexibility and don t break the bank. Cloud-based AI platforms, which cut down on the need for expensive on-site infrastructure, can make AI more reachable for healthcare organizations of any size. Plus, government funding and incentives might help cover some of those implementation costs. There s a real shortage of skilled people in AI and healthcare technology. Many healthcare providers in Australia have trouble finding qualified people to put in place and manage AI systems. Without the right expertise, AI solutions might not live up to their full potential. Solution: Putting money into training programs for healthcare staff and AI specialists is absolutely critical. Partnering with AI in healthcare Australia experts or offering specialized training can give healthcare professionals the knowledge they need to run and optimize AI technologies. Working together with educational institutions can help build up a steady supply of skilled workers in AI healthcare. Using AI in healthcare brings up ethical concerns, especially when it comes to making decisions. AI systems have to be transparent, explainable, and accountable, particularly when they re involved in clinical decisions. The changing regulatory landscape around AI, especially in healthcare, also creates challenges for organizations wanting to implement these technologies. Solution: Healthcare providers need to work closely with regulators to make sure their AI systems follow evolving laws and standards. Putting in place AI integration in healthcare Australia solutions that can be audited and explained will help keep things transparent and build trust. Working with ethical AI specialists can also make sure AI systems line up with responsible practices and ethical guidelines in healthcare. Like with any major technological shift, there can be pushback from healthcare professionals who are used to doing things the traditional way. Bringing AI in healthcare in Australia might face skepticism or reluctance, particularly from senior clinicians or administrative staff. Solution: To get past this resistance, healthcare organizations need to focus on teaching stakeholders about the benefits of AI and how it enhances, rather than replaces, human expertise. Running pilot programs and showing off early success stories can help build confidence amng stakeholders in context to leveraging AI in healthcare Australia solutions. Getting key stakeholders involved in the implementation process and providing ongoing support will also make the transition easier. Another big challenge is that there s no real standardization in AI technologies across the healthcare industry. With so many different AI tools and platforms out there, it can be tough for healthcare organizations in Australia to pick the right one, especially without clear guidelines on which systems work best. Solution: Creating industry-wide standards and working with professional organizations to set benchmarks for AI in healthcare will help make adoption smoother. Making sure that generative AI in healthcare in Australia solutions can work together and follow best practices will let healthcare organizations choose tools that are reliable, scalable, and ready for the future. Australia s huge geography creates challenges for getting healthcare services to rural and remote communities. While AI has the potential to extend healthcare access, there s a gap in infrastructure and technology in these areas, making it hard for AI tools to reach underserved populations. Solution: To fix this, AI solutions designed for AI in healthcare use cases in Australia should focus on accessibility and connectivity in remote areas. Mobile-friendly AI tools, telemedicine integrations, and partnerships with local healthcare providers can help bridge the gap, making sure remote communities get to benefit from advances in AI healthcare. Despite what AI in healthcare could do, many healthcare organizations aren t sure about the return on investment (ROI). Given the high costs of implementation and ongoing maintenance, it s really important to show how AI can lead to significant long-term savings, better efficiency, and improved patient outcomes. Solution: To measure ROI effectively, healthcare organizations can roll out AI solutions in phases and track the impact on key metrics like operational efficiency, patient satisfaction, and cost savings. Running pilot programs and keeping a close eye on the results will help justify AI investments and make a stronger business case for future adoption of AI applications in Australian healthcare. AI depends on high-quality, accurate data to work effectively. However, many healthcare systems in Australia still deal with fragmented and incomplete patient data, which can hurt how well AI solutions work. Poor data quality can lead to wrong predictions, missed diagnoses, and poor patient outcomes. Solution: Making data quality better through improved data collection, integration, and cleaning processes is essential. By using AI healthcare solutions that can handle messy or unstructured data, healthcare organizations can boost the effectiveness of their AI tools and make sure patients get better care. [Also Read: Preventing AI Model Collapse: Addressing the Inherent Risk of Synthetic Datasets] Getting AI into your healthcare operations can completely change patient care, make things run better, and cut costs. Here s a step-by-step process to help you successfully bring AI in healthcare in Australia into your organization and get the most out of what it can do. Begin by figuring out the specific problems in your healthcare operations that AI can actually solve. Whether you re looking at making diagnostics better, optimizing how you manage resources, or improving patient care, having clear goals will help steer the AI integration process in the right direction. Pick AI technologies that actually match up with what you re trying to accomplish. Think about solutions like AI-powered diagnostics, predictive analytics, or AI healthcare solutions in Australia that really fit your operational needs and what you want to achieve for patient care. AI needs tons of high-quality datasets to work properly. Make sure your healthcare facility can access clean, well-organized data from EMR/EHR systems in Australia and other clinical sources. You also need to put data privacy and security at the top of your list to stay compliant with regulations. Team up with AI experts and solution providers who really know their way around artificial intelligence in healthcare Australia. Their knowledge will be crucial for customizing AI tools to fit your specific needs and making sure they integrate smoothly with what you already have running. Before you go all-in with implementation, test out AI solutions with a smaller pilot project first. This gives you a chance to see how well the system works, fine-tune how it functions, and measure what kind of impact it has on important performance indicators like operational efficiency and patient outcomes. Once your pilot proves successful, start bringing AI tools into your healthcare operations across the board. Whether it s AI applications in Australian healthcare for administrative tasks or clinical decision support, make sure the tools fit well into your existing workflow so they can have the biggest impact. AI tools only work if your team actually knows how to use them right. Provide thorough training to make sure healthcare professionals understand how to use AI for better patient care, diagnostics, and decision-making. AI systems need constant monitoring and fine-tuning. Keep track of key metrics like AI in healthcare use cases in Australia and check performance regularly to make sure the technology is getting the results you want and actually providing value. Once your AI integration is working well, expand AI use to other parts of your healthcare operations. From resource management to patient engagement, AI healthcare Australia solutions can be put to work in lots of different areas for broader impact. As AI keeps developing and changing, Australia s healthcare landscape is getting ready for even bigger transformations. We re moving past those early experimental applications and heading into a future where AI gets woven deep into the entire system from operating rooms to country-wide health strategies. Here are the major trends that will share the future of AI healthcare in Australia: Get ready for AI to kick off a whole new era of precision medicine, where treatments get customized to each person s genetic makeup and medical background. When it comes to surgery, AI in medicine in Australia will make ultra-precise procedures possible with AI-assisted surgical robotics, cutting down on complications and making recovery times much faster. Public health is about to get a massive overhaul. AI will help monitor disease patterns, predict when outbreaks might happen, and guide interventions across entire populations. AI applications in Australian healthcare will support much better planning, helping the government and healthcare providers handle future crises way more effectively. AI-powered virtual therapists and mental health chatbots are really picking up steam, especially in remote and underserved areas. Businesses in the AI healthcare Australia space will become crucial for delivering accessible, on-demand mental health services throughout the country. Hospitals of the future will come equipped with AI for real-time healthcare decision support. From examining lab reports to suggesting personalized treatment options, AI will help clinicians make faster, data-backed decisions when every second really matters. With healthcare services going digital so quickly, AI in Australian healthcare will become essential for catching fraud. AI can spot suspicious billing, identify weird patterns in claims, and protect both businesses and patients from financial abuse. A major focus going forward is AI-powered integration across healthcare systems. Whether it s connecting hospitals, labs, or clinics, AI will enable smooth data sharing, allowing every provider to access accurate, current patient information exactly when they need it most. While it s already being used, AI-enhanced chronic disease management will expand to cover more conditions and much larger patient populations. From diabetes to heart monitoring, scalable AI tools will help cut down on hospital visits and improve long-term results. Discover how we developed DiabeticU, a dedicated diabetes management app that allows users to take control of their health by leveraging an AI-powered virtual assistant! Behind-the-scenes processes like patient scheduling, inventory tracking, and administrative workflows will become more and more automated with AI healthcare Australia software solutions. This will reduce costs and let medical professionals spend more time on patient care instead of dealing with paperwork. Looking toward the future, AI in healthcare in Australia will make telehealth even more dynamic, with real-time diagnostics, virtual triage, and smart care routing that personalizes treatment while improving access, especially in rural and regional areas. As more businesses want clear answers about AI s value, expect to see increased adoption of generative AI in healthcare in Australia with clear, measurable ROI. Tools that track efficiency, patient outcomes, and financial performance will help justify long-term investments and scale innovation. Partner with us to integrate AI into your operations and lead the future of healthcare. Australian healthcare businesses are jumping on the AI bandwagon more and more to make patient care better, operations smoother, and overall healthcare delivery more effective. These real-world case studies show how AI is getting woven into the healthcare sector on a large scale to tackle challenges and spark innovation. 1. St. Vincent s Health Australia: AI-Powered Patient Flow Optimization St. Vincent s Health Australia, which ranks among the biggest healthcare providers in the country, has brought in AI-driven solutions to make patient flow work better across all its hospitals. By tapping into predictive analytics, this hospital network can predict patient admissions, cut down wait times, and make resource allocation much more effective. How AI is Leveraged: 2. Monash Health: AI for Early Detection and Monitoring Monash Health, Victoria s biggest public health service, is using AI to get better at catching patient deterioration early and improve how clinical decisions get made. By putting AI algorithms to work, Monash Health can spot high-risk patients and step in sooner, making patients safer and reducing emergency admissions. How AI is Leveraged: 3. NSW Health: AI in Predictive Analytics for Resource Management NSW Health has embraced AI for predictive analytics when it comes to managing hospital resources across its whole network of healthcare facilities. AI tools are used to predict patient volumes, optimize how staff get scheduled, and manage medical supplies, making sure operations run efficiently and costs stay down. How AI is Leveraged: Appinventiv stands out as the perfect partner for Australian healthcare businesses that want to tap into AI s incredible potential. With our extensive knowledge in AI solutions and a solid history of success, we help businesses work through the complicated parts of the healthcare sector and put game-changing AI technologies into action. Appinventiv has earned its place as the go-to partner for businesses that are ready to embrace AI in healthcare in Australia and push innovation forward across the entire sector. Get in touch with us now! A. The cost of implementing AI in healthcare in Australia generally ranges from AUD 50,000 to AUD 400,000 or more, depending on several factors. These include the type of AI solution (e.g., predictive analytics, diagnostic support, virtual assistants), integration with existing EMR/EHR systems, data volume and quality, and the level of customization required. Additionally, costs may increase based on regulatory compliance, security protocols, ongoing training, and post-deployment support. Cloud-based AI platforms or modular solutions may offer more affordable entry points for smaller clinics, while enterprise-grade systems for hospitals and networks will be on the higher end of the spectrum. A. AI is revolutionizing healthcare in Australia by enhancing diagnostic accuracy, improving patient care, and streamlining operational processes. With the integration of AI in healthcare in Australia, healthcare providers can automate administrative tasks, predict patient outcomes, and offer personalized treatment plans. As AI continues to evolve, its impact on the Australian healthcare system will expand, leading to more efficient, accessible, and cost-effective care. A. Australian hospitals are increasingly adopting AI healthcare software solutions to enhance operational efficiency. AI is being used to: A. AI can be seamlessly integrated with EMR/EHR systems in Australia to improve patient data management and enhance clinical decision-making. Key integration strategies include: The Australian government offers several grants and partnerships to support AI healthcare innovations. Some of these include: These funding opportunities encourage the development of innovative AI solutions to improve healthcare delivery across the country. Chirag Bhardwaj is a technology specialist with over 10 years of expertise in transformative fields like AI, ML, Blockchain, AR/VR, and the Metaverse. His deep knowledge in crafting scalable enterprise-grade solutions has positioned him as a pivotal leader at Appinventiv, where he directly drives innovation across these key verticals. Chirag s hands-on experience in developing cutting-edge AI-driven solutions for diverse industries has made him a trusted advisor to C-suite executives, enabling businesses to align their digital transformation efforts with technological advancements and evolving market needs. AI in Warehouse Management: Benefits, Use Cases, and Real World Examples Key takeaways: AI-Driven Optimization: AI enhances warehouse efficiency by automating workflows, reducing errors, and optimizing inventory with real-time data. Cost Reduction: AI cuts operational costs by minimizing labor, storage, and return expenses, with McKinsey noting 5-20% logistics cost savings. Improved Forecasting: AI-powered demand forecasting prevents stockouts and overstocking, improving customer satisfaction and cash flow. Space  How to Use AI in Personalized Treatment Plans for Diseases? Key takeaways: AI enables precise, individualized treatment plans by analyzing patient data, genomic profiles, and clinical histories, moving away from generalized protocols. AI models help anticipate medical events and tailor care, improving outcomes through proactive, personalized interventions. AI integrates diverse data sources (EHRs, genetics, imaging) to provide faster, more accurate decision-making in clinical workflows. AI  RAG vs Fine Tuning: Which AI Approach is Best for Your Business? Key takeaways: RAG delivers real-time answers using external data great for fast-changing content. Fine Tuning builds in expertise ideal for regulated, high-accuracy tasks. RAG is quick to launch, Fine Tuning wins on long-term efficiency. Fine Tuning ensures control, RAG offers flexibility and reach. Hybrid models blend both perfect for enterprise-grade AI tools. Choose based on use case RAG for  B-25, Sector 58,Noida- 201301,Delhi - NCR, India 79, Madison Ave Manhattan, NY 10001,USA Appinventiv Australia, East Brisbane QLD 4169, Australia 3rd Floor, 86-90 Paul Street EC2A 4NE London, UK Tiger Al Yarmook Building, 13th floor B-block Al Nahda St - Sharjah Suite 3810, Bankers Hall West,888 - 3rd Street Sw Calgary Alberta Full stack mobile (iOS, Android) and web app design and development agency Appinventiv is the Registered Name of Appinventiv Technologies Pvt. Ltd., a mobile app development company situated in Noida, U.P. India at the street address - B- 25, Sector 58, Noida, U.P. 201301. All the personal information that you submit on the website - (Name, Email, Phone and Project Details) will not be sold, shared or rented to others. Our sales team or the team of mobile app developers only use this information to send updates about our company and projects or contact you if requested or find it necessary. You may opt out of receiving our communication by dropping us an email on - info@appinventiv.com 1600+ transformation engineers delivered 3000+ game-changing products. We chose Appinventiv to build our financial literacy and money management app from start to finish. From the first call, we were very impressed with Appinventiv s professionalism, expertise, and commitment to delivering top-notch results. It has been a pleasure working with Appinventiv. The team is not only extremely versatile and competent but also very professional, courteous, and responsive. We certainly plan to continue working with Appinventiv for an indefinite period. We took a big leap of faith with Appinventiv who helped us translate our vision into reality with the perfectly comprehensive Edamama eCommerce solution. We are counting to get Edamama to launch on time and within budget, while rolling out the next phase of the platform with Appinventiv. I just want to take a moment to thank the entire Appinventiv team for your incredible support. We truly appreciate everything you've done, and we're excited to continue working together as we grow here at KODA After researching numerous companies, we finally found Appinventiv, and it was the best decision we could have made. They successfully addressed the challenges with our existing app and provided solutions that exceeded our expectations. We approached Appinventiv with a clear vision to build a robust and future-ready platform that could seamlessly integrate with the busy lifestyle of our customers while uplifting their overall experience and giving us a competitive edge. 1600+ transformation engineers delivered 3000+ game-changing products. Connect with our consultation experts to get: Insights specific to your business needs Roadmap to overcome your challenges Opportunities to scale your business in this niche.",2
Bias recognition and mitigation strategies in artificial intelligence healthcare applications | npj Digital Medicine - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTFAtaWVUUUxRR2ROWEVweTlmZEVYd3FxdVhnZWtqRVN3dFNDV3RuVjE3VDVhSVRBZ2hHRWZMcUNlSjRITDVpLUd2MVFTc0ZHSFFhLV9aT3FFc1JjMGx5TmVJ?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement npj Digital Medicine volume 8, Article number: 154 (2025) Cite this article 23k Accesses 24 Citations 42 Altmetric Metrics details Artificial intelligence (AI) is delivering value across all aspects of clinical practice. However, bias may exacerbate healthcare disparities. This review examines the origins of bias in healthcare AI, strategies for mitigation, and responsibilities of relevant stakeholders towards achieving fair and equitable use. We highlight the importance of systematically identifying bias and engaging relevant mitigation activities throughout the AI model lifecycle, from model conception through to deployment and longitudinal surveillance. As of May 13, 2024, the Food and Drug Administration (FDA) update indicated an unprecedented surge in the approval of AI-enabled Medical Devices, listing 191 new entries while reaching a total of 882, predominantly in the field of radiology (76%), followed by cardiology (10%) and neurology (4%)1. These approvals reflect AI s growing role in healthcare, including applications such as analyzing medical images, monitoring health metrics through wearable devices, and predicting outcomes from Electronic Medical Records. This illustrates the rapid growth of AI technologies to improve and personalize patient care, not only in the field of medical imaging and diagnostics but across all aspects of healthcare delivery. This growth has been driven by the unique adeptness of AI models to navigate large healthcare datasets and learn complex relationships with reduced processing speed, enabling superior task performance compared to traditional statistical methods or rule-based techniques. However, these models can and have gained complexity, presenting challenges distinct from those encountered by simpler or traditional statistical tools. Specifically, deep learning (DL) models are commonly opaque (i.e., black-box) in nature, lacking explainability or a clear identification of features that influence model performance, thus limiting opportunities for human oversight or an evaluation of biological plausibility2. Central to these challenges is the issue of bias, which can manifest itself in numerous forms to exacerbate existing healthcare disparities. Regulatory bodies, including the European Commission, FDA, Health Canada, the World Health Organization (WHO), have intensified their efforts to establish stricter frameworks for the development and deployment of AI in healthcare, recognizing a critical need to uphold the core principles of fairness, equity, and explainability3,4,5,6,7. Such frameworks aim to systematically identify and mitigate bias to ensure that AI models adhere to ethical principles and do not perpetuate or amplify historical biases or discrimination against vulnerable patient populations. This comprehensive narrative review delves into the unique types of bias commonly encountered in AI healthcare modeling, explores their origins, and offers appropriate mitigation strategies. We begin by highlighting the central role that bias mitigation plays in achieving fairness, equity, and equality for healthcare delivery. This is followed by a detailed review of where and how bias can introduce itself throughout the AI model lifecycle and conclude with strategies to quantify and mitigate bias in healthcare AI. We followed a critical review methodology to objectively explore and consolidate literature related to AI bias in healthcare8. Our review focused on English articles published from 1993 to 2024, sourced from databases such as PubMed, Google Scholar, and publisher platforms like Elsevier. To refine our search, Boolean operators were used, combining keywords inclusive of  Medical AI,   Healthcare AI,   AI bias,   AI ethics,   Responsible AI,   Healthcare disparities,   Fairness in medical AI,   Equity, equality, and fairness in healthcare,   Bias mitigation strategies in AI,   Algorithmic bias,   AI model life cycle,  and  Data diversity.  Each specific type of bias in AI (e.g., selection bias, representation bias, confirmation bias, etc.) was also searched. Additionally, reference lists from selected articles were examined to identify further relevant studies on bias in healthcare AI applications. Studies offering differing perspectives or contradictory evidence were carefully selected to ensure a balanced and comprehensive view of bias in healthcare AI. We included studies published within a broad timeframe to capture both historical and contemporary perspectives on fairness, equity, and bias in healthcare AI. Incorporating foundational references, such as a 1993 publication on equity and equality in health9 was crucial, as these concepts form the basis for understanding and evaluating fairness in contemporary AI applications. Our search yielded 233 potentially relevant articles. After initial screening of titles and abstracts, this was narrowed down to 152 articles with direct relevance to AI bias in healthcare. Following thorough full-text review, 94 articles were selected for our final review, as shown in Fig. 1. We employed a thematic-based approach to identify and categorize recurring themes, patterns, and insights related to bias types and mitigation strategies. This figure illustrates the process used to filter and review literature from 233 initial articles, narrowing down to 94 articles that directly explored AI bias and mitigation strategies in healthcare. In the context of healthcare AI, bias can be defined as any systematic and/or unfair difference in how predictions are generated for different patient populations that could lead to disparate care delivery10. Through this, disparities related to benefit or harm are introduced or exacerbated for specific individuals or groups, eroding the capacity for healthcare to be delivered in a fair and equitable manner11. The concept of  bias in, bias out , a derivative of the classic adage  garbage in, garbage out,  is often implicated when AI model failures are observed in real world settings12, highlighting how biases within training data often manifest as sub-optimal AI model performance out in the wild13. However, bias may be introduced into all stages of an algorithm s life cycle, including their conceptual formation, data collection and preparation, algorithm development and validation, clinical implementation, or surveillance. This complexity is compounded by the inadequacy of methods for routinely detecting or mitigating biases across various stages of an algorithm s life cycle, emphasizing a need for comprehensive and holistic bias detection frameworks14,15. In 2023 Kumar, et al., conducted a study systematically evaluating the burden of bias in contemporary healthcare AI models14. Using the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) selection strategy16, a standardized methodology to estimate risk of bias (ROB), they sampled 48 studies distributed across tabular, imaging, and hybrid data models. They reported that 50% of these studies demonstrated a high ROB, often related to absent sociodemographic data, imbalanced or incomplete datasets, or weak algorithm design. Only 20%, or 1 in 5 studies were considered to have a low ROB. A similar study, performed by Chen, et al., using the PROBAST (Prediction model Risk Of Bias ASsessment Tool) framework17 (For further details, refer to Supplementary Table 1), examined 555 published neuroimaging-based AI models for psychiatric diagnosis, identifying only 86 studies (15.5%) included external validation while 97.5% included only subjects from high-income regions. Overall, 83% of studies were rated at high ROB18. These studies emphasize a critical need for improved awareness of bias in healthcare AI, and the routine adoption of mitigation strategies capable of bridging model conception through to fair and equitable clinical adoption. Fairness, equality, and equity are core principles of healthcare delivery that are directly influenced by bias. Fairness in healthcare encompasses both distributive justice and socio-relational dimensions, requiring a holistic consideration of each individual s unique social, cultural, and environmental factors, these going beyond the concept of equality - which aims to ensure equal access and outcomes19,20. Equity recognizes that certain groups may require tailored resources or support to attain comparable health benefits9. Navigating these nuances and potential trade-offs between core principles is essential, as blanket approaches to fairness may inadvertently reinforce existing disparities21. Defining and measuring fairness metrics, such as demographic parity, equalized odds, equal opportunity, and counterfactual fairness, is a complex challenge that requires a deep understanding of the healthcare context as well as the lived experiences of diverse patient populations. Failure to apply these metrics appropriately can lead to unintended consequences that may undermine the ethical foundations of equitable care, such as perpetuating healthcare disparities, misallocating resources, or reinforcing systemic biases that disproportionately impact vulnerable populations22. Differentiating equality from equity is essential to understanding the influences that AI bias can impose on healthcare disparities. These often-competing principles, as illustrated in Fig. 2, must be iteratively considered to achieve the best possible balance9,23. This figure illustrates the key differences between Equality and Equity in healthcare support. It presents two scenarios: a  Equality  is depicted where each individual, regardless of their needs, receives the same level of support, symbolized by equal height green platforms for all figures. b  Equity  is shown where supports are varied according to individual needs, represented by green platforms of different heights, ensuring that each person reaches the support they need. This visualization underscores the importance of tailoring healthcare resources to address specific needs to achieve true equity. In the following sections, we explore the various forms of bias commonly encountered in healthcare AI. Our discussion, supplemented by relevant examples, has been structured to consider biases introduced from; (i) human origin; (ii) algorithm development; and (iii) algorithm deployment that may exist within each stage of the AI model lifecycle. A graphical illustration of these stages is provided in Fig. 3. This figure maps the stages of the AI model life cycle in healthcare, highlighting the common phase at which biases can be introduced. The AI life cycle is divided into six phases: conception, data collection, pre-processing, in-processing (algorithm development and validation), post-processing (clinical deployment), and post-deployment surveillance. Each phase is prone to specific biases that can affect the fairness, equity, and equality of healthcare delivery. The dominant origin of biases observed in healthcare AI are human. While rarely introduced deliberately, these reflect historic or prevalent human perceptions, assumptions, or preferences that can manifest across various future stages of AI model development, potentially with profound impact24. For example, data collection activities influenced by human bias can lead to the training of algorithms that replicate historical healthcare inequalities, leading to cycled reinforcement where past injustices are perpetuated into future practice25. The different types of human biases that can be introduced are discussed below and summarized in Table 1. Implicit bias occurs when subconscious attitudes or stereotypes about a person s or group s characteristics, such as birth sex, gender identity, race, ethnicity, age, and socioeconomic status, become embedded in how individuals behave or make decisions. This can present surreptitious but powerful influences on medical AI systems trained from these decisions, particularly when features contributing to this bias are not routinely captured by training data26. This is commonly the case for patient self-reported characteristics, such as gender identity and ethnicity (referring to shared cultural practices, perspectives, and beliefs) that are commonly absent from or inconsistently coded by Electronic Health Records (EHR)27. Systemic bias represents an important dimension of human bias. While related to implicit bias, systemic bias extends to encompass broader institutional norms, practices, or policies that can lead to societal harm or inequities. The origins of systemic bias are more structural in nature and act at higher societal levels than implicit bias, often requiring modification in legislation or institutional policies to address28,29. For example, systemic bias may manifest as inadequate medical resource funding for un-insured individuals, underserved communities, or racial and ethnic minority groups, while implicit bias may be observed in a clinician s own subconscious assumptions about a patient s capacity to comply with medical care based on a stereotype. It is important to recognize that human biases can provide influence beyond the contamination of training data. How each model is conceived and designed, and how it is ultimately used or monitored in clinical practice may similarly be influenced by human implicit or systemic biases. During model development, developers may consciously or subconsciously select, interpret, or give more weight to data that confirms their beliefs, overemphasizing certain patterns while ignoring other patterns that don t fit expectations. This form of human-mediated bias is called confirmation bias30,31. How human biases change over time is an important consideration given strong dependencies on the use of large-scale historic training data that are accrued over extended periods of time. While implicit and systemic biases shift with societal influence, models trained from historical data may inadvertently re-introduce unwanted biases into contemporary care. For example, older datasets influenced by an ethnicity bias may lead to AI models that generate skewed predictions across minority groups, despite more inclusive contemporary ideologies. This results in training-serving skew, where shifts in bias alter data distributions between the time of training and model serving32. A similar temporally sensitive bias, known as  Concept shift,  can occur due to changes in the perceived meanings of data. How clinicians perceive and code diseases or outcomes can change dynamically over time, introducing a unique, human-level bias surrounding what data means and is being used for. Addressing this bias can be challenging, requiring involvement from clinicians with experience spanning these shifting practice periods. Effort must be undertaken to ensure awareness of historical data that contains outdated practices, underrepresented groups, or past healthcare disparities not representing current real-world scenarios33. Beyond human influences, numerous additional biases can be introduced to data used in AI model training, altering its representation of the target environment or population and leading to skewed or unfair outcomes34. A summary of biases introduced during data collection are provided in Table 2. Representation bias describes a lack of sufficient diversity in training data and is a dominant form of bias limiting the generalizability of healthcare AI models into unique environments or populations25. This bias can arise from an underlying implicit or systemic bias in the healthcare system or a history of underrepresentation for a minority group, either directly or indirectly from a reluctance to share information or participate in clinical trials35,36,37. These factors can lead to historical gaps in healthcare data, which can now be projected forward into AI healthcare models. For example, convolutional neural networks (CNNs) trained from large chest X-ray datasets sourced from academic healthcare facilities have been shown to underdetect disease in specific patient populations, inclusive of females, Black, Hispanic, and patients of low socioeconomic status38. Representation bias can also be established through selection or sampling bias. Selection bias occurs when the process of how data is chosen or collected inadvertently favors certain groups or characteristics, leading to a non-representative sample. An example of this is the  healthy volunteer  selection bias observed in the UK Biobank, where participants are generally healthy and therefore do not represent patients typically encountered by healthcare systems39. Sampling bias is a form of selection bias resulting from non-random sampling of subgroups, establishing data patterns that are non-generalizable to new populations. Participation bias or self-selection bias are also strong contributors to representation bias in research generated datasets. For example, patients who are sicker, have higher comorbidity, or are unable to participate in longitudinal research protocols for geographic or economic reasons may not be represented10. Large scale data resources required for AI model development and validation are often sourced from multiple hospital sites, each with unique methods of data acquisition. Such variations are often not related to biological factors, rather are related to differences in data acquisition or processing. These measurement biases result in systematic differences that alter the representation of variables, being most commonly encountered in medical diagnostics33. In radiology, for example, imaging hardware manufacturer, model, software versions and acquisition parameters all meaningfully alter the characteristics of diagnostic images40. Similarly, in pathology, variations in tissue preparation, staining protocols, and scanner-specific parameters can impact the data used in cancer-diagnostic tasks41. AI models can inadvertently learn patterns associated with these non-biological variations, causing them to deviate from their primary objective of diagnosing medical conditions. In contrast, models trained from only one data source, adhering to a consistent set of acquisition parameters, may underperform when applied to data from another source34. Algorithmic biases can be considered as those inherent to the pre-processing of a training dataset or during the conceptual design, training, or validation phases of an algorithm. They can stem from the inappropriate selection of non-diverse datasets, features, or selection of algorithm processes set by model developers34. These biases, along with examples, are further detailed in Table 3. A type of algorithmic bias strongly impacting model generalizability is aggregation bias, which occurs during the data preprocessing phase10. Data aggregation is the act of transforming patient data into a format more suitable for algorithm development, including imputing missing values, selecting key variables, combining data from various sources, or engineering new data features. When population data is merged to form a common, model-ready input, biases can emerge through the selection of input features that are maximally available across all subjects, establishing a  one-size-fits-all  approach42. One example of this bias is managing missing or outlier values, such as patient weight. This variable may not be available for certain patients with disabilities, particularly those using wheelchairs, or may be under-representative for those with limb amputations or terminal illnesses43. Models trained using aggregate data may mistakenly assume uniformity across diverse patient groups, may impute missing variables, and inherently overlook unique characteristics or needs within specific subpopulations. Feature selection bias is the selective introduction or removal of variables during model development based on pre-conceived ideas or beliefs surrounding the planned task. Extending beyond human confirmation bias, the forced inclusion of variables hypothesized to influence performance or deemed of high priority, commonly referred to as Sensitive Variables, is a common practice foundationally motivated to avoid bias. Variables, such as age, sex, and race are often deemed  sensitive  because they represent personal characteristics that predictive algorithms should ideally not discriminate against. Accordingly, their inclusion in models may be aimed at representing priority demographic groups who might behave uniquely due to a variety of differences not described by other input features. These sensitive variables therefore act as  proxy variables  (i.e. surrogates) for more complex features not adequately described by training data to enhance the accuracy and equity of healthcare AI models30,44. As an example, including birth sex in cancer prediction models may be suggested to ensure they do not predict a male sex-specific cancer (e.g., prostate cancer) in a female patient45. However, the use of proxy variables over more appropriate source data can paradoxically propagate bias. For example, a study by Obermeyer, et al., exposed racial bias in an algorithm used for predicting healthcare resource needs using a proxy variable of healthcare cost consumption. This model systematically predicted lower healthcare resource need for Black versus White patients despite similar risk levels due to lower historic healthcare spending in Black patient populations46. Expanded discussion of this is provided in Case Study 1 (Box 1). This highlights the profound impact that proxy variable choice can have on algorithmic bias, emphasizing a need for careful selection in the context of both study questions and intended model use. Mitigating Feature Selection Bias in AI Risk Prediction. A widely used AI risk prediction algorithm in the U.S. healthcare system, analyzed by Obermeyer et al. in 2019, included data from 43,539 White patients and 6,079 Black patients (2013 2015). The algorithm, designed to identify high-risk patients based on predicted healthcare costs, exhibited racial bias, underestimating the needs of Black patients. The study found that Black patients had 26.3% more chronic illnesses than White patients at the same risk score level (4.8 vs. 3.8 conditions). This bias stemmed from using healthcare costs as a proxy for illness severity; systemic barriers like reduced healthcare access, financial constraints, and lower trust levels led to lower costs for Black patients, causing the algorithm to misjudge their risk. To address this, researchers recalibrated the algorithm to use direct health indicators, such as chronic condition counts, instead of costs. This change nearly tripled the enrollment of high-risk Black patients in care management programs, from 17.7% to 46.5%, promoting more equitable healthcare. However, ongoing surveillance is necessary, as reliance on historical data and evolving healthcare dynamics could allow biases to re-emerge46. With AI system adoption incentives in the form of workflow efficiency, an over-reliance on AI systems and a progressive de-skilling of the workforce is of genuine concern47. Several biases can be introduced following model deployment directly related to these factors. These biases are listed with examples in Table 4. In the context of healthcare AI applications, automation bias reflects the inappropriate user adoption of inaccurate AI predictions due to the nature of their automated delivery. This bias can manifest in two forms: omission errors and commission errors. Omission errors occur when clinicians fail to notice or ignore errors made by AI tools, especially when the AI makes decisions based on complex details difficult for humans to detect or easily interpret. This is often exacerbated in fast-paced environments, such as radiology. Commission errors, on the other hand, arise when clinicians inappropriately place greater trust in, or follow an AI model s judgment despite conflicting clinical evidence48. Feedback Loop bias occurs when clinicians consistently adopt and accept AI recommendations, even when inaccurate, and these labels are then captured and reinforced by future training cycles. For example, the presentation of an AI labeled dataset to physicians for adjustment, rather than de-novo labeling of the raw data, can reinforce and propagate prior model biases49. In contrast to an over-reliance on AI systems, dismissal bias, commonly referred to as  alert fatigue , can occur when end-users begin to overlook or de-value AI-generated alerts or suggestions. This end-user introduced bias is often exacerbated by high rates of false positives that lead to a progressive distrust of a system s capabilities. This deployment bias has been shown to lead to the dismissal of appropriate critical warnings, with potential risk of patient harm47,49. Establishing standardized and repeatable approaches for mitigating bias is an expanding societal responsibility for AI-healthcare developers and providers23. This task must be recognized as both longitudinal and dynamic in nature, shifting throughout time based on evolving clinical practice, local population needs, and broader societal influence. A valuable concept for establishing frameworks for bias mitigation in AI healthcare applications is considering the  AI Model life cycle , defining key stages where biases can enter, propagate, or persist10. An AI model s life cycle, as illustrated in Fig. 3, includes a conception, data collection and pre-processing, in-processing (algorithm development and validation), post-processing (clinical deployment), and post-deployment surveillance phase. Systematically considering bias across these sequential phases requires a multifaceted approach tailored to identify, quantify, and mitigate its impact on the core principles of fairness, equity, and equality, and maintain the ethical integrity of healthcare AI. Establishing definitions for what a meaningful bias is (e.g., one that is sufficient to mandate mitigation or usage warnings) is not a uniform task and must be independently assessed on a case-by-case basis. However, considerations should be given to setting nominal accuracy performance difference thresholds (e.g., demographic group differences) to enable the routine, automated, and iterative surveillance of AI healthcare models across relevant demographics15. In the following sections, we provide a concise overview of each phase of the AI model lifecycle, respective recommendations for bias mitigation, and discuss the potential challenges and limitations of such strategies. Figure 4 provides an overview of detection and mitigation strategies relevant to each of these phases. Recommendations have been consolidated from various research teams and organizations, offering complementary insights into the effective implementation, management, and potential obstacles of adopting AI systems. We have also compiled a comprehensive overview of additional bias mitigation strategies, guidelines, and frameworks in AI healthcare models, which can be found in Supplementary Table 1. This figure outlines the key strategies for addressing biases at different phases of the AI model life cycle. It highlights critical interventions from the initial conception phase through to post-deployment surveillance, ensuring that each phase in the AI development process incorporates practices aimed at promoting fairness, equity, and effectiveness. The strategies are categorized by the model s lifecycle phases, providing a roadmap for systematic bias mitigation in AI applications. As illustrated in Fig. 3, bias surveillance should begin at time of model conception, demanding a clear, clinically oriented research question from which areas of bias can then be envisioned. This, and all future phases of the model life cycle, should be systematically reviewed by a diverse and representative AI healthcare team including clinical experts, data scientists, institutional stakeholders, and members of underrepresented patient populations. During conception, teams should align with and document adherence to Diversity, Equity, and Inclusion (DEI) principles of the local institution while identifying plans for mitigating imbalances in team membership50 (For further details, refer to Supplementary Table 1). Meetings focussed on refining the research question, populations affected, datasets available, and intended outcomes should concurrently consider any unintended consequences for specific groups defined by race, ethnicity, gender, sex at birth, age, or other socio-demographic characteristics. In these discussions, it is critical to actively eliminate implicit, systemic, and confirmation biases ensuring research questions and AI models are designed to maximally satisfy the principles of fairness and equity across diverse socio-demographic and socio-economic groups while maintaining ethical practice51. Bias mitigation strategies for the conception phase experience unique challenges given their need for upstream introduction. Embedding bias awareness during conceptualization requires prior education and training for all contributing members of the AI development team, which can be challenging to implement and sustain. Beyond improved awareness, critical thinking activities should be routinely engaged to overcome confirmation bias that can exist within teams, maintaining mindfulness of sensitive attribute biases such as age, gender, or ethnicity. This requires constant vigilance and willingness to question established norms. These challenges highlight a need for standardized approaches to stewarding early concepts prior to data processing, demanding a pause for consideration of systemic biases ingrained into historical policies or practice, and ensuring that target populations are fairly and equitably considered50. Data collection efforts should aim to generate datasets that best reflect the diversity of the population each model is intended to serve. It is essential to design data collection processes to capture an appropriate breadth of demographics, thereby allowing for the reporting and meaningful consideration of each dataset s relevance to the target population. Special consideration should be paid to nuances in patient subgroups, such as ethnic diversity or unique characteristics, such as disabilities52. To address common biases of this phase, such as representation, sampling, selection, and measurement biases, practitioners are advised to refer to the specific mitigation strategies outlined in Fig. 4. For example, when training is dominated by historical data known to have inherent limitations or biases, a prospectively captured external validation dataset should be considered to ensure generalizability across relevant sub-cohorts53. During data collection, it is recommended to; use a variety of data sources to enhance diversity; engage accessible data initiatives such as Open Science Practices34 and STANDING together54 (For further details, refer to Supplementary Table 1); make informed decisions on the use of retrospective versus prospective data (acknowledging their respective biases and challenges); assess the accuracy and reliability of data to identify potential biases; and carefully consider inclusion and exclusion criteria as well as recruitment procedures of any clinical trial data being used. The latter commonly leads to an exclusion of specific groups of patients, limiting generalizability to real world practice23. Achieving unbiased, broadly representative healthcare datasets remains a formidable challenge. Addressing data sparsity for underrepresented populations may not be feasible, while laudable efforts to prospectively improve data quality is time consuming. Both sampling and participation biases are notably difficult to eliminate, given that certain groups may be less inclined to participate in data collection or sharing activities, despite outreach efforts. Additionally, standardizing data collection methods to reduce measurement bias is resource intensive. While initiatives like Open Science Practices and STANDING34,54 aspire to improve the diversity of data resources available for healthcare AI, vulnerable groups may remain under-represented given a relative abundance of resources from Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations. Finally, certain socio-demographic features, such as gender orientation, remain poorly captured by health systems and are not reliably inferred. These ongoing limitations emphasize a rapidly evolving need for targeted data collection and democratization strategies to facilitate fair and equitable healthcare AI23,52. The pre-processing phase encompasses a range of tasks that are undertaken to clean and prepare raw data for model development. The mitigation of bias during this phase requires careful attention to the management of missing data, selection of relevant variables, and feature engineering to ensure appropriate data diversity, representation, and the generation of balanced sub-samples for model validation10,55. Failure to execute these techniques appropriately can introduce variance or sensitivity to data shifts, underscoring the importance of meticulous attention to prevent bias propagation33. Specifically, it is recommended to review data collection methods and demographic distributions to maximize demographic representation, assess accuracy and stability of input variables across minority groups, and consider appropriate data augmentation techniques to address sub-group imbalance. Biases such as aggregation, missing data, feature selection, and representation biases are particularly prevalent during this phase, demanding focused attention to ensure these do not compromise model integrity15,56. Details surrounding specific pre-processing bias mitigation strategies are presented in Fig. 4. Applying these strategies must be done with appropriate understanding of their limitations and should be tailored to specific contexts as inappropriate choices or execution can inadvertently amplify rather than mitigate bias. For example, while aiming to reduce aggregation bias, shifts toward disaggregated data can lead to overly granular datasets prone to noise, leading to reduced model generalizability42. When handling missing data bias, multiple imputation may introduce inaccuracies, particularly for non-random missingness or substantial data gaps43. Data augmentation to address sparsity can generate synthetic data that fails to appropriately reflect true diversity, thus reinforcing biases42. Accordingly, nuanced understandings of bias mitigation techniques and their appropriate use is required during this phase. In-processing represents all activities surrounding the training and validation phase of an AI algorithm. Potential biases introduced during this phase, including algorithmic, validation, and representation bias, must be intentionally sought and addressed10. We provide an example in Case Study 2 (Box 2). This demands iterative participation from the healthcare AI team to anticipate scenarios that could seed biased model behavior50 Beyond stratified sub-group analyses, counterfactual examples should be considered to test these hypotheses during validation, purposely altering a candidate feature (e.g., ethnicity) to assess its systematic (biased) influence on model performance57 (For further details, refer to Supplementary Table 1). However, generating meaningful counterfactual examples requires a deep understanding of the dataset and relationships between features. Moreover, if not carefully implemented, counterfactual examples can lead to overfitting or produce unrealistic scenarios, thereby reducing the model s real-world applicability. Additional limitations include difficulties in finding sufficient representative data for counterfactual examples, which could skew the results57. An alternate, albeit resource and time intensive method to identifying algorithmic bias is  Red Teaming 58. This is a process where an independent team attempts to identify biases or other vulnerabilities in an AI model, determining whether certain conditions, such as unique demographic distributions, alters performance. This may not be practical for all organizations, especially for small sized teams with limited budgets. Under-representation of minority classes is a ubiquitously encountered challenge for healthcare datasets that should be acknowledged during model training and validation59. To address class imbalance, strategies such as resampling to balance class distributions60, synthetic data generation (such as Synthetic Minority Over-sampling Technique (SMOTE))61, and the application of cost-sensitive learning to emphasize minority class errors should be considered62. The latter is a technique where misclassifying examples from the minority class is penalized more heavily than examples from the majority class, assisting in balancing model performance. However, these strategies have limitations. SMOTE, for example, generates synthetic samples by interpolating between minority class case examples, which can result in unrealistic data points that do not accurately reflect true variability61. Random Under-Sampling, on the other hand, risks discarding potentially valuable data from the majority class, leading to information loss60. Cost-sensitive learning can lead to overfitting, especially when costs assigned to the minority classes are high62. Regardless of the techniques employed, appropriate evaluation metrics for imbalanced datasets should be used, such as F1 score and precision-recall curves. These can also serve as cost functions during training to improve model generalizability63. Stratified cross-validation can assist in establishing representative class proportions within each fold to improve model generalizability64. This technique is reliant on the availability of large data resources and may not fully account for minority subgroups when not sufficiently represented. Fairness metrics like demographic parity, equal opportunity, equalized odds, and causal fairness (Table 5) can be leveraged to quantify and monitor for algorithmic bias. However, the application of these approaches can result in a  fairness-accuracy trade-off , as striving for equitable treatment across different groups may result in a reduction in overall model accuracy59,60. Federated learning techniques have gained popularity to improve model access to diverse datasets across unique healthcare environments, enabling a more collaborative and decentralized approach that ensures data privacy while reducing resource needs for model training65 (For further details, refer to Supplementary Table 1). However, while enhancing model generalizability, federated learning inherently limits team access for data pre-processing or quality assurance tasks, limiting its appropriateness for specific applications. Adversarial training, which involves training a model to be less influenced by sensitive attributes (e.g., race, gender) by introducing adversarial examples, can be effective in reducing representation bias (For further details, refer to Supplementary Table 1). However, this technique can be computationally intensive and may lead to reduced model accuracy if adversarial examples do not accurately reflect real-world variation, again introducing a  fairness-accuracy trade-off 44. Transfer learning is an approach that can be used to efficiently train models to perform tasks leveraging knowledge gained from historic models trained to perform similar or unique tasks. This can be used to fine-tune externally trained models using smaller quantities of local data, reducing the potential for external algorithmic bias. However, it must be recognized this may transfer unrecognized biases inherent to the original environment s dataset, potentially perpetuating these biases into new healthcare systems53. Model architecture choices can directly influence the transparency or interpretability of generated predictions. For example, while decision tree models provide clear feature usage insights, deep learning models offer limited insights66. However, techniques like LIME (Local Interpretable Model-agnostic Explanations)67 and SHAP (Shapley Additive exPlanations) can help decipher feature importance in complex models68 (For further details, refer to Supplementary Table 1). While valuable, these techniques can sometimes be inconsistent or fail to capture the true decision-making process of highly complex models, potentially leading to a false sense of interpretability66. The use of ensemble methods, where multiple models are combined to improve prediction accuracy, is one example where computational complexity is increased and interpretability reduced59. In general, model complexity should be minimized, and architectures chosen that maximize explainability, permitting the greater detectability and awareness of bias66. Following model optimization, external validation should be used whenever feasible to assess performance across diverse environments, patient demographics, and clinical characteristics. Validation across multiple independent settings is recommended for models intended for use beyond the local institution, such as for commercial distribution. Obtaining access to diverse external datasets for validation is often challenging due to privacy concerns, data-sharing restrictions, and variability of data quality across different institutions, however, is of paramount importance when models are intended for use beyond local environments25. The size and number of unique cohorts required for external validation varies based on the model s application and target population s diversity, but remains central to confirming performance consistency53. Finally, it is essential to meticulously document each algorithm s development methodologies, ensuring clear descriptions of the target population, its accuracy, and limitations69. This phase encompasses a model s implementation in live clinical environments70. Human-machine interface design and choice of platform deployment will influence accessibility, while planned reimbursement models can threaten equitable delivery across socio-economic groups. Adherence to Human-in-the-loop (HITL) strategies, where human experts review all model predictions, is recommended for clinical decision making71,72. Transparent disclosure of each model s training population demographic distributions is recommended to declare potential biases and to avoid using models in under-represented populations. The reporting of model performance for relevant sub-populations, as permitted by data resources, is also strongly recommended. Model threshold adjustments can deliver improved responsiveness to user-entered data, such as a patient s or clinician s preferences, recognizing individual opinions or beliefs surrounding a given prediction task73. As stated earlier, tools to enhance model explainability for end-users are available and may improve both trust and adoption. Simple but informative ways to incorporate such insights should be considered to improve transparency and interpretability. For image-based predictions, saliency maps can be employed to highlight regions where model predictions are most strongly influenced. For traditional machine learning (ML) models, SHAP values can be used to demonstrate the relative importance and influence of data features74. It must be recognized that these tools may fall short in explaining complex model behaviors and may provide oversimplified or misleading insights that could falsely increase end-user trust. Structured pre-deployment testing across different clinical environments and populations is recommended to identify unforeseen biases in human-machine interactions. This includes shadow deployment in live clinical environments where model results do not influence clinical behavior but are assessed for their calibration, end-user adoption, and user experience to identify barriers to fair and equitable use75,76. This process can take time and delay the model s full implementation. An example of implementing these strategies is seen in the DECIDE-AI guidelines (Developmental and Exploratory Clinical Investigation of DEcision support systems driven by Artificial Intelligence)77, which offer a structured approach for the early-stage clinical deployment of AI decision support systems (For further details, refer to Supplementary Table 1). By focusing on human-AI interaction, transparent reporting, and iterative validation, these guidelines aim to ensure AI models are both effective and safe in clinical practice. Such frameworks are critical to ensure the transparency and safety of AI models in clinical environments, however, are resource-intensive and time-consuming. Therefore, centralization of institutional resources and processes to support these pathways is essential. This final phase encompasses post-deployment activities for AI models in active healthcare environments, inclusive of performance surveillance, model maintenance, and re-calibration (data augmentation and re-weighting)78. Mechanisms to monitor user engagement, decision impact, and model accuracy versus standard care pathways should be adopted and purposely bridged to patient demographics to identify biased sub-group behavior or downstream inequities in clinical benefit. This is a life-long process, recognizing the potential for concept drift, feedback loop-bias, degradation in fairness metrics, or new biases emerging over time (Fig. 4). As a novel and emerging challenge for healthcare institutions, attention must be raised to administrators and practitioners that data destined for live AI models must be considered as a regulated data product, demanding ongoing quality assurance and maintenance. In this context, adhering to established guidelines and frameworks is essential to ensure sustained accuracy and equity of AI algorithms79. The FDA s Proposed Regulatory Framework for AI/ML-based Software as a Medical Device (SaMD) emphasizes a need for real-world performance monitoring, inclusive of tracking model performance to identify established or emerging bias80. While this places meaningful responsibility on AI model providers, healthcare institutions must hold responsibility to maintain an awareness of their local populations and clinical practice shifts that are commonly opaque to commercial platforms. Accordingly, programmatic approaches to AI model surveillance in clinical environments is an expanding priority for healthcare providers81. Addressing Representation Bias (Racial) in Deep Learning-Based Cardiac MRI Segmentation. A deep learning model (nnU-Net) for cardiac MRI segmentation, trained on UK Biobank data of 5,903 subjects (80% White, 20% Black, Asian, Chinese, Mixed, and Other), initially showed racial bias, with the Dice Similarity Coefficient (DSC) at 93.5% for White subjects but as low as 84.5% for Black and Mixed-race subjects. Researchers used three distinct strategies to address this: (1) stratified batch sampling (balancing racial groups in each batch during pre-processing phase) improved DSC for Black subjects from 85.88% to 93.07% and for Mixed-race from 84.52% to 93.84%, though overall accuracy decreased slightly; (2) fair meta-learning (using a secondary classifier to predict race during in-processing phase) raised DSC to 89.60% for Black and 88.03% for Mixed-race but increased complexity; and (3) protected group models (training separate models for each group during in-processing phase) achieved the best results, with DSC reaching 92.15% for Black and 93.17% for Mixed-race subjects, reducing bias to a standard deviation of 0.89. However, this approach required racial information during inference, limiting its practicality99. Given the rapid expansion of AI healthcare innovation, there is immediate need for the meaningful incorporation of DEI principles across all phases of the AI model life cycle, inclusive of structured bias surveillance and mitigation frameworks82. This must be inclusive of actively educating and training a more diverse and representative AI developer community, implementing, and expanding institutional programs focussed on AI model assessment and surveillance, and development of AI healthcare-specific clinical practice guidelines. Adequately addressing these needs will remain a challenge due to the rapid pace of AI advancement relative to legislative, regulatory, and practice guideline development. Regardless, policy makers, clinicians, researchers, and patient advocacy groups must coordinate to enhance diversity in AI healthcare models. Incorporating DEI principles must be perceived as an essential priority, particularly given a lack of representation in current regulatory guidelines for AI applications50,83. Moreover, there is a critical need to integrate AI and machine learning content into medical training curricula. This will prepare healthcare professionals for a future where data-driven decision-making is increasingly considered the standard of care. Understanding AI, its potential biases, and ethical implications will therefore be crucial for these individuals to appropriately contribute to its refinement and appropriate clinical use84. In the evolving landscape of healthcare delivery, one increasingly influenced by AI technology, recognizing, and mitigating bias is a priority. While essential for achieving accuracy and reliability from AI innovations, addressing bias is core to upholding the ethical standards of healthcare, ensuring a future where care is delivered with fairness and equity. This ensures that AI will serve as a tool for bridging gaps in healthcare, not widening them. Data sharing is not applicable to this article as no datasets were generated or analyzed during the current study. This review article is based on previously published studies and does not contain original research data. All data supporting the findings of this study are available within the article and its references. Health, C. for D. and R. Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices. FDA (2024). Al Kuwaiti, A. et al. A review of the role of artificial intelligence in healthcare. J. Pers. Med 13, 951 (2023). Article PubMed PubMed Central Google Scholar WHO calls for safe and ethical AI for health. https://www.who.int/news/item/16-05-2023-who-calls-for-safe-and-ethical-ai-for-health. WHO outlines considerations for regulation of artificial intelligence for health. https://www.who.int/news/item/19-10-2023-who-outlines-considerations-for-regulation-of-artificial-intelligence-for-health. Da Silva, M., Flood, C. M., Goldenberg, A. & Singh, D. Regulating the Safety of Health-Related Artificial Intelligence. Health. Policy 17, 63 77 (2022). Google Scholar AI pitfalls and what not to do: mitigating bias in AI | British Journal of Radiology | Oxford Academic. https://academic.oup.com/bjr/article/96/1150/20230023/7498925. Directorate-General for Parliamentary Research Services (European Parliament), Lekadir, K., Quaglio, G., Tselioudis Garmendia, A. & Gallin, C. Artificial Intelligence in Healthcare: Applications, Risks, and Ethical and Societal Impacts. (Publications Office of the European Union, 2022). Grant, M. J. & Booth, A. A typology of reviews: an analysis of 14 review types and associated methodologies. Health Inf. Libraries J. 26, 91 108 (2009). Article Google Scholar Culyer, A. J. & Wagstaff, A. Equity and equality in health and health care. J. Health Econ. 12, 431 457 (1993). Article CAS PubMed Google Scholar Nazer, L. H. et al. Bias in artificial intelligence algorithms and recommendations for mitigation. PLOS Digital Health 2, e0000278 (2023). Article PubMed PubMed Central Google Scholar DeCamp, M. & Lindvall, C. Latent bias and the implementation of artificial intelligence in medicine. J. Am. Med. Inform. Assoc. 27, 2020 2023 (2020). Article PubMed PubMed Central Google Scholar Hanson, B. et al. Garbage in, garbage out: mitigating risks and maximizing benefits of AI in research. Nature 623, 28 31 (2023). Article CAS PubMed Google Scholar Burlina, P., Joshi, N., Paul, W., Pacheco, K. D. & Bressler, N. M. Addressing Artificial Intelligence Bias in Retinal Diagnostics. Transl. Vis. Sci. Technol. 10, 13 (2021). Article PubMed PubMed Central Google Scholar Kumar, A. et al. Artificial intelligence bias in medical system designs: a systematic review. Multimed. Tools Appl 83, 18005 18057 (2024). Article Google Scholar Chin, M. H. et al. Guiding Principles to Address the Impact of Algorithm Bias on Racial and Ethnic Disparities in Health and Health Care. JAMA Netw. Open 6, e2345050 (2023). Article PubMed PubMed Central Google Scholar Page, M. J. et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 372, n71 (2021). Article PubMed PubMed Central Google Scholar Wolff, R. F. et al. PROBAST: A Tool to Assess the Risk of Bias and Applicability of Prediction Model Studies. Ann. Intern Med 170, 51 58 (2019). Article PubMed Google Scholar Chen, Z. et al. Evaluation of Risk of Bias in Neuroimaging-Based Artificial Intelligence Models for Psychiatric Diagnosis: A Systematic Review. JAMA Netw. Open 6, e231671 (2023). Article PubMed PubMed Central Google Scholar Giovanola, B. & Tiribelli, S. Beyond bias and discrimination: redefining the AI ethics principle of fairness in healthcare machine-learning algorithms. AI Soc. 38, 549 563 (2023). Article PubMed Google Scholar Pu, L. Fairness of the Distribution of Public Medical and Health Resources. Front Public Health 9, 768728 (2021). Article PubMed PubMed Central Google Scholar Fletcher, R. R. Nakeshimana, A. & Olubeko, O. Addressing Fairness, Bias, and Appropriate Use of Artificial Intelligence and Machine Learning in Global Health. Front. Artif. Intell. 3, 561802 (2021). Article PubMed PubMed Central Google Scholar Xu, J. et al. Algorithmic fairness in computational medicine. eBioMedicine 84, 104250 (2022). Article PubMed PubMed Central Google Scholar Abr moff, M. D. et al. Considerations for addressing bias in artificial intelligence for health equity. npj Digit. Med. 6, 1 7 (2023). Article Google Scholar Jackson, M. C. Artificial Intelligence & Algorithmic Bias: The Issues with Technology Reflecting History & Humans Notes & Comments. J. Bus. Tech. L. 16, 299 316 (2021). Google Scholar Celi, L. A. et al. Sources of bias in artificial intelligence that perpetuate healthcare disparities A global review. PLOS Digital Health 1, e0000022 (2022). Article PubMed PubMed Central Google Scholar FitzGerald, C. & Hurst, S. Implicit bias in healthcare professionals: a systematic review. BMC Med Ethics 18, 19 (2017). Article PubMed PubMed Central Google Scholar Implicit bias of encoded variables: frameworks for addressing structured bias in EHR GWAS data | Human Molecular Genetics | Oxford Academic. https://academic.oup.com/hmg/article/29/R1/R33/5899023?login=true. Feagin, J. & Bennefield, Z. Systemic racism and U.S. health care. Soc. Sci. Med. 103, 7 14 (2014). Article PubMed Google Scholar Payne, B. K. & Hannay, J. W. Implicit bias reflects systemic racism. Trends Cogn. Sci. 25, 927 936 (2021). Article PubMed Google Scholar Elston, D. M. Confirmation bias in medical decision-making. J. Am. Acad. Dermatol. 82, 572 (2020). Article PubMed Google Scholar Althubaiti, A. Information bias in health research: definition, pitfalls, and adjustment methods. J. Multidiscip. Healthc. 9, 211 217 (2016). Article PubMed PubMed Central Google Scholar Feng, Q., Du, M., Zou, N. & Hu, X. Fair Machine Learning in Healthcare: A Review. Preprint at https://doi.org/10.48550/arXiv.2206.14397 (2024). Chen, R. J. et al. Algorithmic fairness in artificial intelligence for medicine and healthcare. Nat. Biomed. Eng. 7, 719 742 (2023). Article PubMed PubMed Central Google Scholar Norori, N., Hu, Q., Aellen, F. M., Faraci, F. D. & Tzovara, A. Addressing bias in big data and AI for health care: A call for open science. Patterns (N. Y) 2, 100347 (2021). Article PubMed Google Scholar Ekpo, E. et al. Underrepresentation of Women in Reduced Ejection Heart Failure Clinical Trials With Improved Mortality or Hospitalization. JACC: Adv. 3, 100743 (2024). PubMed Google Scholar Gomez, S. E., Sarraju, A. & Rodriguez, F. Racial and Ethnic Group Underrepresentation in Studies of Adverse Pregnancy Outcomes and Cardiovascular Risk. J. Am. Heart Assoc. 11, e024776 (2022). Article PubMed PubMed Central Google Scholar Scharff, D. P. et al. More than Tuskegee: Understanding Mistrust about Research Participation. J. Health Care Poor Underserved 21, 879 897 (2010). Article PubMed PubMed Central Google Scholar Gaube, S. et al. Do as AI say: susceptibility in deployment of clinical decision-aids. npj Digit. Med. 4, 1 8 (2021). Article Google Scholar Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K. & Galstyan, A. A survey on bias and fairness in machine learning. ACM Comput. Surv. (CSUR) 54, 1 35 (2021). Article Google Scholar Castro, D. C., Walker, I. & Glocker, B. Causality matters in medical imaging. Nat. Commun. 11, 3673 (2020). Article CAS PubMed PubMed Central Google Scholar Dehkharghanian, T. et al. Biased data, biased AI: deep networks predict the acquisition site of TCGA images. Diagnostic Pathol. 18, 67 (2023). Article Google Scholar Cirillo, D. et al. Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare. npj Digit. Med. 3, 1 11 (2020). Article Google Scholar Nijman, S. et al. Missing data is poorly handled and reported in prediction model studies using machine learning: a literature review. J. Clin. Epidemiol. 142, 218 229 (2022). Article PubMed Google Scholar Siddique, S. et al. Survey on Machine Learning Biases and Mitigation Techniques. Digital 4, 1 68 (2024). Article Google Scholar Lu, M. Y. et al. AI-based pathology predicts origins for cancers of unknown primary. Nature 594, 106 110 (2021). Article CAS PubMed Google Scholar Obermeyer, Z., Powers, B., Vogeli, C. & Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health of populations. Science 366, 447 453 (2019). Article CAS PubMed Google Scholar Yoo, J., Hur, S., Hwang, W. & Cha, W. C. Healthcare Professionals  Expectations of Medical Artificial Intelligence and Strategies for its Clinical Implementation: A Qualitative Study. Health. Inf. Res 29, 64 74 (2023). Article Google Scholar Neri, E., Coppola, F., Miele, V., Bibbolino, C. & Grassi, R. Artificial intelligence: Who is responsible for the diagnosis? Radio. Med 125, 517 521 (2020). Article Google Scholar Ueda, D. et al. Fairness of artificial intelligence in healthcare: review and recommendations. Jpn J. Radio. 42, 3 15 (2024). Article Google Scholar Cachat-Rosset, G. & Klarsfeld, A. Diversity, Equity, and Inclusion in Artificial Intelligence: An Evaluation of Guidelines. Appl. Artif. Intell. 37, 2176618 (2023). Article Google Scholar Wiens, J. et al. Do no harm: a roadmap for responsible machine learning for health care. Nat. Med 25, 1337 1340 (2019). Article CAS PubMed Google Scholar Ramirez, A. H. et al. The All of Us Research Program: Data quality, utility, and diversity. Patterns (N. Y) 3, 100570 (2022). Article PubMed Google Scholar Yang, J., Soltan, A. A. S. & Clifton, D. A. Machine learning generalizability across healthcare settings: insights from multi-site COVID-19 screening. npj Digit. Med. 5, 1 8 (2022). Article Google Scholar Ganapathi, S. et al. Tackling bias in AI health datasets through the STANDING Together initiative. Nat. Med 28, 2232 2233 (2022). Article CAS PubMed Google Scholar Kamiran, F. & Calders, T. Data preprocessing techniques for classification without discrimination. Knowl. Inf. Syst. 33, 1 33 (2012). Article Google Scholar Albahra, S. et al. Artificial intelligence and machine learning overview in pathology & laboratory medicine: A general review of data preprocessing and basic supervised concepts. Semin. Diagnostic Pathol. 40, 71 87 (2023). Article Google Scholar Wachter, S., Mittelstadt, B. & Russell, C. Counterfactual explanations without opening the black box: Automated decisions and the GDPR. Harv. JL & Tech. 31, 841 (2017). Google Scholar Feffer, M., Sinha, A., Deng, W. H., Lipton, Z. C. & Heidari, H. Red-Teaming for generative AI: Silver bullet or security theater? In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. Vol. 7, 421 437 https://doi.org/10.1609/aies.v7i1.31647 (2024). Wang, Y.-C. & Cheng, C.-H. A multiple combined method for rebalancing medical data with class imbalances. Computers Biol. Med. 134, 104527 (2021). Article Google Scholar Kim, A. & Jung, I. Optimal selection of resampling methods for imbalanced data with high complexity. PLoS One 18, e0288540 (2023). Article CAS PubMed PubMed Central Google Scholar Chawla, N. V., Bowyer, K. W., Hall, L. O. & Kegelmeyer, W. P. SMOTE: Synthetic Minority Over-sampling Technique. J. Artif. Intell. Res. 16, 321 357 (2002). Article Google Scholar Ling, C. X. & Sheng, V. S. Cost-Sensitive Learning and the Class Imbalance Problem. Scholz, D. et al. (2024). Imbalance-aware loss functions improve medical image classification. in. Wilimitis, D. & Walsh, C. G. Practical Considerations and Applied Examples of Cross-Validation for Model Development and Evaluation in Health Care: Tutorial. JMIR AI 2, e49023 (2023). Article PubMed PubMed Central Google Scholar Nguyen, D. C. et al. Federated Learning for Smart Healthcare: A Survey. ACM Comput. Surv. 55, 1 37 (2023). Google Scholar Linardatos, P. Papastefanopoulos, V. & Kotsiantis, S. Explainable AI: A Review of Machine Learning Interpretability Methods. Entropy 23, 18 (2021). Article Google Scholar Zhang, Y., Song, K., Sun, Y., Tan, S. & Udell, M.  Why Should You Trust My Explanation?  Understanding Uncertainty in LIME Explanations. Preprint at https://doi.org/10.48550/arXiv.1904.12991 (2019). Lundberg, S. M., Erion, G. G. & Lee, S.-I. Consistent Individualized Feature Attribution for Tree Ensembles. Preprint at https://doi.org/10.48550/arXiv.1802.03888 (2019). Amann, J. et al. Explainability for artificial intelligence in healthcare: a multidisciplinary perspective. BMC Med Inf. Decis. Mak. 20, 310 (2020). Article Google Scholar Challen, R. et al. Artificial intelligence, bias and clinical safety. BMJ Qual. Saf. 28, 231 237 (2019). Article PubMed PubMed Central Google Scholar Mittermaier, M., Raza, M. & Kvedar, J. C. Collaborative strategies for deploying AI-based physician decision support systems: challenges and deployment approaches. NPJ Digital Med. 6, 137 (2023). Article Google Scholar Mosqueira-Rey, E., Hern ndez-Pereira, E., Alonso-R os, D., Bobes-Bascar n, J. & Fern ndez-Leal,  . Human-in-the-loop machine learning: a state of the art. Artif. Intell. Rev. 56, 3005 3054 (2023). Article Google Scholar Pfohl, S. et al. Net benefit, calibration, threshold selection, and training objectives for algorithmic fairness in healthcare. in Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency 1039 1052 https://doi.org/10.1145/3531146.3533166 (Association for Computing Machinery, New York, NY, USA, 2022). Dykstra, S. et al. Machine learning prediction of atrial fibrillation in cardiovascular patients using cardiac magnetic resonance and electronic health information. Front. Cardiovasc. Med. 9, 998558 (2022). Article CAS PubMed PubMed Central Google Scholar Bizzo, B. C. et al. Addressing the Challenges of Implementing Artificial Intelligence Tools in Clinical Practice: Principles From Experience. J. Am. Coll. Radiol. 20, 352 360 (2023). Article PubMed Google Scholar Daye, D. et al. Implementation of Clinical Artificial Intelligence in Radiology: Who Decides and How? Radiology 305, 555 563 (2022). Article PubMed Google Scholar Vasey, B. et al. Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI. Nat. Med 28, 924 933 (2022). Article CAS PubMed Google Scholar Thomas, L. et al. Real-world post-deployment performance of a novel machine learning-based digital health technology for skin lesion assessment and suggestions for post-market surveillance. Front Med (Lausanne) 10, 1264846 (2023). Article PubMed Google Scholar Widner, K. et al. Lessons learned from translating AI from development to deployment in healthcare. Nat. Med 29, 1304 1306 (2023). Article CAS PubMed Google Scholar Health, C. for D. and R. Artificial Intelligence and Machine Learning in Software as a Medical Device. FDA (2023). Abr moff, M. D. et al. Foundational Considerations for Artificial Intelligence Using Ophthalmic Images. Ophthalmology 129, e14 e32 (2022). Article PubMed Google Scholar Siala, H. & Wang, Y. SHIFTing artificial intelligence to be responsible in healthcare: A systematic review. Soc. Sci. Med. 296, 114782 (2022). Article PubMed Google Scholar Nyariro, M., Emami, E. & Abbasgholizadeh Rahimi, S. Integrating Equity, Diversity, and Inclusion throughout the lifecycle of Artificial Intelligence in health. in 13th Augmented Human International Conference 1 4 https://doi.org/10.1145/3532530.3539565 (Association for Computing Machinery, New York, NY, USA, 2022). Grunhut, J., Marques, O. & Wyatt, A. T. M. Needs, Challenges, and Applications of Artificial Intelligence in Medical Education Curriculum. JMIR Med. Educ. 8, e35587 (2022). Article PubMed PubMed Central Google Scholar Lai, J. C., Pomfret, E. A. & Verna, E. C. Implicit bias and the gender inequity in liver transplantation. Am. J. Transpl. 22, 1515 1518 (2022). Article Google Scholar Zbierajewski-Eischeid, S. J. & Loeb, S. J. Myocardial infarction in women: promoting symptom recognition, early diagnosis, and risk assessment. Dimens Crit. Care Nurs. 28, 1 6 (2009). ; quiz 7 8. Article PubMed Google Scholar McGowan, S. K., Sarigiannis, K. A., Fox, S. C., Gottlieb, M. A. & Chen, E. Racial Disparities in ICU Outcomes: A Systematic Review. Crit. Care Med. 50, 1 (2022). Article PubMed Google Scholar Daneshjou, R. et al. Disparities in dermatology AI performance on a diverse, curated clinical image set. Sci Adv 8, eabq6147. Heslin, K. C. et al. Trends in Opioid-related Inpatient Stays Shifted After the US Transitioned to ICD-10-CM Diagnosis Coding in 2015. Med. Care 55, 918 (2017). Article PubMed Google Scholar Puyol-Ant n, E. et al. Fairness in Cardiac Magnetic Resonance Imaging: Assessing Sex and Racial Bias in Deep Learning-Based Segmentation. Front. Cardiovasc. Med. 9, 859310 (2022). Article PubMed PubMed Central Google Scholar Guo, L. N., Lee, M. S., Kassamali, B., Mita, C. & Nambudiri, V. E. Bias in, bias out: Underreporting and underrepresentation of diverse skin types in machine learning research for skin cancer detection-A scoping review. J. Am. Acad. Dermatol 87, 157 159 (2022). Article PubMed Google Scholar Rusanov, A., Weiskopf, N. G., Wang, S. & Weng, C. Hidden in plain sight: bias towards sick patients when sampling patients with sufficient electronic health record data for research. BMC Med Inf. Decis. Mak. 14, 51 (2014). Article Google Scholar Enzenbach, C., Wicklein, B., Wirkner, K. & Loeffler, M. Evaluating selection bias in a population-based cohort study with low baseline participation: the LIFE-Adult-Study. BMC Med. Res. Methodol. 19, 135 (2019). Article PubMed PubMed Central Google Scholar Ka mierczak, I., Zajenkowska, A., Rogoza, R., Jonason, P. K. &  ciga a, D. Self-selection biases in psychological studies: Personality and affective disorders are prevalent among participants. PLOS ONE 18, e0281046 (2023). Article PubMed PubMed Central Google Scholar Delgado, J. et al. Bias in algorithms of AI systems developed for COVID-19: A scoping review. J. Bioeth. Inq. 19, 407 419 (2022). Article PubMed PubMed Central Google Scholar Dratsch, T. et al. Automation Bias in Mammography: The Impact of Artificial Intelligence BI-RADS Suggestions on Reader Performance. Radiology 307, e222176 (2023). Article PubMed Google Scholar Wysocki, O. et al. Assessing the communication gap between AI models and healthcare professionals: Explainability, utility and trust in AI-driven clinical decision-making. Artif. Intell. 316, 103839 (2023). Article Google Scholar Drew, B. J. et al. Insights into the Problem of Alarm Fatigue with Physiologic Monitor Devices: A Comprehensive Observational Study of Consecutive Intensive Care Unit Patients. PLoS One 9, e110274 (2014). Article PubMed PubMed Central Google Scholar Puyol-Anton, E. et al. Fairness in cardiac MR image analysis: an investigation of bias due to data imbalance in deep learning-based segmentation. Med. Image. ComputComput. Assist. Interv.   MICCAI 12903, 413 423 (2021). Google Scholar Download references Libin Cardiovascular Institute, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada Fereshteh Hasanzadeh & James A. White Department of Medicine, Cumming School of Medicine, University of Calgary, Calgary, AB, Canada Colin B. Josephson & James A. White Morgan State University, Center for Equitable AI & Machine Learning Systems, Baltimore, MD, USA Gabriella Waters Department of Cardiovascular Medicine, Mayo Clinic, Jacksonville, FL, USA Demilade Adedinsewo Department of Cardiovascular Medicine, Stanford University, Stanford, CA, USA Zahra Azizi Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar F.H. conducted all literature reviews, synthesized co-author contributions, and drafted all components of the manuscript. J.A.W. led conceptual framework development, contribution coordination, and performed structural revisions throughout the writing process. C.B.J., G.W., D.A., and Z.A. each provided domain-relevant expertise and guidance toward manuscript structure and content, performed content revision, and made edits to the final collated manuscript. All authors have read and approved the final version of the manuscript. Correspondence to James A. White. Dr. Demilade Adedinsewo is supported by the Mayo Building Interdisciplinary Research Careers in Women s Health (BIRCWH) Program funded by the National Institutes of Health [grant number K12 AR084222]. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. All other authors declare no competing financial or non-financial interests. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions Hasanzadeh, F., Josephson, C.B., Waters, G. et al. Bias recognition and mitigation strategies in artificial intelligence healthcare applications. npj Digit. Med. 8, 154 (2025). https://doi.org/10.1038/s41746-025-01503-7 Download citation Received: 17 June 2024 Accepted: 06 February 2025 Published: 11 March 2025 DOI: https://doi.org/10.1038/s41746-025-01503-7 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative npj Digital Medicine (2025) Circular Economy and Sustainability (2025) Advertisement npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",2
"Agentic AI in Healthcare: Use Cases, Cost & Challenges - appinventiv.com",https://news.google.com/rss/articles/CBMiZEFVX3lxTE5JZHQ5SWs0RVNTM2xOVkJFMUxsdldZNzRCUnZGX2huVDA5aXFrSF8xZ2YtXzFEVFQycnAtZUhhbEdqZFZJQkVITkZ1WlRzX0tVa1lYVE5YZnAwWDlQcF9iUVNSQUg?oc=5&hl=en-US&gl=US&ceid=US:en,"We believe in change driven by technology and innovation. Meet the brains behind our smooth running and powerful machine. Join our team of experts to make a difference in the real world. Learn about Appinventiv's product lifecycle development process. Our software development services are built to evolve your business idea into a successful growth story that deploy customized solutions in a wide range of industries to steadfast success for top globally leading brands A leading digital platform to offer engaging shopping experience to users A transforming ERP Solution for the world's largest furniture retailer A mobile app to digitalize & expand KFC's digital footprint A refined UX strategy for Domino's to increase their conversion rate by 23% The MIT Innovation award-winning app with $52 Million funding reshaping the employment landscape A SaaS-based financial literacy and smart money management platform for kids We believe in change driven by technology and innovation. Join our team of experts to make a difference in the real world. Learn about Appinventiv's product lifecycle development process. Meet the brains behind our smooth running and powerful machine. that deploy customized solutions in a wide range of industries Our software development services are built to evolve your business idea into a successful growth story A leading digital platform to offer engaging shopping experience to users A transforming ERP Solution for the world's largest furniture retailer A mobile app to digitalize & expand KFC's digital footprint A refined UX strategy for Domino's to increase their conversion rate by 23% The MIT Innovation award-winning app with $52 Million funding reshaping the employment landscape A SaaS-based financial literacy and smart money management platform for kids to steadfast success for top globally leading brands Key Takeaways Imagine walking into a hospital where systems don t just process information   they make decisions, anticipate needs, and take action on behalf of both patients and providers. Not in a decade. Not in theory. But now. This isn t the latest plot twist in a sci-fi series   it s the emerging reality powered by Agentic AI in healthcare. While traditional AI has played a critical role in diagnostics, imaging, and data crunching, Agentic AI takes things several steps further. It doesn t just respond, it initiates. It doesn t just support, it collaborates, plans, and adapts, like a digital colleague embedded within the healthcare workflow. The healthcare industry, long burdened by administrative overload, fragmented systems, and rising patient expectations, is beginning to see what happens when intelligent agents are given to act. And the transformation is anything but subtle. In this article, we ll explore how Agentic AI healthcare is reshaping clinical operations, patient engagement, and care delivery   introducing an era where intelligent automation doesn t just optimize healthcare, it redefines it. To really grasp the impact Agentic AI is having on healthcare, it would help to take a step back and compare it with what we ve known so far. Traditional AI agents have been around for a while now, and they ve made a meaningful dent in healthcare, there s no denying that. Tools that analyze medical images for early signs of disease, chatbots that help patients book appointments, systems that suggest treatment options based on large datasets   all of these are powered by traditional AI. But their roles are usually limited to reactive tasks. They wait for input. They respond based on what they ve been trained on. They re impressive   but they re not exactly thinking ahead or connecting the dots on their own. These systems are excellent at pattern recognition and classification tasks. Feed them data, and they ll give you results. But they won t decide what to do with those results, they won t coordinate with other systems. And they definitely won t re-evaluate their approach if conditions change. Healthcare for AI agentic, on the other hand, operates on a very different level. These are goal-driven systems that combine reasoning, planning, and adaptive decision-making to carry out complex, often multi-step tasks, sometimes entirely autonomously. Let s say a hospital wants to reduce avoidable re-admissions for patients with congestive heart failure. A traditional AI might analyze patient histories and generate a risk score. Helpful? Sure. But it stops there. Agentic AI systems in healthcare would start with that risk score, and then move forward. It could: That s not just analytics   it s agency. AI agents for healthcare are a system that understands a high-level objective and executes a series of intelligent actions to achieve it, involving both humans and machines as needed. Another major difference is in how these systems evolve. Traditional AI/ML models often need periodic retraining with new data, and they typically operate within the boundaries of their training set. Agentic AI systems, however, are designed to learn from interaction and adapt their behavior   kind of like a junior care coordinator who quickly learns the ropes and gets better with every shift. In essence: And this distinction isn t just theoretical, it has massive implications for patient care, clinical efficiency, and system-level transformation. When you build an AI agent for the healthcare industry it doesn t just help humans make better decisions; it starts to take on decisions and workflows that were previously bottlenecked by bandwidth or complexity. Unlike traditional AI that focuses on narrow tasks, agentic AI in healthcare is designed to function as an autonomous decision-maker within complex healthcare environments. It perceives context, weighs multiple variables, adapts in real time, and takes action, much like a skilled human agent would. Let s unravel how autonomous agents work behind the curtains: The system begins by ingesting data from multiple sources: EHRs, medical devices, diagnostics, and even wearables. It doesn t just collect this information; it interprets it in real time. This deep contextual awareness allows the AI to spot subtle patterns, respond to shifting patient conditions, and stay continuously updated as new data flows in. This lays the foundation for adaptability   the system isn t bound to static workflows. If a patient s vitals suddenly change or lab results deviate from expectations, the agentic AI for healthcare adjusts its recommendations or actions accordingly. At its core, agentic AI healthcare blends deterministic logic with probabilistic models. Instead of rigid rule-following, it calculates possible outcomes, weighs risks, and makes decisions even when data is incomplete or ambiguous. That means it can, for example, assess whether a deteriorating patient needs immediate escalation, even before a nurse arrives. Here s where autonomy and intelligent reasoning come into play, healthcare for AI agentic doesn t just assist, it acts. And it does so with built-in accountability by assigning confidence levels to its decisions. Once a decision is made, the agentic AI systems in healthcare can act   automatically triggering alerts, modifying care plans, or initiating workflows across connected systems. This closes the loop between sensing and doing, allowing real-time, meaningful intervention without waiting for human input. This is not just passive analysis, it s active, hands-on participation in care delivery. What sets AI agents for healthcare apart is its ability to learn from outcomes. Every action and its result feeds back into the system to refine future responses. If a specific dosage adjustment improves patient recovery in certain conditions, the system remembers and applies that insight going forward. The benefits of agentic AI in healthcare are making a transformative impact across multiple aspects of healthcare, empowering providers, researchers, and patients with systems that are not only intelligent but also adaptive, autonomous, and predictive. These domains represent areas where the technology is revolutionizing the landscape, offering personalized, proactive, and precise care. Let s explore these core areas that make the efforts to build an AI agent for the healthcare industry fruitful. Beyond suggestions, CDSS agents of tomorrow won t just  recommend , they ll simulate. Rather than scoring medications by guidelines, they ll visualize treatment paths as branching timelines tailored to the patient s physiology, genomics, and environment. Say a patient s been diagnosed with a rare autoimmune disorder. Instead of choosing based on precedent, agentic AI would run thousands of real-time simulations using synthetic biology models of the patient s immune system, microbiome, and drug-response curves. It shows: Day 0 to Month 6, here s what happens if we go with Drug A, B, or CRISPR editing   side effects, hospitalizations, even socioeconomic impacts. The physician doesn t just treat; they time-travel through the possibilities. One of the most significant agentic AI use cases in healthcare is the improvement of diagnostic accuracy. Traditional diagnostic processes, while thorough, can be prone to human error or delays, especially when dealing with complex conditions. Agentic AI, with its ability to autonomously analyze large datasets, ranging from medical imaging to genetic data, significantly enhances the accuracy and speed of diagnoses. For instance, AI systems such as Zebra Medical Vision have developed algorithms that can analyze medical imaging to detect early signs of diseases like cancer, cardiovascular conditions, and neurological disorders. These AI systems are trained on millions of imaging scans, learning to spot patterns and anomalies that are sometimes too subtle for the human eye to catch. With this early detection capability, clinicians can intervene much sooner, leading to better patient outcomes and lower treatment costs. In a groundbreaking collaboration, Google Health is leveraging AI to enhance mammography interpretation. In a study, Google s deep learning model outperformed human radiologists in breast cancer detection, demonstrating a significant reduction in false positives and false negatives. This not only helps in providing a quicker, more accurate diagnosis but also improves patient confidence and reduces unnecessary procedures. Also Read: How AI Expedites the Medical Diagnosis Process Agentic AI applications in healthcare specially in treatment and patient care are immense, shifting healthcare from a one-size-fits-all model to a personalized, dynamic system where interventions can be adapted in real time based on the patient s evolving condition. This results in more effective treatments and reduced healthcare costs. For chronic diseases such as diabetes, hypertension, and asthma, continuous monitoring is critical. Examples of agentic AI in healthcare like Livongo Health enable real-time tracking of vital health metrics, such as glucose levels and blood pressure. These systems analyze the data to provide actionable insights and trigger interventions, such as adjusting medication or suggesting lifestyle changes. For example, if a diabetic patient s blood sugar readings become dangerously high, the system automatically recommends corrective measures or alerts the healthcare provider. Livongo s AI-driven platform empowers patients to manage their conditions proactively, reducing hospital readmissions and improving quality of life. By automating routine monitoring and providing real-time feedback, it allows patients to make data-driven decisions in managing their health. Instead of reacting to fluctuations in heart rate or oxygen levels, future agents will actively manage physiological states like inflammation, cortisol levels, or neurotransmitter imbalances in real time, tuning the patient toward optimal states like a pilot stabilizing a plane. For ICU patients, an agent would adjust not just ventilator pressure or IV dosage but use neuromodulatory feedback loops (e.g. via optogenetics or targeted vagus nerve signals) to maintain calm, reduce cytokine storms, and synchronize circadian cycles. It s not  alert fatigue    it s  alert orchestration.  Such agentic AI systems in healthcare have the tendency to become the silent conductor of biological harmony. AI s role in healthcare administration cannot be overstated. From optimizing hospital workflows to enhancing patient scheduling, agentic AI systems in healthcare are streamlining operations and improving the efficiency of healthcare providers. This reduces administrative burdens, allowing staff to focus more on direct patient care. Hospitals face the challenge of managing scarce resources like ICU beds, ventilators, and staff. Benefits of agentic AI in healthcare can be gauged through AI systems like Qventus which are helping healthcare providers optimize hospital operations by predicting patient volume, staff needs, and resource allocation. By analyzing historical data, Qventus  AI can forecast surges in patient admissions, enabling hospitals to proactively allocate resources and manage staff workload, ensuring that critical resources are always available when needed. During the COVID-19 pandemic, such healthcare for AI agentic systems were instrumental in helping hospitals adjust to rapidly changing patient volumes, ensuring that hospitals could provide timely care even during peak times. Also Read: Hospital Management System Development   Benefits, Process, Features, Costs Traditionally a long and expensive journey, the drug discovery process is now being transformed by artificial intelligence, particularly, agentic AI. By analyzing massive datasets, AI can identify potential drug candidates, predict their effectiveness, and even optimize clinical trial designs. Transcripta Bio, a pharmaceutical startup, utilizes AI to match existing medications with new use cases, focusing on rare diseases. Their Drug-Gene Atlas database enables the identification of new applications for existing drugs, and they have successfully repositioned five FDA-approved drugs for new indications within two years. Moreover, AI agents for healthcare can analyze data from thousands of clinical trials to identify patterns, providing new insights into drug efficacy and side effects. This reduces trial-and-error, ultimately speeding up the development of life-saving drugs. Robot-assisted surgery, empowered by agentic AI healthcare, is revolutionizing surgery by providing higher precision, reduced recovery times, and enhanced patient outcomes. With AI, robotic surgical systems can analyze patient data in real time and adjust surgical techniques based on the patient s specific anatomy. The da Vinci Surgical System by Intuitive Surgical is one of the most advanced examples of AI-powered robotic surgery. Surgeons use the robotic system to perform complex procedures with greater precision and smaller incisions, which leads to faster recovery times and less postoperative pain. The system s AI continually monitors and adjusts the instruments during surgery, ensuring that the surgery is as precise as possible. Recent studies have shown that robotic-assisted surgeries can reduce complications and improve long-term patient outcomes. The da Vinci system s real-time feedback allows surgeons to refine their techniques during operations, leading to higher success rates in surgeries such as prostatectomies and heart valve repairs. Forget static dashboards or alerts. Agentic AI in chronic care will become a persistent presence, growing as your physiology changes, your life context shifts, and treatments evolve. Imagine a teen diagnosed with juvenile arthritis. Instead of managing medications reactively, the agent evolves over decades. It adapts dosing strategies when the user hits puberty, shifts care planning during pregnancy, alters goals during menopause, and communicates lifestyle trade-offs based on socioeconomic changes. Not just a  care plan    a lifelong co-evolving AI caregiver. Agentic AI won t stop at planning incisions. It will design the post-op future first   and then work backward. Before a knee replacement, the agent models recovery trajectories based on the patient s gait, muscle density, pain thresholds, and home setup. It tells the surgeon:  Make a 2cm medial cut, rotate 3  inward, she ll walk unassisted in 21 days.  The agent designs the cut not for now, but for how you want to feel later. Agentic AI is pushing the boundaries of what s possible in the sector. From diagnosing diseases faster and more accurately to creating personalized treatment plans that evolve with the patient, AI is transforming the industry. These benefits of agentic AI in healthcare are not only improving the precision of care but also making healthcare more efficient, affordable, and accessible. Let s explore how we can tailor these capabilities. Adopting Agentic AI for healthcare isn t just about embracing the next big trend   it s about reshaping how decisions are made, patients are treated, and care systems evolve over time. But for those considering the leap, three big questions often arise: What will it cost? Are we ready for it? And what roadblocks should we anticipate? Below, we break down these crucial areas   not from a theoretical lens, but from the practical realities of building and deploying intelligent healthcare agents in 2025 and beyond. The cost of implementing agentic AI healthcare depends on how advanced and autonomous you want your systems to be. It s not a one-size-fits-all model   and that s exactly what makes pricing both flexible and nuanced. For example, if you re starting small with task-specific agents   say, automating follow-ups or basic patient routing   you might be looking at $50K $300K. These systems often rely on existing APIs, pretrained models, and minimal customization. But when you move into more intelligent territory, such as clinical decision support, personalized recommendations, or adaptive care assistants   the cost can scale to $300K $800K, especially as integration, data complexity, and compliance checks increase. Now, if you re building enterprise-grade, closed-loop systems   agents that can analyze diagnostics, respond to patient feedback in real-time, or even assist in clinical research   then you re looking at $1M+. These solutions require robust architecture, multimodal reasoning, custom agent workflows, and often, their own governance and risk layers. These costs to build AI agents for the healthcare industry are increasingly seen as investments   not just in tech, but in long-term operational savings, outcome improvements, and competitive edge. AI agents for healthcare don t just plug into existing systems   they evolve alongside them. Which means your organization has to be ready at multiple levels, something that your partnered healthcare application development services provider will be able to help with. First, take a look at your data maturity. Are your clinical and operational datasets structured, clean, and interoperable? If your data is siloed or inconsistent, your agents won t perform well, or worse, they might draw inaccurate conclusions. Second, when building AI agents for healthcare, focus on workflow clarity. Agents need clearly defined outcomes   whether it s reducing patient wait times, improving diagnostic accuracy, or optimizing surgical schedules. Without a goalpost, the agent s autonomy becomes noise. Team readiness is just as important. AI adoption isn t only technical; it s cultural. Clinical and ops teams need to understand what the system will do, how it will behave, and where human oversight fits in. Then there s infrastructure. Can your organization support high-compute workloads, secure data sharing, and real-time processing? Cloud-native systems and scalable data layers can make a big difference here. And finally, compliance. You ll want to think ahead about consent frameworks, patient explainability, and compliance with HIPAA, GDPR, or any region-specific AI regulations. Even with capital and motivation in place, you ll face challenges that are unique to agentic AI for healthcare. Here are the major ones that founders, CTOs, and innovation leaders need to prepare for: While LLMs and vision models are more accessible than ever, healthcare-specific agentic models remain hard to find   especially ones that can be fine-tuned securely. Accessing high-quality patient data while ensuring privacy and encryption is an ongoing balancing act. Healthcare is one of the most regulated sectors globally. Introducing autonomous agents means you must meet not only AI-specific standards but also healthcare-specific compliances around patient safety, data traceability, and decision explainability. Also Read: How to Make a HIPAA-Compliant App? If your agents don t seamlessly plug into EHR systems, care delivery platforms, or scheduling tools, adoption will stall. Agentic AI healthcare that operate in silos become glorified chatbots, not strategic tools. Not every hospital or health startup has access to secure compute, scalable cloud storage, or GPU-backed systems. If your environment isn t ready, even the smartest agent won t work well. Agentic systems are vulnerable to manipulation, especially when connected to external data sources or user inputs. Defensive architectures, threat detection, and reinforcement learning with safety checks are key. When agentic AI systems make or recommend life-impacting decisions, who s responsible? Establishing human-in-the-loop control, audit trails, and accountability structures is a non-negotiable. Agentic systems require a governance layer that keeps autonomy in check. Define clear roles for human intervention, override protocols, and feedback loops that evolve with the system. Ultimately, building AI responsibly means designing with clarity, control, and compliance from day one. Transparency, fairness, and traceability shouldn t be afterthoughts   they should be built into the blueprint. Agentic AI isn t just another leap in automation   it s the reimagining of healthcare as we know it. From self-directed diagnostic engines to AI caregivers that adapt in real time, the horizon of healthcare is shifting fast. But for organizations to tap into this future, they need more than just AI, they need strategic implementation from a skilled custom agentic AI development company who understand tech, healthcare systems, and the evolving regulatory environment. That s where Appinventiv comes in. We don t just build AI-powered systems. We co-create intelligent agents that think, act, and evolve with your healthcare workflows. Our experience spans full-stack AI product development, cloud infrastructure setup, data engineering, regulatory compliance, and continuous post-deployment optimization. Whether you re looking to: And we re not just reacting to where healthcare is going, we re helping shape it. Our R&D teams are actively exploring the future of agentic AI in healthcare: As a healthcare application development services firm, we see the future of healthcare as not only intelligent, but also personal, transparent, and ethical. Agentic AI is the means   your vision is the goal. Let s make both real. A. Agentic AI is being used to build intelligent systems that can autonomously perceive, reason, and act within clinical environments. Unlike traditional AI models, these agents operate with a high degree of autonomy, enabling them to assist with diagnoses, suggest treatment plans, monitor patient progress, and even coordinate across hospital departments. They re also increasingly being embedded in virtual health assistants, diagnostic engines, and workflow optimization tools, bringing real-time intelligence to every layer of care delivery. A. Real-world applications of Agentic AI for healthcare include: Hospitals and healthtech providers across the US and Europe are already piloting such solutions to reduce readmissions and increase diagnostic accuracy. A. Agentic AI improves patient outcomes by enabling faster, more accurate, and personalized interventions. These systems don t just follow static rules, they learn and adapt in real time, identifying patterns across massive datasets that human clinicians might miss. For example, an agentic AI could detect early signs of sepsis hours before symptoms become critical, or fine-tune a cancer treatment regimen based on a patient s individual response, lifestyle, and history. The result is smarter decisions, quicker care, and higher chances of recovery   all with reduced strain on care teams. Chirag Bhardwaj is a technology specialist with over 10 years of expertise in transformative fields like AI, ML, Blockchain, AR/VR, and the Metaverse. His deep knowledge in crafting scalable enterprise-grade solutions has positioned him as a pivotal leader at Appinventiv, where he directly drives innovation across these key verticals. Chirag s hands-on experience in developing cutting-edge AI-driven solutions for diverse industries has made him a trusted advisor to C-suite executives, enabling businesses to align their digital transformation efforts with technological advancements and evolving market needs. AI in Warehouse Management: Benefits, Use Cases, and Real World Examples Key takeaways: AI-Driven Optimization: AI enhances warehouse efficiency by automating workflows, reducing errors, and optimizing inventory with real-time data. Cost Reduction: AI cuts operational costs by minimizing labor, storage, and return expenses, with McKinsey noting 5-20% logistics cost savings. Improved Forecasting: AI-powered demand forecasting prevents stockouts and overstocking, improving customer satisfaction and cash flow. Space  How to Use AI in Personalized Treatment Plans for Diseases? Key takeaways: AI enables precise, individualized treatment plans by analyzing patient data, genomic profiles, and clinical histories, moving away from generalized protocols. AI models help anticipate medical events and tailor care, improving outcomes through proactive, personalized interventions. AI integrates diverse data sources (EHRs, genetics, imaging) to provide faster, more accurate decision-making in clinical workflows. AI  RAG vs Fine Tuning: Which AI Approach is Best for Your Business? Key takeaways: RAG delivers real-time answers using external data great for fast-changing content. Fine Tuning builds in expertise ideal for regulated, high-accuracy tasks. RAG is quick to launch, Fine Tuning wins on long-term efficiency. Fine Tuning ensures control, RAG offers flexibility and reach. Hybrid models blend both perfect for enterprise-grade AI tools. Choose based on use case RAG for  B-25, Sector 58,Noida- 201301,Delhi - NCR, India 79, Madison Ave Manhattan, NY 10001,USA Appinventiv Australia, East Brisbane QLD 4169, Australia 3rd Floor, 86-90 Paul Street EC2A 4NE London, UK Tiger Al Yarmook Building, 13th floor B-block Al Nahda St - Sharjah Suite 3810, Bankers Hall West,888 - 3rd Street Sw Calgary Alberta Full stack mobile (iOS, Android) and web app design and development agency Appinventiv is the Registered Name of Appinventiv Technologies Pvt. Ltd., a mobile app development company situated in Noida, U.P. India at the street address - B- 25, Sector 58, Noida, U.P. 201301. All the personal information that you submit on the website - (Name, Email, Phone and Project Details) will not be sold, shared or rented to others. Our sales team or the team of mobile app developers only use this information to send updates about our company and projects or contact you if requested or find it necessary. You may opt out of receiving our communication by dropping us an email on - info@appinventiv.com 1600+ transformation engineers delivered 3000+ game-changing products. We chose Appinventiv to build our financial literacy and money management app from start to finish. From the first call, we were very impressed with Appinventiv s professionalism, expertise, and commitment to delivering top-notch results. It has been a pleasure working with Appinventiv. The team is not only extremely versatile and competent but also very professional, courteous, and responsive. We certainly plan to continue working with Appinventiv for an indefinite period. We took a big leap of faith with Appinventiv who helped us translate our vision into reality with the perfectly comprehensive Edamama eCommerce solution. We are counting to get Edamama to launch on time and within budget, while rolling out the next phase of the platform with Appinventiv. I just want to take a moment to thank the entire Appinventiv team for your incredible support. We truly appreciate everything you've done, and we're excited to continue working together as we grow here at KODA After researching numerous companies, we finally found Appinventiv, and it was the best decision we could have made. They successfully addressed the challenges with our existing app and provided solutions that exceeded our expectations. We approached Appinventiv with a clear vision to build a robust and future-ready platform that could seamlessly integrate with the busy lifestyle of our customers while uplifting their overall experience and giving us a competitive edge. 1600+ transformation engineers delivered 3000+ game-changing products. Connect with our consultation experts to get: Insights specific to your business needs Roadmap to overcome your challenges Opportunities to scale your business in this niche.",2
"AI with agency: a vision for adaptive, efficient, and ethical healthcare - Frontiers",https://news.google.com/rss/articles/CBMilwFBVV95cUxQS1BQWXl3ak9OQS1jZXI0OFNSaHF4R2FVdDhla0RDSXEzNzBON0dHSnloTmNhNmNuZG5IcDA1R256cEU5enV2VV9NOFVtakZ1TTZCOWdVNVhyU19zSzVtN1NIMG9SRjJ5V3FjUGQ1bU9LZ2VBYk1FOXg0V1dCMEFKQktwcEpCeU12OFZiLVQ2UWhiZHpXNkk4?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better OPINION article Front. Digit. Health, 07 May 2025 Sec. Health Informatics Volume 7 - 2025 | https://doi.org/10.3389/fdgth.2025.1600216 This article is part of the Research TopicAI in Healthcare: Transforming Clinical Risk Prediction, Medical Large Language Models, and BeyondView all articles The healthcare industry continues to face significant operational challenges in patient care, resource allocation, and administrative processes. For instance, despite spending 16.8% of its gross domestic product on healthcare by 2015, the United States reported higher rates of preventable hospitalizations and lower life expectancy compared to countries that spent nearly half as much (1). In fact, the average life expectancy in the United States was 78.8 years, falling short of the 80.6-year average among OECD countries (1). Moreover, 73.2% of insured adults reported experiencing at least one administrative burden that led them to delay or forgo medical care (2). These inefficiencies stem not only from financial concerns but also from deeply embedded flaws in the administrative and technological infrastructure of the healthcare system. Such persistent inefficiencies highlight a need not merely for automation, but for intelligent, adaptive systems capable of navigating complexity in real time. A major driver of these challenges is administrative overhead. Healthcare institutions allocate approximately 20% of their budgets to administrative tasks, while American physicians spend around 13% of their work time on similar responsibilities (3). Compounding this issue are fragmented workflows, excessive documentation, and poorly integrated clinical systems, which increase physician burnout and the likelihood of clinical errors (4). For example, some computerized provider order entry systems are not tailored to patient needs, requiring 10% more physical effort than manual order selection (4). Additionally, order sets can become outdated quickly, reducing their clinical value. These challenges underscore the need for a more intelligent system to ease administrative burdens and support effective clinical decision-making. What is needed is not just an AI system that follows static rules, but one that learns, evolves, and operates with autonomy. Agentic artificial intelligence (AI) offers a promising solution by autonomously managing complex healthcare tasks, reducing human error, and enhancing efficiency (5). Using machine learning (ML) algorithms, agentic AI adapts to real-time healthcare environments (6). Unlike conventional AI, which depends on fixed rules, agentic AI acts on its own to achieve healthcare goals and continuously updates its behavior as new information comes in. It can streamline workflows, enhance diagnostic accuracy, and reduce administrative workload (6, 7). Some agentic AI systems have been shown to lower cognitive workload by up to 52% (4). Predictive models powered by agentic AI can identify patients at risk of disease progression or complications, resulting in fewer hospitalizations, reduced healthcare costs, and better outcomes (8). For instance, AI-based monitoring systems can detect subtle changes in vital signs, predict deterioration, and alert clinicians before critical issues develop, enabling timely intervention. Because agentic AI is goal-driven and adapts over time, it is especially well-suited to handle the complexity of hospital environments and ever-changing patient needs. In addition, agentic AI can optimize hospital resource management by dynamically adjusting staffing, supply distribution, and patient flow based on real-time data (5). This perspective introduces an agentic AI framework designed to automate, optimize, and personalize medical services. Unlike traditional ML models, agentic AI continuously learns from routine data and adjusts its responses to match evolving healthcare demands. This combination of adaptiveness and autonomy makes agentic AI not just an innovation, but a necessity for the future of healthcare delivery. By reducing administrative burdens, enhancing clinical decision-making, and streamlining operations, agentic AI has the potential to transform healthcare delivery and improve both outcomes and cost-effectiveness. AI enhances administrative efficiency by automating routine tasks such as medical documentation, insurance claims processing, patient scheduling, and staff coordination (9, 10). In the United States, healthcare professionals spend approximately 25% of their working hours on administrative duties (9). This administrative burden contributes to physician burnout, which in turn affects the quality of care delivered to patients (11). Automating repetitive and time-consuming administrative tasks can therefore free up valuable time for medical professionals to focus more on direct patient care. Agentic AI builds upon this by not only executing administrative tasks but also proactively refining and restructuring workflows. AI-driven automation addresses this issue by significantly reducing manual workloads, enabling healthcare providers to allocate their time more intelligently and efficiently (12, 13). One of the most time-consuming administrative tasks is the documentation of clinical history. Physicians often spend more than an hour documenting electronic health records (EHRs) for every hour spent with patients, contributing to both burnout and inefficiencies (14). To address this, AI-powered natural language processing and voice recognition technologies are being integrated into EHR systems to assist with medical transcription (15, 44). These tools allow physicians to dictate patient notes, which are then structured into standardized documents by AI. When these systems use agentic AI, they can learn each clinician's preferences over time to improve documentation quality. This reduces documentation errors, improves the accuracy of patient records, and minimizes delays in administrative processing (16). These innovations also support interdepartmental coordination, improve hospital logistics, and enhance physician productivity (17, 18). In addition, automated documentation tools help ensure regulatory compliance by maintaining records that meet legal and ethical standards, thereby reducing the risk of malpractice claims and institutional penalties (9). AI also improves the processing of insurance claims by identifying errors, ensuring compliance, and detecting fraudulent activity. Traditional claims processing is often time-consuming, prone to human error, and financially inefficient, leading to delayed reimbursements and financial losses for healthcare providers (19). AI-based systems can analyze large volumes of claims data, flag inconsistencies, and verify compliance with regulations, ultimately reducing administrative workloads (45). These optimizations lead to fewer rejected claims, better cash flow, and improved patient satisfaction due to faster approvals (20). AI also detects suspicious patterns in claims, helping to prevent financial losses from fraud (46). Agentic AI would let these systems spot fraud and errors more accurately by learning from feedback and by adjusting to new billing rules in real time. Thus, by automating insurance processing, AI strengthens the financial health of healthcare institutions and allows clinical staff to redirect their focus toward patient care. Additionally, it improves billing transparency and speeds up reimbursements (21). In patient scheduling, AI increases efficiency by predicting no-shows, optimizing real-time appointment availability, and adjusting schedules accordingly. One study found that AI-supported reminders reduced no-show rates from 19.3% to 15.9%, which helped ensure timely treatment and better use of provider time (10). Furthermore, agentic AI would go even further by learning from patient behavior and clinic flow changes. It could adjust scheduling priorities and timing with minimal human input. These systems reduce waiting times, minimize administrative delays, and prevent resource wastage, improving the overall delivery of healthcare services. Virtual assistants powered by AI also help patients schedule appointments and handle inquiries, reducing the workload of administrative staff and improving the patient experience (22). These tools promote smoother operations, better coordination between providers and patients, and more effective resource use. Moreover, AI-powered assistants enhance accessibility by helping patients with disabilities or language barriers navigate healthcare services (23). Finally, AI strengthens staff coordination by optimizing workforce management, automating administrative scheduling tasks, and projecting staffing needs based on real-time patient demand. AI scheduling systems analyze historical data, seasonal patterns, and patient inflow trends to assign personnel efficiently and maintain proper coverage during peak periods (24). This helps prevent employee burnout while ensuring that high-quality patient care is maintained (25). Predictive analytics can also assist administrators in anticipating staffing shortages and reallocating resources dynamically (26). Agentic staff coordination systems would respond to changing demands. It would reallocate staff and update schedules on their own in real time. This helps maintain workforce resilience and ensures effective clinical coverage. By integrating AI into staff coordination, healthcare facilities can enhance workforce efficiency, improve operational resilience, and raise the standard of care. However, it is crucial that AI is implemented with care, so that over-optimization does not result in staff reductions that compromise patient care (27). Agentic systems can be designed to prevent such over-optimizations by balancing administrative efficiency with patient-first policies. The application of AI in Clinical Decision Support Systems (CDSS) has enhanced diagnostic accuracy by reducing medical errors and improving patient outcomes (28). AI-powered CDSS provides clinicians with real-time predictive data, evidence-based recommendations, and risk assessments. One study found that such systems led to a 5% change in treatment decisions, driven by more accurate diagnoses and improved decision-making processes (16). By leveraging large datasets and integrating genetic, lifestyle, and environmental factors, AI enables the automation of personalized treatment plans that enhance patient care (29). With agentic AI, these systems move beyond fixed rules by constantly updating their diagnostic models based on real-world clinical data, making their recommendations more accurate and tailored to current conditions. One of the most impactful applications of AI in CDSS is diagnostic imaging, where it has surpassed human radiologists in detecting certain diseases (30). AI-powered imaging tools have significantly improved tuberculosis screening in low-resource settings, particularly where access to radiologists is limited (30). AI has also transformed early cancer detection by identifying tumors at earlier, more treatable stages, thereby allowing clinicians to develop timely and individualized treatment strategies (31). In ophthalmology, AI helps detect and grade diabetic retinopathy, increasing access to eye screening and minimizing delays in diagnosis (32). These technologies are especially valuable in regions with limited access to trained medical professionals, offering a cost-effective and scalable solution. In parasitology, for instance, automated diagnostic systems have demonstrated high accuracy in identifying host infection and estimating parasite load, showing particular promise for image-based analysis in low-resource settings (33). Such systems reduce the need for manual interpretation, accelerate diagnostic workflows, and extend effective care to underserved populations. By automating diagnostic processes, agentic AI systems not only facilitate earlier interventions and diagnostic precision but also autonomously learn from each new imaging dataset, which improves its interpretive accuracy over time. However, concerns about interpretability remain. Clinicians must be able to understand and verify AI-generated outputs to ensure the reliability and safety of diagnostic recommendations (13). Agentic AI systems must therefore prioritize explainability to preserve clinician trust and avoid over-reliance on just algorithms. The FUTURE-AI guidelines emphasize that explainability is desirable not only from a technological and clinical perspective, but also from ethical and legal standpoints. They recommend defining explainability needs with end users and implementing human-in-the-loop mechanisms to detect bias, flag implausible outputs, and override AI decisions whenever necessary (34). Beyond imaging, AI-based CDSS has made substantial contributions in pediatrics, particularly in the early detection of sepsis and other life-threatening conditions. Trained on large datasets, these AI models enhance the accuracy of sepsis detection without increasing false alarms or contributing to alert fatigue (35). AI also holds promise in mental health care by enabling early diagnosis, treatment planning, and personalized therapy recommendations for psychiatric conditions (12). These systems can monitor mood and behavioral trends continuously, supporting timely interventions and improved patient well-being. Agentic AI enables these systems to recalibrate detection thresholds over time as they process behavioral data. This improves the accuracy of mental health interventions and reduces misclassification. However, if the training data lacks demographic diversity, AI may inadvertently reinforce biases in mental health diagnosis, risking misdiagnosis or inadequate care for underrepresented populations (21). Agentic models must therefore have bias-detection and adapt their predictions across varying patient subgroups to ensure fairness and accuracy. Another important area where AI supports clinical decision-making is in prescription accuracy and drug management. AI algorithms can analyze patient history, genetic information, and drug interaction data to identify potential adverse drug reactions before they occur, thereby reducing medication errors and patient risk (22). In complex cases involving polypharmacy, particularly among elderly or chronically ill patients, AI can detect harmful drug combinations and adjust dosages accordingly. These systems are especially beneficial in resource-limited settings where specialized pharmacists are unavailable (21). By incorporating AI into prescription workflows, clinicians can minimize errors, improve medication adherence, and optimize therapeutic outcomes. Agentic AI would further improve this process by continuously learning from patient outcomes, adapting dosage recommendations, and automatically flagging evolving drug sensitivities. Finally, AI enhances clinical workflows through integration with EHRs. Machine learning algorithms can evaluate patient histories, lab results, and current health data to identify complications and suggest personalized treatment adjustments (14). In intensive care units, AI-powered monitoring systems continuously assess patient conditions and can predict deterioration before critical events occur (36). In surgical contexts, AI decision support tools provide real-time recommendations based on imaging and individualized risk profiles (14). These developments show that AI in clinical decision support goes beyond diagnostics to enhance treatment precision, workflow efficiency, and overall clinical outcomes. With agentic capabilities, these systems ingest streaming patient data and refine treatment strategies in real time. This high level of adaptiveness is essential for managing acute and rapidly changing conditions. AI also plays a growing role in real-time disease monitoring and adaptive treatment planning (37). Agentic CDSS would bridge the gap between episodic interventions and continuous care by creating personalized treatment paths that evolve alongside changes in patient physiology. One of the most pressing operational challenges in healthcare is overcrowding in the emergency department (ED). AI offers a solution by predictively analyzing resource needs, using data on historical admission rates, seasonal illness patterns, and external factors such as flu outbreaks and weather conditions (10). It also enables real-time triage by identifying high-risk patients and prioritizing them for immediate care. Agentic AI enhances this process by autonomously refining triage protocols in response to evolving case patterns and treatment outcomes: this means that the system learns from each interaction and improves for future prioritizations. This reduces waiting times for emergency cases and improves overall healthcare operations by enabling hospitals to deliver timely and effective care (35). However, overreliance on AI-based triage systems presents risks, particularly when models fail to recognize rare conditions or when algorithmic bias results in the misclassification of patient severity (27). Beyond the ED, AI improves hospital-wide workflow efficiency by automating tasks such as discharge planning, bed assignment, and surgical suite scheduling. Agentic AI could expand this functionality by autonomously identifying inefficiencies, learning from its outcomes, and reoptimizing them in real time to balance the changing patient volumes with institutional constraints. AI-driven tracking systems allow for real-time monitoring of patient movement, providing hospitals with a comprehensive view of operational processes rather than isolated data points. This visibility enables administrators to identify bottlenecks and redistribute staff or resources dynamically in response to patient demand (18). AI systems can also forecast departmental overloads and suggest resource reallocation strategies, reducing staff fatigue and optimizing hospital occupancy (46). For example, the YOLOv5s model has demonstrated an effective trade-off between speed and detection accuracy in clinical imaging tasks, making it suitable for real-time applications in resource-constrained environments (38). In surgical departments, AI can support operating room scheduling to allocate teams efficiently, minimize downtime between procedures, and improve surgical throughput (30). When guided by agentic design, scheduling systems can become patient-responsive, balanced with individualized clinical need and real-time prioritization. Despite these benefits, AI-based resource planning must be carefully balanced with ethical considerations to avoid cost-driven decisions that compromise patient care quality (11). AI also contributes to predictive maintenance of medical equipment. Machine learning algorithms can analyze operational data from devices such as MRI scanners, ventilators, and robotic surgery systems to detect early signs of malfunction. This enables hospitals to schedule maintenance proactively, reducing the risk of equipment failures and costly downtime that can disrupt patient care (45). Predictive maintenance reduces unexpected repairs, lowers operational costs, and improves the availability of life-saving technologies. AI tools can also continuously monitor imaging devices to avoid diagnostic delays, ensuring these tools are always ready for critical use (26). Equipped with agentic AI, these tools can autonomously adapt to usage trends, environmental conditions, and failure probabilities. They could evolve their maintenance schedules without human prompting. However, implementing AI-based maintenance systems requires significant initial investment, which may be challenging for under-resourced healthcare systems (39). In addition, AI optimizes supply chain operations within healthcare institutions. Predictive analytics enables AI inventory systems to anticipate demand, automate ordering, and prevent supply shortages. During the COVID-19 pandemic, AI-enabled logistics systems played a vital role in efficiently transporting ventilators, vaccines, and personal protective equipment to the areas of greatest need (9). AI-based quality control systems also monitor storage conditions for temperature-sensitive drugs and vaccines, helping to ensure product integrity. Real-time tracking of drug expiration dates prevents overstocking and reduces financial losses due to waste (14). Agentic AI makes supply chains more flexible and responsive by adjusting procurement and distribution based on real-time local patient needs, helping improve care in underserved areas. Although these tools enhance drug safety and reduce costs, improper implementation may widen healthcare disparities by favoring well-resourced institutions over underfunded ones (23). AI has also transformed surgical operations through the development of robotic-assisted procedures. Systems like the da Vinci Surgical System offer enhanced precision, stability, and visualization for minimally invasive surgeries (12). These systems eliminate hand tremors and allow surgeons to perform complex procedures with greater accuracy. In addition, AI assists with preoperative planning by analyzing patient imaging and health records to develop personalized surgical plans, improving outcomes and reducing postoperative complications (40). AI-enabled surgical robots can also automate routine steps, enabling surgeons to focus on more critical aspects of the procedure (32). Agentic robotic systems further elevate surgical safety and precision by intraoperatively learning from sensor feedback and adjusting their actions dynamically. However, the high cost of robotic surgery limits its accessibility, which may increase disparities in care across institutions (21). The application of AI in healthcare has demonstrated substantial economic potential, with estimates suggesting it could contribute between $100 billion and $150 billion annually to the U.S. healthcare system (19, 40). However, these projections largely reflect the impact of conventional AI systems. The integration of agentic AI, capable of autonomous goal pursuit and adaptive learning, unlocks even greater value by allowing healthcare systems to optimize cost structures in real time. These savings arise from improved drug management, reduced hospital readmissions, better resource utilization, and the automation of administrative tasks. By enhancing clinical decision-making and optimizing hospital operations, AI increases healthcare efficiency while lowering unnecessary medical expenditures (9). Agentic AI systems can elevate this further by autonomously reallocating resources and dynamically updating cost-saving strategies based on live institutional data. Thus, agentic AI can go beyond static optimizations to automatically managing costs in response to emerging trends. However, adopting AI requires significant upfront investments in infrastructure, training, and regulatory compliance, which can impose financial strain on smaller healthcare providers (26). The integration of AI can help mitigate these challenges by tailoring resource prioritization to each healthcare institution. In particular, agentic AI can learn from local constraints to develop institution-specific efficiency models that maintain care quality while minimizing operational strain. One of the primary ways AI reduces costs is through early disease detection and more efficient diagnostics. AI-powered imaging technologies have improved the detection of conditions such as cancer, tuberculosis, and diabetic retinopathy, reducing reliance on invasive and expensive diagnostic procedures (30, 32). Early diagnosis also reduces the need for costly late-stage treatments. Additionally, AI can automate image comparisons and assist radiologists in identifying abnormalities, making the diagnostic process more cost-effective (31). However, disparities in access to AI technologies mean that low-resource institutions may not experience these benefits equally (39, 41). Agentic AI could help bridge this gap by autonomously adapting diagnostic pathways to align with the resources available in low-resource settings. AI-driven medication management also contributes to cost savings. By analyzing patient history and genetic information, AI can predict adverse drug reactions, which reduces complications and preventable hospitalizations, both of which are major contributors to healthcare costs (21). For chronic conditions such as diabetes and hypertension, AI helps optimize dosages and reduce trial-and-error prescribing, thereby increasing treatment efficacy and reducing waste from ineffective medications (16). Agentic AI extends these benefits by continually learning from patient outcomes and dynamically refining drug regimens to prevent adverse reactions and optimize cost-effective therapies. Unnecessary procedures and hospital readmissions are significant cost drivers in healthcare. AI helps reduce these by improving diagnostic accuracy and ensuring that appropriate treatment is provided from the beginning. AI-based decision support tools have been shown to lower misdiagnosis rates, leading to fewer redundant tests and imaging procedures (28). For instance, AI can accurately distinguish between benign and malignant tumors, reducing the need for invasive biopsies (46). In cardiology, AI models assess heart disease risk and support early interventions, which can prevent costly emergency procedures (25). Agentic AI further contributes by proactively initiating preventive care workflows and adjusting recommendations as new patient data becomes available, helping institutions avoid costly reactive care. AI also strengthens hospital supply chain management by improving inventory control and minimizing waste. Predictive analytics can forecast supply demands and maintain optimal inventory levels (15). During the COVID-19 pandemic, AI-driven logistics systems efficiently allocated ventilators, vaccines, and other critical resources to high-need areas (26). AI systems can also monitor storage conditions and track drug expiration dates to prevent overstocking and financial loss due to expired supplies. While these tools support cost reduction, financial optimization should never compromise patient care. Over-automation in budgeting may lead to decisions that prioritize cost over quality, risking negative outcomes for patients (27). A balanced approach is needed to ensure sustainable economic benefits. Agentic AI can help strike this balance by encoding ethical constraints into its optimization logic so that cost-saving measures do not threaten patient outcomes. Fraud detection and automated billing are additional cost-saving areas supported by AI. These systems can identify suspicious billing patterns, detect duplicate or erroneous claims, and prevent losses due to insurance fraud (9, 20). As a result, hospitals benefit from faster reimbursements and reduced administrative burdens, improving their overall financial health (42). Moreover, agentic systems can improve fraud detection by continuously refining anomaly detection models in response to shifting billing behaviors and fraud techniques. In the long term, AI contributes to healthcare system sustainability and improves accessibility. AI-enabled telemedicine reduces the need for in-person consultations, lowering transportation and infrastructure costs, especially in rural or underserved areas (39). Additionally, AI-powered chatbots and virtual assistants can handle non-urgent patient inquiries, allowing clinicians to focus on more critical cases (15). With agentic capabilities, these virtual systems could evolve into fully adaptive triage and support agents that could be capable of navigating complex patient needs across multiple healthcare domains. AI also plays a growing role in drug discovery and clinical trials, helping reduce research and development costs. Traditional drug development is time-consuming and costly, often taking years and billions of dollars. AI accelerates this process by analyzing large datasets to identify promising drug candidates more efficiently (12). Agentic AI enhances this process by iteratively modifying experimental hypotheses and adapting trial designs in real time, thereby reducing the cost of failure and improving discovery pipelines. This contributes to faster access to lifesaving medications and reduces the cost of innovation, making cutting-edge treatments more accessible. Agentic AI offers a fundamentally distinct paradigm from traditional AI by incorporating autonomy, adaptability, and goal-directed behavior into healthcare applications. AI is poised to revolutionize healthcare by enhancing administrative efficiency, improving clinical decision-making, streamlining operations, and supporting economic sustainability. In administration, AI can automate scheduling, documentation, insurance claims, and billing, which reduces the burden on physicians and minimizes human error, but these efficiencies are greatly enhanced when AI systems operate with agentic qualities that enable real-time adjustments based on changing priorities and feedback loops. AI-powered Clinical Decision Support Systems can improve diagnostic accuracy through image interpretation, simplify medication management, and predict clinical deterioration, thereby reducing adverse drug events and medical errors. Operationally, AI can streamline hospital resource allocation, enables effective triage, predicts equipment failures, and enhances supply chain logistics to ensure continuous availability of medical resources. When agentic AI powers these tools, they become responsive systems that evolve in tandem with patient, institutional, and epidemiological changes. And economically, AI can reduce healthcare costs by minimizing unnecessary procedures, decreasing drug waste, and improving fraud detection, which would support better financial planning for healthcare systems. Overall, AI-driven automation has the potential to make healthcare delivery more efficient, precise, and less burdensome for human professionals. By automating repetitive administrative tasks, AI can allow healthcare workers to focus more on direct patient care. Agentic AI adds a layer of resilience to these systems by making them not only automated but continuously optimized in real-world conditions. Agentic AI ensures that such automation remains adaptive even in extenuating circumstances through the optimization of tasks based on clinical priorities. AI-supported clinical decision-making can enhance patient care by delivering real-time, data-driven insights that facilitate early diagnoses, personalize treatments, and improve drug management. Predictive analytics can further maximize operational efficiency by forecasting hospital demand, optimizing patient flow, and enabling proactive resource allocation. The combined effect of these capabilities can support the creation of a more efficient, patient-centered, and financially sustainable healthcare system. With agentic AI at its core, the healthcare system can become an adaptive network that self-improves to meet both patient and provider needs. Despite its many benefits, the integration of AI into healthcare presents several challenges. Ethical concerns around patient privacy and data security remain a top priority. AI relies on large datasets, which increases the risk of data breaches, unauthorized access, and misuse of sensitive health information. Furthermore, AI algorithms trained on biased datasets can perpetuate disparities in diagnosis and treatment, particularly among minority populations. Compliance with regulations such as the U.S. Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR) must be ensured through continuous oversight, transparent development, and rigorous validation of AI models. In addition to regulatory compliance, developers must investigate and address application-specific ethical issues, especially when AI systems may introduce bias or affect vulnerable populations. Establishing clear responsibilities for AI-related errors is also essential, especially when autonomous systems influence high-stakes clinical decisions. The EU's Act, for instance, categorizes all healthcare AI tools as high-risk and mandates strong obligations around safety, transparency, and performance monitoring. Meeting these obligations requires extensive legal oversight, recurring assessments, and continuous model validation to remain compliant. Furthermore, the legal landscape remains unsettled regarding liability for AI-driven clinical harm. When an autonomous system contributes to a misdiagnosis or adverse outcome, it is unclear whether accountability lies with the institution, the developer, or the clinician, which obviously creates a significant ethical and legal dilemma. Another challenge is interoperability. Many healthcare institutions still use legacy EHR systems, making it difficult to integrate AI solutions effectively. Seamless data exchange between AI systems and existing infrastructure will require standardized protocols and investment in technological upgrades. In addition to interoperability issues, hospitals face other technical barriers to deploying agentic AI. These include the need for real-time data processing capabilities, advanced computational infrastructure, and explainability tools that clinicians can trust. Many existing systems are not designed to support continuous learning models or provide transparent reasoning behind recommendations, which slows adoption, particularly in clinical settings where explainability, real-time responsiveness, and interoperability with legacy EHRs are non-negotiable. These requirements often demand low-latency computation, GPU-accelerated processing, and full-stack AI engineering teams, which many hospitals currently lack. A recent return-on-investment (ROI) study of hospital AI implementation estimated the total five-year cost of deploying an AI platform, including software, infrastructure, and IT time, at approximately $1.78 million, resulting in a 451% ROI. Even over a single year, hospitals experienced a 335% ROI, despite reduced time for full benefit realization (43). These figures highlight that although agentic AI systems require significant upfront financial and technical investment, their long-term efficiency gains can justify the cost. However, this level of investment includes more than just software. It requires infrastructure upgrades, clinician training, deployment coordination across departments, and long-term maintenance planning. For under-resourced hospitals, even partial implementation may be out of reach without phased strategies or external support, raising concerns about equitable access and widening gaps between institutions that can afford to adopt AI and those that cannot. However, implementation also requires careful planning, extensive cross-departmental coordination, and months of deployment time, an estimated 160 working days across IT, radiology, and downstream services, before value is fully realized (43). These practical realities must be accounted for in policy and infrastructure planning, especially for health systems operating on tight budgets. It is also critical to ensure that AI enhances rather than replaces human clinical judgment. Overreliance on AI could erode clinical decision-making skills among healthcare workers. AI should be used as a support tool that augments human expertise rather than serving as an autonomous substitute. Additionally, over-dependence on AI may result in reduced human oversight, increasing the risk of harm if the model produces flawed recommendations. Professional validation and the interpretability of AI-generated decisions are necessary to ensure patient safety. Successful deployment of agentic AI in clinical settings also demands robust traceability and risk management strategies. Traceability measures such as continuous logging, auditing, and performance monitoring are vital for the long-term safety and governance of agentic AI. Furthermore, interdisciplinary collaboration is essential: developers should involve clinicians, legal experts, and ethicists at every stage to ensure systems remain clinically useful and socially responsible. Another concern is the potential for AI to widen global healthcare inequalities. High-resource institutions are more likely to benefit from AI advancements, while low-income countries may lack the infrastructure and technical capacity to implement such systems. Errors in AI models, such as misdiagnoses or inappropriate treatment recommendations, can also cause serious patient harm. These tools must undergo thorough clinical validation and be continuously tested across diverse patient populations to ensure safety and reliability. Agentic AI addresses many of these challenges by continuously adapting to new data and by the possibility of incorporating robust ethical guidelines into its autonomous decision-making processes. What sets agentic AI apart is its potential to dynamically enforce ethical constraints and fairness mechanisms at runtime. This allows healthcare to be safer and more equitable. To build trust and improve the usability of AI, models must become more transparent and interpretable. Explainable AI (XAI) can help clinicians understand and evaluate AI-generated recommendations. Explainability must be addressed from technological, clinical, legal, and ethical perspectives, ensuring clinicians can scrutinize both AI outputs and their underlying logic. This becomes especially important in agentic AI, where systems are not merely generating suggestions but are independently making context-sensitive decisions that require clinical oversight. In the context of agentic AI, explainability is imperative since clinicians have to be able to scrutinize not only the outcomes but also the autonomous decision-making processes that drive the recommendations. AI also holds the potential to personalize treatments by integrating genetic, lifestyle, and environmental data into care plans. This approach is especially promising for chronic disease and oncology management, where personalized interventions can significantly improve outcomes. The increasing use of Internet of Things (IoT) devices and wearable technologies makes continuous health monitoring possible, allowing AI to track patient data in real time. These insights support early disease detection, enable timely interventions, and improve chronic disease management. Through the use of agentic AI, these systems would not only monitor patient data but also autonomously adjust treatment recommendations in response to real-time health changes. This adaptivity transforms healthcare from a reactive model into a dynamic, self-learning ecosystem. Thus, the emergence of agentic AI represents a revolutionary leap in healthcare technology that bridges the gap between reactive and proactive healthcare models. It is capable of continuously learning and adapting: making it indispensable for addressing the ever-changing nature of healthcare. To fully harness the potential of agentic AI, stakeholders must invest in robust infrastructure, interdisciplinary collaborations, and ongoing research to ensure these systems remain transparent, ethical, and effective. Furthermore, regulatory standards must evolve to support the integration of agentic AI: these systems should only augment human expertise rather than replace it. By embracing agentic AI, healthcare can transition toward a system that is continuously learning, adapting, and innovating to deliver high-quality and equitable patient care. In summary, agentic AI is not simply another tool for healthcare automation. It represents a leap toward adaptive, self-improving, ethical, and patient-care-oriented systems. As such, future research should focus on ensuring that AI becomes accessible across a wide range of healthcare settings, including those with limited resources. Low-cost, scalable, and ethically designed AI models are essential for equitable global adoption. While traditional AI has already delivered clear improvements to healthcare administration, decision-making, and cost-efficiency, its successful integration will depend on balancing automation with human oversight, ensuring ethical use, and delivering high-quality care that is accessible to all. Ultimately, agentic AI stands to become the backbone of a healthcare system that does not just respond to change, but anticipates and shapes it in service of better outcomes for all. Embracing agentic AI offers the promise of a healthcare ecosystem that is continuously learning, adapting, and evolving to meet current and future challenges. VH: Writing   original draft, Writing   review & editing. HK: Funding acquisition, Writing   review & editing. MT: Project administration, Supervision, Writing   review & editing. NA: Conceptualization, Supervision, Writing   review & editing. The author(s) declare that financial support was received for the research and/or publication of this article. The publication of this article was funded by Multimedia University, Malaysia. MT was employed by Yo-Vivo Corporation. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The author(s) declare that no Generative AI was used in the creation of this manuscript. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. 1. Papanicolas I, Woskie LR, Orlander D, Orav EJ, Jha AK. The relationship between health spending and social spending in high-income countries: how does the US compare? Health Aff. (2019) 38(9):1567 75. doi: 10.1377/hlthaff.2018.05187 PubMed Abstract | Crossref Full Text | Google Scholar 2. Kyle MA, Frakt AB. Patient administrative burden in the US health care system. Health Serv Res. (2021) 56:755 65. doi: 10.1111/1475-6773.13861 PubMed Abstract | Crossref Full Text | Google Scholar 3. Cutler DM, Ly DP. The (paper) work of medicine: understanding international medical costs. J Econ Perspect. (2011) 25(2):3 25. doi: 10.1257/jep.25.2.3 PubMed Abstract | Crossref Full Text | Google Scholar 4. Zhang Y, Padman R, Levin JE. Paving the COWpath: data-driven design of pediatric order sets. J Am Med Inform Assoc. (2014) 21(2):e304 11. doi: 10.1136/amiajnl-2013-002316 PubMed Abstract | Crossref Full Text | Google Scholar 5. Acharya DB, Kuppan K, Divya B. Agentic AI: autonomous intelligence for complex goals a comprehensive survey. IEEE Access. (2025) 13:18912 3. doi: 10.1109/ACCESS.2025.3532853 Crossref Full Text | Google Scholar 6. Jiang F, Jiang Y, Zhi H, Dong Y, Li H, Ma S, et al. Artificial intelligence in healthcare: past, present and future. Stroke Vasc Neurol. (2017) 2:e000101. doi: 10.1136/svn-2017-000101 Crossref Full Text | Google Scholar 7. Wubineh BZ, Deriba FG, Woldeyohannis MM. Exploring the opportunities and challenges of implementing artificial intelligence in healthcare: a systematic literature review. Urol Oncol: Semin Orig Invest. (2024) 42:48 56. doi: 10.1016/j.urolonc.2023.11.019 PubMed Abstract | Crossref Full Text | Google Scholar 8. Khaleelullah S, Kumar H, Rahul G, Naik R, Teja S. Dietrx: machine learning enhanced disease specific nutrition and precautions. Proceedings of the 8th International Conference on Inventive Systems and Control (ICISC). IEEE (2024). doi: 10.1109/ICISC62624.2024.00065 Crossref Full Text | Google Scholar 9. Bekbolatova M, Mayer J, Ong CW, Toma M. Transformative potential of AI in healthcare: definitions, applications, and navigating the ethical landscape and public perspectives. Healthcare. (2024) 12(2):125. doi: 10.3390/healthcare12020125 PubMed Abstract | Crossref Full Text | Google Scholar 10. Knight DRT, Aakre CA, Anstine CV, Munipalli B, Biazar P, Mitri G, et al. Artificial intelligence for patient scheduling in the real-world health care setting: a metanarrative review. Health Policy Technol. (2023) 12(4):100824. doi: 10.1016/j.hlpt.2023.100824 Crossref Full Text | Google Scholar 11. Esmaeilzadeh P. Challenges and strategies for wide-scale artificial intelligence (AI) deployment in healthcare practices: a perspective for healthcare organizations. Artif Intell Med. (2024) 151:102861. doi: 10.1016/j.artmed.2024.102861 PubMed Abstract | Crossref Full Text | Google Scholar 12. Alowais SA, Alghamdi SS, Alsuhebany N, Alqahtani T, Alshaya AI, Almohareb SN, et al. Revolutionizing healthcare: the role of artificial intelligence in clinical practice. BMC Med Educ. (2023) 23:689. doi: 10.1186/s12909-023-04698-z PubMed Abstract | Crossref Full Text | Google Scholar 13. Liu S, Wright AP, Patterson BL, Wanderer JP, Turer RW, Nelson SD, et al. Using AI-generated suggestions from ChatGPT to optimize clinical decision support. J Am Med Inform Assoc. (2023) 30(7):1237 45. doi: 10.1093/jamia/ocad072 PubMed Abstract | Crossref Full Text | Google Scholar 14. Reddy S. Generative AI in healthcare: an implementation science informed translational path on application, integration and governance. Implement Sci. (2024) 19:27. doi: 10.1186/s13012-024-01357-9 PubMed Abstract | Crossref Full Text | Google Scholar 15. Sahithya B, Prasad MS, Krishna KM, Devarlla AC, Yashavanth TR. Empowering healthcare with AI: advancements in medical image analysis, electronic health records analysis, and AI-driven chatbots. 3rd International Conference for Innovation in Technology (INOCON), Karnataka, India (2024). doi: 10.1109/INOCON60754.2024.10511753 Crossref Full Text | Google Scholar 16. Al-Antari MA. Artificial intelligence for medical diagnostics existing and future AI technology!. Diagnostics. (2023) 13(4):688. doi: 10.3390/diagnostics13040688 PubMed Abstract | Crossref Full Text | Google Scholar 17. Tierney AA, Gayre G, Hoberman B, Mattern B, Ballesca M, Kipnis P, et al. Ambient artificial intelligence scribes to alleviate the burden of clinical documentation. NEJM Catal Innov Care Deliv. (2024) 5(3):1 15. doi: 10.1056/CAT.23.0404 Crossref Full Text | Google Scholar 18. Dicuonzo G, Donofrio F, Fusco A, Shini M. Healthcare system: moving forward with artificial intelligence. Technovation. (2023) 120:102510. doi: 10.1016/j.technovation.2022.102510 Crossref Full Text | Google Scholar 19. Wolff J, Pauling J, Keck A, Baumbach J. The economic impact of artificial intelligence in health care: systematic review. J Med Internet Res. (2020) 22(2):e16866. doi: 10.2196/16866 PubMed Abstract | Crossref Full Text | Google Scholar 20. Erion G, Janizek JD, Hudelson C, Utarnachitt RB, McCoy AM, Sayre MR, et al. CoAI: cost-aware artificial intelligence for health care. Nat Biomed Eng. (2022) 6(12):1384 98. doi: 10.1038/s41551-022-00872-8 PubMed Abstract | Crossref Full Text | Google Scholar 21. Francisco K, Apuhin A, Tan MJ, Byers M, Maravilla N, Karim HA, et al. Can personalized medicine coexist with health equity? Examining the cost barrier and ethical implications. arXiv [Preprint]. arXiv:2411.02307 (2024). Available at: https://doi.org/10.48550/arXiv.2411.02307 (Accessed April 15, 2025). Google Scholar 22. Wu Y, Zhang L, Bhatti UA, Huang M. Interpretable machine learning for personalized medical recommendations: a LIME-based approach. Diagnostics. (2023) 13(16):2681. doi: 10.3390/diagnostics13162681 PubMed Abstract | Crossref Full Text | Google Scholar 23. Joshi S, Sharma M,  ywio ek J, Das R, Rosak-Szyrocka J, Muduli K, et al. Modeling conceptual framework for implementing barriers of AI in public healthcare for improving operational excellence: experiences from developing countries. Sustainability. (2022) 14:11698. doi: 10.3390/su141811698 Crossref Full Text | Google Scholar 24. Shahzad MF, Xu S, Naveed W, Nusrat S, Zahid I. Investigating the impact of artificial intelligence on human resource functions in the health sector of China: a mediated moderation model. Heliyon. (2023) 9:e21818. doi: 10.1016/j.heliyon.2023.e21818 PubMed Abstract | Crossref Full Text | Google Scholar 25. Cheng M, Li X, Xu J. Promoting healthcare workers  adoption intention of artificial-intelligence-assisted diagnosis and treatment: the chain mediation of social influence and human computer trust. Int J Environ Res Public Health. (2022) 19:13311. doi: 10.3390/ijerph192013311 PubMed Abstract | Crossref Full Text | Google Scholar 26. Aminizadeh S, Heidari A, Dehghan M, Toumaj S, Rezaei M, Jafari Navimipour N, et al. Opportunities and challenges of artificial intelligence and distributed systems to improve the quality of healthcare service. Artif Intell Med. (2024) 149:102779. doi: 10.1016/j.artmed.2024.102779 PubMed Abstract | Crossref Full Text | Google Scholar 27. Wang L, Zhang Z, Wang D, Cao W, Zhou X, Zhang P, et al. Human-centered design and evaluation of AI-empowered clinical decision support systems: a systematic review. Front Comput Sci. (2023) 5:1187299. doi: 10.3389/fcomp.2023.1187299 Crossref Full Text | Google Scholar 28. Ouanes K, Farhah N. Effectiveness of artificial intelligence (AI) in clinical decision support systems and care delivery. J Med Syst. (2024) 48(1):74. doi: 10.1007/s10916-024-02098-4 PubMed Abstract | Crossref Full Text | Google Scholar 29. Ilan Y. Improving global healthcare and reducing costs using second-generation artificial intelligence-based digital pills: a market disruptor. Int J Environ Res Public Health. (2021) 18(2):811. doi: 10.3390/ijerph18020811 PubMed Abstract | Crossref Full Text | Google Scholar 30. van Leeuwen KG, de Rooij M, Schalekamp S, van Ginneken B, Rutten MJCM. How does artificial intelligence in radiology improve efficiency and health outcomes? Pediatr Radiol. (2022) 52(11):2087 93. doi: 10.1007/s00247-021-05114-8 PubMed Abstract | Crossref Full Text | Google Scholar 31. Gali  I, Habijan M, Leventi  H, Romi  K. Machine learning empowering personalized medicine: a comprehensive review of medical image analysis methods. Electronics. (2023) 12(21):4411. doi: 10.3390/electronics12214411 Crossref Full Text | Google Scholar 32. Rajesh AE, Davidson OQ, Lee CS, Lee AY. Artificial intelligence and diabetic retinopathy: aI framework, prospective studies, head-to-head validation, and cost-effectiveness. Diabetes Care. (2023) 46(10):1728 39. doi: 10.2337/dci23-0032 PubMed Abstract | Crossref Full Text | Google Scholar 33. AlDahoul N, Abdul Karim H, Kee SL, Tan MJT. Localization and classification of parasitic eggs in microscopic images using an EfficientDet detector. IEEE International Conference on Image Processing (ICIP) (2022). p. 4253 7. doi: 10.1109/ICIP46576.2022.9897844 Crossref Full Text | Google Scholar 34. Lekadir K, Frangi AF, Porras AR, Glocker B, Cintas C, Langlotz CP, et al. FUTURE-AI: international consensus guideline for trustworthy and deployable artificial intelligence in healthcare. Br Med J. (2025) 388:e081554. doi: 10.1136/bmj-2024-081554 PubMed Abstract | Crossref Full Text | Google Scholar 35. Ramgopal S, Sanchez-Pinto LN, Horvat CM, Carroll MS, Luo Y, Florin TA. Artificial intelligence-based clinical decision support in pediatrics. Pediatr Res. (2023) 93(2):334 41. doi: 10.1038/s41390-022-02226-1 PubMed Abstract | Crossref Full Text | Google Scholar 36. Lobo PL, Kavitha P, Saranya K, Selvaraju C, Kumar C, Venkatachalam D. Artificial intelligence (AI) based on real time patient monitoring system in intensive care unit and its applications. 8th International Conference on Inventive Systems and Control (ICISC) Coimbatore, India (2024). p. 328 32. doi: 10.1109/ICISC62624.2024.00063 Crossref Full Text | Google Scholar 37. Tan MJT, Kasireddy HR, Satriya AB, Abdul Karim H, AlDahoul N. Health is beyond genetics: on the integration of lifestyle and environment in real-time for hyper-personalized medicine. Front Public Health. (2025) 12:1522673. doi: 10.3389/fpubh.2024.1522673 PubMed Abstract | Crossref Full Text | Google Scholar 38. Escobar FIF, Alipo-on JRT, Novia JLU, Tan MJT, Abdul Karim H, AlDahoul N. Automated counting of white blood cells in thin blood smear images. Comput Electr Eng. (2023) 108:108710. doi: 10.1016/j.compeleceng.2023.108710 Crossref Full Text | Google Scholar 39. Macariola AD, Santarin TMC, Villaflor FJM, Villaluna LMG, Yonzon RSL, Fermin JL, et al. Breaking barriers amid the pandemic: the status of telehealth in Southeast Asia and its potential as a mode of healthcare delivery in the Philippines. Front Pharmacol. (2021) 12:754011. doi: 10.3389/fphar.2021.754011 PubMed Abstract | Crossref Full Text | Google Scholar 40. Jiao W, Zhang X, D Souza F. The economic value and clinical impact of artificial intelligence in healthcare: a scoping literature review. IEEE Access. (2023) 11:123445 57. doi: 10.1109/ACCESS.2023.3327905 Crossref Full Text | Google Scholar 41. Tan MJT, Lichlyter DA, Maravilla NMAT, Schrock WJ, Ting FIL, Choa-Go JM, et al. The data scientist as a mainstay of the tumor board: global implications and opportunities for the global south. Front Digit Health. (2025) 7:1535018. doi: 10.3389/fdgth.2025.1535018 PubMed Abstract | Crossref Full Text | Google Scholar 42. Zayas-Cab n T, Haque SN, Kemper N. Identifying opportunities for workflow automation in health care: lessons learned from other industries. Appl Clin Inform. (2021) 12(3):686 97. doi: 10.1055/s-0041-1731744 PubMed Abstract | Crossref Full Text | Google Scholar 43. Bharadwaj P, Nicola L, Breau-Brunel M, Sensini F, Tanova-Yotova N, Atanasov P, et al. Unlocking the value: quantifying the return on investment of hospital artificial intelligence. J Am Coll Radiol. (2024) 21(10):1677 85. doi: 10.1016/j.jacr.2024.02.034 PubMed Abstract | Crossref Full Text | Google Scholar 44. Baviskar D, Ahirrao S, Potdar V, Kotecha K. Efficient automated processing of the unstructured documents using artificial intelligence: a systematic literature review and future directions. IEEE Access. (2021) 9:72894 936. doi: 10.1109/ACCESS.2021.3072900 Crossref Full Text | Google Scholar 45. Senapati T, Sarkar A, Chen G. Enhancing healthcare chain management through artificial intelligence-driven group decision-making with Sugeno-Weber triangular norms in a dual hesitant q-rung orthopair fuzzy context. Eng Appl Artif Intell. (2024) 135:108794. doi: 10.1016/j.engappai.2024.108794 Crossref Full Text | Google Scholar 46. Johnson M, Albizri , Simsek S. Artificial intelligence in healthcare operations to enhance treatment outcomes: a framework to predict lung cancer prognosis. Ann Oper Res. (2022) 308:275 305. doi: 10.1007/s10479-020-03872-6 Crossref Full Text | Google Scholar Keywords: agentic AI, artificial intelligence, clinical decision support systems, electronic health records, healthcare administration, healthcare costs, hospital operations, personalized medicine Citation: Hinostroza Fuentes VG, Karim HA, Tan MJT and AlDahoul N (2025) AI with agency: a vision for adaptive, efficient, and ethical healthcare. Front. Digit. Health 7:1600216. doi: 10.3389/fdgth.2025.1600216 Received: 26 March 2025; Accepted: 16 April 2025;Published: 7 May 2025. Edited by: Reviewed by: Copyright:   2025 Hinostroza Fuentes, Karim, Tan and AlDahoul. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Hezerul Abdul Karim, hezerul@mmu.edu.my; Myles Joshua Toledo Tan, mylesjoshua.tan@medicine.ufl.edu; Nouar AlDahoul, nouar.aldahoul@nyu.edu Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on",2
5 ways AI is changing healthcare - Microsoft,https://news.google.com/rss/articles/CBMihgFBVV95cUxQZ3dTejZtd2VBZkFIV1hEdTVkWHEwaFVuNXdRM09zSE1mYlhDMWhjSUtrdzRsTWRpUVdlVlVQaXBfVl9iOEJETUlaMVpkemQ0T3JYUURCQXZ0NkwyNWY3NW5wVGFaS3prb0o3aEU3RmdYQ2F0N01JNERVZFhBckZhUGJqS3MwQQ?oc=5&hl=en-US&gl=US&ceid=US:en,"As emerging AI tools are being used in healthcare, doctors are spending more time looking at their patients instead of computer screens. Researchers are interpreting medical images faster and more accurately. Operating room schedulers are fitting in more lifesaving surgeries. With these early advances, clinicians, administrators, researchers and developers say they re already seeing AI s positive impact with innovations in how they approach patient care, handle administrative tasks and coordinate with care teams.  This is technology that spans all aspects of our business,  says Eric Shelley, vice president of analytics and digital solutions at Northwestern Medicine. Here are five ways AI is transforming healthcare, from the business office to the exam room. AI tools are personalizing office visits, says Dr. Jorge Scheirer, a physician and the chief medical information officer at St. Luke s University Health Network in Pennsylvania. He and his colleagues use Microsoft Dragon Copilot, the healthcare industry s first unified voice AI assistant, to focus fully on patients rather than computer screens. Scheirer can query charts ahead of appointments for insights and reminders, and then the system securely records, transcribes and summarizes   sometimes catching pertinent comments he hadn t heard. It pulls up any health records or vetted medical resources he asks for, helps find the right medical codes, and drafts after-visit notes and referrals for him to proof and sign.  It s uncanny how good a job it does,  Scheirer says. Dragon Copilot s time savings also alleviate clinician burnout, he says. Scheirer often worked until 10:30 p.m. to complete regulatory documentation. Now patients get their after-visit notes sooner, and he s home in time for dinner with his wife. Researchers are testing new tools that may help them identify tumors and diseases. Providence, the University of Washington and Microsoft developed multimodal AI models that may be able to help researchers interpret medical imaging and answer questions about it in natural language. GigaPath stitches all the tiny details in microscopic slides together into one complete picture, unlike traditional tools that focus on single sections. That could help with earlier disease detection and more personalized treatments. BiomedParse analyzes all types of medical scans to detect and identify abnormalities   even spotting things human eyes miss   and answer questions about highlighted areas. That could help speed up the diagnostic process and make it more accurate.  These technologies put us on the path to a future where vision becomes part of the intelligence we have,  says Dr. Carlo Bifulco, chief medical officer at Providence Genomics in Oregon. With AI chat capabilities,  you literally will have conversations with the medical images.  Morning huddles are now more informed, as everyone from operating room coordinators to pharmacy managers uses daily data reports to provide better, more efficient healthcare across Northwestern Medicine s 11 hospitals in and around Chicago. About 400 Power BI data visualization reports hosted on the Microsoft Fabric platform provide  a snapshot of the state of the health system,  says Eric Shelley, Northwestern s vice president of analytics and digital solutions. They track emergency visits, scheduled surgeries and patient appointments to help allocate resources. Fabric collects data in one secure place, from any software the organization s various groups might be using, and provides a shared workspace so Northwestern s data teams can manage it for accuracy. The  clean data  gives more confidence to hospital workers and allows them to use AI programs to screen reports about everything from medication dosing errors to safety incidents, helping prioritize responses and monitor trends, Shelley says. Integrating AI with medical research and clinical practice can make healthcare more effective, improving outcomes and reducing costs, says Jonathan Carlson, vice president and managing director of Microsoft Health Futures. AI-powered research is driving progress in precision medicine and personalized treatment plans, Carlson says. AI tools can sort through mounds of data faster than whole teams of researchers could, spotting patterns and making data-based predictions. That information can help doctors match patients with the right clinical trials faster, for example, or find existing medications likely to work on specific tumor mutations.  We can use this increasingly holistic image of a patient both to help the clinician reason about,  Hey, what s the next thing I should do to understand this patient?  Carlson says,  and then,  How do I compare that patient with the population and have a better idea of what s actually going to work?  Assessing a patient s health requires more than medical text comprehension. That s where AI models can help, by integrating and analyzing data sources across modalities such as images, video and audio. Microsoft Azure s multimodal medical imaging foundation models allow healthcare organizations to build AI tools specific to their needs. These models, developed in collaboration with Microsoft Research and strategic partners, reduce the extensive computing and data requirements typically involved in creating tools from scratch. The RAD-DINO model, for example, converts chest X-rays into digital formats that can be processed and organized to help better identify diseases. MedImageInsight helps classify and sort medical imaging, and MedImageParse-3D helps analyze and interpret MRIs and CT scans. The ECG-FM model detects patterns in electrocardiograms and HistAI helps with tasks identifying the organ a sample came from and whether it s healthy or diseased.",2
AI Symptom Checkers: What Every Person Must Know About These Healthcare Tools - nerdbot,https://news.google.com/rss/articles/CBMirAFBVV95cUxNUTltN2VPRDFEQkMzcnp4TnZLQUl5d1QxT3Y3Qi01ZVBGNmUxZEN2NUlqczdfR2p0TG50ZVprYU9tRlhMcHlLYTJyVkg5N3M4ZG9QakpjOHF4MnZBaGhrUFJmc2F1MHRFeGxUVVZmb0xpTGdlaEJwVlVKMk1JdksxLWpkdTRQY2t1S1Z3OTRkSzF6LUFzRm44Q3ZEb3h2OE1XSUdsM2RPdG5uajZS?oc=5&hl=en-US&gl=US&ceid=US:en,"The rising cost of healthcare often leads people to avoid doctor visits. They wait until they are seriously ill before making an appointment, which can do significant harm to their health. Innovative solutions are needed to help drive healthcare costs down while ensuring everyone has access to quality care. Many individuals today use an AI symptom checker to determine whether they need to visit a doctor or can treat their ailment at home. These symptom checkers enable them to remain in the comfort of their own homes while receiving medical advice as needed. They no longer need to search online to learn more about what they are possibly dealing with. The symptom checker helps narrow down the possibilities to just a few. If additional treatment is needed, the symptom checker connects the individual to a doctor for further help. What is a Symptom Checker? People might be confused by what a symptom checker is. Aren t these tools currently available? For many years, people could enter symptoms into a computer and it would give them possible diagnoses. A symptom checker differs in that it provides a free, private, and immediate virtual medical consultation. They don t need to log into the site or have insurance to participate in this consultation. The symptom checker is powered by AI that utilizes structured medical reasoning to determine the patient s condition. If a real doctor is needed following the initial consultation, the symptom checker can transfer the patient to one in minutes. A person no longer needs to wait days or weeks to receive a diagnosis. They learn whether they can be treated virtually or must make an appointment with a local physician. If the individual must see a local doctor, they can bring the information from the virtual visit to reduce intake times. In 2019, approximately 7% of all daily Google searches were related to healthcare topics. That number is likely to have increased in recent years. People want rapid and reliable medical information, and an AI symptom checker can provide it. A person might assume AI symptom checkers will eventually replace doctors. Nothing is further from the truth. They empower patients to take control of their health while reducing the burden on healthcare workers by addressing simple medical conditions that can be treated at home. Doctors then have more time to handle complex cases that require human intervention. Are AI Symptom Checkers as Accurate as Traditional Diagnoses? Individuals might worry that the AI symptom checker won t be accurate and their health will suffer. Technology has advanced significantly in recent years, so this concern can be laid to rest. A person enters their symptoms, and the algorithm uses this information to generate a list of potential diagnoses. The person can then determine the following steps to resolve the issue they are experiencing. When a person sees a healthcare provider in person, they report the symptoms they are experiencing. The provider may ask follow-up questions and do an exam. They might order additional tests to collect more information. When all data has been gathered, they make a diagnosis and offer treatment options. AI symptom checkers have become more accurate in recent years. They are continuously updated as improvements are made in machine learning. AI symptom checkers are designed to provide suggestions. If a person feels they need a confirmed diagnosis, the symptom checker can connect them to a qualified healthcare professional. Some AI symptom checkers today are highly accurate, matching the diagnosis and treatment plans of human doctors with precision. Furthermore, some AI symptom checkers today have an additional layer of oversight. Conversations are monitored in real time. If there are signs of a medical emergency, if the person discloses something of a sensitive nature, or the system feels anything is beyond its scope, the virtual consultation is halted and the individual is directed to seek immediate help. Convenience and Speed Individuals who try a symptom checker often find themselves impressed with what they see. Using this tool is very convenient, as it can be accessed anywhere with an internet connection and remains available 24 hours a day, seven days a week. They don t need to visit a traditional medical facility in the middle of the night or find a ride if they don t have transportation. There is no wait time either. AI Symptom Checker Limitations These symptom checkers make diagnoses based on information provided by the users. If a person doesn t accurately report their symptoms, the suggested diagnoses won t be accurate. Furthermore, AI cannot replace human doctors, as it cannot take a person s pulse or consider the emotional aspects of an individual s health. Human doctors are needed for comprehensive assessments. When Should a Person Use an AI Symptom Checker? People may wonder when to use an AI symptom checker and when an in-person visit is a better option. Use this tool when the symptoms being experienced are mild or when more information is needed to determine whether medical care is required. If symptoms are serious or last for an extended period, it s time for an in-person visit with a medical professional. AI Symptom Checkers and Bias In addition to concerns about accuracy due to patients misreporting symptoms, there are also worries that symptom checkers don t fully account for demographic differences. They are built around datasets derived from Western healthcare symptoms. As a result, diagnoses may not be as accurate for underrepresented populations. Another problem with symptom checkers is that some conditions present differently based on the patient s age and sex. This tool may underdiagnose heart attacks in women because it prioritizes pain over the more subtle symptoms of a heart attack that women frequently experience. They also aren t as accurate when diagnosing pediatric conditions because the data used when building these models is adult-centric. AI symptom checkers also struggle with language barriers and cultural differences. Linguistic and cultural backgrounds influence how people describe their symptoms. Non-native English speakers may receive less accurate diagnoses and less precise recommendations, which can potentially lead to delayed medical attention. Problems such as these must be addressed rapidly to expand healthcare access to all. Every person should try an AI symptom checker so they are familiar with how these tools work. If they need to use the tool at a later date, they will already know how it works and can get help sooner. AI symptom checkers offer numerous benefits and few drawbacks, so they are likely to become more commonplace in the future. The necessity for safety and security in relation to cars and other vehicles is crucial in today's society. This demand for security starts from the point of buying an automobile and extends to when we drive it. To achieve this level of safety and security, we require VINs and VIN  In ""NV Apps/Review"" Imagine a world where patients can monitor their vitals, consult with doctors, and manage their treatment plans all from the palm of their hand. This isn't a glimpse into the future; it's the reality that mobile health apps are creating right now. In an era where smartphones are ubiquitous, mobile health  In ""NV Health/Lifestyle/Travel"" Hospice care, fundamentally centered on providing comfort and support to terminally ill patients, has traditionally been a deeply personal and human-centric field. However, as technology advances, it is clear that modern tools can significantly enhance the quality of care. Integrating technology into hospice revenue management allows caregivers to streamline operations,  In ""NV Health/Lifestyle/Travel"" Jack Wilson is an avid writer who loves to share his knowledge of things with others. None found Type above and press Enter to search. Press Esc to cancel.",2
Preparing healthcare data for AI models - Wolters Kluwer,https://news.google.com/rss/articles/CBMikgFBVV95cUxPMVpaVVNrV04yRVFIUWZyU2pxTm5FVm9QUFRsYmN1UDEzTXQ0MGpQTjh6b0p6cFVJNzRGNnF0dzl4TzB4OU43S0pIaTRKOXNvSHZkRzlQNjY3S1pzMkhGRnlTZXN0UGlDRVBBVDdDajVWVnd6SjdOZW5WaHlEQWM0WnlCY2RiXzU5UmdCVmUzM0VMdw?oc=5&hl=en-US&gl=US&ceid=US:en,"Wolters Kluwer Wolters Kluwer is a global leader in information, software solutions and services for professionals in healthcare; tax and accounting; financial and corporate compliance; legal and regulatory; corporate performance and ESG. Select Language Trusted clinical technology and evidence-based solutions that drive effective decision-making and outcomes across healthcare. Specialized in clinical effectiveness, learning, research and safety. Enabling tax and accounting professionals and businesses of all sizes drive productivity, navigate change, and deliver better outcomes. With workflows optimized by technology and guided by deep domain expertise, we help organizations grow, manage, and protect their businesses and their client s businesses. Enterprise software to drive financial and sustainability performance, manage risks, meet reporting requirements, improve safety and productivity, and reduce environmental impact. Our solutions for regulated financial departments and institutions help customers meet their obligations to external regulators. We specialize in unifying and optimizing processes to deliver a real-time and accurate view of your financial position. Enabling organizations to ensure adherence with ever-changing regulatory obligations, manage risk, increase efficiency, and produce better business outcomes. Serving legal professionals in law firms, General Counsel offices and corporate legal departments with data-driven decision-making tools. We streamline legal and regulatory research, analysis, and workflows to drive value to organizations, ensuring more transparent, just and safe societies. The integration of artificial intelligence (AI) in healthcare is accelerating at a tremendous pace. However, the efficient utilization of AI in healthcare largely depends on the quality of the data. According to studies, 80% of healthcare data exists in unstructured formats, making it challenging for AI algorithms or large language models to extract meaningful insights. The phrase ""garbage in, garbage out"" aptly describes this situation. To truly harness the capabilities of generative AI in healthcare, it's essential to address and overcome the challenges related to data quality and to maintain clean data. When we talk about AI data preparation for healthcare, it's a two-phase process training the AI models and then implementing these trained models for useful insights. One of the major hurdles in using healthcare data for training AI models is the consistency of data quality and accuracy. Healthcare data from different care settings lacks standardized formats and accuracy, which often results in data misinterpretation or loss of valuable insights. Moreover, medical or lab data usually contains inaccuracies, incomplete information, and lacks validity. These data quality issues can mislead the AI models into perceiving patterns that don't actually exist, which can further lead to inaccurate or misleading results. Therefore, it's crucial to understand and address these pitfalls while preparing data for machine learning models. A common problem during AI data preparation arises from semi-structured and unstructured data. With 80% of healthcare data existing in unstructured formats, like clinical notes, there is a need to map that raw information to industry standards. Because of these challenges, it's crucial for healthcare organizations to put in place certain tools or processes for assessing, cleaning, and standardizing their data before utilizing it for AI technologies. Clinical terminology tools that codify clinical notes to industry standards can help improve the data quality going into AI models. The successful integration of AI in healthcare largely depends on the quality of the data. Training AI models with unclean or messy data can lead to several complications such as a decrease in accuracy and the inclusion of bias. Insufficient or overly simplified data related to minority populations can cause bias to be built into the model, which may lead to wrong assumptions and poor recommendations. Maintaining data quality is crucial when it comes to preparing data for AI models. Six core elements that organizations should focus on when implementing AI tools are accuracy, validity, data integrity, completeness, consistency, and timeliness. By ensuring these qualities, healthcare organizations can prepare their data for AI in an efficient way, with minimal error. A strong data governance process including aligning data and validating codes to an industry standard is essential for maintaining data quality for AI models. It can help in distinguishing between good and bad data. For example, it's important to verify lab results against appropriate codes to avoid incorrect codes getting into the system. We have found in one data set that the data quality was as low as 30% accurate as it contained invalid codes and incorrect codes for labs. Normalizing and mapping lab data to LOINC can help in consolidating information from multiple sources and authors, thereby ensuring the accuracy of data. The clinical terminologies used to standardize healthcare data like LOINC, ICD-10, CPT, and SNOMED, release code set updates 600+ times a year. Having a single source of truth for clinical terminology is key to ensure that the data used to train AI models is correct. Continuous assessment of data used for training the model helps in identifying gaps or bias within the data. Healthcare data can present challenges due to its complexity, so creating a process where the data is properly assessed and cleaned is crucial. AI's potential in healthcare is vast, but the basics of data quality must not be overlooked as they determine the success of AI platforms. Through effective data governance and normalization practices, healthcare organizations can maximize AI capabilities and ensure the most accurate outputs for the betterment of patient care. Health Language Data Solutions can help ensure your healthcare data is prepared to power your AI tools. Speak to an expert today to help understand your data quality. When you have to be right   2025 Wolters Kluwer N.V. and/or its subsidiaries. All rights reserved.",2
"Health Care AI, Intended To Save Money, Turns Out To Require a Lot of Expensive Humans - KFF Health News",https://news.google.com/rss/articles/CBMimwFBVV95cUxPRGpsWW42VExXdVVMVVUxMWE1ZGg1TW5HWTNuWFFYU2ZMOXhUMnR2WkRERDhabVU0M0MtNnlGYkRJTkh4UHdQSXRhZHF0ZG00LVgtNHlabDU3a0VzQVVmZ2pnVGUtQWhoaFVwWjMwbjV0cWVLeFRMUnJtSkpDVlJSWXpLdHA0Q3p6VDROVGp0b3dUa0RCUTIwN0M1VQ?oc=5&hl=en-US&gl=US&ceid=US:en,"Republish This Story Disponible en Espa ol Preparing cancer patients for difficult decisions is an oncologist s job. They don t always remember to do it, however. At the University of Pennsylvania Health System, doctors are nudged to talk about a patient s treatment and end-of-life preferences by an artificially intelligent algorithm that predicts the chances of death. This story also ran on CBS News. It can be republished for free. But it s far from being a set-it-and-forget-it tool. A routine tech checkup revealed the algorithm decayed during the covid-19 pandemic, getting 7 percentage points worse at predicting who would die, according to a 2022 study. There were likely real-life impacts. Ravi Parikh, an Emory University oncologist who was the study s lead author, told KFF Health News the tool failed hundreds of times to prompt doctors to initiate that important discussion   possibly heading off unnecessary chemotherapy   with patients who needed it. He believes several algorithms designed to enhance medical care weakened during the pandemic, not just the one at Penn Medicine.  Many institutions are not routinely monitoring the performance  of their products, Parikh said. Algorithm glitches are one facet of a dilemma that computer scientists and doctors have long acknowledged but that is starting to puzzle hospital executives and researchers: Artificial intelligence systems require consistent monitoring and staffing to put in place and to keep them working well. In essence: You need people, and more machines, to make sure the new tools don t mess up.  Everybody thinks that AI will help us with our access and capacity and improve care and so on,  said Nigam Shah, chief data scientist at Stanford Health Care.  All of that is nice and good, but if it increases the cost of care by 20%, is that viable?  Subscribe to KFF Health News' free Morning Briefing. Government officials worry hospitals lack the resources to put these technologies through their paces.  I have looked far and wide,  FDA Commissioner Robert Califf said at a recent agency panel on AI.  I do not believe there s a single health system, in the United States, that s capable of validating an AI algorithm that s put into place in a clinical care system.  AI is already widespread in health care. Algorithms are used to predict patients  risk of death or deterioration, to suggest diagnoses or triage patients, to record and summarize visits to save doctors work, and to approve insurance claims. If tech evangelists are right, the technology will become ubiquitous   and profitable. The investment firm Bessemer Venture Partners has identified some 20 health-focused AI startups on track to make $10 million in revenue each in a year. The FDA has approved nearly a thousand artificially intelligent products. Evaluating whether these products work is challenging. Evaluating whether they continue to work   or have developed the software equivalent of a blown gasket or leaky engine   is even trickier. Take a recent study at Yale Medicine evaluating six  early warning systems,  which alert clinicians when patients are likely to deteriorate rapidly. A supercomputer ran the data for several days, said Dana Edelson, a doctor at the University of Chicago and co-founder of a company that provided one algorithm for the study. The process was fruitful, showing huge differences in performance among the six products. It s not easy for hospitals and providers to select the best algorithms for their needs. The average doctor doesn t have a supercomputer sitting around, and there is no Consumer Reports for AI.  We have no standards,  said Jesse Ehrenfeld, immediate past president of the American Medical Association.  There is nothing I can point you to today that is a standard around how you evaluate, monitor, look at the performance of a model of an algorithm, AI-enabled or not, when it s deployed.  Perhaps the most common AI product in doctors  offices is called ambient documentation, a tech-enabled assistant that listens to and summarizes patient visits. Last year, investors at Rock Health tracked $353 million flowing into these documentation companies. But, Ehrenfeld said,  There is no standard right now for comparing the output of these tools.  And that s a problem, when even small errors can be devastating. A team at Stanford University tried using large language models   the technology underlying popular AI tools like ChatGPT   to summarize patients  medical history. They compared the results with what a physician would write.  Even in the best case, the models had a 35% error rate,  said Stanford s Shah. In medicine,  when you re writing a summary and you forget one word, like  fever    I mean, that s a problem, right?  Sometimes the reasons algorithms fail are fairly logical. For example, changes to underlying data can erode their effectiveness, like when hospitals switch lab providers. Sometimes, however, the pitfalls yawn open for no apparent reason. Sandy Aronson, a tech executive at Mass General Brigham s personalized medicine program in Boston, said that when his team tested one application meant to help genetic counselors locate relevant literature about DNA variants, the product suffered  nondeterminism    that is, when asked the same question multiple times in a short period, it gave different results. Aronson is excited about the potential for large language models to summarize knowledge for overburdened genetic counselors, but  the technology needs to improve.  If metrics and standards are sparse and errors can crop up for strange reasons, what are institutions to do? Invest lots of resources. At Stanford, Shah said, it took eight to 10 months and 115 man-hours just to audit two models for fairness and reliability. Experts interviewed by KFF Health News floated the idea of artificial intelligence monitoring artificial intelligence, with some (human) data whiz monitoring both. All acknowledged that would require organizations to spend even more money   a tough ask given the realities of hospital budgets and the limited supply of AI tech specialists.  It s great to have a vision where we re melting icebergs in order to have a model monitoring their model,  Shah said.  But is that really what I wanted? How many more people are we going to need?  Darius Tahir: DariusT@kff.org, @dariustahir Share This Story: We want to hear from you: Contact Us Republish This Story By Darius Tahir January 10, 2025 We encourage organizations to republish our content, free of charge. Here s what we ask: You must credit us as the original publisher, with a hyperlink to our kffhealthnews.org site. If possible, please include the original author(s) and KFF Health News  in the byline. Please preserve the hyperlinks in the story. It s important to note, not everything on kffhealthnews.org is available for republishing. If a story is labeled  All Rights Reserved,  we cannot grant permission to republish that item. Have questions? Let us know at KHNHelp@kff.org Trump Voters Wanted Relief From Medical Bills. For Millions, the Bills Are About To Get Bigger. Journalists Drill Down How Federal Cuts Will Affect Medicaid, Cancer Research, and Uninsured Rates Fearing Medicaid Coverage Loss, Some Parents Rush To Vaccinate Their Kids Listen: Some Scientists Speak Out on Deep Cuts to National Cancer Institute, While Others Flee   2025 KFF. All rights reserved. Powered by WordPress VIP Thank you for your interest in supporting Kaiser Health News (KHN), the nation s leading nonprofit newsroom focused on health and health policy. We distribute our journalism for free and without advertising through media partners of all sizes and in communities large and small. We appreciate all forms of engagement from our readers and listeners, and welcome your support. KHN is an editorially independent program of KFF (Kaiser Family Foundation). You can support KHN by making a contribution to KFF, a non-profit charitable organization that is not associated with Kaiser Permanente. Click the button below to go to KFF s donation page which will provide more information and FAQs. Thank you! Notifications",2
The Healthcare AI Adoption Index - Bessemer Venture Partners,https://news.google.com/rss/articles/CBMiakFVX3lxTE9lNHhZU0xCeVRKTjNhd29wMkVtVWpCbHQ3ZTlWWnpRWFRIWlZRcVlWLTBBUm5fV3dncy1sbE4zaWFiSGdiWTUxcHo1eHdKQXNiWG13YjN4V1ZfY0hLV1ZTa1Z6UGVqN0NMbEE?oc=5&hl=en-US&gl=US&ceid=US:en,"The Healthcare AI Adoption Index Top takeaways The state of AI adoption today Experimentation but not yet production The development strategy How startups can win Introducing the AI Dx Index How buyers can navigate the fast-evolving AI ecosystem Healthcare enters the era of AI co-development Appendix The healthcare industry has reached its AI inflection point, a technological shift mirroring the earlier transition to electronic health records (EHRs) but this time unfolding at warp speed. The changeover from paper to electronic health records spanned more than a decade, aided by the HITECH Act of 2009, which paid incentives to doctors to adopt EHRs. Yet progress was slow, plagued by reticence to change and complex, disparate processes. In stark contrast, just two and a half years after LLMs became accessible to the masses, AI is now a top priority in healthcare. It s shaping boardroom agendas, fueling startups (i.e., healthcare AI applications), and easing daily burdens for clinicians, scientists, and administrators. Curiosity has quickly turned into action, with a wave of experimentation underway to reinvent care delivery, diagnosis, and drug development. Bessemer Venture Partners, in partnership with Amazon Web Services (AWS) and Bain & Company, surveyed more than 400 leaders across Payer, Pharma, and Provider segments to understand how healthcare buyers are approaching GenAI applications what jobs-to-be-done they prioritize, how they re making adoption decisions, and where their projects sit on the adoption curve. Unlike the earlier shift to EHRs, today s GenAI innovation isn t just coming from healthcare AI applications but rather from inside healthcare organizations themselves. Internal development teams are building their own tools by partnering with horizontal AI labs (i.e., Anthropic), Big Tech companies, or by procuring new solutions from incumbent systems of record and healthcare IT (HCIT) vendors. We found that there is a flood of internally and externally developed GenAI proof of concept (POC) projects, but few make it to implementation at scale. Early-stage application layer startups must learn to navigate healthcare s evolving go-to-market landscape, while buyers need to separate the signal from the noise and build dynamic AI strategies. This report is a snapshot of today s healthcare AI landscape and we plan to continue tracking the adoption, evolution, and impact of these solutions. To that end, we give our  diagnosis  of the current state of opportunities with the AI Dx Index, which can help founders, buyers, and investors accurately assess where our industry is in the AI transformation curve and where the most significant areas for opportunity in the near term lie across nearly 60 jobs-to-be-done. Healthcare organizations overwhelmingly see AI as a strategic priority and they re backing that belief with investment. In our survey, 95% of respondents said GenAI will be transformative, with 85% of Provider and 83% of Payer leaders expecting it to reshape clinical decision-making within three to five years. This is faster than we anticipated, given the industry s regulatory complexity and low tolerance for error. Across all segments, 84% believe AI will impact clinical decisions, 80% expect it to reduce labor costs through automation, and many see the potential for revenue gains. However, only 57% of Pharma executives believe AI will drive most new therapy discoveries in the next decade, reflecting the complexity and long timelines of drug development. Executives are putting real money behind their AI ambitions; the data shows that there s an upfront investment to AI innovation and staying ahead of the curve compared to competitors means starting yesterday. (We describe budgeting trends further below.) Still, it's early days. Only half of the organizations surveyed have a clear AI strategy, while 57% have an AI governance committee, and Payers are leading the way. That said, 54% are already seeing meaningful ROI within the first year of GenAI implementation. To understand where GenAI can deliver the most impact, we surveyed executives on 59 key jobs-to-be-done across the healthcare value chain: We found that nearly half (45%) of the use cases across these jobs are still in the ideation or POC phase, with far fewer actually in production. For a full description and breakdown of adoption levels of use cases across these three segments, refer to the Appendix. Providers are ahead in POC experimentation, while most Payer and Pharma use cases remain in the ideation phase. Unlike the EHR adoption wave of the last few decades, Providers haven t had the need for regulation or government payments to incentivize them to adopt AI. AI-powered ambient scribes for clinical documentation illustrate this rapid adoption. While EHRs improved data handling, they also increased administrative burden, creating an opening for AI. This wave is defined by speed, enthusiasm, and a focus on boosting productivity. Among providers surveyed, 30% have system-wide deployments, another 22% are in implementation, and another 40% are actively piloting solutions. Many organizations are running dozens of GenAI POC projects at once, but only 30% of completed POCs have made it into production. Providers have the most projects making it into production among the segments we tracked, and large Providers in particular are leading the charge with 46% of their POCs going to production. So why are so many organizations stuck in the experimentation phase? This wave of AI adoption has been driven by  test and learn  urgency, with boards and CEOs pushing teams to discover possible use cases. Mid-to-large Providers are the exception, as they are early adopters with more resources to bring AI into production. Despite enthusiasm, executives across all segments pointed to four main barriers to scaling AI: However, budgeting has yet to be a roadblock to GenAI experimentation. According to the survey, 65% of AI projects are being funded through a centralized budget and 35% through departmental dollars. Most importantly, the majority across each segment stated that budget was not a barrier to scaling AI projects from POC into production scale. Healthcare systems and buyers know the urgency of AI and so budgets are being cleared for the sake of experimentation because of these strategic priorities. On average, 60% of respondents (65% Payers, 57% Pharma, and 56% Providers) are seeing GenAI budgets growing faster than general IT budgets. C-suite leaders are at the helm, controlling the purse strings and determining 70% of the decisions on AI use cases. Given these dynamics, startups need to prioritize strategic relationships with the C-suite, no matter the end user. (More startup lessons later.) Many AI projects today are being developed in-house with horizontal AI labs and large tech partners. While startups are no longer the only source of innovation, healthcare AI entrepreneurs have the opportunity to reimagine how to work with buyers to serve emergent AI strategies. Over half (54%) of executives we surveyed feel comfortable with and want to work with early-stage startups, but, on average, 48% prefer a more innovative startup over an established technology player and 55% of executives would only consider working with startups with proven track records. The takeaway? Startups must rethink their go-to-market strategy as they build market traction and prove results with peer healthcare organizations healthcare buyers want innovation but are still risk-averse. It's the classic catch-22: You need social proof to sell, but you can t get social proof until you do. Less than 15% of projects are currently being developed at and procured from startups, as organizations believe they can build AI tools on their own or procure incremental AI features from incumbents. 51% of Payer projects are being developed internally, compared to 43% of Pharma projects and 27% of Provider projects, with variation by use cases. Off-the-shelf models and horizontal automation platforms are great for creating simple minimum viable products (MVPs) and automated process improvements that don t require consistent high accuracy. Startups, however, can offer more applications that are specific, specialized, and more accurate. (More best practices below in the  How to win  section.) We expect the failure rate of early-stage startups in this space to stabilize over time, in turn helping more healthcare AI startups gain traction with buyers. Cloud infrastructure players are the core platform atop which AI-first applications are being developed either by internal teams or startups and we chose to partner with AWS on this report for its unique position in the health AI ecosystem. AWS and other cloud infrastructure partners can curate a list of best-in-class startups to work with, ensure security, and improve integration standardization and data readiness for AI applications. Two example programs include the AWS Partner Network and AWS Marketplace. If you re an entrepreneur building in healthcare, here s a hard pill to swallow: Buyers aren t going to assume your tool is best-of-breed just because you ve got some venture funding and an exciting pitch. Only 32% of our survey respondents agreed that startups have best-of-breed GenAI solutions that are superior to those developed by large technology incumbents. Startups are not just competing with other startups, but with a full ecosystem that includes potential buyers  internal teams, horizontal AI labs, incumbent systems of record like Epic and Veeva, and HCIT companies deploying AI features. Founders, here s how your AI application startup can stand out in this increasingly crowded field in healthcare. Winning startups don t just sell a point solution, they land with a high-impact use case and then expand, adding capabilities to adjacent processes upstream or downstream of the core workflow. This enables a company to deepen touch points either by serving more needs of a particular user or by sitting at the intersection point between stakeholders (e.g., collaborations between internal teams or external parties, like payer-provider interactions). To help AI startups prioritize healthcare use cases, we created the AI Dx Index, based on survey data. It factors in market opportunity, urgency, and current AI adoption to give startups a strategic edge when engaging healthcare buyers. The Index includes three components: Based on our survey data, here s how we position these use cases on the index: As AI adoption increases, a use case moves to the right on the x-axis, and its opportunity may shrink as the problem gets solved and manual work is reduced. Let s look at documentation support (like AI scribes) as an example. This category has a high Adoption Score (53) because over 60% of organizations are already using it. While it's still a big pain point, especially around physician burnout, much of the manual work is already being automated, so its Opportunity Score is now in the medium range (43). Sellers and buyers will view the 2x2 chart differently. Entrepreneurs and investors should focus on the top left, where high Opportunity Scores overlap with early AI adoption. Healthcare buyers, meanwhile, will gravitate toward the middle or top right, where solutions show traction unless they re open to co-developing early-stage projects. When selecting the right entry point, prioritize use cases where you can access unique data, engage key stakeholders, and align with the appropriate budget owner. Also consider how mission-critical the process is these high-impact areas offer significant opportunity but come with higher stakes and slower adoption. To succeed, your solution must demonstrate reliable, accurate results from day one. Review slides below to see how each of the jobs-to-be-done surveyed score on the three components of the AI Dx Index. The best way to move beyond a POC is to show clear impact, which can start with case studies from similar environments or even other departments within the buyer s organization. Our research states the most compelling startups and organizations assess and measure ROI ahead of a POC or pilot by: To avoid getting stuck in POC limbo, startups must demonstrate both financial and non-financial impact. We discovered that buyers expect fast results, with 60% of respondents across segments expecting to see positive ROI in under 12 months. And the onus will be on the startup to acknowledge its customer s investment of time and resources 75% of survey respondents say startups fail to recognize the real cost of a POC. Lastly, startups must also get ahead of scaling challenges like data governance, security, and integration by involving key stakeholders early, including legal and IT. We learned in this survey that a whopping 64% of buyers are willing to co-develop solutions with startups.  Co-development  in this context might look like embedding sales engineers and developers into the selling process. But how should startups shift from traditional healthcare go-to-market cycles to co-development without becoming glorified custom software development shops? Enterprise healthcare buyers expect to have some level of control and ride shotgun with companies. Specifically, we mean: To avoid the churn common in AI services, startups should position their product as a reimagined end-to-end workflow that evolves with customer needs, highlighting domain expertise over technical novelty. Startups can build defensibility by investing in deep integrations with related software especially when their solution connects multiple systems and handles novel data sources beyond a single record system. These integrations boost retention, reduce security risks, and set startups apart from incumbents limited to adding features within existing platforms. We believe that within a year or two many internally built POCs will struggle to scale or deliver reliable accuracy, which will then prompt a shift toward proven solutions, often validated by peer case studies. While it s easy to stitch together off-the-shelf models to build an MVP, sustaining 99% accuracy at scale is far harder. The more complex the process, the less effective horizontal tools become. That said, there s still low-hanging fruit in single-step workflows ripe for general automation. Startups should target high-frequency, high-accuracy use cases and pair them with a seamless, user-friendly UI. Finally, startups must lead with empathy. While buyers are eager to adopt AI, many face internal resistance from colleagues wary of disruption. Our survey revealed key barriers to full deployment: lack of in-house expertise, data readiness, security compliance, costly integrations, and added IT burden. Startups should position their tools as empowering not replacing teams, and explain deployment clearly, without technical jargon. Traditional healthcare software vendors have captured only a fraction of the value they create: Software makes up just <5% of healthcare administrative spend, limited by IT budgets and point-solution sales (aside from systems of record). However, GenAI offers a more scalable way to tap into the remaining 80 90%. The AI wave gives founders a real opportunity to align pricing and business models with the value they deliver. We have written extensively about these new business models in the age of AI, and you can read about them here and here. A particularly intriguing business model that has emerged in this space is what we refer to as AI services-as-software, which allows you to get compensated for the work you provide. Aligning your model to the value you create underscores the importance of measuring clear, attributable ROI. Over time, we hope AI-first companies can tap into opex budgets especially since, as the survey shows, funding isn t a barrier when ROI is well-defined and continuously proven. If you re a healthcare executive reading this, you have likely been inundated by AI pitches given the rapid pace of innovation similar to the majority of our survey respondents. At the same time, you are likely under pressure to drive AI progress through managing dozens of projects and POCs. We ve developed a shortlist of principles to navigate common challenges, leveraging insights from our report partner Bain & Company, which advises hundreds of healthcare clients on AI strategy. Like any technological shift, success with AI requires behavioral change especially in large organizations. While large companies have the resources to invest, they re also held back by bureaucracy and legacy processes. And AI s potential to disrupt workflows can spark fear, despite its promise. Prime your organization for the potentially massive value that can be created and have strong change management plans in place to help drive adoption of new technology. For example, the CTO of a large health system we surveyed said it well: Storytelling, rather than imposition of a new technology, is required by AI champions to bring stakeholders across their business onboard. Leaders can help establish the willingness to experiment, and mid-level employees can showcase the benefits of AI-powered tools to their colleagues to help drive adoption. Moving quickly and adapting as necessary is the name of the game. This involves collaborating with a range of different stakeholders, from foundational tech partners to agile startup partners. Given the pace of change, building all tools internally is unlikely to be a winning strategy for most organizations. It is also critical to be nimble and adaptive yourself. We know this can be especially tough for large organizations, but AI is still in its early stages. Success depends on having a flexible strategy with clear priorities, making it easier to prioritize effort. And remember, not every AI project will succeed many internal efforts and startups will fail. But some will thrive. Watch how your peers are partnering and use those case studies to guide your own decisions. Ecosystem partners can also help to make your governance, tech stack, and cybersecurity AI-ready while enforcing responsible AI policies. We know you re feeling urgency (and you should be!), but don t forget we are still very early in the AI journey, which will take us from today s chatbots to AI systems that will one day be able to run organizations. Get ahead of your competitors by developing technology where differentiation matters, adopting tools across your organization and maximizing the value of your unique, proprietary data. AI is no longer a distant promise in healthcare. It s already here, moving fast, and beginning to reshape how care is diagnosed, delivered, and managed. But unlike past waves of innovation, this one is defined by co-development, not just procurement. Today, healthcare organizations aren t simply buying AI they re building it, often in partnership with horizontal AI labs, cloud providers like AWS, and a select group of startups. Increasingly, they re bypassing traditional IT budgets and vendors, tapping into services spend as AI becomes core to workforce productivity and operational performance. The pace is staggering. Since AI s watershed moment in 2023, many health systems are now running dozens of pilots at once. Yet most projects, whether startup-led or internally-developed, won t scale. That s expected. The winners will be those that: Our AI Dx Index is designed to help founders and healthcare buyers identify the most promising use cases those with the greatest pain points, lowest automation, and highest potential for transformation. Over time, we expect these scores to evolve as more projects move from ideation to implementation, and as the gap between hype and real value narrows. The results of this research highlights healthcare s inflection point. If you're a startup, buyer, or investor shaping this transformation, we want to hear from you. Let us know what you re seeing, what s working, and what we should be measuring next. Reach out to steve@bvp.com or sguerra@bvp.com we re ready to continue the conversation. Venture insights that matter",2
AWS Health Data & AI Day Dublin 2025: EHDS implementation and healthcare innovation | Amazon Web Services - Amazon Web Services,https://news.google.com/rss/articles/CBMiwAFBVV95cUxPbE5RZkR0cUN0cVYweEs2bkxJSnVpTmN6cjZJblV0M2ZuQzVtYkNHSEloLXB0OWZmdWNwbmxxamo1c3RvemxWWkw1UGJualdPdDJ6ZUFSZGV0Ni1iMmsxY0FTczBHZWtXZndIZ0xHelNtWExEQ2Utanh2MnhTTmhuLXdxQ1liOUVuMmY3NEhwc2NOME1ZMkxOU1NEczhPc09OYVpXX3NoN2Vmd2RWVVQ2WkU2YWJjbnZmZHlfYnRoRDk?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies and similar tools that are necessary to provide our site and services. We use performance cookies to collect anonymous statistics, so we can understand how customers use our site and make improvements. Essential cookies cannot be deactivated, but you can choose  Customize  or  Decline  to decline performance cookies. If you agree, AWS and approved third parties will also use cookies to provide useful site features, remember your preferences, and display relevant content, including relevant advertising. To accept or decline all non-essential cookies, choose  Accept  or  Decline.  To make more detailed choices, choose  Customize.  Essential cookies are necessary to provide our site and services and cannot be deactivated. They are usually set in response to your actions on the site, such as setting your privacy preferences, signing in, or filling in forms. Performance cookies provide anonymous statistics about how customers navigate our site so we can improve site experience and performance. Approved third parties may perform analytics on our behalf, but they cannot use the data for their own purposes. Functional cookies help us provide useful site features, remember your preferences, and display relevant content. Approved third parties may set these cookies to provide certain site features. If you do not allow these cookies, then some or all of these services may not function properly. Advertising cookies may be set through our site by us or our advertising partners and help us deliver relevant marketing content. If you do not allow these cookies, you will experience less relevant advertising. Blocking some types of cookies may impact your experience of our sites. You may review and change your choices at any time by selecting Cookie preferences in the footer of this site. We and selected third-parties use cookies or similar technologies as specified in the AWS Cookie Notice. We and our advertising partners ( we ) may use information we collect from or about you to show you ads on other websites and online services. Under certain laws, this activity is referred to as  cross-context behavioral advertising  or  targeted advertising.  To opt out of our use of cookies or similar technologies to engage in these activities, select  Opt out of cross-context behavioral ads  and  Save preferences  below. If you clear your browser cookies or visit this site from a different device or browser, you will need to make your selection again. For more information about cookies and how we use them, read our Cookie Notice. To opt out of the use of other identifiers, such as contact information, for these activities, fill out the form here. For more information about how AWS handles your information, read the AWS Privacy Notice. We will only store essential cookies at this time, because we were unable to save your cookie preferences.If you want to change your cookie preferences, try again later using the link in the AWS console footer, or contact support if the problem persists. AWS Blogs On June 24, 2025, Amazon Web Services (AWS) will welcome healthcare professionals to our Dublin offices for AWS Health Data & AI Day a full-day event focused on the European Health Data Space (EHDS) and the transformative potential of artificial intelligence (AI) in healthcare. The event will bring together healthcare leaders, technical experts, and innovators to explore how secure, standardized, and interoperable data sharing can improve healthcare delivery and patient outcomes across Europe. Attendees will take part in two dedicated tracks Executive and Technical designed to foster collaboration between business and technical stakeholders. This cross-functional engagement is critical to unlocking the value of EHDS and advancing the responsible use of AI across healthcare systems. The Executive Track will explore EHDS implementation strategies and real-world approaches to AI adoption in clinical and operational settings. The day will open with an overview of healthcare trends in Ireland, delivered by Niamh Ferris, AWS healthcare lead for Ireland. This will be followed by a case study on the HSE Health App, presented by Gar Mac Cr osta, digital advisor for strategy & architecture at HSE, alongside Kevin Devine and Tara Nolan from Nearform, who led the product s design and delivery. Bernhard Geist, EMEA principal business development manager for Electronic Health Records (EHR) at AWS, will share insights into the evolving health data platform landscape and the broader EHDS vision. Dougal Fleming, CTO UK & Ireland at Dedalus, will then examine data interoperability frameworks aligned with EHDS goals. After lunch, the conversation will shift toward AI in healthcare. Kate Surgue, head of data science at Vhi, will walk through her team s AI journey, highlighting key lessons and impact. A fireside chat will follow, featuring Tracy McClelland from Dedalus, Jonathan Larbey from T-Pro, and Daniel Harrison, AI & GenAI strategist at AWS, who will share perspectives on implementing AI to improve patient care. The Executive Track will conclude with a session on responsible AI in healthcare, presented by a leader from Accenture, covering governance and ethical best practices. The Technical Track is tailored for healthcare IT professionals, developers, and architects working with AWS services. It opens with a deep dive into AWS s portfolio for EHDS and running health data platforms at scale, presented by Khrystyna Shlyakhtovska and Bernhard Geist, EMEA Healthcare business development managers for EMEA healthcare at AWS. Two hands-on workshops will follow, led by Enda Kenny, Donny Wilson, and Chris Haddad AWS s leading Irish and global solution architects for healthcare. The first workshop will offer practical experience implementing security and compliance requirements for generative AI in healthcare settings using AWS services. The second will provide a hands-on session focused on AWS HealthLake implementation. Date and time: Tuesday, June 24, 2025, 9:00 AM   4:00 PM Venue: Amazon Ireland   DUB19, Block 2 Charlemont Row Dublin, D D02 F6X6 Register today for the Executive Track for healthcare leaders and decision-makers. Register today for the Technical Track Registration for healthcare tech professionals. Attendees should have experience with AWS services and an active AWS account. Don t forget your laptop for workshop participation. The event is no cost, but places are limited. Register today to secure your place at the AWS Health Data & AI Day Dublin! Niamh serves as the healthcare lead for the Republic of Ireland at AWS, where she supports public sector organizations in adopting cloud technologies to enhance healthcare delivery and outcomes. With a focus on digital transformation, she collaborates with healthcare providers, startups, and industry stakeholders to implement innovative solutions leveraging AWS's cloud capabilities. Niamh holds a Master's degree in Political Communication from Dublin City University and has rich experience in fundraising, marketing communications, marketing, public relations, and research. Sin ad is head of marketing for Ireland and Northern Ireland at AWS, where she drives demand generation and pipeline growth through close alignment with sales. With more than 16 years of experience in B2B SaaS and tech, she previously led global marketing at Origina and co-founded the performance agency Donutz Digital. Sin ad holds a Master s in Electronic Commerce from DCU and was awarded the 2025 AWS Legend Award for marketing excellence. Khrystyna is a business development manager at AWS and head of AWS for European Health Data Space (EHDS). The focus of her work is to help organizations on their way to digital transformation and cross-organizational data-sharing initiatives in healthcare and public sector across Europe. With a background in both law and economics, she has built a career at the intersection of technology, regulation, and strategic partnerships. She holds master s degrees in Business Administration, Economics and Law from Lviv State University of Internal Affairs and Carl von Ossietzky University Oldenburg and is a scholarship alumna of the Friedrich-Ebert-Foundation, Germany.",2
Open-source AI tool matches commercial systems in medical scan reporting - News-Medical,https://news.google.com/rss/articles/CBMivAFBVV95cUxOeXdMSF85Y0syOE5HVXN4ajI0RjRXSDdTd2V2U2w2d1FGSzdHZDhQalNxUTZxbG1WakxUSUE0dzRfdmlHbUlqYmJhNU9Wc3l0X1ZycFJ3YWlrb2psMEJFNnJZVkhZOXJHMmpCWGlLcklORHFGVWhuWU15bnUtcTFyU1JJS1RFcDRUQ1FOVlFyU2JtOHEzYTQ0Vml3T2NITkhLbXJFdnV1aDY4dGJkMngtamJmU0hDd3VUTFlkVg?oc=5&hl=en-US&gl=US&ceid=US:en,"Be the first to rate this article A new study from the University of Colorado Anschutz Medical Campus shows that free, open-source artificial intelligence (AI) tools can help doctors report medical scans just as well as more expensive commercial systems without putting patient privacy at risk. The study was published today in the journal npj Digital Medicine. The research highlights a promising and cost-effective alternative to widely known tools like ChatGPT which are often expensive and may require sending sensitive data to outside servers. This is a big win for healthcare providers and patients. We've shown that hospitals don't need pricey or privacy-risky AI systems to get accurate results."" Aakriti Pandita, MD, lead author of the study and assistant professor of hospital medicine at the University of Colorado School of Medicine Doctors often dictate notes or write free-text reports when reviewing medical scans like ultrasounds. These notes are valuable but they are not always in a format that is required for various clinical needs. Structuring this information helps hospitals track patient outcomes, spot trends and conduct research more efficiently. AI tools are increasingly used to make this process faster and more accurate. But many of the most advanced AI systems, such as GPT-4 from OpenAI, require sending patient data across the internet to external servers. That's a problem in healthcare, where privacy laws make protecting patient data a top priority. The new study found that free AI models, which can be used inside hospital systems without sending data elsewhere, perform just as well, and sometimes better, than commercial options. The research team focused on a specific medical issue: thyroid nodules, lumps in the neck, often found during ultrasounds. Doctors use a scoring system called ACR TI-RADS to evaluate how likely these nodules are to be cancerous. To train the AI tools without using real patient data, researchers created 3,000 fake, or ""synthetic,"" radiology reports. These reports mimicked the kind of language doctors use but didn't contain any private information. The team then trained six different free AI models to read and score these reports. They tested the models on 50 real patient reports from a public dataset and compared the results to commercial AI tools like GPT-3.5 and GPT-4. One open-source model, called Yi-34B, performed as well as GPT-4 when given a few examples to learn from. Even smaller models, which can run on regular computers, did better than GPT-3.5 in some tests. ""Commercial tools are powerful but they're not always practical in healthcare settings,"" said Nikhil Madhuripan, MD, senior author of the study and Interim Section Chief of Abdominal Radiology at the University of Colorado School of Medicine. ""They're expensive and using them usually means sending patient data to a company's servers which can pose serious privacy concerns."" In contrast, open-source AI tools can run inside a hospital's own secure system. That means no sensitive information needs to leave the building and there's no need to buy large and expensive GPU clusters. The study also shows that synthetic data can be a safe and effective way to train AI tools, especially when access to real patient records is limited. This opens the door to creating customized, affordable AI systems for many areas of healthcare. The team hopes their approach can be used beyond radiology. In the future, Pandita said similar tools could help doctors review CT reports, organize medical notes or monitor how diseases progress over time. ""This isn't just about saving time,"" said Pandita. ""It's about making AI tools that are truly usable in everyday medical settings without breaking the bank or compromising patient privacy."" University of Colorado Anschutz Medical Campus Pandita, A., et al. (2025). Synthetic data trained open-source language models are feasible alternatives to proprietary models for radiology reporting. npj Digital Medicine. doi.org/10.1038/s41746-025-01658-3. Be the first to rate this article Posted in: Device / Technology News | Medical Research News | Healthcare News Cancel reply to comment Ege Kavalali and Natalie Guzikowski Discover how super-resolution technology can be used to sudy neurotransmission at inhibitory synapses. Lloyd M. Smith In the interview, Lloyd M. Smith discusses proteoforms, an area of research worthy of the next Human Genome Project. Professor James J. Collins In this interview, Professor James J. Collins, founder of the field of Synthetic Biology, discusses his journey to founding the field of synthetic biology and the potential of living diagnostics. News-Medical.Net provides this medical information service in accordance with these terms and conditions. Please note that medical information found on this website is designed to support, not to replace the relationship between patient and physician/doctor and the medical advice they may provide. Last Updated: Sunday 27 Jul 2025 News-Medical.net - An AZoNetwork Site Owned and operated by AZoNetwork,   2000-2025 Your AI Powered Scientific Assistant Hi, I'm Azthena, you can trust me to find commercial scientific answers from News-Medical.net. To start a conversation, please log into your AZoProfile account first, or create a new account. Registered members can chat with Azthena, request quotations, download pdf's, brochures and subscribe to our related newsletter content. A few things you need to know before we start. Please read and accept to continue. Please check the box above to proceed. Great. Ask your question. Azthena may occasionally provide inaccurate responses. Read the full terms. Terms While we only use edited and approved content for Azthena answers, it may on occasions provide incorrect responses. Please confirm any data provided with the related suppliers or authors. We do not provide medical advice, if you search for medical information you must always consult a medical professional before acting on any information provided. Your questions, but not your email details will be shared with OpenAI and retained for 30 days in accordance with their privacy principles. Please do not ask questions that use sensitive or confidential information. Read the full Terms & Conditions. Provide Feedback",2
Healthcare’s Capacity Crisis: How AI Agents Enhance Clinician Productivity - HIT Consultant,https://news.google.com/rss/articles/CBMirwFBVV95cUxPY3RnR1FfM0Y2UGUybEs3aGFUcktxcmlfRmRVUUVzWjF5dUhZMk8yaENrY0E3bVdZMURDUjQ1YXlnYVZkZ21NYUZWM291bHY4eXNpVGVUdU9NeHQzM1ZjUG9QMnNmck84XzljYlhaSFNoRmVHMk1uVVhFbWlaY0RGV19tQ3VfelBVQzJ2OW4tN1V3N2dQMGFGTlVCWWdiSVNPWG1MUFluUVlPYnlXSlZj?oc=5&hl=en-US&gl=US&ceid=US:en,"by Jeff Elton Vice Chairman of Concert AI 07/23/2025 Leave a Comment America s healthcare system is fighting battles on several fronts: surging patient needs across specialties, an aging clinical workforce, and escalating provider burnout. Projections indicate a shortfall exceeding three million healthcare workers by 2026. This is supported by a 2023 McKinsey study, which found that 40% of inpatient registered nurses were planning to leave their positions. At the same time, nearly 60% of oncologists cite burnout as one of the primary reasons they may seek earlier retirement, only exacerbating an already staggering gap in care, as nearly 32 million Americans live in counties without locally available cancer care. Similar patterns have emerged across specialties, with fewer radiologists graduating. The implications extend far beyond access capacity limitations drive up costs, limit patients  access to potentially lifesaving clinical trials, compromise care quality, and strain the entire healthcare ecosystem. This is a societal issue and one that requires immediate attention. When we look deeper into these signs of dissatisfaction, a clearer picture emerges. Physicians spend more than 35% of their time on non-patient-facing tasks. Registered Nurses (RNs) continue to cite the need for more manageable workloads as a key factor in their decision to remain in or leave their positions. Their administrative burdens will only grow as new diseases emerge, the population ages, and cancer rates rise. To date, few technologies have had a beneficial impact on clinician productivity and work burden. However, today s AI technologies stand ready to deliver the same efficiencies found in drug discovery to fundamentally transform healthcare delivery capacity. Incremental efficiency gains and amplifying clinicians  capabilities may not sound revolutionary at first glance, but those new AI advancements will be able to resolve this capacity crisis and ensure high-quality physician-led and patient-centric care. AI Agents: Extending Clinical Teams to Drive Patient-Centric Care The most frequent concern physicians express, especially those in oncology, is how to alleviate the burden on clinicians. How can you cut back on the tasks that take them away from direct patient care? In these same discussions, I hear about the tremendous advancements the industry is making and how AI has begun to reshape the foundation of healthcare. These rapidly advancing AI solutions are already addressing mission-critical capacity issues and will create an entirely new foundation of data and intelligence. That foundation will consist of domain-tuned and specific Large Language Models (LLMs) with and without transparent reasoning combined with agentic AI. AI agents are available to prepare data and an array of traditionally administrative or non-performed tasks that, in turn, will be used by other agents and agents deployed as assistants to care providers. From intake processes to discharge instructions and patient follow-up, AI agents can offload these repetitive yet crucial steps in the patient journey. In oncology, where every day matters for so many, the ability to spend more time with patients and speed up clinical trials can be life-changing. Imagine walking into an oncology clinic and seeing each of the doctors and nurses actively engaged with patients, providing guidance and personal treatment support, while AI agents quietly manage routine tasks in the background. Consider a clinical trial where AI predicts which participants are at risk of dropping out and recommends interventions to improve retention. Or imagine the profound impact these agents can have when they are used to offload even higher-level tasks behind the scenes. Employing a kind of corporate-like hierarchy, AI agents can each have a distinct domain specialty, with higher-level supervisor agents keeping tabs on specialists. An example of this would be to design a  transaction agent  to specifically look at molecular diagnostic reports, extract the critical biomarker information, and then format it into a usable condition. A higher-level  supervisor agent  would oversee that particular agent to make sure they do their work correctly. These scenarios are no longer hypothetical they are becoming a reality that will shape a more patient-centric future. Solving the healthcare capacity crisis requires more than just technology. It demands collaboration between domain experts, clinicians, technologists, and researchers to ensure that AI solutions align with real-world needs. As AI continues to advance at an unprecedented pace, these collaborations will be essential to unlocking the full potential of AI agents. They are not a replacement for the knowledge, experience, intuition, and empathy that clinicians bring to patient care. Instead, they can empower clinical teams. By reducing administrative burdens and enhancing decision-making, AI agents hold the promise of bringing balance to the clinician s workload, freeing them up to spend more time doing the most important work the work that initially drew them to healthcare caring for patients. Most healthcare providers now view AI adoption as a strategic imperative. By investing in AI solutions that optimize workflows and augment human expertise, healthcare systems and research institutions can reduce burnout, improve retention, and accelerate the pace of medical innovation. The foundations for this transformation are being put into place now. Based on the progress made in the last 12 months alone, we anticipate substantial advances quarter over quarter in the coming three years. The future of healthcare isn t about technology for the sake of technology. It s about prioritizing patient care and bringing life-saving therapies to market faster. AI agents are here, and it s exciting to be part of harnessing this amazing technology to make healthcare more accessible for everyone. About Jeff Elton, Ph.D. Jeff Elton, Ph.D., is Vice Chairman of ConcertAI, an AI SaaS solutions company providing research and patient-centric solutions for life sciences innovators and the world s leading providers. Prior to ConcertAI, Jeff was Managing Director, Accenture Strategy/Patient Health; Global Chief Operating Officer and SVP Strategy at Novartis Institutes of BioMedical Research, Inc.; and partner at McKinsey & Company. He is also a founding board member and senior advisor to several early-stage companies. Jeff is currently a board member of the Massachusetts Biotechnology Council. He is the co-author of the widely cited book, Healthcare Disrupted (Wiley, 2016). Jeff has a Ph.D. and M.B.A. from The University of Chicago. Get in-depth healthcare technology analysis and commentary delivered straight to your email weekly Latest insightful articles delivered straight to your inbox weekly. Submit a Tip or Pitch Selecting the Right EMR: A Practical Guide to Streamlining Your Practice and Enhancing Patient Care Virta Health CEO: GLP-1s Didn t Kill Weight Watchers, Its Broken Model Did Latest insightful articles delivered straight to your inbox weekly Copyright   2025. HIT Consultant Media. All Rights Reserved. Privacy Policy |",2
"AI in healthcare: unrealised benefits, irreversible consequences - Medical Republic",https://news.google.com/rss/articles/CBMipwFBVV95cUxONlBCVXkybXRjR2NiV0pySkNjdXBYSDhZakxjMnNMLWU2bXktc0JneFhjWDVEVGFjSG5vZGEwZnF4a3dWQzBHY0ZULUlUM0dIekZ5cE90YnAzVmFibDM4ZTFsaGxTT21YQmZzbGh6a1I4cDM5RS1ENExVbzJJWTdpUTBOUGY4bEFYTWVGQW9CeldUMVdlZHI3UmY4aGRWVmxOTHAza3lndw?oc=5&hl=en-US&gl=US&ceid=US:en,"3 minute read By The Department of Health, Disability and Ageing appears set on allowing AI into healthcare in some capacity. Here s what stands in its way. A new government review has laid out the biggest concerns about artificial intelligence within the healthcare industry, including the emerging risk of patient data being re-identified. Completed earlier this year but published this week, the Department of Health, Disability and Ageing review looked at Australia s various legislative and regulatory settings on AI and consulted various industry stakeholders including the RACGP, the Pharmacy Guild of Australia and the AMA. In some senses, AI in healthcare is a horse that has already bolted   or is at least nosing its way out of the stable door. According to the report, AI is already being used in consult rooms as a scribe, in aged care homes as a companion, on healthcare websites as a chatbot, as a predictive tool in medical records and as a screening tool in oncology and dermatology. The other facets of healthcare which could eventually be touched by AI, according to the DoHDA report, include insurance, billing, training, consent, privacy and health data. This last formed a major area of focus.  Stakeholders highlighted that, given the many data sources now available, even the most robust techniques for deidentification may no longer be sufficient to safeguard patient privacy,  the report said.  Some respondents pointed out that reidentifying of patient data is a likely outcome.  Several clinical stakeholders raised that certain patient data, such as skin scans and genetic data, is impossible to deidentify.  In these instances, deidentification cannot be assumed to be a safeguard for patient privacy.  People who live in smaller communities   and are therefore more easily identified   and children were specifically mentioned as being at higher risk of suffering the consequences if their health data was to leak.  Data leakage is one way where the breached data generally cannot be retrieved or deleted, so the damage may continue for years after the original exposure or leakage,  the report said.  For children, who are not able to consent initially, the impacts may be felt for the long term.  There is also the possibility of AI introducing changes to patient records that are difficult to reverse or irreversible, such as errors arising from  hallucinations  or inaccuracies.  The report also alluded to disagreement amongst the consultation cohort as to whether the patient or the health service provider owns patient data and whether patients should be remunerated if their data is sold on. Just over half of the respondents said they thought personal healthcare information should be kept in Australia. There were also different attitudes toward what constituted  low risk ; the report pointed out that while the risk from an AI scribe mistake may seem very low, it becomes riskier over time and is inversely correlated with performance. In terms of future direction, the report identified a need for  national and centralised policy leadership to steward equitable benefit realisation . In other words, there needs to be official advice tailored to the health sector.  The existence of low-quality and misleading information about AI in health care can adversely impact decision making,  the report said.  Further, the use of AI to generate information about health care can result in poor-quality outputs.  Having access to a trusted source of accurate, reputable, and timely information about AI in health care would support its safe and responsible use.  It also raised the possibility of an incentive framework for the medical technology industry to follow that would reward AI development and usage practices that deliver high quality, accurate and safe products to the market. More than 70% of the consultation respondents supported a dedicated body that would oversee AI in healthcare. Receive daily updates on the latest news affecting Australian GPs End of content No more pages to load",2
Examining human-AI interaction in real-world healthcare beyond the laboratory - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTE4xa29ObUZKQkdJNjdmQU5pb3F1S3Z5bHhPRlZCTnZtZTRyNkVfMU9Tb3BQWllaOWNRM05hUVVJei04dy1yWW9MaHZCb1ZDN0ZUMHlPMlRRYnJWSEh2Q0pj?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement npj Digital Medicine volume 8, Article number: 169 (2025) Cite this article 6263 Accesses 5 Citations 10 Altmetric Metrics details Artificial Intelligence (AI) is revolutionizing healthcare, but its true impact depends on seamless human interaction. While most research focuses on technical metrics, we lack frameworks to measure the compatibility or synergy of real-world human-AI interactions in healthcare settings. We propose a multimodal toolkit combining ecological momentary assessment, quantitative observations, and baseline measurements to optimize AI implementation. Artificial intelligence (AI) is transforming healthcare through its numerous applications, including disease diagnosis, subtyping, and prognosis, as well as decision-support tools, automation, and AI-driven administrative tasks1. Technical advances in large language models (LLMs) have further fueled progress, enabling the application of chatbots and reasoning engines to healthcare2. Many AI-based tools are already used in clinical practice, such as image analysis tools for radiology or pathology3,4. These AI tools can automate tedious work, thereby freeing up human time for more meaningful tasks, for example, by replacing the second reader in a mammography examination5. In addition, AI tools can go beyond what a human can do, for example, by extracting quantitative information from routine clinical data and predicting treatment response to specific medications6,7. Currently, the landscape of AI is evolving even further towards models with even broader, generalist capabilities8,9 as well as increasingly autonomous agents10. These AI models are not just a piece of software that is steered by a human for a simple task, but they can execute complex chains of tasks and guide their own process, much like a human co-worker, or as Zou and Topol have recently posited, a new teammate11. The pace of development of medical AI is very fast, but in the near future, all medical AI tools have one thing in common: The aim of these AI applications is to interact with human users in hybrid human-AI workflows. It is in the interaction with human healthcare providers that the value of medical AI systems is realized. The toolkit combines real-time and non-real-time methods to comprehensively evaluate healthcare providers  interactions with AI systems. Real-time assessment captures immediate self-reported experiences, behavioral patterns, and physiological responses during actual AI system use in clinical settings. Non-real-time methods establish baseline characteristics through standardized physiological measurements and detailed questionnaires on professional background, digital competence, AI-related attitudes, acceptance, and adherence. Together, these complementary approaches enable quantitative evaluation of human-AI interaction, and therefore optimization of AI implementation in healthcare. However, formal conceptual frameworks for assessing the quality of these human-AI interactions using subjective and physiological measures from a user perspective are lacking. While promising approaches exist for assessing patient user experience12,13, they cannot be directly applied to healthcare professionals in real-world settings. The complexity of clinical workflows and interactions with AI systems demands a clear concept of how to measure human-AI interaction in the real world. What we cannot measure, we cannot optimize therefore, by measuring human-AI interaction quantitatively, we can ultimately improve it. This was emphasized in a recent review by Khan et al.14. Most research focuses on solely technical metrics of AI performance such as sensitivity or specificity in diagnostic tasks. However, this neglects important metrics of human-AI interaction, particularly human task performance and user experience. Task performance is an objective measure of the efficiency, effectiveness, and accuracy with which a user completes a task using an AI system. User experience following a definition in the ISO 9241-210 standard describes a user s holistic experience of the interaction including person s perceptions and responses resulting from the use or anticipated use of an AI system. Understood this way, user experience goes beyond usability. The latter focuses on the subjective instrumental evaluation of how effectively, efficiently, and satisfactorily goals are achieved. While usability is a necessary component of user experience, it represents only one part of the broader, more holistic experience (Turner, 2017). Previous work has touched upon this topic but has not provided comprehensive solutions. In a recent seminal work by Vaccaro et al.,  When Combinations of Humans and AI Are Useful: A Systematic Review and Meta-Analysis,  the authors provided a comprehensive overview of task performance in human-AI interactions across various domains, including healthcare15. Similarly, other studies examined user experience of human-AI interaction, across domains and within healthcare16. However, there is an important limitation to the vast majority of the existing evidence: It measures human-AI interaction in laboratory settings, but not in real-world environments. Why is this a problem? In healthcare, the realities of clinical practice such as workload (time pressures, distractions, ergonomic constraints), institutional standards (disease variability, clinical workflows, institutional ideologies), and economic constraints  differ substantially from controlled laboratory conditions. Findings from laboratory studies often fail to translate reliably to clinical workflows. This calls into question whether there exists a formalized, quantitative understanding of human-AI interaction in healthcare. We need to measure task performance and user experience in the real world, not in simulated environments. This is especially important because task performance and user experience are interlinked17: For example, poorly designed systems reduce both user satisfaction and efficiency, as seen with electronic health record (EHR) systems. Here, central aspects of user experience, namely usability and satisfaction, have been shown to influence the adoption of efficiency strategies among physicians18. While a variety of AI applications like large language models (LLMs) for clinical summarization19 may offer smoother integration and fewer disruptions compared to the transition from paper charts to EHRs, it remains essential to evaluate their impact in real-world clinical practice rather than simulated environments. As with many previous technologies, we cannot rule out that a technology which is well-behaved in the laboratory has unintended consequences and causes user frustration in the real-world setting. A number of studies have put forth formalized quantitative toolkits to measure task performance in human-AI interactions in medicine20. Indeed, task performance is usually the main and only aim of many AI studies in healthcare. Common metrics of task performance include diagnostic performance measures (e.g., sensitivity, specificity, area under the curve), task completion metrics (e.g., task completion time, success rate, error rate), clinical workflow efficiency (e.g., patient throughput or time spent on documentation before and after AI integration), and safety and error reduction metrics (e.g., number of adverse events avoided or improvements in patient outcomes). There are ongoing debates about how to evaluate specific aspects of task performance, such as whether to focus on human augmentation (where the human-AI system outperforms humans alone) or human AI synergy (where the system performs worse than at least one of its components, either the human or the AI alone) as pointed out by Vaccaro et al. 15. Most metrics either already represent core clinical practice measures, such as patient outcomes and workflow efficiency, or can be easily transferred from laboratory settings to real-world applications. Measures like task completion time, success rate, and error rate remain consistent across both contexts, making their applicability in clinical practice relatively straightforward. In contrast, adapting methods for assessing user experience from laboratory studies to real-world healthcare settings is far more challenging due to the complexity and variability of clinical environments. This is likely one of the main reasons why efforts to measure user experience in real-world healthcare have not kept pace with AI developments. From our perspective, the challenge is not to decide what should be measured in terms of user experience. Key components of user experience are well established and validated through previous work, each highlighting different aspects of user experience. Hassenzahl et al.21 pointed out that user experience is shaped by the balance between pragmatic quality (usability and functionality) and hedonic quality (pleasure and identity formation). Robert and Lesage22 emphasized, besides the introduction of additional dimensions of user experience, the importance of both anticipated and ongoing user experience for shaping the overall experience. The Components of User Experience (CUE) model by Th ring and Mahlke23 identifies three distinct dimensions, namely task-related and non-task-related qualities, and emotion, and stresses the role of the system, the user, and the context of interaction. In our view, the real issue lies in determining how these aspects can be measured in real-world healthcare settings in a pragmatic and doable way. While some of these approaches mentioned above provide concrete guidance on how to measure user experience, a comprehensive toolkit that captures all user experience relevant aspects in real-world settings is still lacking. We argue that well-established and validated behavioral and social science tools can be utilized to evaluate user experience across all types of human-AI interaction in healthcare. The insights gained from this evaluation can then be used for the development of structured and standardized processes for designing and improving future health AI, ultimately optimizing AI s real-world value. Surprisingly, this area of research remains largely underexplored. To address the existing gaps in understanding user experience during human-AI interactions in the real world of healthcare, we propose a multimethod toolkit based on existing frameworks which are summarized in Fig. 1 and Table 1. At its core is ecological momentary assessment (EMA)24, which captures physiological and psychological data in real-time within their natural context. We suggest complementing classical EMA by quantitative observations and non-real-time assessments of healthcare providers  baseline psychophysiological characteristics. According to ISO 9241-210, user experience is influenced not only by system interaction but also by baseline psychophysiological factors, highlighting the importance of these non-real-time methods. Regarding these baseline factors, the principle of parsimony should guide the selection, focusing solely on individual factors that have either been shown to influence acute psychophysiological markers of interest in the general population25,26,27 or proven relevant specifically for healthcare providers, such as factors affecting general adherence to guidelines28,29, the adoption of AI, or general attitudes and beliefs about AI30,31. Together, these methods would form a comprehensive toolkit that captures healthcare providers  real-time and real-world experiences outside the laboratory. This approach minimizes the burden on clinicians by incorporating non-participatory observational and physiological measures. To ensure broad clinician acceptance, providing clinicians with thorough information on the purpose, data privacy management during data collection, and implementation of the toolkit is essential. Table 2 presents an overview of the tools and outcome variables included in our proposed approach. Importantly, we suggest limiting the toolkit to a pragmatic set of quantitative and scalable instruments. The number of AI methods for healthcare is rapidly growing. Keeping up with this growth and providing quantitative evidence for human-AI interactions at scale requires instruments that are practical for large participant samples. In the realm of physiological assessment, this would favor the use of wearable consumer electronics, such as smart watches, over bespoke devices using professional diagnostic-grade electrodes due to lower cost, faster setup, and reduced disruption in real world environments. This would also favor the use of short, quantitative, validated items, pushed to participants during or after the interaction with AI systems, rather than semi-structured or structured one-on-one interviews. As an example, it would be impractical and even disruptive to require primary care physicians to wear electrocardiogram electrodes and be subjected to lengthy unstructured interviews after using an AI application aimed at better managing their patients. Rather, it would be clearly preferable to have them wear smartwatches and answer quick, structured questionnaires on their mobile devices during and after their use of AI applications in their daily practice. Only if we can easily scale psychophysiological measurements of human-AI interactions to hundreds or thousands of doctors will we be able to provide quantitative real-world evidence for healthcare AI. The idea of integrating EMA into the investigation of human-AI interactions is not new. It has been discussed before, for instance by Chen et al.32. However, it has yet to be effectively implemented in human-AI interactions in healthcare. Where EMA has been applied, the focus has predominantly been on psychological self-report data, often neglecting physiological measures. This is surprising given that previous conceptualizations of EMA have emphasized the need for complementary assessment of both psychological and physiological data. Only by integrating these data sources can we accurately assess user experience, especially since self-reports are prone to biases and rely heavily on an individual s ability to perceive or articulate their internal states. The interpretation of physiological measures requires careful integration with psychological measures: While physiological data can help to identify episodes that may be critical for user experience, psychological measures are essential for determining the underlying reason, as very different causes can exhibit similar arousal patterns in the markers of interest. Incorporating contextual information, such as motion and body position can further enhance this accuracy. Preferably, the selection of specific markers should be guided by the respective aspect of experience that is of primary interest. Priority should be given to markers that have been frequently associated with these experiences and can be reliably measured using current technology. These include cardiovascular, respiratory, and autonomic measures. With regard to the choice of the right technology, sometimes also referred to as mobile Health (mHealth) devices, many consumer devices do not provide sufficient data quality or use proprietary, undisclosed algorithms for preprocessing physiological signals, and some raise concerns regarding data privacy. In contrast, devices offering high-quality raw physiological data that meet strict data protection standards are often expensive and lack the wearing comfort of consumer options, which can affect the willingness of healthcare professionals to participate and complicate large-scale data collection. Therefore, the selection of both markers and technologies should be carefully considered based on the specific questions and circumstances of the respective user experience evaluation. The real-time collection of healthcare providers  psychological data via self-rated items inevitably disrupts workflows to some degree but remains crucial for understanding user experience during human-AI interactions. The aim should be to comprehensively capture key aspects of psychological experience, including psychophysiological, cognitive, and affective dimensions, alongside other perceptions and contextual information as control variables. To minimize the burden on healthcare providers, we ideally propose embedding these assessments seamlessly within the respective AI system. For example, a radiologist using a decision support system could answer relevant items assessing endpoints such as usability, stress, workload, and diagnostic confidence directly within the system, without additional workflow disruptions. Should this not be feasible, following standard EMA procedures24, items can be delivered through app-enabled devices. Additionally, selecting only a minimal set of validated, hypothesis-driven items ensures that assessments remain focused and minimally intrusive. While EMA is effective for capturing real-time data, fully structured observations offer additional objective insights without requiring active participation from healthcare providers, reducing their effort and integrating seamlessly into clinical workflows. Unlike unstructured observations, they categorize numerical data for predefined variables into standardized schemes. LM-based video analysis could support this by converting unstructured observational data into structured formats, addressing interrater reliability issues and enabling large-scale, reliable observations. A practical application example would be the use of computer cameras to record healthcare professionals during documentation in a Clinical Information System (CIS) and analyze changes in facial expressions, body posture, and movements to infer psychological states. Critically, data protection rights of both healthcare professionals and patients must always be respected. As we propose a scalable toolkit to evaluate human-AI interaction in real-world healthcare, we must consider: who will analyze the vast data generated? Observing hundreds of healthcare professionals using AI decision-support systems will produce an enormous dataset, including psychological and physiological measures. Unlike small-scale lab studies, real-world evaluations require automated approaches to process large-scale observational data. To tackle this, we propose using AI to analyze the data. LLMs can already today make the analysis of clinical data much more efficient than a manual evaluation33. Also, AI can already interpret time series data obtained with wearable sensors34. Similarly, video-language models (VLMs) can assess recorded interactions, identifying events, but also human reactions to them. Ultimately, instead of relying on human evaluators, AI tools can perform sentiment analysis, behavior tracking, and physiological signal interpretation efficiently. AI-based analysis must be transparent, interpretable, and aligned with healthcare needs, but it will likely be the most efficient way to cope with the amount of measurement data when our proposed toolkit is deployed at scale. AI-based methods are regulated according to medical device laws and guidance. Therefore, we have to take the regulatory landscape into account when we measure human-AI interaction. Relevant regulatory frameworks, include the Human Factors or Usability Engineering (ISO/IEC 62366-1:2015) framework. This has long had approaches for the quantitative evaluation of human-AI interactions with devices, systematizing the application of knowledge about human behavior, abilities, limitations, and other human characteristics into the design process, including software-driven user interfaces. These principles extend to the design of user documentation and user training to enhance safe and effective use. They recognize the close dependency between user experience and task performance, as safety issues with medical devices are often linked to user errors resulting from poor design and inadequate user experience. However, their focus is primarily on laboratory settings rather than real-world use of medical devices in healthcare, highlighting a weakness in medical device regulation35. However, most principles and concepts for device development and evaluation predate the application of AI in medicine, and are therefore not tailored to AI. Often, these regulation frameworks are generic, and do not address the specifics of AI-enabled medical devices and their user interfaces. These frameworks were developed to be applicable to the wide variety of medical device types, some physical, some consisting of hardware and software, and some which are purely software. AI-enabled software brings new challenges in user experience engineering as it interacts in complex ways with human decision-making that did not apply in prior medical devices. Hence, in this way, AI-enabled decision-support tools act as human-AI interfaces, and therefore interfaces that influence cognitive and emotional states of medical decision-makers. It is therefore critical that these AI tools are developed in a manner that avoids automation bias, complacency bias and deskilling9,35. These unique aspects of AI-enabled decision-support tools are highly important and safety and acceptable user experience (such as the avoidance of stress for workers) therefore warrants specific standards and guidance - which does not yet exist. Some standards and guidance are in preparation36,37 that begin to address some of these issues, but they do not explore newer challenges, such as the AI-enabled decision-support tools based on large language models that are entering regulatory approval pathways38, which have highly adaptive user interactions and experiences unlike any previous medical device39. Also, these draft standards are based on the experience of a small number of developers and the empirical knowledge of small expert groups rather than being based on real-world assessments. Overcoming these weaknesses requires a foundation from academic studies that map out and quantify human-AI interaction in the real world. For these reasons, medical AI needs specifically designed frameworks for quantitatively evaluating real-world human-AI interactions which better take account of systemic interaction of healthcare providers with all AI contact points, instead of isolated focus on the design of individual devices. Under current frameworks the holistic experience of the human is neglected and much greater consideration of their mid- and long-term interaction with AI, and of their learning curve and adaptation to these technologies. Isolated studies have explored the human factors challenges of implementing AI systems, developing individual frameworks for human factors evaluation due to the absence of accepted standard approaches. In one such study, Google Health explored real-world challenges of an on-market deep learning algorithm for detecting diabetic retinopathy in an observational study in clinical environments in Thailand40,41. The authors identified a high image-quality related rejection rate of images that human readers could screen, with resultant increased staff workload, elevated stress, and longer patient waiting times. This is a typical example where user experience design, through better interaction between the system and user in required image properties, can increase performance and efficiency and reduce stress. The authors advocated for human-centered evaluative research to be conducted alongside prospective model accuracy evaluations. The value of medical AI lies in its interaction with human users, yet the lack of evaluation frameworks tailored to real-world environments remains a significant hurdle, because existing approaches fail to capture the complexity and contextual dependencies of clinical workflows in real-life settings. Here, we propose a multimodal toolkit which bridges the gap between laboratory research and clinical reality by integrating real-time physiological monitoring, observational data, and user experience assessment. In our concept, we focus on healthcare professionals as the users of AI systems. However, patients themselves are also interacting with AI systems and this interaction deserves substantial scientific attention in future studies. We intend to validate our proposed framework empirically in future field studies, and we encourage the scientific community to adopt and build on these concepts and likewise validate this approach across diverse healthcare settings to refine AI implementation further. No datasets were generated or analysed during the current study. Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine. Nat. Med. 28, 31 38 (2022). Article CAS PubMed Google Scholar Truhn, D., Reis-Filho, J. S. & Kather, J. N. Large language models should be used as scientific reasoning engines, not knowledge databases. Nat. Med. 29, 2983 2984 (2023). Article CAS PubMed Google Scholar Hosny, A., Parmar, C., Quackenbush, J., Schwartz, L. H. & Aerts, H. J. W. L. Artificial intelligence in radiology. Nat. Rev. Cancer 18, 500 510 (2018). Article CAS PubMed PubMed Central Google Scholar Bera, K., Schalper, K. A., Rimm, D. L., Velcheti, V. & Madabhushi, A. Artificial intelligence in digital pathology - new tools for diagnosis and precision oncology. Nat. Rev. Clin. Oncol. 16, 703 715 (2019). Article PubMed PubMed Central Google Scholar L ng, K. et al. Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, single-blinded, screening accuracy study. Lancet Oncol. 24, 936 944 (2023). Article PubMed Google Scholar Echle, A. et al. Deep learning in cancer pathology: A new generation of clinical biomarkers. Br. J. Cancer 124, 686 696 (2021). Article PubMed Google Scholar Shmatko, A., Ghaffari Laleh, N., Gerstung, M. & Kather, J. N. Artificial intelligence in histopathology: Enhancing cancer research and clinical oncology. Nat. Cancer 3, 1026 1038 (2022). Article PubMed Google Scholar Moor, M. et al. Foundation models for generalist medical artificial intelligence. Nature 616, 259 265 (2023). Article CAS PubMed Google Scholar Gilbert, S. & Kather, J. N. Guardrails for the use of generalist AI in cancer care. Nat. Rev. Cancer 24, 357 358 (2024). Article CAS PubMed Google Scholar Lee, Y., Ferber, D., Rood, J. E., Regev, A. & Kather, J. N. How AI agents will change cancer research and oncology. Nat. Cancer 5, 1765 1767 (2024). Article PubMed Google Scholar Zou, J. & Topol, E. J. The rise of agentic AI teammates in medicine. Lancet 405, 457 (2025). Article PubMed Google Scholar Scheder-Bieschin, J. et al. Improving emergency department patient-physician conversation through an artificial intelligence symptom-taking tool: Mixed methods pilot observational study. JMIR Form. Res. 6, e28199 (2022). Article PubMed PubMed Central Google Scholar Knapp, A., Harst, L., Hager, S., Schmitt, J. & Scheibe, M. Use of patient-reported outcome measures and patient-reported experience measures within evaluation studies of telemedicine applications: Systematic review. J. Med. Internet Res. 23, e30042 (2021). Article PubMed PubMed Central Google Scholar Khan, S. D. et al. Frameworks for procurement, integration, monitoring, and evaluation of artificial intelligence tools in clinical settings: A systematic review. PLOS Digit. Health 3, e0000514 (2024). Article PubMed PubMed Central Google Scholar Vaccaro, M., Almaatouq, A. & Malone, T. When combinations of humans and AI are useful: A systematic review and meta-analysis. Nat. Hum. Behav. 8, 2293 2303 (2024). Article PubMed PubMed Central Google Scholar Asan, O. & Choudhury, A. Research trends in artificial intelligence applications in Human Factors health care: Mapping review. JMIR Hum. Factors 8, e28236 (2021). Article PubMed PubMed Central Google Scholar Liu, H. et al. Artificial intelligence and radiologist burnout. JAMA Netw. Open 7, e2448714 (2024). Article PubMed PubMed Central Google Scholar Holmgren, A. J. et al. Electronic health record usability, satisfaction, and burnout for family physicians. JAMA Netw. Open 7, e2426956 (2024). Article PubMed PubMed Central Google Scholar Van Veen, D. et al. Adapted large language models can outperform medical experts in clinical text summarization. Nat. Med. 30, 1134 1142 (2024). Article PubMed PubMed Central Google Scholar Yu, F. et al. Heterogeneity and predictors of the effects of AI assistance on radiologists. Nat. Med. 30, 837 849 (2024). Article CAS PubMed PubMed Central Google Scholar Hassenzahl, M. The thing and I. Unerstanding the relationship between user and product. in Funology: From Usability to Enjoyment (eds. Blythe, M., Overbeek, C., Monk, A. F. & Wright, P. C.) 31 42 (Kluwer Academic Publishers, 2003). Robert, J.-M. & Lesage, A. From usability to user experience with interactive systems. in The Handbook of Human-Machine Interaction 303 320 (CRC Press, 2017). Th ring, M. & Mahlke, S. Usability, aesthetics and emotions in human technology interaction. Int. J. Psychol. 42, 253 264 (2007). Article Google Scholar Shiffman, S., Stone, A. A. & Hufford, M. R. Ecological momentary assessment. Annu. Rev. Clin. Psychol. 4, 1 32 (2008). Article PubMed Google Scholar Boucsein, W. et al. Publication recommendations for electrodermal measurements: Publication standards for EDA. Psychophysiology 49, 1017 1034 (2012). Article PubMed Google Scholar Laborde, S., Mosley, E. & Thayer, J. F. Heart rate variability and cardiac vagal tone in psychophysiological research - recommendations for experiment planning, data analysis, and data reporting. Front. Psychol. 8, 213 (2017). Article PubMed PubMed Central Google Scholar Pickering, T. G. et al. Recommendations for blood pressure measurement in humans and experimental animals: part 1: blood pressure measurement in humans: a statement for professionals from the Subcommittee of Professional and Public Education of the American Heart Association Council on High Blood Pressure Research: Part 1: Blood pressure measurement in humans: A statement for professionals from the subcommittee of professional and public education of the American heart association council on high blood pressure research. Circulation 111, 697 716 (2005). Article PubMed Google Scholar Cabana, M. D. et al. Why don t physicians follow clinical practice guidelines? A framework for improvement. JAMA 282, 1458 1465 (1999). Article CAS PubMed Google Scholar Hoorn, C. J. G. M., Crijns, H. J. G. M., Dierick-van Daele, A. T. M. & Dekker, L. R. C. Review on factors influencing physician guideline adherence in cardiology. Cardiol. Rev. 27, 80 86 (2019). Article CAS PubMed Google Scholar Dai, T. & Singh, S. Artificial intelligence on call: The physician s decision of whether to use AI in clinical practice. SSRN Electron. J. https://doi.org/10.2139/ssrn.3987454 (2021). Chen, M. et al. Acceptance of clinical artificial intelligence among physicians and medical students: A systematic review with cross-sectional survey. Front. Med. (Lausanne) 9, 990604 (2022). Article PubMed Google Scholar Chen, C. et al. Toward a unified metadata schema for ecological momentary assessment with voice-first virtual assistants. Proc 3rd Conf Conversat User Interfaces CUI 2021 (2021) 2021, (2021). Tayebi Arasteh, S. et al. Large language models streamline automated machine learning for clinical studies. Nat. Commun. 15, 1603 (2024). Article CAS PubMed PubMed Central Google Scholar Ruan, F. Y., Zhang, A., Oh, J. Y., Jin, S. & Jacobson, N. C. AI foundation models for wearable movement data in mental health research. ArXiv, (2025). Welzel, C. et al. Holistic human-serving digitization of health care needs integrated automated system-level assessment tools. J. Med. Internet Res. 25, e50158 (2023). Article PubMed PubMed Central Google Scholar Engler, M., Johner, C., Krepcke, M., Martinez Torres, M. I. & Stangenberg, M. Usability engineering for medical devices using Artificial Intelligence and machine learning technology. Preprint at https://doi.org/10.5281/ZENODO.14203190 (2024). Center for Devices & Radiological Health. Artificial Intelligence-Enabled Device Software Functions: Lifecycle Management and Marketing Submission Recommendations. U.S. Food and Drug Administration https://www.fda.gov/regulatory-information/search-fda-guidance-documents/artificial-intelligence-enabled-device-software-functions-lifecycle-management-and-marketing (2025). AI Airlock pilot cohort. GOV.UK https://www.gov.uk/government/publications/ai-airlock-pilot-cohort/ai-airlock-pilot-cohort. Gilbert, S., Harvey, H., Melvin, T., Vollebregt, E. & Wicks, P. Large language model AI chatbots require approval as medical devices. Nat. Med. 29, 2396 2398 (2023). Article CAS PubMed Google Scholar CIEHF. Human Factors in Healthcare AI. https://ergonomics.org.uk/resource/human-factors-in-healthcare-ai.html. Beede, E. et al. A human-centered evaluation of a deep learning system deployed in clinics for the detection of diabetic retinopathy. in Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (ACM, New York, NY, USA, 2020). https://doi.org/10.1145/3313831.3376718. Davis, F. D. & Grani , A. The Technology Acceptance Model: 30 Years of TAM. (Springer Nature, Cham, Switzerland, 2024). Brod, C. Techno Stress: The Human Cost of the Computer Revolution. (Longman Higher Education, Harlow, England, 1984). Ragu-Nathan, T. S., Tarafdar, M., Ragu-Nathan, B. S. & Tu, Q. The consequences of technostress for end users in organizations: Conceptual development and empirical validation. Inf. Syst. Res. 19, 417 433 (2008). Article Google Scholar Scherer, K. R. What are emotions? And how can they be measured? Soc. Sci. Inf. (Paris) 44, 695 729 (2005). Article Google Scholar Brown, S. B. R. E., Brosschot, J. F., Versluis, A., Thayer, J. F. & Verkuil, B. New methods to optimally detect episodes of non-metabolic heart rate variability reduction as an indicator of psychological stress in everyday life. Int. J. Psychophysiol. 131, 30 36 (2018). Article PubMed Google Scholar Schwerdtfeger, A. R. & Rominger, C. Feelings from the heart: Developing HRV decrease-trigger algorithms via multilevel hyperplane simulation to detect psychosocially meaningful episodes in everyday life. Psychophysiology 58, e13914 (2021). Article PubMed PubMed Central Google Scholar Richer, R. et al. Machine learning-based detection of acute psychosocial stress from body posture and movements. Sci. Rep. 14, 8251 (2024). Article CAS PubMed PubMed Central Google Scholar Nadeem, M. et al. Vision-enabled large language and deep learning models for image-based emotion recognition. Cogn. Comput. 16, 2566 2579 (2024). Article Google Scholar Vaiani, L., Cagliero, L. & Garza, P. Emotion recognition from videos using multimodal large language models. Future Internet 16, 247 (2024). Article Google Scholar Download references This work was supported by the European Commission under the Horizon Europe Program, as part of project ASSESS-DHT (101137347) via funding to S.G, the European Research Council (ERC; NADIR, 101114631 to J.N.K); and the Federal Ministry of Education and Research (BMBF), Germany, as part of the Zukunftscluster SEMECO (03ZU1210GA). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union. Neither the European Union nor the granting authority can be held responsible for them. Open Access funding enabled and organized by Projekt DEAL. Else Kroener Fresenius Center for Digital Health, Faculty of Medicine and University Hospital Carl Gustav Carus, TUD Dresden University of Technology, Dresden, Germany Magdalena Katharina Wekenborg, Stephen Gilbert & Jakob Nikolas Kather Department of Medicine I, Faculty of Medicine and University Hospital Carl Gustav Carus, TUD Dresden University of Technology, Dresden, Germany Jakob Nikolas Kather Medical Oncology, National Center for Tumor Diseases (NCT), University Hospital Heidelberg, Heidelberg, Germany Jakob Nikolas Kather Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar All authors wrote this article and agreed with its submission. Correspondence to Jakob Nikolas Kather. M.K.W. declares no competing interests. S.G. declares a nonfinancial interest as an Advisory Group member of the EY-coordinated  Study on Regulatory Governance and Innovation in the field of Medical Devices  conducted on behalf of the DG SANTE of the European Commission. S.G. declares the following competing financial interests: he has or has had consulting relationships with Una Health GmbH, Lindus Health Ltd., Flo Ltd, ICURA ApS, Rock Health Inc., Thymia Ltd., FORUM Institut f r Management GmbH, High-Tech Gr nderfonds Management GmbH, DG SANTE, Prova Health Ltd and Ada Health GmbH and holds share options in Ada Health GmbH. S.G. is a News and Views Editor for npj Digital Medicine. S.G. played no role in the internal review or decision to publish this article. J.N.K. declares consulting services for Bioptimus, France; Owkin, France; DoMore Diagnostics, Norway; Panakeia, UK; AstraZeneca, UK; Mindpeak, Germany; and MultiplexDx, Slovakia. Furthermore, he holds shares in StratifAI GmbH, Germany, Synagen GmbH, Germany, and has received a research grant by GSK, and has received honoraria by AstraZeneca, Bayer, Daiichi Sankyo, Janssen, Merck, MSD, BMS, Roche, Pfizer and Fresenius. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions Wekenborg, M.K., Gilbert, S. & Kather, J.N. Examining human-AI interaction in real-world healthcare beyond the laboratory. npj Digit. Med. 8, 169 (2025). https://doi.org/10.1038/s41746-025-01559-5 Download citation Received: 04 January 2025 Accepted: 10 March 2025 Published: 19 March 2025 DOI: https://doi.org/10.1038/s41746-025-01559-5 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",2
30+ Key Questions Healthcare Leaders Should Ask Before Investing in AI-Based HealthTech - appinventiv.com,https://news.google.com/rss/articles/CBMilwFBVV95cUxOZjh5UmJFcF8zdEhRNndBTHNieWZqSmlYemllQm9jU0JaeGNjZXoteEhEMkRMUmdxOHExMFpTVXNld1RNa2VjeUdla1ItQ2gxQ1ZpcXpQVnNGb01POUU1OVdkS3RNRk9VT2FmVXhRV1VDX1dSTlJlcmN0LTdFenBwOTFxOEFFS1lvUjBzTVpldy1Ua2EyY05v?oc=5&hl=en-US&gl=US&ceid=US:en,"We believe in change driven by technology and innovation. Meet the brains behind our smooth running and powerful machine. Join our team of experts to make a difference in the real world. Learn about Appinventiv's product lifecycle development process. Our software development services are built to evolve your business idea into a successful growth story that deploy customized solutions in a wide range of industries to steadfast success for top globally leading brands A leading digital platform to offer engaging shopping experience to users A transforming ERP Solution for the world's largest furniture retailer A mobile app to digitalize & expand KFC's digital footprint A refined UX strategy for Domino's to increase their conversion rate by 23% The MIT Innovation award-winning app with $52 Million funding reshaping the employment landscape A SaaS-based financial literacy and smart money management platform for kids We believe in change driven by technology and innovation. Join our team of experts to make a difference in the real world. Learn about Appinventiv's product lifecycle development process. Meet the brains behind our smooth running and powerful machine. that deploy customized solutions in a wide range of industries Our software development services are built to evolve your business idea into a successful growth story A leading digital platform to offer engaging shopping experience to users A transforming ERP Solution for the world's largest furniture retailer A mobile app to digitalize & expand KFC's digital footprint A refined UX strategy for Domino's to increase their conversion rate by 23% The MIT Innovation award-winning app with $52 Million funding reshaping the employment landscape A SaaS-based financial literacy and smart money management platform for kids to steadfast success for top globally leading brands Key takeaways: For most healthcare leaders, the promise of AI sounds almost too good to ignore   smarter diagnostics, streamlined workflows, 24/7 virtual support, and faster clinical decisions. But step inside a real hospital s back office, and the picture looks different. Legacy EHR systems resist modern integrations. Clinical teams operate in silos from IT. Compliance is rigid. And vendor conversations often focus more on what s  possible  than what s truly practical. The gap between AI s promise and the day-to-day operational reality remains wide. Meanwhile, the AI-in-healthcare space is overflowing with possibilities. New solutions launch every quarter, each one claiming to transform operations or personalize care. But for healthcare leaders, investing in AI isn t about chasing the next big thing. It s about asking the right questions early, so the technology improves care delivery, supports clinicians, and avoids getting lost in complexity. So how do you make the right decision? The truth is, there s no one-size-fits-all approach. Each organization operates under its own set of constraints from infrastructure limitations to regulatory pressure and workforce readiness. That s why in this blog, we explore key questions every healthcare leader should ask before investing in AI-based HealthTech, to ensure every solution adopted doesn t just solve today s needs but builds toward tomorrow s care models. But before diving into those questions, it s important to understand just how fast this space is evolving. The AI-healthcare landscape isn t driven by hype alone it s backed by hard numbers and real momentum. Understanding what to ask is step one. Knowing how to act is where we come in. At Appinventiv, we help healthcare leaders navigate the AI maze! The global AI in healthcare market is surging and is expected to reach $187.69 billion by 2030, driven by the urgent need for precision and efficiency in care delivery. Healthcare leaders clearly recognize this shift. In fact, more than 80% of health system executives expect generative AI to have either a  significant  (26%) or  moderate  (55%) impact on their organizations in 2025. With over 70% of healthcare providers, payers, and tech firms already launching proofs-of-concept or live use cases from AI chatbots to automated documentation GenAI is rapidly becoming the new operational standard, not just a future trend. Enterprises leverage AI for predictive analytics, while startups deploy chatbot-driven apps to ease ER bottlenecks. Integration with EHRs demands robust APIs, enabling seamless workflows for hospital networks or lean startup platforms. Another key driver of AI adoption is the cost efficiency it brings to hospital administration, particularly through the automation of claims processing. Moreover, emerging HealthTech AI trends include blockchain for secure data sharing and IoT for remote vitals monitoring, fueling personalized care. In summary, the growth of AI in healthcare isn t just about technology   it s about solving systemic problems at scale, with a focus on patient care and safety. For AI to create a measurable impact, leaders must move beyond pitch decks and start asking the right questions   ones rooted in clinical reality, data maturity, and long-term scalability. What follows isn t just a checklist, it s a strategic lens a set of core considerations designed to help you evaluate whether an AI solution is ready for your organization s challenges today, and built to grow with you tomorrow. In the high-stakes world of AI-based HealthTech, healthcare compliance is the foundation of trust and impact. With over 369,107 HIPAA violation complaints filed with the Office for Civil Rights (OCR) since 2003, and the PHI of 276,775,457 individuals exposed in 2024 alone, healthcare AI regulations demand rigorous scrutiny. Ask these questions to secure your AI investment in healthcare: How does the AI solution ensure compliance with HIPAA, GDPR, and other regional data protection laws? To protect patient data at any cost, HIPAA-compliant AI-based solutions should implement encryption and de-identification of such data. Ask how vendors can help to achieve certifications and conduct audits to comply with HIPAA, GDPR, and local and international healthcare compliances to protect the privacy of patient information through AI without the risk of a financial penalty. How does the AI vendor handle data access and control? Ask how vendors ensure patient data privacy in AI, with secure access protocols and clear ownership for your organization, using secure cloud or on-premises storage. Enterprises managing global systems require centralized control, while startups need lightweight solutions that scale without complexity. This helps startups build trust and enterprises manage global patient networks, especially as healthcare providers heavily rely on inadequate protocols. What is the vendor s approach to maintaining compliance with healthcare industry regulations? The laws can change with the passage of time, and your healthcare AI products should be able to adjust to this shift. One of the key reasons why proactive monitoring is important is that a strategic partner can ensure that the solutions comply with laws in different regions. This is important to those enterprises that already have an international presence and also to startups that plan to be rapidly expanding. Seamless technology adoption and integration are the backbone for enterprises and startups building AI healthcare apps. HealthTech systems must ensure that solutions enhance patient care without breaking operations. These questions guide your investment in technologies for building healthcare AI solutions: What is the level of integration with existing health information systems (EHR, EMR, telemedicine platforms)? The well-developed AI healthcare infrastructure is integrated into systems, such as Epic or Teladoc, to improve diagnostics and telehealth, making them more accurate. Advanced API integration is necessary for enterprises to help connect complex networks using the core of the business. Make sure your vendor customizes AI to your technology stack in order to achieve seamless integration and continuity of care. What are the integration timelines and potential disruptions to current workflows? AI systems sit inside an already complex ecosystem of EHRs, PACS, LIMS, billing systems, and compliance protocols. If integration takes months or causes downtime in critical systems, the risk can outweigh the reward. Consequently, the vendors must be able to lay out potential plans that reflect the timeline based on your demands. Every new technology comes with a learning curve, and therefore, make sure the vendors are willing to train the concerned team and roll out programs gradually in phases so that the workflow is not disrupted. What is the scalability of the AI solution for multi-site or multi-region healthcare operations? Make sure that the solution is scalable and can support not only multiple locations or serve in multiple lines but also enable the support of any future changes. The scalability of healthcare AI allows enterprises to grow by merging with operations around the world. Ensure flexibility, i.e., request vendors how their solutions respond to the increased patient data or trends like telehealth in the long term. 64% of healthcare organizations that have adopted AI technology like Generative AI are already seeing an increase in ROI (Source: McKinsey). However, as AI adoption grows in healthcare, it demands strategic planning to ensure value for enterprises investing in building these solutions. HealthTech organizations must strike a balance between investment and measurable returns, aligning with patient care goals. Ask these questions to anchor your strategy: What are the initial and ongoing costs associated with AI implementation and maintenance in healthcare? Healthcare investment in AI includes not only development and integration costs but also the cost of updating and maintaining AI models. More sophisticated systems, such as predictive diagnostics, are more expensive for enterprises; new companies can use cost-effective systems such as chatbots. Ask vendors for transparent budgets detailing recurring costs to avoid surprises and align with your financial goals. How significant will the ROI be by adopting AI in certain departments (i.e., diagnostics, customer service, administrative tasks)? Identify defined KPIs to quantify healthcare AI ROI, including better radiology diagnostic performance, faster patient wait times in customer service, or decreased administrative workforce. Ensure that vendors can connect AI to deliver targeted results, such as enhanced patient retention or a streamlined supply chain. What will be the duration of waiting for tangible outcomes of investments in AI? Businesses can see results from patient engagement tools in months, whereas AI-based integration in whole departments may take a year in enterprises, all depending on technology use and integration. HealthTech organizations must adapt to these new technological environments; therefore, they must confirm that vendors offer training and clear schedules to deliver results without derailing operations. Since healthcare relies on a variety of data to articulate its outcomes, the concept of AI in healthcare data management becomes crucial among businesses developing solutions. Big data in the HealthTech spectrum should include a robust data strategy that presents information on the one hand and remains ethical on the other. Ask these questions to inform how you want to proceed: The AI system works on structured and unstructured types of healthcare data. How does it work? Both unstructured and structured forms of data, including patient records, medical images, and notes by physicians, are incorporated into successful HealthTech data management processes. HealthTech solutions have to integrate complex information and handle a range of data using AI in real time. Ensure that vendors support different types of data to enable accurate clinical applications. What is the data quality assurance procedure, data cleansing, and validations? The quality of healthcare AI lies in proper inputs. Therefore, vendors must implement rigorous cleansing like removing duplicates and validation protocols to ensure reliable outputs, such as precise diagnostics. Ask your vendors how they maintain data integrity, critical for startups proving credibility and enterprises scaling trust. What is the structure of the AI formulation in handling big data and harnessing power in healthcare? Big data in HealthTech involves the processing of population health records or information from clinical trials to enable informed action. Enterprises need systems to analyze vast datasets with effective solutions for targeted analytics. Confirm vendors  design AI to scale with growing data volumes. What are the key ethical issues in building AI solutions for healthcare? Data bias should be addressed in AI healthcare ethics to ensure fairness and equity. Biased datasets can skew diagnostics, harming patients. Vendors must use diverse data and regular audits to promote equitable outcomes, vital for startups and enterprises to build trust. What are the limitations of AI in healthcare? AI limitations in healthcare include a lack of emotional bonds in patient care. AI cannot replicate human empathy, so providers must pair it with human interaction, ensuring startups and enterprises maintain compassionate care alongside technical precision. As AI technologies take on increasingly critical roles in diagnostics, decision-making, and patient communication, ensuring ethical implementation becomes non-negotiable. From fairness in data training to transparency in decision logic, healthcare AI must align with ethical standards that protect patients, promote equity, and enhance trust. Ask these questions to guide your organization s ethical AI adoption: How does the vendor address algorithmic bias in healthcare AI models? Bias in training data such as underrepresentation of certain age, gender, or ethnic groups can lead to dangerous inaccuracies. Ask whether the vendor uses diverse, inclusive datasets and whether they routinely audit model performance for disparities across demographics. What frameworks or guidelines are followed to ensure fairness and explainability in AI decisions? AI models used in patient care must provide interpretable outputs. Ensure that the vendor incorporates explainable AI (XAI) methods and adheres to ethical frameworks like the WHO s  Ethics & Governance of AI for Health  or the FDA s Good Machine Learning Practice (GMLP). Does the AI system allow clinicians to override or question automated decisions? To maintain clinical autonomy, healthcare AI systems must be built with override capabilities and clear reasoning trails. Ask if clinicians can access confidence scores, justification metrics, or alternative outcomes before accepting AI-generated recommendations. How are patients informed about the use of AI in their care decisions? Transparency also extends to patients. Vendors should support features that allow organizations to disclose AI involvement in care, especially in diagnosis or treatment suggestions. Ask how the solution ensures informed consent while respecting digital literacy levels. What ongoing ethical review or monitoring mechanisms are in place post-deployment? Ethical responsibility concerns don t end at deployment. Ask how vendors monitor unintended consequences, such as shifts in diagnosis accuracy across population groups, and whether they have an internal ethics board or conduct regular third-party reviews. With the rise of cybersecurity threats in healthcare, AI in healthcare security is crucial for enterprises developing medical software. Patient data privacy in HealthTech ensures trust and compliance, protecting sensitive data while enabling innovation. Ask these questions to fortify your strategy: How does the AI solution ensure the security of sensitive patient data? When it comes to AI in healthcare, no concern is more critical or more regulated than patient data security. HIPAA, GDPR, and country-specific data laws leave no room for error, and any breach can damage both finances and trust. Before investing, leaders need to understand exactly how the AI system handles PHI across its lifecycle from ingestion and training to deployment and storage. We help you design, vet, and implement AI solutions that meet HIPAA, GDPR, and other global standards. What are the AI solution s capabilities for detecting and responding to cybersecurity threats? Healthcare AI cybersecurity requires real-time anomaly detection to identify breaches, such as unauthorized access to patient data. Enterprises benefit from AI-driven threat response across networks; startups need rapid alerts for smaller systems. Ensure vendors embed proactive monitoring to address risks swiftly. How do you ensure that AI models remain secure throughout their lifecycle? Ongoing security is vital. Vendors must conduct regular audits and vulnerability assessments to protect AI in healthcare security from evolving threats. Enterprises need continuous updates for global operations and require agile maintenance to stay secure without heavy costs. Ask for detailed lifecycle protection plans. What is the biggest challenge AI implementation is currently facing in healthcare? Security and privacy remain the top challenges in the healthcare industry. Breaches impact patient trust, which is a critical concern for enterprises managing reputations. Address this by prioritizing patient data privacy in HealthTech through transparent data practices and staff training, ensuring AI enhances care without compromising safety. As AI reshapes patient care, its use cases in healthcare are redefining how organizations deliver, scale, and compete. When applied strategically, AI doesn t just improve clinical outcomes it rearchitects operations, optimizes resource allocation, and creates entirely new standards for patient experience and care delivery. Ask these questions to harness AI s full potential: What are the specific AI use cases that will generate the most value for our healthcare organization? Pinpoint high-impact AI for healthcare efficiency applications like AI-driven triage to prioritize urgent cases or predictive analytics to forecast disease progression. Enterprises can overhaul hospital workflows or integrate hyper-personalized care delivery services, depending on the requirements. Identifying the right-fit use cases whether it s predictive diagnostics, workflow automation, or real-time patient monitoring ensures investments support both short-term wins and long-term transformation. What are the ways through which the AI model will optimize clinical decision support systems (CDSS)? AI can bring real-time insights into CDSS by identifying patterns in EHR data, surfacing evidence-backed recommendations, and reducing alert fatigue through intelligent filtering. The goal is not just support, but smarter, context-aware decisions that improve clinical outcomes. Ensure that vendors integrate AI into workflows without overwhelming clinicians. In non-clinical functions (e.g., scheduling, administrative duties), how can AI be used to increase efficiency and decrease spending? Artificial intelligence changes the way the administration operates, from scheduling patients to simplifying claims procedures, thereby reducing mistakes and delays to a minimum. Intelligent bots can handle repetitive workflows, while predictive algorithms forecast staffing needs or flag billing errors freeing up human teams to focus on more complex, value-driven work. Ask vendors how AI delivers measurable savings, like faster patient throughput or reduced administrative overhead. What are the strengths and weaknesses of applying AI to healthcare? AI brings undeniable strengths speed, scale, and data-driven precision. It accelerates diagnostics, streamlines workflows, and unlocks predictive insights that were previously impossible. But its limitations are equally real. Clinical nuance, data quality issues, and algorithmic bias can affect performance. Not every task benefits from automation, and poorly implemented AI can add friction instead of solving it. Understanding both sides is crucial to set the right expectations before rollout. As large language models (LLMs) like ChatGPT and Gemini mature, healthcare organizations are beginning to explore their use in patient engagement, clinical decision support, medical transcription, and more. These AI systems bring unmatched natural language understanding and generation capabilities, but their integration into healthcare environments requires careful scrutiny to ensure accuracy, safety, and compliance. [Also Read: An Explorative Guide to LLM Fine Tuning   Implications, Applications, Methods, Best Practices] Ask these questions to assess whether LLMs are ready to support your healthcare operations: How accurate are LLM-based models in understanding and generating clinical content? LLMs trained on general-purpose data may struggle with domain-specific vocabulary or misinterpret clinical context. Ask whether the model has been fine-tuned on medical literature, and whether it can reliably understand structured data (like EHRs) and unstructured inputs (like physician notes or patient questions). We help healthcare organizations evaluate and implement LLMs fine-tuned for medical accuracy ensuring better context, fewer errors, and real clinical value. How does the LLM handle sensitive patient data in line with HIPAA and GDPR regulations? Because LLMs may store or process sensitive information, vendors must demonstrate rigorous security and privacy safeguards. Ask how personally identifiable information (PII) is protected during inference, how data is anonymized or de-identified, and whether the system avoids storing PHI unless explicitly permitted. Can the LLM integrate with existing clinical systems such as CDSS, EHRs, or chatbots? To be truly valuable, LLMs must seamlessly connect with your existing infrastructure. Ask whether the model supports integration via APIs or FHIR standards, and if it can pull real-time clinical data to provide relevant outputs within workflows such as triage, documentation, or discharge planning. What measures are in place to mitigate hallucinations or inaccurate outputs from LLMs? LLMs are known to occasionally generate confident-sounding but incorrect information known as hallucinations. Ask vendors what guardrails are in place to prevent this, such as fact-checking layers, retrieval-augmented generation (RAG), or human-in-the-loop review systems before clinical outputs are finalized. How does the LLM ensure fairness and avoid bias across diverse patient populations? Bias can surface if the LLM hasn t been tested across different age, gender, racial, or socio-economic segments. Ask if the vendor has performed fairness evaluations, used representative datasets for training, and implemented post-processing techniques to reduce skewed outputs. How can LLMs enhance patient engagement and improve communication in care delivery? LLMs can be powerful tools for personalized health education, symptom triage, or multilingual support. Ask how the AI adapts language based on patient profiles, simplifies complex medical jargon, or provides round-the-clock support through conversational interfaces in apps or portals. The success of AI in healthcare depends heavily on the vendor s understanding of real-world constraints, not just their technology stack. For enterprises building AI for healthcare support, a proven partner ensures seamless adoption and lasting impact. Ask these questions to select an experienced vendor that provides total assistance: What is the vendor s experience in the healthcare AI sector, and can they provide relevant case studies? Healthcare presents unique challenges, from regulatory scrutiny to complex data interoperability. Vendors with a proven track record in this space understand those nuances and can preempt potential pitfalls. Case studies showing successful deployment, adoption, and measurable clinical or operational impact demonstrate whether the vendor can turn pilot projects into enterprise-grade solutions. Look for partners who speak the language of clinicians and compliance, not just code. What level of ongoing support will be provided post-implementation? Each new technology has a learning curve, so robust post-implementation support is essential. Enterprises need comprehensive services troubleshooting, regular updates, and staff training to maintain complex systems. Ensure vendors commit to tailored maintenance and customization to keep your AI solution aligned with evolving goals. What type of customer success or account management will we receive during the deployment phase? In healthcare, the deployment phase can be the most sensitive part of the process. Whether you re integrating with legacy EHRs or onboarding multiple clinical teams, strong vendor-side support is essential. Ask about dedicated account managers, clinical onboarding resources, and change management expertise. Will you have a single point of contact? Is there a support team trained in healthcare-specific workflows? How fast do they respond when something breaks? The right partner doesn t just hand over the tech they stay present through go-live, resolve integration hiccups, and align with your internal rollout goals. Great customer success means fewer delays, smoother training, and faster ROI. To keep AI-driven diagnostics and triage sharp in healthcare s fast-evolving landscape, AI in healthcare performance monitoring demands precision tools and adaptive processes. Enterprises integrating AI across hospital systems and startups deploying targeted solutions need robust monitoring to ensure clinical reliability. These questions sharpen your AI strategy: How will we monitor the performance of AI models in real time? Effective monitoring uses platforms like performance dashboards to track metrics such as precision in AI-driven cardiac risk assessments. Enterprises require centralized systems to oversee AI across multiple specialties; startups need compact tools to evaluate tools like chatbot response accuracy. Ask vendors for solutions with real-time alerts for anomalies, ensuring AI aligns with patient outcome goals. What is the process for continuously improving and optimizing AI models post-deployment? Ongoing optimization relies on structured retraining, like updating models for diabetes outcome prediction with new patient glucose data. Enterprises benefit from automated retraining pipelines handling vast EHR datasets; startups need efficient workflows for refining tools like mental health screening algorithms. Ensure vendors detail how they incorporate fresh data to maintain model accuracy without disrupting care delivery. How will AI solutions be adjusted to accommodate changing healthcare regulations or emerging trends? Adaptive AI for HealthTech must comply with evolving standards like the FDA s AI/ML-based Software as a Medical Device framework or emerging genomic testing protocols. Enterprises need vendors to integrate regulatory updates across global operations; startups require flexible recalibration for applications like AI-assisted radiology. Confirm vendors conduct regular model audits to address trends like personalized medicine, keeping AI effective and compliant. Imagine a hospital chain expanding to rural clinics or a startup adding AI-driven drug discovery AI healthcare scalability must match ambition with adaptability. Enterprises and startups need AI that scales effortlessly and evolves with healthcare s next wave. These questions ensure your solution is built for tomorrow: How scalable is the AI solution as our healthcare organization grows or expands its service offerings? Your AI must handle exponential growth, like scaling a triage system from one clinic to a multi-state network. Businesses must have tools capable of handling massive patient-related information in various fields such as cardiology or orthopedics. Moreover, they must also enable startups to modernize telehealth care for chronic disease monitoring. Ask vendors how their solution can achieve performance with an increasing workload. What are the technological advancements and healthcare trends of the AI solution, and how well does it future-proof the solution? Future-ready HealthTech AI will be able to run alongside other emerging technologies, such as edge computing to monitor vitals in real-time or blockchain to share patient data in a secure way. Businesses have to be able to adapt to the global change and change their frameworks to niche breakthroughs, such as using AI in immunotherapy. Make sure that vendors remain compatible with future systems through the use of open APIs. What is the future of AI in healthcare? AI-driven healthcare evolution will prioritize hyper-personalized care, like tailoring cancer treatments via genomic data. Enterprises will unify AI across global facilities; startups will innovate with focused tools like virtual mental health coaches. Ask vendors how their AI aligns with these long-term shifts. How will AI change healthcare? AI healthcare transformation will streamline care delivery, from automating discharge planning to enable virtual ICUs. Enterprises can optimize system-wide efficiency; startups can disrupt with patient-centric tools. Therefore, confirm your vendors place your AI-driven healthcare solutions to lead in this new era of care. At Appinventiv, we have a decade of experience redefining the healthcare industry, powered by AI technology. We have built over 1000 AI-powered applications and digital products for various industry giants like KFC, IKEA, and KPMG, and secured over $800 million in funding for startups. Named Top Android & Chatbot Development Company 2025 and Fastest-Growing Company 2025 by Clutch, we craft healthcare AI solutions that slash wait times and medical care precision. Want to know how we achieve that? Take a look at the case study of our multi-request format platform for in-hospital patients, YouCOMM, which we developed. This platform helps patients connect with nurses. It is a fully customizable app that prioritizes accessibility and inclusivity by offering various options, including voice commands and head gestures. We also developed DiabeticU, a diabetes management app that offers users real-time tracking, educational resources, and smart lifestyle suggestions based on their health data. At the core of the app is an AI-powered wellness recommendation engine that analyzes real-time user data including glucose levels, meal patterns, and activity to deliver personalized lifestyle suggestions. These recommendations help users make smarter decisions about their nutrition, medication, and daily routines, ultimately reducing complications and improving long-term outcomes. By combining behavioral tracking with AI-led insights, the platform elevates chronic care into continuous, personalized wellness. Our healthcare software development services are HIPAA-compliant and integrate seamlessly with EHRs, while scalable architectures support trends like IoT patient monitoring. We have  GDPR-Ready  architecture for global HealthTech platforms, ensuring compliance across international markets. With dedicated support and continuous model retraining, we empower healthcare enterprises and startups to tackle regulatory risks and drive lasting impact in the future of medicine. With dedicated support and continuous model retraining, we empower healthcare enterprises and startups to tackle regulatory risks and drive lasting impact in the future of medicine. Each new technology has a learning curve, and these questions stand to guide you in choosing a company for healthcare app development services that acts as a strategic partner to your vision. By choosing a healthcare app development partner like us, you will be able to develop future-proof AI applications that will have a significant effect on your company. Q. How does AI ensure compliance with healthcare regulations? A. AI in healthcare compliance must have end-to-end encryption and frequent audit sessions to meet HIPAA and GDPR standards. To abide by the changing regulations, vendors should supply compliance certifications and automated updates to make sure that an enterprise will not face any fines and startups will be able to build trust-based relationships without any legal factors involved. Q. What ROI can healthcare organizations expect from AI? A. Healthcare AI ROI depends on clear KPIs: Q. How does AI integrate with existing healthcare systems? A. Seamless AI HealthTech integration connects AI to EHRs like Epic or Cerner using robust APIs. For enterprises, this unifies multi-site workflows; for startups, lightweight solutions enable quick deployment. Vendors must offer phased rollouts and training to prevent disruptions, ensuring smooth clinical operations. Q. What expertise should we seek in an AI vendor? A. AI healthcare vendor expertise is critical. Seek vendors with case studies in: Q. Can AI scale with our organization s growth? A. Scalable AI supports growth from single clinics to global networks. Enterprises need platforms handling massive patient data for specialties like cardiology; startups require modular tools for pivoting to telehealth or genomics. Vendors must ensure performance stability and adaptability to trends such as IoT integration, keeping their solutions future-ready. CTO s Take on Complying with US HealthTech Regulations: Navigating HIPAA, HITECH, and Beyond Key takeaways: Build HIPAA, HITECH, and other regulatory requirements into your platform s architecture from the start. Stay ahead of emerging regulations like FHIR and SMART on FHIR to ensure seamless integration. Implement encryption, access controls, and secure APIs as core components of your platform s foundation. Vet every third-party vendor to ensure they meet compliance requirements  Healthcare IT Outsourcing: Key Benefits, Best Practices, and Implementation Process Key Takeaways Reduce Costs by 30-40%: Convert capital IT expenses to predictable operational costs, eliminating the need for large in-house teams and infrastructure. Focus on Core Patient Care: Free up internal resources from IT management to concentrate on core healthcare services, innovation, and growth. Ensure Compliance & Mitigate Risk: Leverage expert partners to navigate HIPAA  How AI-Powered Virtual Health Assistants Are Enhancing Remote Patient Monitoring Key takeaways: Remote patient monitoring has the potential to reduce hospital readmissions by up to a quarter, shifting healthcare from reactive to proactive care through continuous, real-time health tracking. The U.S. health intelligent virtual assistant market is expected to reach $1.87 billion by 2030, driven by the growing demand for continuous care, virtual consultations, and  B-25, Sector 58,Noida- 201301,Delhi - NCR, India 79, Madison Ave Manhattan, NY 10001,USA Appinventiv Australia, East Brisbane QLD 4169, Australia 3rd Floor, 86-90 Paul Street EC2A 4NE London, UK Tiger Al Yarmook Building, 13th floor B-block Al Nahda St - Sharjah Suite 3810, Bankers Hall West,888 - 3rd Street Sw Calgary Alberta Full stack mobile (iOS, Android) and web app design and development agency Appinventiv is the Registered Name of Appinventiv Technologies Pvt. Ltd., a mobile app development company situated in Noida, U.P. India at the street address - B- 25, Sector 58, Noida, U.P. 201301. All the personal information that you submit on the website - (Name, Email, Phone and Project Details) will not be sold, shared or rented to others. Our sales team or the team of mobile app developers only use this information to send updates about our company and projects or contact you if requested or find it necessary. You may opt out of receiving our communication by dropping us an email on - info@appinventiv.com 1600+ transformation engineers delivered 3000+ game-changing products. We chose Appinventiv to build our financial literacy and money management app from start to finish. From the first call, we were very impressed with Appinventiv s professionalism, expertise, and commitment to delivering top-notch results. It has been a pleasure working with Appinventiv. The team is not only extremely versatile and competent but also very professional, courteous, and responsive. We certainly plan to continue working with Appinventiv for an indefinite period. We took a big leap of faith with Appinventiv who helped us translate our vision into reality with the perfectly comprehensive Edamama eCommerce solution. We are counting to get Edamama to launch on time and within budget, while rolling out the next phase of the platform with Appinventiv. I just want to take a moment to thank the entire Appinventiv team for your incredible support. We truly appreciate everything you've done, and we're excited to continue working together as we grow here at KODA After researching numerous companies, we finally found Appinventiv, and it was the best decision we could have made. They successfully addressed the challenges with our existing app and provided solutions that exceeded our expectations. We approached Appinventiv with a clear vision to build a robust and future-ready platform that could seamlessly integrate with the busy lifestyle of our customers while uplifting their overall experience and giving us a competitive edge. 1600+ transformation engineers delivered 3000+ game-changing products. Connect with our consultation experts to get: Insights specific to your business needs Roadmap to overcome your challenges Opportunities to scale your business in this niche.",2
"Generative AI’s healthcare professional role creep: a cross-sectional evaluation of publicly accessible, customised health-related GPTs - Frontiers",https://news.google.com/rss/articles/CBMilgFBVV95cUxOUnNrQmowMlpSSG00eHNLNGU3aFJ4TDB0cHlNY1FHWlVZQUdlcEx6a1I3S0xvSFhOeXBVUXlHVUV0RmZnZUt4RXh0VS1wOThOY3l5M3pjYzQwZ3Rld1Raaldoc3RYWXVjNHRDeHVxb2wzOElLdnpFV0NGMnFJQUE0TkwzYzlZU2VIdkUwcmE3aEhwdGpFQnc?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better ORIGINAL RESEARCH article Front. Public Health, 09 May 2025 Sec. Digital Public Health Volume 13 - 2025 | https://doi.org/10.3389/fpubh.2025.1584348 Introduction: Generative artificial intelligence (AI) is advancing rapidly; an important consideration is the public s increasing ability to customise foundational AI models to create publicly accessible applications tailored for specific tasks. This study aims to evaluate the accessibility and functionality descriptions of customised GPTs on the OpenAI GPT store that provide health-related information or assistance to patients and healthcare professionals. Methods: We conducted a cross-sectional observational study of the OpenAI GPT store from September 2 to 6, 2024, to identify publicly accessible customised GPTs with health-related functions. We searched across general medicine, psychology, oncology, cardiology, and immunology applications. Identified GPTs were assessed for their name, description, intended audience, and usage. Regulatory status was checked across the U.S. Food and Drug Administration (FDA), European Union Medical Device Regulation (EU MDR), and Australian Therapeutic Goods Administration (TGA) databases. Results: A total of 1,055 customised, health-related GPTs targeting patients and healthcare professionals were identified, which had collectively been used in over 360,000 conversations. Of these, 587 were psychology-related, 247 were in general medicine, 105 in oncology, 52 in cardiology, 30 in immunology, and 34 in other health specialties. Notably, 624 of the identified GPTs included healthcare professional titles (e.g., doctor, nurse, psychiatrist, oncologist) in their names and/or descriptions, suggesting they were taking on such roles. None of the customised GPTs identified were FDA, EU MDR, or TGA-approved. Discussion: This study highlights the rapid emergence of publicly accessible, customised, health-related GPTs. The findings raise important questions about whether current AI medical device regulations are keeping pace with rapid technological advancements. The results also highlight the potential  role creep  in AI chatbots, where publicly accessible applications begin to perform   or claim to perform   functions traditionally reserved for licensed professionals, underscoring potential safety concerns. Generative artificial intelligence (AI) applications, such as OpenAI s ChatGPT, Google s Gemini, and Anthropic s Claude, are advancing rapidly with increasingly sophisticated abilities and outputs across a broadening array of fields (1 4). These advances stem from breakthroughs in natural language processing, particularly following the development of large language models that can be fine-tuned to perform medical tasks and provide health information (5 7). This has enabled the emergence of health-focused chatbots with the potential to transform public access to health information by offering clear, reliable, tailored, empathetic and real-time responses across multiple languages (1 4, 8, 9). While these AI technologies offer new possibilities, they also present challenges for regulatory bodies like the U.S. Food and Drug Administration (FDA), European Union Medical Device Regulation (EU MDR) and the Australian Therapeutic Goods Administration (TGA) (3, 9 14). Generative AI tools may fall under medical device regulations, requiring transparent and robust evidence of clinical validation for their efficacy and risks, if they provide diagnostic or therapeutic advice, offer clinical recommendations, or directly influence healthcare decisions made by patients or clinicians (15 17). The rapid evolution of generative AI models presents unique challenges for regulatory frameworks (3, 10 14). Due to the broad range of capabilities of models like ChatGPT, Gemini, and Claude, these systems are intended to have safeguards and terms of use to avoid unintentionally meeting criteria for medical device regulation. However, emerging evidence suggests that both healthcare professionals and the public are increasingly using these systems to inform diagnoses and guide care strategies (18 20). Another important consideration is the growing ease with which the public can access and customise the original foundation models, and then release publicly accessible AI applications for specific tasks (9, 21). For instance, OpenAI s GPT store allows individuals to easily create and publicly share customised GPT applications (21). However, the extent to which these tailored applications maintain safety and clearly communicate their limitations remains unclear, particularly in health-related contexts. This research seeks to address this gap by evaluating the OpenAI GPT store for customised GPTs designed or described as providing healthcare-related information or assistance. Our goal was to provide a snapshot of the accessibility and functionality descriptions of these GPTs, facilitating discussions among healthcare professionals about their potential risks and benefits. A notable consideration is the naming of these publicly accessible AI applications, which, if unclear, may suggest they are taking on roles that traditionally require demonstrated healthcare professional competence and/or formal regulatory registration. Using a cross-sectional observational study design, the OpenAI GPT store (21) was searched from September 2nd to 6th, 2024, to identify publicly accessible, customised GPTs with purported health-related functions. The search terms included: clinician, doctor, physician, nurse, healthcare, medical, psychiatrist, psychologist, therapist, mental health, counselor, vaccine, immunization, immunologist, vaccination, oncologist, hematologist, cancer, cardiologist, heart, and cardiology. The intent was to identify a broad range of publicly accessible, customised health-related GPTs, as well as examples tailored for highly specialized areas of medical practice. GPTs included in our evaluations were those that appeared designed to assist or provide information to patients or healthcare professionals. GPTs that were not health-related or were described as solely for academic research purposes were excluded. For each of the identified health-related GPTs, available information on the GPT name, displayed description, user rating, number of conversations, capabilities, creator, and URL was recorded. Two healthcare researchers (authors B.C and A.M.H) independently reviewed the GPT names and their displayed descriptions. Each GPT was then grouped according to its apparent target audience (healthcare professionals, patients, or both healthcare professionals and patients) and health specialty (general medicine, psychology, cardiology, oncology, immunology, or other). Identified health-related GPTs were also evaluated for the presence of healthcare professional titles in their name and/or displayed descriptions. The FDA, EU MDR and TGA lists of approved or registered AI/machine learning medical devices were searched to determine if any of the identified health-related GPTs were listed (17, 22, 23). The top 10 most-used health-related GPTs were subjected to an exploratory analysis, where each GPT was questioned in relation to its description, target audience, regulatory approval status, supporting research evidence, instructions, and specific knowledge files. Supplementary File 1 provides the specific questions (along with the full responses) asked of each of the top 10 most-used health-related GPTs identified. The conducted search identified 1,055 publicly accessible, customised GPTs with described health-related purposes (Supplementary Figure 1). Supplementary File 2 provides usage, descriptive characteristics, and URL information for each of these GPTs. Of the 1,055 identified GPTs, 587 were related to psychology, 247 to general medicine, 105 to oncology, 52 to cardiology, 30 to immunology, and 34 to other health specialties. Of the 1,055 GPTs, 589 were tailored to assist or provide information to patients, 128 for healthcare professionals, and 338 for both healthcare professionals and patients. These 1,055 GPTs had been used in over 360,000 cumulative conversations, with 36 GPTs having been used more than 1,000 times and 10 having been used more than 5,000 times. None of these 1,055 publicly accessible, customised GPTs were identified as approved medical devices by the FDA, EU MDR or TGA (17, 22, 23). Of the 1,055 GPTs, 624 included healthcare professional titles within their name and/or displayed description, including Therapist (n = 170), Psychologist (n = 139), Doctor (n = 104), Counselor (n = 76), Nurse (n = 66), Psychiatrist (n = 60), Dr. (n = 22), Counselor (n = 14), Cardiologist (n = 11), Oncologist (n = 6), Hematologist (n = 4), Clinician (n = 2), Immunologist (n = 2), Hematologist (n = 1), and Radiologist (n = 1). Table 1 provides examples of GPTs with healthcare professional titles in their names and/or displayed descriptions. For the 431 GPTs that did not include healthcare professional titles within their names and/or displayed descriptions, many still related to highly specialized medical tasks including, but not limited to:  Medical Diagnosis Assistant ,  Cardiology-focused echocardiography expert ,  expert on vaccines ,  A GPT expert in head and neck cancer staging ,  therapeutic companion offering mental health support ,  Expert in X-Ray and MRI Imaging Analysis . Table 1. Examples of identified publicly accessible, customised GPTs with healthcare professional titles in their names and/or displayed descriptions. Table 2 provides the name, usage, and displayed descriptions for the 10 most-used health-related GPTs identified, along with a summary of their responses to questions regarding their description, target audience, regulatory approval status, and supporting research evidence. Full responses are in Supplementary File 1. Each of these 10 GPTs had been used more than 5,000 times, with six having been used more than 10,000 times and one, named  Therapist   Psychologist (non-medical therapy) , having been used more than 200,000 times. Cumulatively, these 10 GPTs had been used in over 300,000 conversations, representing over 80% of the total conversations across all 1,055 GPTs identified. The  Therapist   Psychologist (non-medical therapy)  GPT alone accounted for approximately 55% of the cumulative uses. Notably, 6 of the 10 most-used health-related GPTs had names that suggested they were taking on healthcare professional roles by including terms like  Therapist,   Psychologist,   Registered Nurse,  and  Medical Doctor . For each of these six GPTs, their displayed descriptions appeared to reinforce this suggestion. Table 2. Name, usage, and displayed description details, along with a summary of responses to questions regarding description, target audience, regulatory approval status, and supporting research evidence for the 10 most-used health-related GPTs identified in this study. Of the 10 most-used health-related GPTs, two indicated that they would not divulge information related to their description, target audience, regulatory approval status, supporting research evidence, instructions, or specific knowledge files. Of the remaining eight GPTs, six responded that they were designed to provide information to patients or healthcare professionals, while two were designed to assist with tasks related to medical notetaking. None of the eight GPTs were able to provide specific research evidence to support their safety, and none provided information regarding their regulatory approval status, although seven argued that such approvals were not required for various reasons. This study identified over 1,000 GPTs publicly accessible on the OpenAI GPT store customised to provide health-related information or assistance to patients or healthcare professionals across general medicine, psychology, oncology, cardiology, and immunology. Collectively, these GPTs have been used in over 360,000 conversations, with the 10 most used GPTs accounting for over 300,000 uses. Notably, over half of the identified GPTs included healthcare professional titles within their names and/or descriptions, suggesting that these applications may be assuming responsibilities traditionally reserved for licensed professionals. For instance, this may reflect AI role creep, whereby chatbots expand their responsibilities to perform those typically carried out by licensed professionals. Regulatory bodies such as the FDA, EU MDR, and TGA oversee the approval of AI medical devices (15 17). However, models like ChatGPT, Gemini, and Claude are generally classified as informational systems not requiring such evaluations (24, 25). With the rapid evolution of generative AI, both the public and healthcare professionals are increasingly using AI for healthcare advice and administrative assistance (8, 18 20, 26), highlighting an important need for auditing and proactive monitoring to ensure their safety in the community (3, 9, 13, 14, 27 29). Beyond regulation, responsible integration into healthcare also requires careful ethical consideration ensuring accuracy, protecting user privacy, promoting transparency, and minimizing bias at both the model and developer levels (30, 31). Another important consideration is the growing ease with which the public can customise foundation AI models and release new applications (9, 21). A recent study found 22 customised ophthalmic GPTs on the OpenAI platform (32), with our study, the largest yet, identifying over 1,000 customised health-related GPTs. Among these, 10 GPTs had been involved in over 300,000 conversations, offering functions described across symptom assessment, first aid, cognitive behavioral therapy, diagnostic assistance, and drafting of medical notes for the British National Health Service (NHS). Combined with identifying over 600 GPTs displaying healthcare professional titles in their names and/or descriptions, this study raises important questions about the boundaries on AI being deployed into the community and whether medical device regulations are lagging behind current technological advancements. Notably, in many countries and jurisdictions, the use of titles like  Doctor  by humans is regulated and monitored (33 36). However, none of the customised GPTs identified in our study had FDA, EU MDR, or TGA approval. We acknowledge that generative AI, including customised GPTs, do not require regulatory approval from the FDA, EU MDR, or TGA if they do not meet medical device criteria (15 17, 24, 25). This includes cases where they are clearly intended for informational purposes, providing reliable, referenced information that directs users to qualified healthcare professionals for personalized advice. Further, this may include symptom checkers, risk calculators, wellness chatbots, general health advice tools, or medical scribes, where functionalities and responses are clearly not intended for medical diagnosis or treatment. Correspondingly, it is not the intent of this study to suggest that all identified GPTs require regulation or are inherently harmful some are likely innovative, useful, and beyond regulator scope. Rather, the study importantly highlights the rapidly emerging phenomenon of customised, health-related GPTs. From which our findings suggest that a discussion on the appropriateness of the naming and descriptions of publicly accessible AI is warranted. Notably, while our focus was on the OpenAI GPT store, a brief internet search revealed over 10 AI platforms leveraging generative AI APIs, marketing  AI doctors  capable of diagnosing and treating across general medicine and specialized fields (Supplementary File 3). This observation included one platform,  Doctronic   your private and personal AI-powered doctor,  which had been used in over 2.6 million conversations (37). In addition, we acknowledge that large language models developed by major technology companies such as Google s Gemini and Meta s Llama are becoming increasingly accessible to the public and could be readily customized to deliver health information, thereby expanding the landscape of available health-focused AI tools. Undoubtedly publicly accessible generative AI holds immense potential to improve access to health information within the community through advancing abilities to offer clear, reliable, tailored, and empathetic responses in real-time across multiple languages (1 4, 8, 9). However, much like the internet where the usefulness of health information hinges on accessing it from reliable sources the generative AI ecosystem must evolve to prioritize transparency and vigilance within public-facing health-related contexts, regardless of whether applications fall under formal regulation. At this pivotal moment, we can guide generative AI development and deployment to create a safe and trustworthy environment. Key considerations include ensuring that health responses are based on reliable sources, with transparent referencing, and that they direct users to qualified healthcare professionals for personalized advice. To this end, AI developers should involve creators of current trusted medical resources (such as those from health organizations, institutions, and societies) to ensure the information meets practice standards. Furthermore, we propose that AI applications should refrain from using healthcare professional titles in their names or descriptions; instead, terms like  information  for public-facing tools and  assistant  for clinician-facing tools can help avoid confusion about their intended functions. Additionally, prioritizing the multilingual capabilities of AI will help ensure equitable access to health information across diverse populations; neglecting this may allow existing inequities to persist or worsen. Finally, research evidence supporting the accuracy of deployed AI should be readily available, and potential errors and limitations should be clearly indicated, ideally with quantifiable data. Notably, our study found that none of the top 10 most-used health-related GPTs provided specific research evidence to support their safety. Particularly concerning, two of the top 10 both indicating  psychologist  in their names refused to answer questions about their description, target audience, regulatory approval status, supporting research evidence, instructions, or knowledge files. Such behavior would be unacceptable for human psychologists, underscoring the urgent need for the AI ecosystem to prioritize accountability. Limitations of the present study include that the identification of customised, health-related GPTs was dependent on the search terms used and the time at which the search was conducted. Many additional health-related GPTs are likely available on the OpenAI GPT store, noting, for example, that search terms such as  naturopath  and  homeopath  also return customised applications. Additionally, while we assessed the characteristics of the identified customised GPTs including their names, descriptions, number of uses, and intended audience we did not test their functionality or accuracy regarding their purported functions. Interpretation of usage data was also limited, as the content and context of user interactions were not accessible; therefore, usage counts alone may not accurately reflect real-world use. Finally, we acknowledge that classification of GPTs was based on their names and descriptions, which may involve a degree of subjectivity. Addressing these limitations in future studies will be important, along with developing a structured process to identify and evaluate generative AI applications customised for health-related purposes across the internet more broadly than just the OpenAI GPT store. This study provides an important snapshot of the rapidly emerging ecosystem of customised, health-related GPTs on the OpenAI GPT store, identifying over 1,000 publicly accessible applications. While some of these GPTs likely offer useful functions, as suggested by the high use of certain applications, concerns about unregulated  role creep  exist, with over half including healthcare professional titles in their names and/or descriptions. Furthermore, we observed a clear need for improved transparency to ensure these applications provide clear evidence of their accuracy, safety, and limitations to the community. Finally, this study raises questions about whether current AI medical device regulations are adequate or lagging amid rapid technological advancement particularly given that none of the customised GPTs identified had FDA, EU MDR, or TGA approval. The original contributions presented in the study are included in the article/Supplementary material, further inquiries can be directed to the corresponding author. The research undertaken was undertaken with approval from the Flinders University Human Research Ethics Committee. BC: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing   original draft, Writing   review & editing. NM: Conceptualization, Formal analysis, Investigation, Methodology, Resources, Supervision, Validation, Visualization, Writing   review & editing. BM: Writing   review & editing, Conceptualization, Methodology, Project administration, Supervision, Validation. SB: Formal analysis, Validation, Writing   review & editing. GK: Formal analysis, Validation, Writing   review & editing. CP: Formal analysis, Validation, Writing   review & editing. JK: Formal analysis, Validation, Writing   review & editing. IR: Formal analysis, Validation, Writing   review & editing. JL: Formal analysis, Validation, Writing   review & editing. MW: Formal analysis, Validation, Writing   review & editing. RM: Formal analysis, Validation, Writing   review & editing. AR: Formal analysis, Validation, Writing   review & editing. MS: Formal analysis, Validation, Writing   review & editing. AH: Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Resources, Software, Supervision, Validation, Visualization, Writing   review & editing. The author(s) declare that financial support was received for the research and/or publication of this article. AH holds an Emerging Leader Investigator Fellowship from the National Health and Medical Research Council, Australia (APP2008119). The PhD scholarship of BM is supported by the National Health and Medical Research Council (APP2030913). NM salary is supported by funding from the Hospital Research Foundation (2023-S-DTFA-005) and Tour De Cure (RSP-117-FY2023). MS is supported by a Beat Cancer Research Fellowship from the Cancer Council South Australia. The funders had no role in considering the study design or in the collection, analysis, interpretation of data, writing of the report, or decision to submit the article for publication. AR and MS are recipients of investigator-initiated funding for research outside the scope of the current study from AstraZeneca, Boehringer Ingelheim, Pfizer and Takeda. AH is a recipient of investigator-initiated funding for research outside the scope of the current study from Boehringer Ingelheim. AR is a recipient of speaker fees from Boehringer Ingelheim and Genentech outside the scope of the current study. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The authors declare that Generative AI was used in the creation of this manuscript. During the preparation of this work the authors used ChatGPT and Grammarly AI to assist in the formatting and editing of the manuscript to improve the language and readability. The authors take full responsibility for the content of the publication. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. The Supplementary material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fpubh.2025.1584348/full#supplementary-material 1. Haupt, CE, and Marks, M. AI-generated medical advice-GPT and beyond. JAMA. (2023) 329:1349 50. doi: 10.1001/jama.2023.5321 PubMed Abstract | Crossref Full Text | Google Scholar 2. Bedi, S, Liu, Y, Orr-Ewing, L, Dash, D, Koyejo, S, Callahan, A, et al. Testing and evaluation of health care applications of large language models: a systematic review. JAMA. (2024) 333:319 28. doi: 10.1001/jama.2024.21700 PubMed Abstract | Crossref Full Text | Google Scholar 3. Sorich, MJ, Menz, BD, and Hopkins, AM. Quality and safety of artificial intelligence generated health information. BMJ. (2024) 384:q596. doi: 10.1136/bmj.q596 PubMed Abstract | Crossref Full Text | Google Scholar 4. Lee, P, Bubeck, S, and Petro, J. Benefits, limits, and risks of GPT-4 as an AI Chatbot for medicine. N Engl J Med. (2023) 388:1233 9. doi: 10.1056/NEJMsr2214184 PubMed Abstract | Crossref Full Text | Google Scholar 5. Chow, JCL, Wong, V, Sanders, L, and Li, K. Developing an AI-assisted educational Chatbot for radiotherapy using the IBM Watson assistant platform. Healthcare. (2023) 11:2417. doi: 10.3390/healthcare11172417 PubMed Abstract | Crossref Full Text | Google Scholar 6. Chow, JCL, and Li, K. Developing effective frameworks for large language model-based medical Chatbots: insights from radiotherapy education with ChatGPT. JMIR Cancer. (2025) 11:e66633. doi: 10.2196/66633 PubMed Abstract | Crossref Full Text | Google Scholar 7. Menz, BD, Modi, ND, Abuhelwa, AY, Ruanglertboon, W, Vitry, A, Gao, Y, et al. Generative AI chatbots for reliable cancer information: evaluating web-search, multilingual, and reference capabilities of emerging large language models. Eur J Cancer. (2025) 218:115274. doi: 10.1016/j.ejca.2025.115274 PubMed Abstract | Crossref Full Text | Google Scholar 8. Hopkins, AM, Logan, JM, Kichenadasse, G, and Sorich, MJ. Artificial intelligence chatbots will revolutionize how cancer patients access information: ChatGPT represents a paradigm-shift. JNCI Cancer Spectr. (2023) 7:pkad010. doi: 10.1093/jncics/pkad010 PubMed Abstract | Crossref Full Text | Google Scholar 9. Freyer, O, Wiest, IC, Kather, JN, and Gilbert, S. A future role for health applications of large language models depends on regulators enforcing safety standards. Lancet Digit Health. (2024) 6:e662 72. doi: 10.1016/S2589-7500(24)00124-9 PubMed Abstract | Crossref Full Text | Google Scholar 10. Mesk , B, and Topol, EJ. The imperative for regulatory oversight of large language models (or generative AI) in healthcare. NPJ Digit Med. (2023) 6:120. doi: 10.1038/s41746-023-00873-0 PubMed Abstract | Crossref Full Text | Google Scholar 11. Muralidharan, V, Adewale, BA, Huang, CJ, Nta, MT, Ademiju, PO, Pathmarajah, P, et al. A scoping review of reporting gaps in FDA-approved AI medical devices. NPJ Digit Med. (2024) 7:273. doi: 10.1038/s41746-024-01270-x PubMed Abstract | Crossref Full Text | Google Scholar 12. Warraich, HJ, Tazbaz, T, and Califf, RM. FDA perspective on the regulation of artificial intelligence in health care and biomedicine. JAMA. (2024) 333:241 7. doi: 10.1001/jama.2024.21451 PubMed Abstract | Crossref Full Text | Google Scholar 13. Menz, BD, Kuderer, NM, Bacchi, S, Modi, ND, Chin-Yee, B, Hu, T, et al. Current safeguards, risk mitigation, and transparency measures of large language models against the generation of health disinformation: repeated cross sectional analysis. BMJ. (2024) 384:e078538. doi: 10.1136/bmj-2023-078538 PubMed Abstract | Crossref Full Text | Google Scholar 14. Menz, BD, Modi, ND, Sorich, MJ, and Hopkins, AM. Health disinformation use case highlighting the urgent need for artificial intelligence vigilance: weapons of mass disinformation. JAMA Intern Med. (2024) 184:92 6. doi: 10.1001/jamainternmed.2023.5947 PubMed Abstract | Crossref Full Text | Google Scholar 15. The European Union Medical Device Regulation. (2024) Regulation (EU) 2017/745 (EU MDR). Available online at: https://eumdr.com/ (Accessed October 20, 2024). Google Scholar 16. Therapeutic Goods Administration (TGA): (2024) Artificial intelligence (AI) and medical device software. Available online at: https://www.tga.gov.au/how-we-regulate/manufacturing/manufacture-medical-device/manufacture-specific-types-medical-devices/artificial-intelligence-ai-and-medical-device-software (Accessed October 20, 2024). Google Scholar 17. U.S. Food & Drugs Administration. (2024). Artificial intelligence and machine learning (AI/ML)-enabled medical devices 2024. Available online at: https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices (Accessed October 20, 2024). Google Scholar 18. Forbes. (2024). Dr. GPT 84% say ChatGPT Got Their Diagnosis Right. Available online at: https://www.forbes.com/sites/johnkoetsier/2024/01/02/dr-gpt-84-say-chatgpt-got-their-diagnosis-right/ (Accessed October 25, 2024). Google Scholar 19. Ayers, JW, Poliak, A, Dredze, M, Leas, EC, Zhu, Z, Kelley, JB, et al. Comparing physician and artificial intelligence Chatbot responses to patient questions posted to a public social media forum. JAMA Intern Med. (2023) 183:589 96. doi: 10.1001/jamainternmed.2023.1838 PubMed Abstract | Crossref Full Text | Google Scholar 20. Charlotte, RB, Cosima, L, Jens, G, Maria, H, and Kenneth, DM. Generative artificial intelligence in primary care: an online survey of UK general practitioners. BMJ Health Care Inform. (2024) 31:e101102. doi: 10.1136/bmjhci-2024-101102 PubMed Abstract | Crossref Full Text | Google Scholar 21. OpenAI. (2024). OpenAI: Introducing the GPT store. Available online at: https://openai.com/index/introducing-the-gpt-store/ (Accessed September 2, 2024). Google Scholar 22. European Commission: EUDAMED. (2024) European database on medical devices. Available online at: https://ec.europa.eu/tools/eudamed/#/screen/home (Accessed October 20, 2024). Google Scholar 23. TGA. (2024) Therapeutic goods administration (TGA): ARTG search visualisation tool. Available online at: https://compliance.health.gov.au/artg/ (Accessed October 25, 2024). Google Scholar 24. U.S. Food & Drugs Administration (FDA). (2022) Your clinical decision support software: is it a medical device? Available online at: https://www.fda.gov/medical-devices/software-medical-device-samd/your-clinical-decision-support-software-it-medical-device (Accessed October 25, 2024). Google Scholar 25. Therapeutic Goods Administration (TGA): (2024) Excluded software, interpretation of software exclusion criteria. Available online at: https://www.tga.gov.au/sites/default/files/2024-07/excluded-software.pdf (Accessed October 25, 2024). Google Scholar 26. Reddy, S, and Generative, AI. In healthcare: an implementation science informed translational path on application, integration and governance. Implement Sci. (2024) 19:27. doi: 10.1186/s13012-024-01357-9 PubMed Abstract | Crossref Full Text | Google Scholar 27. Hopkins, AM, Menz, BD, and Sorich, MJ. Potential of large language models as tools against medical disinformation reply. JAMA Intern Med. (2024) 184:450 1. doi: 10.1001/jamainternmed.2024.0023 PubMed Abstract | Crossref Full Text | Google Scholar 28. Tam, TYC, Sivarajkumar, S, Kapoor, S, Stolyar, AV, Polanska, K, McCarthy, KR, et al. A framework for human evaluation of large language models in healthcare derived from literature review. NPJ Digit Med. (2024) 7:258. doi: 10.1038/s41746-024-01258-7 PubMed Abstract | Crossref Full Text | Google Scholar 29. Menz, BD, Kuderer, NM, Chin-Yee, B, Logan, JM, Rowland, A, Sorich, MJ, et al. Gender representation of health care professionals in large language model-generated stories. JAMA Netw Open. (2024) 7:e2434997 7. doi: 10.1001/jamanetworkopen.2024.34997 PubMed Abstract | Crossref Full Text | Google Scholar 30. Chow, JCL, Sanders, L, and Li, K. Impact of ChatGPT on medical chatbots as a disruptive technology. Front Artif Intell. (2023) 6:1166014. doi: 10.3389/frai.2023.1166014 PubMed Abstract | Crossref Full Text | Google Scholar 31. Chow, JCL, and Li, K. Ethical considerations in human-centered AI: advancing oncology Chatbots through large language models. JMIR Bioinform Biotechnol. (2024) 5:e64406. doi: 10.2196/64406 PubMed Abstract | Crossref Full Text | Google Scholar 32. Aykut, A, and Sezenoz, AS. Exploring the potential of code-free custom GPTs in ophthalmology: an early analysis of GPT store and user-Creator guidance. Ophthalmol Ther. (2024) 13:2697 713. doi: 10.1007/s40123-024-01014-w PubMed Abstract | Crossref Full Text | Google Scholar 33. (2024) AHPRA and the National Boards: What's an offence under the National law? Available online at: https://www.ahpra.gov.au/Notifications/Reporting-a-criminal-offence/What-is-an-offence.aspx (Accessed October 24, 2024). Google Scholar 34. UK Public General Acts (2023) Medical act 1983; section 49: penalty for pretending to be registered. Available online at: https://www.legislation.gov.uk/ukpga/1983/54 (Accessed October 24, 2024). Google Scholar 35. European Union (2024) Directive 2005/36/ec of the european parliament and of the council; article 52 - use of professional titles. Available online at: https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32005L0036 (Accessed October 24, 2024). Google Scholar 36. Federal Trade Commission (2006) Act: section 5 - unfair or deceptive acts or practices. Available online at: https://www.ftc.gov/sites/default/files/documents/statutes/federal-trade-commission-act/ftc_act_incorporatingus_safe_web_act.pdf (Accessed October 24, 2024). Google Scholar 37. Doctronic. (2024) Available online at: https://www.doctronic.ai/ (Accessed October 24, 2024). Google Scholar Keywords: customised GPTs, Generative AI in healthcare, AI health applications, medical chatbots, AI regulation, OpenAI GPT store Citation: Chu B, Modi ND, Menz BD, Bacchi S, Kichenadasse G, Paterson C, Kovoor JG, Ramsey I, Logan JM, Wiese MD, McKinnon RA, Rowland A, Sorich MJ and Hopkins AM (2025) Generative AI s healthcare professional role creep: a cross-sectional evaluation of publicly accessible, customised health-related GPTs. Front. Public Health. 13:1584348. doi: 10.3389/fpubh.2025.1584348 Received: 27 February 2025; Accepted: 21 April 2025; Published: 09 May 2025. Edited by: Reviewed by: Copyright   2025 Chu, Modi, Menz, Bacchi, Kichenadasse, Paterson, Kovoor, Ramsey, Logan, Wiese, McKinnon, Rowland, Sorich and Hopkins. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Ashley M. Hopkins, ashley.hopkins@flinders.edu.au  These authors have contributed equally to this work  ORCID: Ashley M. Hopkins, orcid.org/0000-0001-7652-4378 Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on Supplementary Material Supplementary file 1.pdf Table 1.xlsx Table 2.xlsx Table 3.xlsx These cookies are necessary for our websites to function as they enable you to navigate around the sites and use our features. They cannot be switched off in our systems. They are activated set in response to your actions such as setting your privacy preferences, logging-in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. These cookies allow us to collect information about how visitors use our website, including the number of visitors, the websites that referred them to our website, and the pages that they visited. We use them to compile reports, to measure and improve the performance of our website, analyze which pages are the most viewed, see how visitors move around the site and fix bugs. If you do not allow these cookies, your experience will not be altered but we will not be able to improve the performance and content of our website. These cookies may be set by us to offer your personalized content and opportunities to cooperate. They may also be used by social media companies we work with to build a profile of your interests and show you relevant adverts on their services. They do not store directly personal information but are based on unique identifiers related to your browser and internet device. If you do not allow these cookies, you will experience less targeted advertising.",2
10 AI healthcare trends to watch in 2025 and beyond - TechTarget,https://news.google.com/rss/articles/CBMihgFBVV95cUxNdDBJWTNCTG56dmFIV0ZRdEF1cVFWMGxHWlZLTEsyYXkwREpBR29SVHFLWF9vUXd5VjBHUFF1clRGNkRWYkdrM3FaX1Y0amoyTGlrZWdPX1lUVlRuV0FCZ1pkem53TWxFY3h5U08wOHZuSlE1dFo4QXQwRFRhTEFYd3lJNG54UQ?oc=5&hl=en-US&gl=US&ceid=US:en,"If it seems like artificial intelligence dominates healthcare headlines, it's with good reason. AI in healthcare is on track for rapid growth; research firm MarketsandMarkets, for example, has predicted an extraordinary compound annual growth rate of 38.6% for the rest of the decade, driven by investments in deep learning, remote monitoring and chronic care management. Meanwhile, McKinsey & Company has pointed to additional AI trends in healthcare, from the rapid rise of generative AI to back-office use cases, such as claims processing and insurance verification. For executives such as Jordan Archer, chief operating officer of Charlotte, N.C.-area multispecialty group Tryon Medical Partners, AI systems are valuable because they facilitate the work that well-trained staff do every day. ""They can handle processes in a more efficient manner and deliver more for patients. As an organization, we can perform better in value-based care contracts,"" he said. Hospitals, health systems and large medical practices are exploring many ways to incorporate AI into both clinical and administrative workflows. Some initiatives are small in scale, using AI tools in their pilot phase or focusing on individual service lines or patient populations. Others span the enterprise, especially if AI functionality has been embedded in practice management or electronic health record (EHR) systems already in place. Here's a look at some of the AI trends expected to reshape healthcare in the years ahead. As with many other industries, healthcare has embraced AI-powered chatbots to complement customer service. Based on how a patient answers a chatbot's intake questions, the chatbot can determine whether an automated system can handle an inquiry -- such as scheduling an appointment, refilling a prescription or paying a bill -- or whether a patient needs to speak to a person based on their medical symptoms or the complexity of their needs. In addition, chatbots can augment patient engagement efforts by providing evidence-based recommendations or translating educational resources to a patient's native language. This article is part of Across the industry, survey data from Rock Health showed more than 30% of primary care physicians use AI for clerical support, such as documenting patient visits and drafting post-visit notes to patients. Close to 25% said they use AI for information management and clinical decision support. Critically, less than 10% of primary care physicians said they don't want to use AI at work. The Cleveland Clinic is one health system that has deployed AI in the exam room, with chatbots embedded in the EHR system to help retrieve information and ambient listening technology capturing notes and creating instructions for prescriptions or lab orders. Roughly 80% of healthcare data is unstructured, meaning it doesn't easily fit into database rows and columns. AI systems can parse unstructured data sources, such as medical images and lab reports, far more quickly than traditional analytics tools. Depending on the algorithm being used, models can provide insight into a potential diagnosis, identify high-risk patients or offer treatment recommendations. AI-powered data analysis can also help hospitals prevent adverse events, such as harmful drug interactions or sepsis infections. Imaging studies drive diagnostic and treatment decisions. However, analysis with the naked eye can be time-consuming and prone to error -- and backlogs are all too common. Image recognition tools can interpret studies such as X-rays, electrocardiograms and CAT scans to identify irregularities and suggest a potential diagnosis. AI systems can also clear studies with normal results so radiologists and other members of the care team can focus on studies that need careful review. Robot-assisted surgery debuted in the 1980s and, after initial trepidation among surgeons, the technology began to take off in the 2000s. The American College of Surgeons (ACS) has noted increased interest in robot-assisted surgery for procedures involving tight, cramped and sometimes inaccessible areas, particularly the prostate and urinary tract. The main benefit is a minimally invasive approach that contributes to shorter hospital stays and faster recovery times. Advances in automation should soon let robotic systems execute minor tasks such as sutures, ACS indicated. Practice guidelines in physical therapy tend to be based on small studies, as it can be difficult to enroll patients at a single site with the same diagnosis and treatment plan. AI can analyze real-world data from PT patients in many settings; the output of this analysis can provide personalized recommendations for individual patients and inform guidelines for specific populations in need of PT services. Meanwhile, incorporating virtual reality into PT treatment can provide immediate feedback to patients about whether a movement is being done properly and to clinicians about how a patient is responding. Whether created from queries to generative AI models or fitness apps gathering data from wearables, personalized exercise programs are poised to help address patients' specific wellness needs and fitness goals. A recent literature review noted individuals participating in various strength, stretching and resistance training programs showed measurable improvement compared to the control group. As with many AI use cases, though, the expert recommendation is to use AI as a supplemental tool for exercise programs but ""not as a substitute for personalized, progressive, and health condition-specific prescriptions provided by healthcare and fitness professionals."" Automating various stages of claims processing -- for instance, applying the right medical codes to a patient visit or filling in demographic information -- means ""claims can go out the door with no human touch on the back end,"" Archer said. For example, Tryon Medical Partners now files about 90% of prior authorization requests automatically, and denial rates are below 2%. Before automation, the process was highly manual, leading to long wait times on the phone -- not to mention mounting frustration from patients. ""This allows us to manage our labor in a more effective way,"" he said. Managing the inventory of vaccines, medications and other supplies with cold storage requirements or expiration dates can be a delicate balancing act. Keep too many items in stock and some can go bad, costing the facility money; keep too few and supplies will run out, negatively impacting patient outcomes. Predictive analytics supplements inventory management by tracking historical data and considering external factors to help anticipate demand and avoid waste. Leaders can also see which suppliers have been most reliable in meeting shipping deadlines as well as product safety requirements. In competitive markets, hospitals and health systems need all the help they can get to reach more patients in their catchment area or identify service lines ripe for expansion. Bringing AI into sales and marketing helps organizations better understand market dynamics, such as referral patterns and specific medical needs within the population. This information can help influence business decisions that drive revenue growth. As the industry looks to reap the benefits of these AI trends in healthcare, stakeholders have shared concerns that large hospitals and health systems are better poised than smaller practices to make the most meaningful strides. ""We know that we need to support both large institutions and smaller practices, making sure that there is equitable access to these tools,"" Dr. Margaret Lozovatsky, vice president of Digital Health Innovations at the AMA, said late last year in an interview with the association's chief experience officer, Todd Unger. The need for support is especially acute when it comes to using AI tools effectively, according to Dr. Brian Anderson, CEO at the Coalition for Health AI. Community-based, rural or federally qualified health centers often lack strong AI governance or the expertise to monitor how AI systems perform. ""How do they make an informed decision to buy models that perform well? How will they manage and monitor models? Can they tune models locally based on their community data?"" he questioned. ""Those are really important things that we will have to tackle collectively."" Brian Eastwood is a Boston-area freelance writer who has been covering healthcare IT for more than 15 years. He also has experience as a research analyst and content strategist. Arguing the pros and cons of AI in healthcare AI medical terminology: Terms to understand How AI is changing telemedicine AI healthcare companies to watch Top AI tools in healthcare A new study shows physician exits from Medicare fee-for-service accelerating since 2010, with primary care physicians leading the... Agentic AI is the next big thing in healthcare, carrying promises to tackle revenue cycle management's biggest challenges, ... CMS announced the CY 2026 OPPS and ASC Payment System Proposed Rule, which includes a 2.4% rate outpatient rate hike, ... Organizations should consider multiple touchpoints for obtaining patient consent for using ambient listening in clinical ... Cancer screening rates are low, but survey data shows more convenient and innovative testing methods could increase patient ... Although the EHR is widely considered the most impactful health IT on patient outcomes, telehealth is emerging as a key tool to ... The health AI company plans to use the new funds to continue developing its CARE foundation model and aiOS clinical AI operating ... With little federal guidance on health AI, industry groups have taken it upon themselves to offer guidance and guardrails to ... The partnership will draw on EHR data to gain insights into the progression and treatment of COPD and asthma. Healthcare is making strides in governance and response planning, but the sector has room to grow when it comes to risk ... The Microsoft SharePoint vulnerability only impacts on-premises SharePoint Server customers, who should apply the emergency patch... Millions of individuals were impacted by the most recent healthcare data breaches reported to HHS. A federal report details the latest telehealth use trends within the Medicare program, showing high satisfaction with the ... The report reveals pharma-telehealth partnerships lead to high prescription rates of sponsor medications, raising concerns about ... Stanford Health Care is collaborating with a virtual-first provider for pulmonary rehabilitation to expand access to chronic care...  2025 TechTarget, Inc. d/b/a Informa TechTarget. All Rights Reserved. Privacy Policy Cookie Preferences Cookie Preferences Do Not Sell or Share My Personal Information",2
4 top use cases for agentic AI in healthcare - TechTarget,https://news.google.com/rss/articles/CBMioAFBVV95cUxPOGJZOHJaR3Zvazdic25tVURnNVhjNlNrSmNReFk0bVNVNWhkOHdkeTFoWEVBYmcwY3V2djgxT0J2RUFsZE5VQVVBdERCbVpocHZWZXBiV203eHVlWXpuSnVWeEFDTWRWbUVsZDFXUElYM3E5X1pJQ0FTR1l4TXk0S3ltU2UtZlBzNXRBUGUzZzBkZlMyVkpsTFdMZEhOVnJI?oc=5&hl=en-US&gl=US&ceid=US:en,"AI agents represent perhaps ""the biggest untapped opportunity"" in healthcare because they work autonomously around... AI agents represent perhaps ""the biggest untapped opportunity"" in healthcare because they work autonomously around the clock, according to Wes Little, executive vice president of analytics and AI at WellSky, a company that offers software, analytics and services to deliver intelligent coordinated care, particularly post-discharge or post-acute care recovery. Agentic AI consists of AI systems that perform autonomous actions and aid decisions. They also help clinicians when there are staffing shortages. Fertility, for example, is an area that is using AI while it deals with a clinician shortage and increased demand for services, according to Irene Alvarado, cofounder and CEO of Berry Fertility, a fertility management app for IVF, embryo transfer and egg freezing. Use cases for agentic AI in healthcare include scheduling, authorizations and patient engagement. Agents also access application program interfaces to perform tasks such as querying an EHR, Alvarado explains. Today's AI agents continue to advance and are now able to read full web pages and understand the information. ""I think we're headed toward an era when the majority of the traffic on the internet will not be humans,"" Little says. ""It'll be agents going out and finding information, collecting it, and then inserting information into other systems."" This article is part of Here's a look at how agentic AI can help with some key functions in healthcare. Healthcare organizations use voice agents to help with administrative tasks such as scheduling patient appointments and sending reminders via text. WellSky's voice agent, for example, is integrated into the company's scheduling application and allows providers to call their patients the day before an appointment, which they didn't have the resources to do before AI agents, Little says. ""There's nothing more costly to a provider, both in terms of actual cost and opportunity cost as when that nurse shows up at the house at 9 a.m., and the patient has forgotten they were going to be there,"" he says. ""It's a missed interaction opportunity, and you have to go about rescheduling. That takes a long time."" WellSky's AI agents use generative AI large language models similar to that of ChatGPT and interact with an application from telephony provider Twilio to make a voice agent's functionality possible. In addition, its agents have improved in just the last few months as far as latency and responding to patients effectively, according to Little. Now AI agents are more advanced because of the ability to have a conversation with the patient, Little explains. AI agents make natural conversations possible compared with traditional automated calls in which a caller simply presses 1 to confirm an appointment or 2 to decline, according to Little. ""Then you have a whole other set of interactions with a [person] who follows up with you,"" Little says. ""Now you can have that whole interaction the first time that patient answers the phone."" Without APIs, AI agents can log in to an EHR to access a calendar to help clinicians fill an open slot, Alvarado says. An AI agent can follow up with patients, send a confirmation email and access SMS reminders, she said. Other companies that offer voice-activated AI agents to help with appointment scheduling include Amelia and Innovaccer. Interoperability and APIs are also making agents effective in areas such as authorization, Little says. Voice agents call up payers to seek authorization for a treatment that will be delivered. ""Preauthorizing that care is hugely valuable for providers who otherwise would have to have a big staff calling those payers and following up to ensure the patients get the care that they need,"" Little says. ""Now a lot of that can be done autonomously via agents."" Agents can interact with payer portals and perform ""web crawling"" to collect the data needed from providers to authorize payment. AI voice agents can save clinicians time during insurance preauthorization through robotic process automation, Alvarado says. Because not all insurance covers fertility, for instance, finance coordinators speak with patients to go over the insurance terms, such as medical necessity, Alvarado said. AI agents are just ""barely starting"" to perform the calls to get a preauthorization from an insurance company. ""In some cases, they're not replacing humans, but they're validating what the humans are researching to save time,"" Alvarado said. When patients begin researching fertility treatments, they may not get started for 4 to 8 months, according to Alvarado. ""Anything that could reduce that time of finding out your insurance benefits is a huge part of what is useful, both for the patients to get treated faster as well as for the clinics to sort of [fill] that slot faster."" Voice agents can help physicians and patients research which medications are covered. AI tools aid nurses by writing drafts of notes or providing summaries of patient encounters. Oracle Health's Clinical AI Agent, for example, uses voice recognition technology to record physician-patient interactions and adds draft notes to the Oracle Health EHR, formerly Cerner. Organizations such as Beacon Health System in South Bend, Indiana, uses ambient AI as part of Oracle Health's Clinical AI Agent to fight burnout and help with administrative tasks. ""It improves the trust between the patient and the physician because our notes are more comprehensive related to what the conversation entailed,"" said Dr. Scott Eshowsky, chief medical information officer at Beacon Health System, in an Oracle case study. When patients download a patient engagement app, AI agents can check in with patients on how they're feeling and capture valuable information that allows providers to offer guidance on potential risks or issues. As agents speak to patients every day, they collect data points that can be fed back into a predictive model, according to Little. ""You're increasing the amount of data that you capture, which is inevitably going to make the analytic engines much more performant at identifying the risks that you want to avoid,"" Little says. If a patient is having difficulty with a hip and knee replacement after discharge, AI agents can collect this information to avoid the chance of readmissions, according to Little. ""People can slip through the cracks once they've left the hospital doors, and this gives you the ability to stay more connected with those patients than you've ever been able to before,"" he says. AI agents can reach out to patients to make sure they understand their upcoming procedures and follow up after, Alvarado says. If an agent gets in touch with a patient, there's often a ""human in the loop"" in which humans must review the action before it occurs, according to Alvarado. As AI agents interact with patients more, they can learn to answer patients' questions more effectively, Alvarado said. ""You can have agents that are self-learning that will take feedback and adjust themselves next time,"" she said. Brian T. Horowitz started covering health IT news in 2010 and the tech beat overall in 1996. Exploring 3 types of healthcare natural language processing Challenges of AI integration in healthcare and their remedies Top ways artificial intelligence will impact healthcare How AI is changing telemedicine The cost of AI in healthcare explained: Is it worth it? Home health agencies report being held back in interoperability, building workarounds to keep up while they evaluate solutions. Providers report significant gains in EHR usability, reduced documentation burden and physician burnout with ambient speech ... While the rate of health information exchange via standardized, digital means increases, the rate of fax-based exchange only dips... Healthcare is making strides in governance and response planning, but the sector has room to grow when it comes to risk ... The Microsoft SharePoint vulnerability only impacts on-premises SharePoint Server customers, who should apply the emergency patch... Millions of individuals were impacted by the most recent healthcare data breaches reported to HHS. DOJ has launched an investigation into UnitedHealth Group after several reports that the payer engaged in inappropriate billing ... Humana will eliminate a third of prior authorizations for outpatient services by 2026 and publicly report its prior authorization... Two reports detail healthcare premium hikes for 2026, which can impact consumers through higher costs and limited employer ... Regulatory loopholes let compounders mass-produce untested GLP-1 drug knock-offs under the guise of personalization, prompting ... HHS Secretary Robert F. Kennedy Jr. backs the removal of thimerosal from U.S. flu vaccines following a CDC advisory committee ... The FDA appointed Stanford physician-scientist and ex-biopharma executive George Tidmarsh as director of its largest division, ...  2025 TechTarget, Inc. d/b/a Informa TechTarget. All Rights Reserved. Privacy Policy Cookie Preferences Cookie Preferences Do Not Sell or Share My Personal Information",2
The future of care - PwC,https://news.google.com/rss/articles/CBMixAFBVV95cUxNcjlLb2Qzb3pGUEVucWN0M0dnMG9DNk1vRTBFSHRWeWNNc1J3X0JzWm5zMmZmZXVhbkpGU0ZLNlRuaFRtc0QyZV9GRGVORnd1ZzNwM040UEJYRzBpUHVGTE5MM2VvYmQ3bUUtM3A1NUFYM1M4eThWdTdKbGdUTkZlNjJYNHpuNGZ4ZG14eExqMGx6TUtrbVk4azNjNnJQaG1qeTJ4bEVLZ0VKR2hGcGdUU3l6bUhoNWxBZ245cE1jdEptYWNQ?oc=5&hl=en-US&gl=US&ceid=US:en,"More Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Featured Value in motion Industry edge Engine by Starling: From launching a bank to launching a software business Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Featured Climate risk, resilience and adaptation Business transformation Sustainability assurance Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Featured Value in motion PwC s 28th Annual Global CEO Survey The Fearless Future: 2025 Global AI Jobs Barometer Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Menu Featured Value in motion Committing to Net Zero Global Transparency Report 2024 Menu Menu Menu Featured PwC s Global Annual Review Committing to net zero The CEO s ESG dilemma Loading Results No Match Found The healthcare industry is evolving into a dynamic ecosystem. Patients, practitioners and industry players all stand to benefit. Healthcare faces multiple challenges that will only intensify in the years ahead. The global population is ageing and growing; by 2035, it could rise to nearly 9 billion people. The incidences of chronic diseases such as diabetes, cancer and dementia are also growing, with global treatment costs for these conditions slated to reach US$47 trillion by 2030 (PDF). Backlogs in state-funded health services are delaying routine treatments, and given the global shortage of healthcare workers, it is unlikely that this problem will resolve itself soon. Though newer medications, such as GLP-1 drugs, offer hope in treating a range of medical conditions, the cost is currently too high for the general population; a course of such obesity drugs in the US would set the average patient back US$500 a month. And inequalities in income and education further aggravate health inequalities. All the while, consumers and patients who have come to expect convenience and high levels of service in their retail experiences are now raising their expectations for their own healthcare. But today s challenges also represent tomorrow s opportunities. Imagine a future in which personalised treatments are available to all; where drug discovery, enabled by AI, is faster, more effective and less costly than it is today; where technology platforms and health services collaborate to make care and procedures more digitally enabled, and hence more accessible for all; where smart personal consumer electronic devices are able to detect the early warning signs of an expanding range of conditions; where wearable technology and innovations in medical devices allow for real-time AI-powered diagnosis. Along with every other aspect of the industrial ecosystem, healthcare is being reconfigured under the pressure of key megatrends: climate change, technological disruption, demographic shifts, a fracturing world and social instability. These megatrends are long term in nature, but their impact is being felt today. That means business leaders must tackle a combination of ever-changing short-term crises and, at the same time, consider how their organisations can adapt to the impact of the megatrends in order to evolve and thrive. The shifts brought about by the megatrends are manifesting themselves in six basic areas of human activity: how we build, make, move and power, and how we feed and care for ourselves. In healthcare, as in each domain, a new ecosystem is emerging. Tomorrow s care ecosystem will look much different from today s healthcare industry, as it evolves to bring together traditional and non-traditional players cooperating and innovating as they envisage, enable and deliver care interventions. A massive shift in value or as we term it, value in motion will be created in this care ecosystem. Entirely new value pools will emerge, with the patient at the centre. Others that are small today could grow large, while some will disappear altogether. Incumbents and start-ups, large and small organisations, public and private companies, and regulators will join the ecosystem from a wide array of legacy sectors spanning both the  real  and the financial economy. The health ecosystem will reconfigure into a larger and more diverse network, capable of meeting rising demand and coping with increased cost pressure and rising consumer expectations. Although industry players have already started to work in this direction, there is much more to be done. Leaders can confront the challenges they face and take advantage of the many opportunities that present themselves by transitioning from keeping a narrow focus on their direct legacy value chain to embracing the broader ecosystem. As PwC imagines new models of care, our work is informed by a core set of principles. Equitable. We must implement care fairly and ethically, and maximise health impact by directing resources to where they re most urgently needed or to where the impact is largest and/or most significant. Innovative. We must develop an environment that incentivises innovation by fostering collaboration among all care ecosystem players. High-quality and safe. We must prioritise providing high-quality care across boundaries in every clinical interaction, ensuring that all products and services are safe to deliver, patient-centred and supported by a well-trained workforce that can meet evolving needs. Efficient and effective. We must control spiralling costs and invest where it matters most, while ensuring that there are no negative impacts to the level of care being provided. Sustainable. We must reduce the care system s carbon footprint; the sector currently accounts for 5% of global emissions. Resilient. We must ensure that the ecosystem can withstand sudden shifts and shocks such as pandemics, supply chain disruptions and geopolitical factors and that it can flex to accommodate innovation. In our view, the global care ecosystem will be reconfigured around four key attributes over the coming decade. It will be more preventative, addressing disease risk factors upstream to avoid deteriorating conditions and to keep people well longer. It will be more personalised, as care shifts from today s one-size-fits-all approach to one tailored to individuals  unique needs. Healthcare will be more predictive and proactive, using advanced technologies to switch from being largely reactive to intervening before issues become critical. And it will be more focused on the point of care, changing where, how and by whom care is delivered with areas such as telehealth, remote surgeries and physical community-based facilities set to experience strong growth. Focusing on disease risk factors and promoting self-directed healthy living and wellness can reduce the need for care. When leaders prioritise prevention, the overall health of the population can be improved, reducing the strain on healthcare systems. In the UK, the volunteer-led Our Future Health programme uses genetic information to develop treatments focused on disease prevention. It s funded through a combination of government grants and investments from life sciences companies and health research charities, and in addition to offering financial support, all stakeholders providing financial backing are also bringing their skills and expertise to help in the discovery of more effective disease prevention and treatment. In Malaysia, the NGO Better Health Programme has developed a digital health app, MyBHP (PDF) (file size: 1.1 MB), aimed at improving the capacity to prevent non-communicable diseases through a healthier diet and weight loss. The app has enlisted local food vendors and food outlets as part of an obesity reduction programme, and serves as a platform for marketing healthier food products. Personalised care can bring considerable improvement to health by enabling more balanced and careful evaluations of potential treatment options. By considering the patient s genetic make-up and contributing factors, such as lifestyle and environment, tailored treatments can address each patient s individual needs and characteristics. Such an approach will allow for more precise diagnoses, the identification of abnormalities for which treatment decisions may be less obvious, effective interventions and a potential reduction in expenditures. In Dubai, meta[bolic], a therapeutics company, integrates data into its GluCare health platform gathered from the Oura wearable ring, which tracks numerous markers, including heart health and metabolic activity. Constant monitoring is being used to prevent complications and identify the early stages of conditions before they worsen. Healthcare organisations need to harness data and technology to take a more predictive and proactive approach to care. Doing so may enable them to treat patients at an earlier stage, before diseases and other health conditions can develop. This can help improve patient health outcomes, reduce the care burden and increase quality. There are numerous ways in which proactive care is being supported. For example, data analytics and testing can identify adverse health conditions earlier, which allows for more timely interventions. Zoe, a health programme that runs in the UK and the US, provides consumers with home testing kits that include a blood glucose monitor to check which foods cause blood sugar spikes. This helps Zoe customers make healthier food choices that better support gut microbiome health. Individuals will be able to manage their health with subscription-based health management models, in which healthcare providers monitor and maintain an individual s health. In the US, 1upHealth s cloud platform is helping improve patient outcomes by providing efficient access powered by health data to deliver more proactive and effective care. It does this by facilitating data sharing among providers, payors, health tech companies and life science firms, along with claims data. Shifting care locations to settings that are more accessible and convenient for patients changes the dynamics of care delivery. This approach includes the use of telehealth, community-based services and remote surgery, which could provide significant cost savings, in addition to making care more patient-centric and efficient. Kaiser Permanente aims to bring convenience to consumers by providing healthcare services at Target Clinics in California, where customers are able to fill prescriptions at the co-located CVS pharmacies within the Target Clinics. The company s experience in providing innovative care solutions has also led it to set up a venture fund for other potential providers to drive even more innovation. In a patient-centric model, organisations leverage critical enablers to deliver care in new ways. A number of critical enablers are needed to overcome the many blockers and drive the reconfiguration of the care ecosystem. It is important for organisations to master their ability to push these levers to accelerate progress. Delivering care in the future will demand a different approach to attracting and retaining healthcare workers. Employers should focus on developing new employment models that incentivise and motivate the workforce. Upskilling staff is equally motivational. It gives health professionals the capacity to integrate new technical skills and widen their skill sets. This is essential for a future-ready healthcare system. Technology plays a significant part in improving health outcomes. Data gathered from wearable devices and monitors is enabling more effective disease prevention and management. Blood glucose monitors, once a specialised solution, are now a mass-market offering. Advances in technology empower patients by giving them wider information access and tools for self-directed health management. Tech is also facilitating remote consultations and even remote surgery. AI applications have use along the value chain to augment care teams. In the front office, AI can be used in clinical ways to help identify at-risk patients for a care team so they can be monitored more closely. IT can be applied to improve data quality via coding, to enable more accurate diagnoses of conditions such as delirium. AI scribes, which automate the transcription of physicians  notes, can alleviate the back-office burden on care teams. Data-sharing is fundamental to generating better health insights. Because of the sensitivity of patient data, sharing must be facilitated through the institution of robust data regulations and safety measures to guarantee privacy and security. Moreover, individuals and companies need to be encouraged and incentivised to share health data. Healthcare players must also harness data for better predictions and planning, and use it as an engine to drive new innovations. Regulation needs to evolve from covering individual countries to being more global in order to account for the potential provision of more cross-border care services. The transition to new healthcare models that focus on early intervention, personalised treatments, integrated care and community-based models will put a renewed focus on funding, which will force tough decisions on how funds are allocated. Pharmaceutical and life sciences companies will need incentives to adapt to the new care models so that their business models remain viable, whether those incentives take the form of value-based pricing or grants and subsidies for preventative drugs. All stakeholders in the care ecosystem need to recognise how a reconfiguration will benefit all healthcare players in the long term. Empowering consumers is central to the future of healthcare. This means providing individuals with the information, tools, agency and education they need to make informed decisions about their care. Patient education also plays a role by emphasising the need for preventative and proactive care and in fostering healthier behaviours. Facilitating the proper use of health data and technology to support self-monitoring can further reduce the care burden through strategies such as virtual ward scenarios, which allow patients to receive hospital-level care at home. Wearable technology further empowers individuals through access to and ownership of personal health data. To take part in and take advantage of the evolving healthcare landscape, it s important to get to work now. All players in the future of healthcare should develop a clear understanding of what the ecosystem will look like. This includes mapping out how and where value will be created or destroyed. Players should then decide how they want to contribute to each ecosystem based on their existing and future capabilities. Business model reinvention will be necessary to shape and reposition the organisation for success within the reconfigured ecosystems that healthcare players have mapped out. Finally, players should actively work to bring other participants into each ecosystem they inhabit by developing new or evolved third-party relationships and collaborative models to help them achieve success. In a series of forthcoming articles, we ll be taking a close look at how to prepare for the coming transformation in healthcare. We ll identify what actions will need to be taken in the workforce, and the impacts of technology and AI, data and insights, regulation and planning, consumer empowerment, and funding and incentives. Global Health Industries Leader, PwC United Kingdom Global Health Industries, Partner, PwC United Kingdom Professional guidance and multiple-industry experience for policymakers, healthcare providers, payors and health-sciences organisations. Solutions for the complex challenges affecting business, delivered by PwC s global network of pharmaceutical, biotech and medtech experts. Emerging healthcare companies can transform value-based care by leveraging tech, data analytics and patient engagement to improve outcomes and reduce costs. A comprehensive introductory guide to Taiwan s healthcare system and its biopharma and medical device sectors In light of today's public healthcare challenges, Malaysia needs innovative solutions for the future. Private healthcare providers can be part of the solution through public-private partnerships. Our report aims to introduce compelling partnership models that could drive Malaysia s healthcare transformation.   2017 - 2025 PwC. All rights reserved. PwC refers to the PwC network and/or one or more of its member firms, each of which is a separate legal entity. Please see www.pwc.com/structure for further details. These cookies are necessary for the website to operate. Our website cannot function without these cookies and they can only be disabled by changing your browser preferences. These cookies allow us to measure and report on website activity by tracking page visits, visitor locations and how visitors move around the site. The information collected does not directly identify visitors. We drop these cookies to help us analyse the data. These cookies help us to understand how effective our marketing campaigns are, and enhance your online experiences with PwC with customisation. Marketing cookies help us provide you with personalised and relevant services or advertising, and track the effectiveness of our digital marketing activities. Social media cookies are set by a range of social media services that we have added to the site to enable you to share our content with your friends and networks. They are capable of tracking your browser across other sites and building up a profile of your interests. This may impact the content and messages you see on other websites you visit. If you do not allow these cookies you may not be able to use or see these sharing tools.",0
Advancing healthcare and scientific discovery with AI - The Keyword,https://news.google.com/rss/articles/CBMid0FVX3lxTE8zQXVMOHlQby10ajBkU3g5NkVCVzVmZmg1RGVaNVBYaDdoSWR6Y2Z3b1kzbUtjdlk3b3hPcEhJTzY0VzlIQm4xX210OGg3emFOajlVTFk3MWF0eGZJYm9yM0trTTFIUFRXY2hXbE1JOVpmRE0tRkNv?oc=5&hl=en-US&gl=US&ceid=US:en,"Mar 04, 2025 4 min read Google is using AI to improve healthcare and scientific discovery. They're developing AI models to make health information more accessible, personalize care, and improve health outcomes globally. Google is also working on an AI co-scientist to help scientists uncover new knowledge and accelerate scientific discovery. Google is using AI to make healthcare better. They're making health information easier to find and understand, and they're developing AI tools to help doctors diagnose patients. They're also working on AI that can help scientists discover new medicines and treatments. Google wants to make sure everyone has access to good healthcare, no matter where they live. Learn how AI is making accurate health information more accessible and personalized and how AI can accelerate scientific discoveries with AI co-scientist. Last week, at the Lake Nona Impact Forum for advancing global health, I discussed the potential of AI to meaningfully improve healthcare and advance science. Our recent AI breakthroughs provide unprecedented opportunities to make healthcare more accessible, personalized and effective for everyone, and to significantly accelerate scientific discovery. Here s an update on our progress, how we re collaborating with partners to bring AI to global healthcare settings and our recently announced AI co-scientist. Google is often the first place people turn to when they are looking for answers to health-related questions, so we strive to make sure everyone has access to relevant, high-quality health information in their moment of need. Using Lens, people can take a picture to search skin conditions that are visually similar to what they see on their own skin, and on YouTube, we piloted AI tools with health creators and organizations like Cleveland Clinic to make it easier for them to publish authoritative, high-quality content. For care providers, we launched MedLM and Search for Healthcare which can provide answers to medical questions. These are available on the Google Cloud Vertex AI platform, helping clinicians make more informed decisions and helping patients receive the precise care they need. Our research into medical factuality ensures that health-related content generated by language models is as reliable and grounded in factual sources as possible. With advancements in multimodality and conversational AI, we re able to reimagine patient care and how it could be personalized for everyone, with an emphasis on preventive healthcare. From X-rays to digital health records, medicine is multimodal. Building on our MedLM research, we developed Med-Gemini, next-generation models for healthcare which have Gemini s superior multimodal and reasoning capabilities and are fine-tuned on de-identified medical data. In published research, Med-Gemini achieved 91.1% accuracy on U.S. medical exam-style questions, and we demonstrated how the models can effectively interpret 3D scans or answer complex clinical questions. We re researching how AI systems could serve as conversational diagnostic partners in clinical settings, using Articulate Medical Intelligence Explorer (AMIE), a research AI system optimized for diagnostic reasoning and conversations. It is designed to take a  clinical history  and ask intelligent questions to help derive a differential diagnosis and handle discussions with empathy, including in subspecialist domains. Mobile and wearable devices are another promising area where generative AI models could provide personalized insights for both healthcare and wellness, using data such as step count and heart rate. We designed the Personal Health Large Language Model, another fine-tuned version of Gemini, which can interpret sensor data and generate insights and recommendations about an individual s sleep and fitness patterns. Early diagnosis of disease is critical for improving health outcomes. Over the past decade, we ve tapped into AI s imaging and diagnostic capabilities and developed AI models to help detect diseases including breast cancer, lung cancer and diabetic retinopathy. Through partnerships, we re now bringing these solutions at scale to clinical settings so that more patients can benefit from timely and accurate screenings. The impact is particularly profound in low-resource medical settings and countries with fewer specialist doctors per capita. Over the next decade, our health-tech partners in India and Thailand aim to deliver 6 million diabetic retinopathy screenings at no cost to patients, and Apollo Radiology International will build on our AI models to provide 3 million free screenings across India for tuberculosis, lung cancer and breast cancer. Adding to our various initiatives to address maternal health in Africa, we re also developing an ML model for cardiotocography, used to predict fetal well-being, and exploring its utility in medical settings with limited resources. We re also laying the technological foundations for broader access to healthcare. Our Health AI Developer Foundations include open-weight models and resources to help developers build AI models for healthcare more efficiently. And solutions powered by our Open Health Stack (OHS)   a suite of open-source tools that makes it easier for developers to create next-gen digital health solutions for healthcare workers   have already been deployed across regions in Africa, South Asia and Southeast Asia to support frontline healthcare workers serving millions of patients. Medicine is rooted in science. Building on AI s ability to synthesize information and perform complex reasoning tasks, we re exploring how it could augment scientific and biomedical discovery through our work on AI co-scientist, a multi-agent AI system based on Gemini 2.0. AI co-scientist is designed to function as a collaborative tool for scientists. It s intended to uncover new, original knowledge and help scientists formulate novel research hypotheses and proposals, building upon prior evidence and tailored to specific research objectives. It has already demonstrated potential in areas such as drug repurposing for acute myeloid leukemia, proposing hypotheses for novel treatment targets for liver fibrosis and explaining mechanisms of horizontal gene transfer underlying antimicrobial resistance   each of which is a complex application and presents a different set of challenges. We continue to realize the incredible potential of AI to advance science and improve, personalize and democratize access to healthcare. The  magic cycle  where we achieve research breakthroughs and translate them to real world impact is accelerating and expanding in scope. We ll pursue this opportunity responsibly, in collaboration with global partners, and continue to share our research   in 2024, we published more than 50 papers in which we shared cutting-edge health research, and we recently shared our 2025 Health Impact Report. Ultimately, we believe AI will continue to help advance healthcare and science for the benefit of billions of people. Today, in partnership with the Earth Fire Alliance and Muon Space, we shared the first images of wildfires detected by FireSat, a revolutionary new satellite constellati  Let s stay in touch. Get the latest news from Google in your inbox. Share your thoughts - take a quick survey. Thank you! Follow Us",2
How AI is helping healthcare startups multiply their patients and chase profits - Business Insider,https://news.google.com/rss/articles/CBMioAFBVV95cUxNRzRCb2xGanprNWRkY3NsbU8tbXQ2d0hUaHNVZTZVQVFtQTZJdGd6VjhQcHpNWUtHbS14bU1ubzJPVEpBN2tFMkhkSjBsRHB3NjVBRnFrb3FTUi1WN052TzVrT2ZqeURWWHpLUG9CNV85bWh4c2RpMzlwc3Z2V3BzZmVORWNLUl9XdDFnTEU4V29kNXNiaDNGSFJEZ0FkTkxk?oc=5&hl=en-US&gl=US&ceid=US:en,"A growing number of healthcare startups are betting that artificial intelligence can help them do more with less   including, in some cases, to care for more patients. The stakes are high: clinician shortages, tight margins, and mounting physician burnout are putting pressure on healthcare delivery businesses. Startups say AI could be the breakthrough that helps clinicians multiply their caseloads and earn extra cash without sacrificing care quality. ""The long-term vision is to continue to peel away aspects of in-person care and deliver the care itself via technology,"" said CEO Daniel Perez, the cofounder and CEO of newly public physical therapy company Hinge Health, in a May interview with BI. But while some companies are leaning all the way in to automate more and more care, others are keeping the tech at arm's length, wary of the safety risks or the patient experience of turning clinician tasks over to AI. Instead, they're pushing AI behind the scenes to automate non-clinical tasks only. For some, that approach is offering a new lease on the startup's life. ""It's allowing companies that historically would not have gotten to profitability, that may have had to go out of business, to sustain and to be able to keep driving impact long term,"" said NOCD cofounder and CEO Stephen Smith. Some of healthcare's biggest AI moves are happening in musculoskeletal care, where companies like Hinge Health and Sword Health are aggressively using the tech to help make their physical therapists more efficient. Sword Health spent 2024 ramping up its AI applications for clinicians. While its physical therapists were expected to manage between 200 to 300 patients at a given time at the beginning of the year, the company wanted those providers to manage 700 patients at a time on average by the end of 2024, BI reported in November. Sword declined to comment for this story. Sword's efforts toward that goal included using AI to pre-write messages for clinicians to send along to patients, per BI's reporting. The company also said at the time that its AI helped physical therapists prioritize patients who might need more attention. Hinge Health has taken a similar approach in implementing AI, including for patient-provider messaging. While Hinge Health declined to share its providers' caseloads for this story, the company says it's used AI to slash 95% of clinician hours spent on physical therapy. Hinge Health also uses computer vision technology to guide patients through PT sessions at home. Following its May IPO, the company says it's actively working on more applications of AI to further automate care delivery, and has a large research and development team focused on that goal. ""At some point, whether 10, 50, or 200 years in the future, care delivery will be automated with technology. And that's a good thing,"" Hinge Health CEO Daniel Perez told BI in May. Virtual pediatric care startup Summer Health is also working to use AI to multiply the number of patients its providers can care for. While founder and CEO Ellen DaSilva doesn't think AI will replace doctors, she said the startup is working on automating some clinical tasks as well as administrative ones. Whereas a healthcare business with minimal technology implemented might expect providers to see five to seven patients an hour, Summer Health's largely text-messaging-based care model initially allowed its providers to see about 10 patients in an hour, DaSilva said. And with AI, ""that number could pretty easily be doubled,"" she said. ""Our providers still have a lot of room to run."" That could be especially good news for specialties like pediatric care where there's a shortage of clinicians. DaSilva said AI can help augment the remaining clinicians to help more patients get care, while hopefully reducing provider burnout along the way. As some healthcare startups go all in on AI, other companies aren't quite as sold on its potential for improving care. Chronic care company Omada Health, which went public in May, has long used AI behind the scenes, like to suggest educational videos in Omada's content library for clinicians to send to their patients. But it's been slower to bring AI in front of patients, waiting until the company could point to significant LLM improvements and train the models on its own clinical data. The company released its first patient-facing AI tool earlier in May, an AI agent that answers members' nutrition questions. But Omada Health cofounder and CEO Sean Duffy said the company has no intention of meaningfully replacing patient-provider interactions across its business with AI. ""I've yet to find someone who feels accountable to Chat GPT,"" Duffy told BI in May at Omada's IPO. ""I always quip that there's a reason that artificial intelligence is a buzzword and artificial empathy is not. You can expect us to always have a proactive people component."" Omada's diabetes care peer, Virta Health, has been using machine learning since 2017, per founder and CEO Sami Inkinen. Between 2015 and 2020, as Virta was figuring out how to use technology like machine learning to deliver better outcomes, it was turning a negative gross margin. But generative AI has helped the startup get that number in the green; today Virta boasts a roughly 60% gross margin, according to Inkinen. Virta has injected AI into many parts of its business, including into patient interactions, with AI-powered personalized care plans and chatbots available to help patients answer nutrition questions 24/7. But Inkinen said AI isn't making clinical decisions and won't replace providers in Virta's business. He added that Virta doesn't have caseload targets for its providers, and said the company is rigidly focused on delivering better patient outcomes and improving its financials along the way. Mental health startup NOCD is using AI to tackle a broad range of administrative tasks, including clinical note dictation and revenue cycle management. CEO Stephen Smith said the tech has helped the company to reach profitability, allowing NOCD to grow its therapist network while keeping operating expenses low. The startup does have a chatbot called Robin that therapists can ask for information during sessions. But that AI doesn't interact directly with patients, nor does it intervene unless the clinician seeks out its help. Smith said he's treading carefully given the company's focus on patients with obsessive-compulsive disorder, which is typically classified as a serious mental illness. While NOCD may be able to increase its providers' caseloads with AI eventually, it's not a short-term goal for the company, he said. ""All these different projects, we know we have to proceed with some caution since we're dealing with a severe group of people,"" Smith said. ""Bringing AI to the clinical delivery of care is going to require a lot of research, largely because we want to make sure it's super, super effective, and that we can create a safe experience for our members and our therapists."" Jump to",2
Building Trust in Healthcare AI: Leaders Weigh in - Philips,https://news.google.com/rss/articles/CBMizAFBVV95cUxQeXFoa0hKVXZQQjluLVhNZ0FSdzZFUTJiRXo5TFY3S1dRekJnbFd5dGNLOVQ3Smo0eUF6WS16MTZFa1RhZ2k0Nmt4OXltbk5ZSGR6QVJkWklINm95UEpBMmZ5dmlkZ2sxcmMwY1BwM1RINTJxdXBLa3lNQ2g5SjM3NmlRdmJyN0ZfMEZwaU5odVBKTUZhRlA3WU8tWFo3Y0k5Y00ySnBURGc0amNfX3pHNVdhM0QtbDV6UzB5ZHBVYjVqcF9VR3JlWHkzc2g?oc=5&hl=en-US&gl=US&ceid=US:en,"Search terms Jun 30, 2025 | 3 minute read At a recent Fortune-hosted dinner in New York, healthcare leaders gathered to discuss insights from Philips' 2025 Future Health Index (FHI) report and explore how artificial intelligence (AI) is transforming healthcare delivery. Fortune s Jason Del Rey moderated the timely conversation with frontline leaders Dr. Shez Partovi (Philips), Dr. Jill Kalman (Northwell Health), and Dr. David Reich (Mount Sinai Hospital). The message was clear: AI earns its place when it gives time back to clinicians and builds confidence with patients. Here are five takeaways from the discussion. The FHI reveals a striking optimism gap: 63% of U.S. clinicians believe AI can improve patient outcomes, while only 48% of patients agree. Partovi emphasized the importance of clinician involvement:  Patients said that their trust goes up if the AI is actually combined with input from physicians and nurses.  The data backs that up 79% of U.S. patients say a physician s explanation increases their comfort with AI. Trust isn t just about what AI can do it s about how it s implemented. When clinicians guide the conversation, patients are reassured and feel supported. According to FHI, more than 75% of U.S. clinicians say they lose time each shift to incomplete or inaccessible data; one-third lose over 45 minutes. Kalman spotlighted the need to eliminate unnecessary tasks through automation, describing prior authorization processes as prime examples:  I love cases like this because it takes incredibly mundane tasks that no one has to do...and there s no risk to the patient.  AI technology should ease time pressure not add complexity. When designed effectively, clinicians see the potential for AI to reduce burnout, and enhance the overall patient experience. Reich highlighted the power of AI tools that handle routine tasks, allowing healthcare providers to focus on more critical interactions. Mount Sinai s AI-powered alert system for aortic dissection exemplifies this approach rapidly identifying life-threatening conditions so surgical teams can act swiftly. Mount Sinai also successfully implemented NutriScan AI, significantly improving the accuracy of malnutrition diagnoses and enhancing dietitians  job satisfaction. When AI handles routine tasks, clinicians can spend more time on meaningful patient care. Implementing AI requires a clear demonstration of value, a point Reich made explicitly: It s very challenging at first to implement something unless I can both demonstrate strong ROI, which isn t always financial but mostly, and scalability.  Partovi illustrated this principle with Philips  Compact Ultrasound 5500CV, which uses AI to halve cardiac scan times enhancing patient throughput, reducing clinician workload, and improving operational efficiency. To succeed in real-world care, AI must deliver clinical, workflow, and financial impact. The importance of collaboration between health systems and tech partners was a key panel theme. Kalman described the shift from traditional vendor models to genuine partnership models in healthcare technology:  Typically in the past...vendors came,  this is our product, and this is what we re gonna do.  It s much more partnerships now. What do you need? How do we develop it together? Let s develop it together, pilot together, scale it together.  Partovi reinforced this point, noting that the richest data sits within provider organizations, making open ecosystems and collaboration essential to successful AI deployments. The FHI report confirms that progress depends on partnership. When solutions are shaped by the people who use them, they re more likely to succeed. AI s promise in healthcare lies in giving clinicians back what they need most: time. When designed with providers and introduced transparently, it becomes more than a tool it becomes a way to deliver better care for more people. Watch the full panel conversation to hear directly from the leaders shaping the future of care. Press release | March 12, 2025 Philips highest ranked medical technology company among Clarivate Top 100 Global Innovators Press release | January 28, 2025 Philips jumps 160 spots on the Forbes 2025  Canada s Best Employers  list Press release | November 13, 2024 Philips Norelco and actor Adam Scott pay tribute to  The King of Beards  this holiday Press release | July 11, 2024 Philips recognized by Forbes as one of Canada s Best Employers for Diversity Press release | March 19, 2024 Leading in health technology innovation: Philips one of the top patent applicants at European Patent Office Press release | March 06, 2024 Health technology leader Philips recognized as Clarivate Top 100 Global Innovator By clicking on the link, you will be leaving the official Royal Philips (""Philips"") website. Any links to third-party websites that may appear on this site are provided only for your convenience and in no way represent any affiliation or endorsement of the information provided on those linked websites. Philips makes no representations or warranties of any kind with regard to any third-party websites or the information contained therein. You are now exiting the Philips United States (US) site and entering the Philips global site. This content is intended for a global audience. It may not apply to the US and should not be interpreted as meeting US standards, executive orders or regulations. Select country/region Our site can best be viewed with the latest version of Microsoft Edge, Google Chrome or Firefox.",2
Artificial Intelligence (AI) in Healthcare Market Size to Hit USD 613.81 Bn by 2034 - GlobeNewswire,https://news.google.com/rss/articles/CBMi6AFBVV95cUxOZzBILWh2TW12dmxseExsYXN2MnY4dXczQjRaNGhKWUc5cVB5MXNlS203ZThnWXp4VFQzbHNKOXQwcEI5VTZkZWpGWU5yT2NCdjRJX01zNHFwRXcyTVFBeDRDWjJTSmdIYzJ5OW5RYlZBYmltN3dDbEZsM3I0NkpiQU90eDgtN3FWZFVwM3NYVm1XOUpoX2taTzJTcXgyQjVxYmx5bTdzRTU1M3c3WWU2YlRoZlZ3QkpBQ2NSbDJjSkd3YUhKUmVuYnFqRVhxZnlaOVBYNG9Sd0FPZHo2cnk4MWE0RkcwMDJT?oc=5&hl=en-US&gl=US&ceid=US:en,"April 02, 2025 10:00 ET | Source: Precedence Research Follow Precedence Research Ottawa, April 02, 2025 (GLOBE NEWSWIRE) -- According to Precedence Research, the artificial intelligence (AI) in healthcare market size was valued at USD 26.69 billion in 2024, calculated at USD 36.96 billion in 2025, and is forecasted to reach around USD 613.81 billion by 2034. The market is expected to see extraordinary growth from a valuation of USD 19.27 billion in 2023, it is estimated to climb to a massive USD 500.47 Bn by 2033. This significant increase signifies a Compound Annual Growth Rate (CAGR) of 38.5% between 2023 and 2033. The Complete Study is Readily Available | Download the Sample Pages @ https://www.precedenceresearch.com/sample/1616 Role of AI in Revolutionizing the Healthcare Sector: Industry Insights Artificial Intelligence (AI) is changing the landscape of healthcare across the globe. With the ability to process and analyze massive amounts of data, AI is revolutionizing patient care, clinical trials, drug discovery, and administrative tasks. From diagnostic tools to personalized treatments, AI technology is helping healthcare professionals make more informed decisions faster. A study published in Nature in 2020 revealed that Google's AI model could significantly reduce diagnostic errors and improve detection rates. This serves as a real-world example of how AI is reshaping healthcare by improving diagnostic precision, leading to more effective and personalized treatment options. Key Drivers and Challenges in the AI Healthcare Market Key Drivers: Challenges: You can place an order or ask any questions, please feel free to contact at sales@precedenceresearch.com | +1 804 441 9344 Demand for Efficient Healthcare System: Market s Largest Opportunity The demand for AI in healthcare is fueled by numerous factors such as the growing need for efficient healthcare systems, increasing patient volumes, rising chronic disease prevalence, and the push towards personalized medicine.  According to the report, healthcare data continues to grow exponentially, with the global data volume estimated to increase to over 175 zettabytes by 2025.  This explosion in healthcare data is one of the key drivers for the integration of AI, which can process vast amounts of unstructured data, providing actionable insights that enhance decision-making and improve outcomes. AI technologies can analyze medical records, imaging scans, and genetic data faster than human practitioners, leading to more accurate diagnoses and tailored treatments. As healthcare systems strive to improve patient care while controlling costs, AI has become a central component of these efforts. Browse Detailed Insight at https://www.precedenceresearch.com/artificial-intelligence-in-healthcare-market Artificial Intelligence (AI) in Healthcare Market Scope How does the generative AI in healthcare market compare to the broader AI in healthcare market?Comparing the Generative AI in Healthcare Market to the broader AI in Healthcare Market reveals several key differences and similarities: Market Size and Growth: AI in Healthcare Market: The market size was valued at USD 26.69 billion in 2024 and is projected to reach USD 613.81 billion by 2034, with a CAGR of 36.83% from 2024 to 2034. Generative AI in Healthcare Market: The global generative AI in healthcare market size was estimated at USD 1.95 billion in 2024 and is expected to hit USD 39.70 billion by 2034, growing at a healthy CAGR of 35.17% from 2025 to 2034. Growth Drivers: Applications Challenges Set up a meeting at your convenience to get more insights instantly! https://www.precedenceresearch.com/schedule-meeting Regional Analysis: What to Expect till 2034? North America is the dominant region for AI in healthcare, with the United States playing a pivotal role in the growth of the market. The combination of cutting-edge research, substantial investments, and a healthcare infrastructure ready to adopt AI technologies positions North America at the forefront of AI adoption in healthcare. U.S. Artificial Intelligence (AI) in Healthcare Market Size and Trends 2024 to 2034 The U.S. artificial intelligence (AI) in healthcare market size reached USD 8.41 billion in 2024 and is projected to surpass over USD 195.01 billion by 2034, growing at a healthy CAGR of 37% from 2024 to 2034. Asia-Pacific is emerging as the fastest-growing market for AI in healthcare. Countries like China and India are witnessing significant advancements in healthcare AI, with government-backed initiatives and a rapidly growing tech sector. India s healthcare sector is increasingly turning to AI to address challenges such as access to quality care, high treatment costs, and a growing population with diverse healthcare needs. China is one of the largest and fastest-growing markets for AI in healthcare, driven by its rapid technological advancements and increasing investment in AI applications. Precedence Research s Expert View on AI s Role in Healthcare Our analysis indicates that AI s impact is particularly profound in areas such as virtual assistants, clinical trials, and patient management. Virtual assistants, driven by advanced natural language processing (NLP) and machine learning technologies, are not only helping healthcare professionals manage daily tasks but also offering personalized support to patients. AI is playing a key role in clinical trials by optimizing patient recruitment, predicting outcomes, and improving the overall efficiency of trial processes. Artificial Intelligence in Healthcare Market Segmental Analysis: By Component Analysis: Software: In 2024, the software segment held the largest share in the AI in healthcare market. Software applications, including predictive analytics, diagnostic algorithms, and clinical decision support systems, are integral in improving healthcare services. AI-powered software systems are not only used for real-time decision-making but also for streamlining administrative tasks and managing hospital workflows efficiently. Services: The services segment, which includes implementation, maintenance, and support services for AI-powered healthcare solutions, is expected to grow at the fastest pace over the forecast period. With the integration of AI into clinical practice and hospital systems, demand for AI-related services, such as data migration, model training, and AI system optimization, is increasing. Hospitals and healthcare providers are recognizing the importance of continuously updating and enhancing AI tools to keep up with evolving technologies. By Application Analysis: Clinical Trials: The clinical trials segment was dominant in 2024. AI technology is being leveraged to streamline the clinical trial process, from participant recruitment to data analysis. AI algorithms can process large datasets, identifying suitable candidates for trials, predicting outcomes, and optimizing trial protocols. This reduces the time it takes to bring a drug to market and lowers costs associated with traditional trial methods.Virtual Assistants: The virtual assistants segment led the market in 2024. Virtual assistants, like chatbots and voice-enabled devices, are rapidly gaining popularity. These tools are increasingly being used for appointment scheduling, patient engagement, medication reminders, and even virtual consultations. Their ability to provide 24/7 assistance and personalized recommendations makes them indispensable to healthcare providers and patients alike. By Technology: The Backbone of AI in Healthcare Machine Learning: Machine Learning (ML) remains the most dominant technology in the AI healthcare market. ML algorithms can analyze medical data, predict disease progression, and even recommend treatment plans based on patient-specific data. From image recognition in radiology to patient monitoring, machine learning forms the backbone of most AI applications in healthcare. Natural Language Processing (NLP): NLP technology has been experiencing rapid growth in the healthcare space. Natural language processing allows machines to understand, interpret, and generate human language, making it crucial for applications like voice recognition, clinical documentation, and automated transcription of medical records. NLP s ability to extract valuable insights from unstructured data (like doctor s notes and patient reports) is making it an indispensable tool for healthcare professionals. By End User: Who Benefits from AI in Healthcare? Hospitals & Healthcare Providers: In 2024, the hospitals and healthcare providers are the dominant end users of AI in healthcare. AI is already being used in various ways, such as enhancing diagnostic accuracy, optimizing patient management, improving administrative efficiency, and reducing hospital readmission rates. Healthcare providers are also incorporating AI into personalized treatment plans, utilizing AI-driven insights to tailor interventions based on a patient s unique characteristics. Patients: The patients segment is seen to grow at a notable rate Patients are becoming more proactive in their healthcare, aided by AI-powered tools. AI-driven apps provide health monitoring, symptom checkers, and virtual consultations, allowing patients to take charge of their own health and manage chronic conditions more effectively. Pharmaceuticals & Biotechnology Companies: AI is being leveraged by pharmaceutical companies for drug discovery, clinical trials, and precision medicine. The technology accelerates the identification of promising drug candidates, reduces the cost and time of drug development, and improves the overall efficiency of research. Healthcare Payers: AI also aids healthcare payers (insurance companies) by streamlining claims processing, fraud detection, and predictive analytics for patient management. AI allows payers to make data-driven decisions about coverage, which improves their ability to manage costs and enhance customer satisfaction. Browse Related Insights: Artificial Intelligence in Healthcare Market Key Players Recent Breakthroughs and Developments: The research report categorizes the AI in Healthcare Market into the following segments and subsegments: By Component By Application By Technology By End User By Geography Thanks for reading you can also get individual chapter-wise sections or region-wise report versions such as North America, Europe, or Asia Pacific. Immediate Delivery Available | Buy This Premium Research Report@ https://www.precedenceresearch.com/checkout/1616 You can place an order or ask any questions, please feel free to contact at sales@precedenceresearch.com | +1 804 441 9344 Precedence Research offers exclusive subscription services designed to provide in-depth data and analytics insights. With a subscription, you gain access to a comprehensive suite of statistical resources, market intelligence, and research tools tailored to your business needs. Whether you're looking for industry trends, competitive analysis, or future market projections, our subscription plans ensure you stay ahead with reliable, up-to-date information. Browse Our Subscription Plans@ https://www.precedenceresearch.com/get-a-subscription About Us Precedence Research is a worldwide market research and consulting organization. We give an unmatched nature of offering to our customers present all around the globe across industry verticals. Precedence Research has expertise in giving deep-dive market insight along with market intelligence to our customers spread crosswise over various undertakings. We are obliged to serve our different client base present over the enterprises of medicinal services, healthcare, innovation, next-gen technologies, semi-conductors, chemicals, automotive, and aerospace & defense, among different ventures present globally. Web: https://www.precedenceresearch.com Our Blogs: Towards Healthcare | Towards Packaging | Statifacts | Towards EV Solutions | Towards Dental | Towards Automotive | Nova One Advisor Get Recent News: https://www.precedenceresearch.com/news For Latest Update Follow Us: LinkedIn | Facebook | Twitter Ottawa, July 25, 2025 (GLOBE NEWSWIRE) -- The global infant formula foods market size stood at USD 91.15 billion in 2024 and is expected to rise from USD 100.45 billion in 2025 to around USD 240.75... Ottawa, July 25, 2025 (GLOBE NEWSWIRE) -- The global rehabilitation equipment market size was valued at USD 17 billion in 2024 and is predicted to hit around USD 37.34 billion by 2034, a study... GlobeNewswire is one of the world's largest newswire distribution networks, specializing in the delivery of corporate press releases, financial disclosures and multimedia content to media, investors, and consumers worldwide.   2025 Digital Media Innovations, LLC. All rights reserved.",2
GE HealthCare's AI-Driven X-Ray Innovations: A Strategic Play to Solve Radiology's Workforce Crisis and Unlock Long-Term Growth - AInvest,https://news.google.com/rss/articles/CBMi3wFBVV95cUxOZEpQeHRZWTQ3MEozLTB1VF9IcEVhd3RjOVVQQ2NydzJEMTB1LXdIRF9mRHQyejZ5YTlQV2xJM25jeW9lREx3bkJsM0xiU093UEZvZ1VmNXZ5QWE0NVY4WXJKanhfOEhCTnk2cUwtV1hqLXVhcWhGSVBiMjR0and3Vk1yYkF3MU5hNmlKZ2swUFJNR2hEZXByX0FrZE9tQW1QR3dWTmV2Z2dGcGZUQWNQbjZ1cWNiSjh1dGV0RGZScWdfYTdRajRvN0xXRWRBeXotZ1BHalR2QWJ6TTZqMUlN?oc=5&hl=en-US&gl=US&ceid=US:en,"News/ Articles/ Articles Details Your AI value-stock bloodhound sniffing out overlooked gems with the upside to make contrarians cheer. The global radiology workforce shortage has reached a tipping point. By 2025, demand for imaging services is projected to outpace radiologist supply by 1.2% annually, driven by aging populations, rising chronic disease prevalence, and post-pandemic healthcare demand surges. With 46% of U.S. radiologists reporting burnout and 81% of healthcare systems citing technologist shortages, the industry faces a perfect storm of operational inefficiencies and unmet patient needs. Enter GE HealthCare, whose AI-driven X-ray innovations are redefining diagnostic imaging as a scalable, efficient, and sustainable solution. Radiology departments are buckling under unsustainable pressures. A 2025 Harvey L. Neiman Institute report highlights a 26.9% projected rise in imaging demand by 2055, outpacing radiologist supply growth of 25.7%. Key pain points include:- Burnout and attrition: 46% of private-practice radiologists and 37.4% in academic settings report burnout, driven by 24/7 reporting demands and high case volumes.- Geographic disparities: Rural hospitals struggle to attract talent, with 72% of teleradiology users citing its role in reducing backlogs.- Operational waste: 35% of imaging studies are repeated daily due to inconsistent protocols, costing the U.S. healthcare system an estimated $2.8 billion annually. These challenges create a $12.3 billion opportunity for AI-driven solutions that address both human and technical bottlenecks. GE HealthCare's 2025 innovations directly tackle the root causes of radiology's crisis. The Definium  Pace Select ET and AMX  Navigate systems integrate AI to automate workflows, reduce errors, and enhance diagnostic accuracy: These tools are not just incremental improvements they represent a fundamental shift in how imaging is delivered. By automating 60% of technologist tasks and enabling 24/7 AI triage, GE HealthCare's systems reduce burnout while expanding access. For example, Miungo Medical in Germany achieved a 31% increase in exam slots after adopting Imaging 360, a platform that optimizes fleet utilization and standardizes protocols. GE HealthCare's collaboration with NVIDIA to develop autonomous X-ray and ultrasound systems is a game-changer. Leveraging NVIDIA's Isaac and Omniverse platforms, these systems will handle patient positioning, image validation, and even basic interpretation with minimal human intervention. This addresses the 4.2 billion annual imaging exams bottlenecked by staffing shortages, particularly in low- and middle-income countries. The implications are profound. Autonomous imaging could reduce the need for 50% of technologist labor in routine exams, freeing professionals to focus on complex cases. Early trials in the UK's NHS show a 40% reduction in radiologist workload for critical care imaging, with AI prioritizing 85% of cases correctly. GE HealthCare's AI-driven strategy aligns with three macroeconomic trends:1. AI adoption in healthcare: The global medical AI market is projected to grow at 27% CAGR through 2030, with GE's 42 FDA-approved AI tools positioning it as a leader.2. Operational efficiency: By reducing exam times and repeat studies, GE's solutions offer a 3:1 ROI for hospitals, making them a priority for cash-strapped healthcare systems.3. Geographic expansion: Autonomous imaging opens new markets in rural and underserved regions, where GE's 2025 partnerships in Africa and Southeast Asia are already expanding access. For investors, the company's $3.2 billion R&D budget and strategic partnerships (e.g., NVIDIA, Everlight Radiology) signal long-term resilience. While short-term risks include AI adoption inertia and regulatory hurdles, the structural demand for imaging driven by an aging population and chronic disease prevalence guarantees sustained growth. GE HealthCare's AI-driven X-ray innovations are not just solving today's radiology crisis they're building the infrastructure for tomorrow's healthcare. By automating workflows, reducing burnout, and democratizing access to imaging, the company is poised to capture a disproportionate share of the $52 billion global medical imaging market. For investors seeking exposure to a sector at the intersection of AI and healthcare, GE HealthCare offers a compelling, data-driven opportunity. Investment Recommendation: Buy with a 3 5-year horizon, targeting key milestones in FDA approvals, international partnerships, and AI adoption rates. Monitor the company's stock price correlation with healthcare AI ETFs for entry points. View More   No comments yet",2
Developing an innovative lung cancer detection model for accurate diagnosis in AI healthcare systems - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTFBJODZVRmcwaVJ5V2laV2ppQlc1S2U5Si12NWsxMFJMb1NhSVdGcnFJSkYtTUprUU9va29kaURjVTR1dDkzYUVpUGtxbWU1WUs3Nm9lNG1KcUdmYThzcERB?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement Scientific Reports volume 15, Article number: 22945 (2025) Cite this article 641 Accesses Metrics details Accurate Lung cancer (LC) identification is a big medical problem in the AI-based healthcare systems. Various deep learning-based methods have been proposed for Lung cancer diagnosis. In this study, we proposed a Deep learning techniques-based integrated model (CNN-GRU) for Lung cancer detection. In the proposed model development Convolutional neural networks (CNNs), and gated recurrent units (GRU) models are integrated to design an intelligent model for lung cancer detection. The CNN model extracts spatial features from lung CT images through convolutional and pooling layers. The extracted features from data are embedded in the GRUs model for the final prediction of LC. The model (CNN-GRU) was validated using LC data using the holdout validation technique. Data augmentation techniques such as rotation, and brightness were used to enlarge the data set size for effective training of the model. The optimization techniques Stochastic Gradient Descent(SGD) and Adaptive Moment Estimation(ADAM) were applied during model training for model training parameters optimization. Additionally, evaluation metrics were used to test the model performance. The experimental results of the model presented that the model achieved 99.77% accuracy as compared to previous models. The (CNN-GRU) model is recommended for accurate LC detection in AI-based healthcare systems due to its improved diagnosis accuracy. Cardiac and respiratory disorders are leading causes of death worldwide particularly in old age1. As the world population increases, the prevalence of various diseases rises, putting an additional burden on healthcare systems. Alternatives to early disease detection are needed to satisfy the growing healthcare needs of the elderly. When diseases are detected early, they can be treated before they become critical. Several large-scale techniques, such as the Nederlands-Leuvens Longkanker Screenings Onderzoek( NELSON) Trial2 and the National Lung Screening Trial (NLST)3, were developed to refine the diagnosis of initial LC. Annual Low-Dose CT Screening, as well as numerous clinical assessments. The NLST and NELSON studies indicate that combining low-dose CT imaging with lung cancer screening (LCS) significantly reduces lung cancer mortality rates, making it a vital method for high-risk populations4. The ability of LCS imaging to detect underdiagnosed viruses in LCS populations is a significant and as of yet unrealized advantage. Cardiovascular Disease (CVD) has similar risk features as LC. As a result, a thorough examination of LCS imaging could aid in the diagnosis of cardiovascular risk. Similarly, the detection of early respiratory diseases that affect survival should be enhanced. Various approaches are employed to identify lung cancer; however, these techniques are not suitable for detecting LC. Artificial intelligence techniques5,6,7 are widely applied nowadays for the analysis of medical data in order to diagnose different chronic diseases8,9. Certain deep learning(DL) models Convolutional neural networks (CNNs), and gated recurrent units (GRU) are suitable for the analysis of medical imaging data because these models extract more deep patterns from images, and the classifier accurately classifies the images as cancerous or non-concourses. In AI-based healthcare systems for critical disease diagnosis, deep learning techniques are incorporated, due to their excellent performance. LC diagnosis from CT scan imaging data is more effective using deep learning models and different researchers have proposed different lung cancer diagnosis models. Here, we discuss the recently published research on LC diagnosis using AI techniques. Niaki et al.10 suggested using the K-nearest-neighbor(KNN) algorithm in conjunction with a genetic algorithm(GA) as a method for LC identification. To improve the KNN classifier s training procedure, the GA is integrated to gather suitable attributes. The model s effectiveness is measured using evaluation measures. The 1000 instances and 23 features of the LC dataset were used. The suggested model, GA-KNN, demonstrated a high level of prediction accuracy. Ibrahim et al.11 proposed an artificial neural network(ANN) based LC diagnosis model. The model achieved 96.67% accuracy. Yang et al.6 development of an LC recognition system by the use of ANN to analyze pathological images. The proposed model achieved an accuracy of 90 percent. Alsinglawi et al.6 proposed a method for diagnosing LC applied Machine Learning (ML) techniques. The validation method k-folds with k=10 CV are used for model validation. The suggested method s outputs were evaluated using valuation metrics, and the Random-forest and SMOTE class-balancing models achieved good accuracy. Gao et al.12 proposed a deep learning based method using a pre-trained MobileNetV2 model for diagnosis of lung cancer. Lung cancer CT scan image data was used for the proposed model finetuning and the model obtained 99.6% accuracy. priya et al.13 proposed a deep learning algorithms based model for lung cancer detection using SE-ResNeXt-50 architecture and Convolutional Neural Networks (CNNs) (SE-ResNeXt-50-CNN). The lung CT images data was used for model validation and the proposed model obtained 99.15% accuracy, 97.58% sensitivity, 99.80% specificity, and 98.54% score. Table 1 gives a brief overview of other DL-based techniques for LC detection. According to the literature review as summarized in Table 1, various DL algorithms-based techniques were considered for accurate diagnosis of LC. However, further improvement is necessary in these methods to detect LC accurately. Therefore, an intelligent method is necessary to design for accurate and efficient recognition of LC in AI-based healthcare systems. In this research study for lung cancer detection Deep learning techniques-based model is proposed (CNN-GRU). In the proposed model, the CNNs and GRUs models are integrated to design the LC detection model. In order to improve the LC detection model, the GRU captures temporal relationships between medical image sequences, such as multiple CT scans over time. To enhance the model s capacity to identify subtle patterns and progressions, the GRU monitors changes in lung tissue and tumor growth while the Convolutional Neural Network (CNN) extracts spatial data. The model is enhanced in accuracy and efficacy by integrating CNN s spatial analysis with GRU s sequential learning. GRU was chosen over LSTM for integration with CNN due to its computational efficiency, faster training, and lower memory requirements, making it ideal for large-scale medical imaging. The (CNN-GRU) model was validated using LC image data. Data augmentation techniques such as rotation, and brightness were used to enlarge the data set size for effective training of the model. The hold-out validation technique is used for the training and testing of the suggested model. Moreover, evaluation metrics were applied for the evaluation of the model. The optimization algorithm SGD and ADAM are incorporated during the model s training. The (CNN-GRU) model experimental results demonstrate the prospective to improve the correctness and effectiveness of diagnosing LC in the AI-based healthcare system. The Major contributions are as follows: Design an integrated deep learning-based (CNN-GRU) model for LC detection. In the model the GRU captures temporal relationships between medical image sequences. To enhance the model s capacity to identify subtle patterns and progressions, the GRU monitors changes in lung tissue and tumor growth while the Convolutional Neural Network (CNN) extracts spatial data. The model is enhanced in accuracy and efficacy by integrating CNN s spatial analysis with GRU s sequential learning. The optimization algorithms SGD and ADAM are utilized during model training to optimize the model parameters. Hold out validation technique is used for model training and validation and tested the model by IQ-OTH/NCCD and CT-Scan images data sets and computes evaluation metrics for model performance evaluation. The (CNN-GRU) model obtained high performance compared to baseline models using statistical t-test. The (CNN-GRU) model demonstrates the prospective to improve the correctness and effectiveness of diagnosing LC in the AI-based healthcare system. In the remaining paper the methodology of the proposed method is given in Sect. Methodology, the proposed model experimental results are reported in Sects. Experiments and Conclusion is the conclusion and future work. The model was validated using CT-Scan images and IQ-OTH/NCCD data sets in this study. The IQ-OTH/NCCD lung cancer dataset consists of CT scan images categorized into three classes: malignant, benign, and normal. The dataset contains a total of 1097 images, with 561 malignant, 120 benign, and 416 normal images. The ratio of malignant, benign, and normal images is given in Fig. 1. These images were utilized in this study for the purpose of lung cancer detection. The IQ-OTH/NCCD-Lung Cancer Dataset classes are shown in Fig. 2. Model cross-validation was performed using the CT-Scan image data set, which has 364 images altogether, of which 238 are cancerous and 238 are non-cancerous. Dataset samples ratio of malignant, benign, and normal classes. IQ-OTH/NCCD-Lung Cancer dataset description. CNNs, or feed-forward neural networks, are widely recognized for their application in the construction of deep learning algorithms25. The CNN structure as shown in Fig. 3 included pooling, convolutional, and fully connected, which perform tasks including feature extraction, dimensionality reduction, and classification. In the LC detection process, we use CNNs because they are great at understanding images like lung scans. CNNs help us find important patterns in the images that can indicate the presence of lung cancer. They are good at learning different levels of details in the images, starting from simple features to more complex ones. The filter moves over the input during the forward pass of the convolution, calculating the activation that is shown by summing the point-wise production of each component to produce an activation at that place. Sliding filters are created using convolutions. Since this process is linear, a dot product could be an effective way to explain it. Equation 1 illustrates how a convolution \(({x}^-*w) (a)\) at t time may be defined for all dimensions with x as input and w as kernel function. where a in \(R^n\) denotes any \(n \ge 1\). Typically, the discrete parameter is t. Thus, Eq. (2) can be used to represent the process of convolution. Nonetheless, researchers frequently employ 2 or 3 dimensional convolutions. Assume we have a 2-dimensional kernel N and a 2-dimensional image I as inputs. CNN architecture. The specialized version of RNN architecture known as a Gated Recurrent Unit (GRU)26 has garnered popularity due to its capacity to overcome some of the limitations associated with conventional RNNs. It was introduced in 2014 by Cho and colleagues, and its effectiveness in modeling sequential data has made it a crucial element in various machine learning and natural language processing tasks. Whereas 2 or 3-dimensional convolutions are used by researchers. Let two-dimensional image I be taken as input and a 2-D kernel K. One of the main problems encountered with standard RNNs is the vanishing gradient issue, which impairs their capacity to identify distant relationships in sequential data. GRUs with built-in gating mechanisms were created expressly to solve this problem. These gates control the information flow inside the network, making it easier to recall or delete pertinent data at each time step. A GRU consists of four key components: an update gate \(z_t\), a reset gate \(r_t\), a hidden state \(h_t\), and a candidate state. The hidden state is the network s memory for prior information; the reset gate regulates which data from the earlier hidden state would be destroyed and the update gate governs what amount of candidate state should be included in the current hidden state. The candidate state is calculated with the current input and the reset gate. The GRU model is depicted in Fig. 4. GRU architecture. Data augmentation techniques such as rotation, and brightness were used to increase and balance the training dataset by making several variations to the original data in this research study27. For model training and validation the holdout cross-validation mechanism28 is used. In this study for all experiments, we divided the datasets of CT scan images 80% and 20% for training and testing of the model. Different assessment measures are used to evaluate the proposed model9,29. Eqs. (3) through (7) outline key assessment measures used to evaluate the proposed model. An integrated deep learning model based on CNN and GRU is proposed for lung cancer detection in AI-based healthcare systems. In the development of the proposed model, CNN and GRU models are integrated to design a model for detecting lung cancer. CNN model is employed to extract deep patterns from the CT scan images through a series of pooling and convolutional layers. The obtained features from data are embedded in the GRUs model for the final prediction of LC. The structure of the CNN-GRU model has 3 convolution layers, 3 batch normalization layers, 3 max-pooling layers, 3 Relu activation functions, 1 flatten layer, 3 GRU layers, 2 Dropout layers, 2 dense layers, and one softmax activation function for final classification. The model (CNN-GRU) was validated using CT Scan image data. Data augmentation techniques such as rotation, and brightness were used to enlarge the data set size for effective training of the model. For the training and testing of the model, the holdout CV method was used. Furthermore, metrics for evaluating performance were applied for the model s evaluation. The optimization algorithms SGD and ADAM were implemented to optimize the training parameters of the model. The structure of the model(CNN-GRU) is given in algorithm 1 and the flowchart is shown in Fig. 5. Lung cancer detection model (CNN-GRU). Lung cancer detection model (CNN-GRU) The model (CNN-GRU) was implemented by conducting several experiments to validate its performance. In these experiments, different data augmentation techniques such as rotation, and brightness were used for data pre-processing. The data set of LC was divided into 80% and 20% for training and testing using holdout cross-validation techniques. Moreover, evaluation metrics were computed for model performance evaluation. In the experimental setting, the hyperparameters were manually modified to improve model performance. The batch size was set to 32, 64, and 120, with 30, 50, and 60 epochs and a learning rate of 0.0001 and 0.001. In addition, a dropout rate of 0.2 was introduced to all experiments to prevent overfitting and improve generalization. These manually selected settings were determined based on empirical observations to obtain the best training stability and accuracy. All experiments were repeated with different hyperparameters until they obtained stable results and we only reported stable results. Also, various tools were employed for simulating these experimentations, such as Python with TensorFlow, and Keras. For the experiments, an Intel  Core  i5-2400 CPU, 4 GB of RAM, and Windows 10 were used on the computer. The experimental results are analyzed here. The original IQ-OTH/NCCD lung cancer dataset had 1097 images, with an imbalance between classes: 561 malignant, 120 benign, and 416 normal. To overcome this issue, data augmentation techniques such as rotations, and color brightness were used to boost the number of images in underrepresented categories. As a result, the enlarged dataset now has 1683 images, with each class balanced at 561 images, resulting in enhanced model performance and reduced bias towards the majority class. The predictive capability of the CNN model was assessed through various experiments employing original and augmented datasets. The LC dataset has 1190 images of three classes(Malignant, Normal, and Benign). The data augmentation techniques rotations and color brightness were used. There are 1683 images included in the new dataset. The model was for 50 epochs, batch size was 120, and other hyperparameters(PR) for all experiments are listed in Table 2. Table 2 shows that the CNN architecture with the SGD optimizer at a rate of 0.0001 obtained 96.12% accuracy, 93.02% specificity, 95.24% sensitivity, 98.87% precision, and 97.73% F1 score. The CNN with the ADAM and at an LR of 0.0001 obtained accuracy (96.89%), specificity (99.78%), sensitivity (100%), precision(99.45%), and F1 score(98.78%). Results of the CNN model are graphically shown in Fig. 6. The results of the CNN model on the augmented data set are reported in Table 2. Table 2 shows that the CNN architecture with the SGD optimizer and at a learning rate of 0.0001 obtained 96.78% accuracy, 98.54% specificity, 99.78% sensitivity, 99.23% precision, and 98.03% F1 score. CNN architecture with the ADAM at a learning rate of 0.0001 obtained accuracy (97.02%), specificity (97.30%), sensitivity (100%), precision (99.89%), and F1 score (98.88%). Figure 7 shows graphically the results of the CNN model on the newly generated dataset. CNN model results on original dataset. CNN model results on the augmented dataset. The results of the GRU model on the original data set are reported in Table 3. Table 3 demonstrated that the GRU model with the SGD optimizer and at a learning rate of 0.0001 obtained accuracy (97.00%), specificity(100.00%), sensitivity(97.07%), precision(99.00%), and F1 score (99.10%). The GRU with the ADAM same LR of 0.0001 obtained accuracy (98.23%), specificity(98.68%), sensitivity(100.00%), precision(97.00%), and F1 score(99.34%). Results of GRU architecture on the original dataset are shown graphically in Fig. 8. The GRU model experimental results on augmented data are reported in Table 3. Table 3 presented that the GRU architecture with the SGD optimizer and at a learning rate of 0.0001 obtained accuracy(97.98%), specificity(99.78%), sensitivity(99.23%), precision(99.23%), F1 score (98.03%). GRU with the ADAM at an LR of 0.0001 obtained accuracy(98.97%), specificity(99.80%), 100.00% sensitivity, precision(99.96%), and F1-score(98.95%). Results of GRU architecture on the augmented dataset are shown graphically in Fig. 9. Results of GRU model on original dataset. Results of GRU model on augmented dataset. The proposed model (CNN-GRU) has been evaluated experimentally using original and augmented data sets. The results of the models with SGD and ADAM optimizers are reported in Table 4. Table 4 demonstrated that the proposed architecture with the SGD optimizer obtained accuracy(98.95%), specificity(100%), sensitivity(99.87%), precision(97.04%), 98.99% F1 score. The model with the ADAM obtained accuracy(99.12%), specificity(99.89%), sensitivity(96.34%), precision(99.23%) , and F1 score(99.32%). on the other side With an augmented data set the model (CNN-GRU) results in Table 4 shows that the Proposed model with the SGD optimizer and at a learning rate of 0.0001 obtained accuracy(98.99%), specificity(98.78%), sensitivity(98.54%), precision(99.73%), F1 score(99.33%). The model with the ADAM at a learning rate of 0.0001 obtained accuracy(99.77%), specificity (100%), sensitivity(99.%), precision(99.98% ), and F1 score(99.97%). Results of the proposed model (CNN-GRU) on original and augmented datasets are shown graphically in Figs. 10 and 11. The CNN-GRU mosel performance is validated with an independent CT-Scan images data set and the model is trained with an augmented IQ-OTH/NCCD data set under the parameters( Eposes=50, Batch size=120, LR=0.0001). The cross-validation results are reported in Table 5. The Model CNN-GRU with SGD optimizer achieved 98.97% accuracy, 97.40% specificity, 99.20% sensitivity, 96.64% precision, and 99.76% f1-score. While ADAM optimizer 99.68% accuracy, 99.78% specificity, 99.86 % sensitivity, 99.90% precision and 99.94% f1-score. The performance of the model with an independent CT-Scan images dataset is a little low as compared to the IQ-OTH/NCCD test data set as reported in Tables 4 and 5. These results show that the proposed model has the capability for generalization. From the above experimental results analysis we concluded that the model (CNN-GRU) with ADAM on augmented data achieved 99.77% accuracy. The high accuracy of the proposed model is due to the integration of the CNN and GRU models and the data augmentation approach. Model (CNN-GRU) model results on original dataset. Model (CNN-GRU) model results on augmented dataset. The accuracy of the model compared with the baseline models in Table 6 and Fig. 12. The model achieved 99.77% accuracy as compared to baseline models. To statistically validate the proposed model(CNN-GRU) performance compared to a baseline, we incorporate hypothesis testing using T-tests (for two groups)30. The T-test determines whether the means of two independent models differ significantly from one another. The null hypothesis \(H_0\) states that the two groups  means are equal. The alternative hypothesis \(H_1\) states that the two groups  means differ. If \(p < 0.05\) the proposed model significantly outperforms the baseline models and If\(p \ge 0.05\), there is no significant difference between the proposed model and baseline models. According to the T-Test, the P-value is equal to 0.000 which means that \(0.000 < 0.05\) and it demonstrates that the proposed model outperformance than baseline models. Due to high accuracy, we recommend the model for LC detection in AI-based healthcare systems. Accuracy comparison with baseline models. The space and Time complexity of model CNN-GRU with optimizer SGD and ADAM with original and augmented data are reported in Table 4 for the detection of Lung cancer. The space complexity is analyzed by taking into account of model trainable parameters because the proposed model uses deep learning techniques. To compute the time complexity, we use the training time of the model. Table 4 presented that CNN-GRU has the worst space complexity since its trainable parameter is 140 million with ADAM optimizer on augmented data, while CNN-GRU has 7.4 hours the best space-time complexity with SDG on original data. Additionally, for the time complexity, the CNN-GRU model has the worst time complexity because its training time is 10.3h hours. Thus the proposed CNN-GRU model predictive accuracy is high but it is computationally more complex due to the complex structure, more training parameters, and huge training data. However, the complexity problem can be handled by using more high-performance technology such as GPU. Lung cancer is a critical clinical issue and around the world, many people are affected by it. The accurate and on-time diagnosis of lung cancer is a critical challenge for medical professionals and researchers. Lung cancer diagnosis on conventional methods is not effective for accurate and on-time diagnosis for reliable treatment and recovery. To tackle these issues researchers nowadays incorporate artificial intelligence mechanisms for initial stage diagnosis of lung cancer using medical big data such as patient medical history, MRI, and CT Scan image data. Deep learning, a major AI technology, requires big data to exhibit the self-learning procedure to multiple computations of data patterns that classify tumors into their related classes. However deep learning-based lung cancer diagnosis requires big and properly balanced label data for effective training and testing of the model. The deep learning model especially the convolutional neural network is a more suitable model for medical image analysis31. The CNN algorithm can extract more deep patterns from images for accurate image classification32. The gated recurrent units (GRU) model is also a suitable model for the analysis of medical imaging data as compared to LSTM for its computational efficiency, faster training, and lower memory requirements, making it ideal for large-scale medical imaging. With a simpler gating mechanism, GRU effectively captures long-term dependencies while requiring less computational power. Additional techniques of deep learning such as attention techniques and data augmentation techniques can also improve the predictive capability of the CNN model for precise diagnosis of Lung cancer in AI-based healthcare systems33. In this study, we proposed a deep learning techniques-based integrated model(CNN-GRU) for lung cancer detection. In the designing of the model, the CNNs and GRUs models are integrated. The GRU captures temporal relationships between medical image sequences, such as multiple CT scans over time. To enhance the model s capacity to identify subtle patterns and progressions, the GRU monitors changes in lung tissue and tumor growth while the Convolutional Neural Network (CNN) extracts spatial data. The model is enhanced in accuracy and efficacy by integrating CNN s spatial analysis with GRU s sequential learning. The (CNN-GRU) model was validated using LC image data. The hold-out validation technique was used for the training and testing of the model. Moreover, evaluation metrics were applied for the evaluation of the model. The optimization algorithm SGD and ADAM are incorporated during the model s training. According to the experimental results in Tables 4 and 5. The CNN-GRU model with SGD optimizer obtained accuracy(98.95%), specificity(100%), sensitivity(99.87%), precision(97.04%), 98.99% F1 score. The model with the ADAM obtained accuracy(98.12%), specificity(99.89%), sensitivity(96.34%), precision(99.23%), and F1 score(99.32%) results on original data as reported in Table 4. On the other side with an augmented data set the model (CNN-GRU) results in Table 4 shows that the model with the SGD optimizer and at a learning rate of 0.0001 obtained accuracy(98.99%), specificity(98.78%), sensitivity(98.54%), precision(99.73%), F1 score(99.33%). The model with the ADAM at a learning rate of 0.0001 obtained accuracy(99.77%), specificity (100%), sensitivity(99.%), precision(99.98% ), and F1 score(99.97%). The integrated model with augmented data increased accuracy from 99.12% to 99.77% with Adam optimizer. The improved accuracy of the proposed model demonstrated that the structure of the model is well suited for deep pattern recognition and classification of images. From the results analysis, we concluded that the proposed model is more suitable for accurate diagnosis of Lung cancer in AI-based health care systems. However, the prospered model obtained greater accuracy, but it had significant technological limitations. It requires huge well-structured data and is computationally intensive, restricting its application in resource-constrained situations. Differences in images have an impact on generalization, and overfitting persists. Furthermore, this work did not investigate sophisticated data augmentation such as elastic deformations, random cropping, and adversarial training, and deep learning methods such as transfer learning and federated learning, which could improve model resilience and flexibility. The proposed CNN-GRU model predictive accuracy is high as compared to the baseline model but it is computationally more complex due to the complex structure, more training parameters, and huge training data. However, the complexity problem can be handled by using more high-performance technologies. Furthermore, deploying AI in healthcare faces challenges such as integration with existing workflows, data privacy compliance, and regulatory approvals. Issues like model bias, interpretability, and clinician trust must also be addressed for successful adoption. To overcome these limitations, future research should explore the integration of advanced techniques, such as multi-modal data fusion, transfer learning, and more sophisticated model architectures. Additionally, efforts should be made to enhance the model s generalization capabilities and interpretability, ensuring that it can be applied effectively across diverse patient populations. By addressing these challenges, a more reliable, accurate, and scalable model for lung cancer diagnosis can be developed, ultimately advancing healthcare systems and improving patient outcomes. To the best of our knowledge, lung cancer (LC) is a critical disease, and early detection is crucial for effective treatment. So an accurate LC diagnosis is a big medical problem in the AI-based healthcare systems. Researchers have proposed various deep learning-based models to tackle the problem of accurate diagnosis of LC. However, these models still need further improvement to accurately diagnose LC. To tackle this problem of accurately detecting lung cancer, we proposed a deep learning techniques-based integrated model (CNN-GRU). The proposed model development combines the (CNNs) and (GRUs) models to create an intelligent model (CNN-GRU) for LC diagnosis. The CNN model is employed to get spatial features of the images through several convolutional and pooling operations. The extracted features from data are embedded in the GRUs model for the final prediction of LC. The proposed (CNN-GRU) model was validated using LC data using the holdout validation technique. The SGD and ADAM optimization algorithms were incorporated during model training to optimize model training parameters. The experimental results of the model presented that the model reached 99.77% accuracy as compared to baseline models. Due to higher diagnosis accuracy, we recommend the (CNN-GRU) model for accurate detection of Lung cancer in AI-based healthcare systems. Transfer learning and federated learning approaches and more advanced data augmentation techniques including elastic deformations, random cropping, and adversarial training will be employed in the future to develop a more reliable model for LC diagnosis in AI-based healthcare systems. The data sets used in this work are available at the Kaggle repository: IQ-OTH/NCCD (https://www.kaggle.com/datasets/adityamahimkar/iqothnccd-lung-cancer-dataset) and CT-Scan images (https://data.mendeley.com/datasets/p2r42nm2ty/2) Vos, T. et al. Global burden of 369 diseases and injuries in 204 countries and territories, 1990 2019: a systematic analysis for the global burden of disease study 2019. The Lancet 396, 1204 1222 (2020). Article Google Scholar De Koning, H., Van Der Aalst, C., Ten Haaf, K. & Oudkerk, M. Pl02. 05 effects of volume ct lung cancer screening: mortality results of the nelson randomised-controlled population based trial. J. Thorac. Oncol. 13, S185 (2018). Article Google Scholar Kramer, B. S., Berg, C. D., Aberle, D. R. & Prorok, P. C. Lung cancer screening with low-dose helical ct: results from the national lung screening trial (nlst) (2011). Snoeckx, A. et al. The radiologist s role in lung cancer screening. Trans. Lung Cancer Res. 10, 2356 (2021). Article Google Scholar Ozdemir, B. & Pacal, I. An innovative deep learning framework for skin cancer detection employing convnextv2 and focal self-attention mechanisms. Res. Eng. 25, 103692 (2025). Google Scholar Wang, S. et al. Artificial intelligence in lung cancer pathology image analysis. Cancers 11, 1673 (2019). Article PubMed PubMed Central Google Scholar Bayram, B., Kunduracioglu, I., Ince, S. & Pacal, I. A systematic review of deep learning in mri-based cerebral vascular occlusion-based brain diseases. Neuroscience (2025). Pacal, I. Investigating deep learning approaches for cervical cancer diagnosis: a focus on modern image-based models. European Journal of Gynaecological Oncology 46 (2025). Pacal, I., Ozdemir, B., Zeynalov, J., Gasimov, H. & Pacal, N. A novel cnn-vit-based deep learning model for early skin cancer diagnosis. Biomed. Signal Process. Control 104, 107627 (2025). Article CAS Google Scholar Maleki, N., Zeinali, Y. & Niaki, S. T. A. A k-nn method for lung cancer prognosis with the use of a genetic algorithm for feature selection. Expert Syst. Appl. 164, 113981 (2021). Article Google Scholar Nasser, I. M. & Abu-Naser, S. S. Lung cancer detection using artificial neural network. Int. J. Eng. Inform. Syst. (IJEAIS) 3, 17 23 (2019). Google Scholar Gao, Z., Tian, Y., Lin, S.-C. & Lin, J. A ct image classification network framework for lung tumors based on pre-trained mobilenetv2 model and transfer learning, and its application and market analysis in the medical field. arXiv preprint arXiv:2501.04996 (2025). Priya, A. & Bharathi, P. S. Se-resnext-50-cnn: A deep learning model for lung cancer classification. Applied Soft Computing 112696 (2025). Moitra, D. & Mandal, R. K. Classification of non-small cell lung cancer using one-dimensional convolutional neural network. Expert Syst. Appl. 159, 113564 (2020). Article Google Scholar Asuntha, A. & Srinivasan, A. Deep learning for lung cancer detection and classification. Multimedia Tools Appl. 79, 7731 7762 (2020). Article Google Scholar Pacal, I. Improved vision transformer with lion optimizer for lung diseases detection. Int. J. Eng. Res. Dev. 16, 760 776 (2024). Google Scholar Kavitha, M., Vidhya, K., Sreeja, S., Roopashri, G. & Muhil, P. Automated lung disease detection, classification and prediction using rnn framework. In E3S Web of Conferences, vol. 491, 03015 (EDP Sciences, 2024). Ozdemir, B., Aslan, E. & Pacal, I. Attention enhanced inceptionnext-based hybrid deep learning model for lung cancer detection. IEEE Access 13, 27050 27069. https://doi.org/10.1109/ACCESS.2025.3539122 (2025). Article Google Scholar Khoirunnisa, A. et al. Implementation of crnn method for lung cancer detection based on microarray data. JOIV Int. J. Informatics Visual. 7, 600 605 (2023). Google Scholar Hatuwal, B. K. & Thapa, H. C. Lung cancer detection using convolutional neural network on histopathological images. Int. J. Comput. Trends Technol 68, 21 24 (2020). Article Google Scholar Wankhade, S. & Vigneshwari, S. A novel hybrid deep learning method for early detection of lung cancer using neural networks. Healthcare Analy. 3, 100195 (2023). Article Google Scholar Vemula, S. T. et al. Deep learning techniques for lung cancer recognition. Eng. Technol. Appl. Sci. Res. 14, 14916 14922 (2024). Article Google Scholar Crasta, Lavina Jean & Rupal Neema, A. .R. . P. A novel deep learning architecture for lc detection and diagnosis from computed tomography image analysis. Healthcare Analy. 5, 100316 (2024). Article Google Scholar Shah, A. A., Malik, H. A. M., Muhammad, A., Alourani, A. & Butt, Z. A. Deep learning ensemble 2d cnn approach towards the detection of lung cancer. Sci. Rep. 13, 2987 (2023). Article ADS CAS PubMed PubMed Central Google Scholar HASAN, M. A., Bhargav, T., SANDEEP, V., REDDY, V. S. & AJAY, R. Image classification using convolutional neural networks. Int. J. Mechan. Eng. Res. Technol. 16, 173 181 (2024). Google Scholar Dey, R. & Salem, F. M. Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS), 1597 1600 (IEEE, 2017). Aboor, A. et al. Ddfc: deep learning approach for deep feature extraction and classification of brain tumors using magnetic resonance imaging in e-healthcare system. Sci. Rep. 14, 6425 (2024). Article ADS Google Scholar Haq, A. et al. A survey of deep learning techniques based parkinson s disease recognition methods employing clinical data. Expert Syst. Appl. 208, 118045 (2022). Article Google Scholar Haq, A. U. et al. Mcnn: a multi-level cnn model for the classification of brain tumors in iot-healthcare system. J. Ambient. Intell. Humaniz. Comput. 14, 4695 4706 (2023). Article PubMed Google Scholar Aminulhaq, Li, J., Memon, M. H., Khan, J. & Ud Din, S. A novel integrated diagnosis method for breast cancer detection. J. Intell. Fuzzy Syst. 38, 2383 2398 (2020). Google Scholar Sultan, H. H., Salem, N. M. & Al-Atabany, W. Multi-classification of brain tumor images using deep neural network. IEEE Access 7, 69215 69225 (2019). Article Google Scholar AminulHaq,. Iimfcbm: Intelligent integrated model for feature extraction and classification of brain tumors using mri clinical imaging data in iot-healthcare. IEEE J. Biomed. Health Inform. 26, 5004 5012 (2022). Article Google Scholar Haq, A. U. et al. Stacking approach for accurate invasive ductal carcinoma classification. Comput. Electr. Eng. 100, 107937 (2022). Article Google Scholar Download references The authors extend their appreciation to the Deanship of Scientific Research at Northern Border University, Arar, KSA for funding this research work through the project number  NBU-FFR-2025-1850-05, and Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2025R303), Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia. School of Artificial Intelligence, Neijiang Normal University of Sichuan, Neijiang, Sichuan, 641100, China Wang Jian Institute of Telecommunications, Computer Science and Photonics, Scoula Superiore Sant Anna (SSSA), Pisa, Via Moruzzi 1, 56124, Italy Amin Ul Haq International Centre for Wavelet Analysis and Its Applications, Big Data Research Institute, University of Electronic Science andTechnology of China (UESTC), Chengdu, Sichuan, China Amin Ul Haq Department of Computer Science, Mohi-Ud Din Islamic University, Azad Jammu and Kashmir, 100600, Pakistan Noman Afzal Information Technology Department, College of Computer and Information Sciences, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, 11432, Saudi Arabia Shakir Khan Department of Information Systems, College of Computer and Information Sciences, Princess Nourah bint Abdulrahman University, P.O. Box 84428, Riyadh, 11671, Saudi Arabia Hadeel Alsolai Department of Computer Science, Faculty of Science, Northern Border University , Arar, 73213, Kingdom of Saudi Arabia Sultan M. Alanazi & Abu Taha Zamani Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Conceptualization, Wang Jian (W.J), Amin ul Haq (A.U.H.), Noman Afzal (N.A), and Shakir Khan (S.K), Hadeel Alsolai (H.A) software, A.U.H.; validation, A.U.H., Sultan M. Alanazi(S.M.A), N.A, Abu Taha Zamani(A.T.Z), W.J, data curation, A.U.H., S.K., H.A, W.J, writing-original draft preparation, A.U.H.; writing-review and editing, W.J, A.U.H., N.A, A.H, S.M.A, A.T.Z, S.K.; visualization, W.J, A.U.H, N.A, A.T.Z, S.M.A, S.K.; supervision, W.j; project administration, A.U.H.; funding acquisition, W.J. All authors have read and agreed to the published this manuscript. Correspondence to Amin Ul Haq. The authors declare no competing interests. Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions Jian, W., Haq, A.U., Afzal, N. et al. Developing an innovative lung cancer detection model for accurate diagnosis in AI healthcare systems. Sci Rep 15, 22945 (2025). https://doi.org/10.1038/s41598-025-03960-2 Download citation Received: 19 December 2024 Accepted: 23 May 2025 Published: 02 July 2025 DOI: https://doi.org/10.1038/s41598-025-03960-2 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement Scientific Reports (Sci Rep) ISSN 2045-2322 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",1
A caution against customized AI in healthcare - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTE5McHlsYm1rQlg3aVBEX2hNQTUzLTRSVEY4YjNSQzJ0NFA3c2g1cmxvRHNOanVkZzlJOFhmMy0xQnFzMF9vYjlFNzZpbGRIalh5UEhMV3JZczBZMGVUZ2ZV?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement npj Digital Medicine volume 8, Article number: 13 (2025) Cite this article 4331 Accesses 2 Citations 15 Altmetric Metrics details This article critiques the shift towards personalized AI in healthcare and other high-stakes domains, cautioning that without careful deliberation, customized AI systems can compromise the diversity and reach of human knowledge by restricting exposure to critical information that may conflict with users  preferences and biases. Customized AI should be applied with caution and intention where access to a wide and diverse range of perspectives is essential for impartial, informed decision making. The trend of personalizing artificial intelligence (AI) systems is gaining momentum as AI companies compete to develop tools with broad consumer appeal and respond to challenging ethical questions about how best to prioritize and align content with diverse human values and beliefs. AI-powered chatbots and answer engines are quickly replacing more traditional search engines as a preferred platform for acquiring knowledge, placing growing pressure on AI developers to present outputs that resonate with what consumers believe to be accurate representation of the  truth.  However, the wide diversity of belief systems and perspectives among users poses significant challenges in deciding which information to prioritize, especially where truths are contested or where evidence diverges. This is especially relevant in healthcare, where stakes are high for pinpointing the precise nature and severity of pathology and effective treatment pathways. A primary utility of AI in healthcare is to identify patterns that are difficult for clinicians to see on their own (e.g., due to human imperceptibility, insufficient resolution, voluminous data); however, they rarely point to a single truth and more often present a range of probabilities. As such, AI systems designed to augment clinical decision making face the same problems of acceptability and uptake as consumer-grade AI-powered chatbots and answer engines. AI developers in healthcare (as in other fields) are similarly responding to these acceptability challenges by offering systems that customize outputs to users  (e.g., physicians ) style and content preferences. What follows is a critique of this shift and a call for applying customized AI with caution and explicit intentionality in critical domains like healthcare where access to a wide and diverse range of perspectives is necessary for impartial, informed and responsible decision making. In 2022, AI moved into the global spotlight after the release of OpenAI s ChatGPT, a large language model (LLM) that generates responses according to a complex calculus of word sequence, context, and coherence. Importantly, these calculations depend strictly on the content, quality, and quantity of training data sets and have no inherent relationship to any objective truth. Given that LLMs are trained on vast portions of the internet, critics1 argue that AI outputs mirror biases embedded in online texts leading to overrepresentation of dominant (e.g., English, Western) viewpoints. The AI community has responded by refining datasets and employing techniques like fairness constraints and adversarial training to ensure more balanced representation of perspectives. However, recent controversies (e.g., over Google Deepmind s AI assistant Gemini depicting America s  founding fathers  as people of color) reveal that some developers are overshooting these attempts, prioritizing fairness and non-bias at the expense of accuracy. Such controversies highlight the need to effectively address algorithmic bias, but also highlight a deeper question about how to shape AI systems that, in turn, shape the content of information we consume. AI systems are rapidly becoming essential tools for human reflection, influencing our understandings, opinions, and decisions around entertainment, leisure, work, and even critical domains like healthcare. Despite their far-reaching political and social implications, important AI design choices are being left to Big Tech companies that are more directly incentivized to expand their user base than to expand human knowledge. Big Tech s response to the consumer value alignment problem is captured by what Open AI s Sam Altman has described as a  simple  solution: Let users decide. Rather than navigate a complex landscape of contested worldviews, AI developers are pioneering a new generation of AI systems that learn from users  feedback to deliver content that aligns with a user s specific style and content preferences. This design choice absolves developers of the task of deciding which information to prioritize, while users receive content they perceive to be more relevant interesting, thereby enhancing the product s appeal. The rapid adoption of algorithmic customization (or  personalization,  used here interchangeably) is demonstrated by the latest trend of enhancing systems  capacity to remember and adapt. Expanding the number of tokens per context window allows AI systems to better recall what users said at the beginning of a conversation or even previous conversations. Researchers2 are also working on  infini-attention  models that leverage data compression to enable infinite memory. The vision is to offer users a bespoke, lifelong companion, powered by their entire digital footprint, promising to  never forget what s important to you  and always be  on your side 3. Mustafa Suleyman, co-creator of Inflection AI s new  personalized intelligence  system, forecasts a future with millions of personalized AI systems, serving not only individuals but also businesses and even governments. While AI customization helps to address the interrelated challenges of value alignment, acceptability, and uptake, lessons from two decades of social media experimentation should caution us about letting algorithms shape the range and diversity of information consumed by humans. As we enter the era of  personalized everything,  AI developers and policymakers seem to overlook the social and epistemic implications of granting algorithms (beyond those used in social media) the power to curate human understandings of the world, and the risks of indiscriminately applying customization across critical sectors like healthcare, banking, education, transportation, manufacturing, agriculture and energy, where customized AI is already appearing. While algorithmic filtering and curation may seem innocuous, greater attention from researchers and policymakers is needed to assess the risks of allowing AI systems to increasingly mediate human understandings and perspectives. The evolution from one-size-fits-all to personalized AI models represents a significant shift in the objectives and capabilities of AI technologies. Early AI systems were confined to fixed, narrow, and pre-defined objectives. However, the landscape of AI has radically changed with advancements in inverse reinforcement learning (IRL) that enable AI systems to operate with a degree of uncertainty about their objectives. Unlike traditional reinforcement learning, which relies on pre-specified rewards and punishments to teach behavior, IRL allows machines to infer and learn underlying preferences and principles from observed human behavior. This capacity has led to an explosion of  recommender systems  taken up by social media and online marketers to deduce user preferences from online activity, fueling a multibillion-dollar industry dedicated to enhancing user engagement on digital platforms. Supercharged with deep learning for further accuracy and effectiveness, IRL can enhance user experience by automatically tailoring content to reduce exposure to contradictory or disquieting information, minimizing discomfort or  friction . While effective, these algorithms are also widely blamed for fostering  echo chambers  and  filter bubbles  that reinforce users  existing beliefs. Legislation following the Cambridge Analytica scandal, which spotlighted the risks of filter bubbles for ideological polarization and behavioral manipulation, focused on strengthening data privacy protections but did not directly address risks of narrowing or distorting information on a wide scale. Other regulations attempting to address filter bubbles directly (the Filter Bubble Transparency Act of 2021) never passed. More recent initiatives in the U.S4. and E.U5. around AI focus on combatting new forms of misinformation like AI-generated deepfakes but continue to overlook subtler risks of information filtering that can critically undermine AI s transformative potential in sectors vital to society, where friction is essential for promoting reflective thinking and awareness of diverse viewpoints and biases. Algorithmic constraints on accessible information can lead not only to echo chambers but also to discrimination, bias, loss of decision autonomy, dependence on AI systems, and diminished human judgement. Further, as AI systems evolve into more independent  agents  capable of handling complex tasks with minimal oversight, the impacts of customization will increasingly manifest in the real world. Rather than confining these concerns to the realms of social media or politics, AI policymakers should consider a broader range of domains where AI customization increasingly shapes human exposure to information. Efforts to make AI inference more inclusive focus on mitigating algorithmic bias or balancing representation in training data sets; but more research and policy should focus on AI model and interface designs that encourage wide-ranging and analytical perspectives, especially for systems that significantly affect human safety and well-being. Below are three recommendations for AI development and policy initiatives to help cultivate  constructive friction  in AI systems and encourage the creation of AI that not only captivates but also fosters critical thinking and broad perspectives. Generative AI systems now allow users to ask questions in natural language and typically receive a  best fit  result in a seamless, conversational thread. As developers work to ensure responses are accurate, the demand for accuracy among consumers remains complex. While users may value accuracy, their online behavior suggests a stronger preference for information that aligns with their beliefs and preferences. Even this is not straightforward, as consumption habits are influenced by interface design features in ways that are not well understood6. The new proliferation of AI chatbots offers a natural testbed to experiment with these tendencies on a massive scale to identify forms of information delivery that promote critical evaluation of misinformation and consideration of alternative perspectives, without fundamentally compromising sustained user engagement. A wealth of decision science and behavioral economics literature can help to design such experiments. For example, empirical research shows that the way choices are presented (i.e.,  choice architecture 7) can significantly influence decision making. Presenting multiple distinct options aids informed decision-making, but an excess can increase cognitive load8. Further, humans imperfectly evaluate information and make decisions influenced by cognitive and emotional biases and contextual factors (i.e.,  bounded rationality )9. With generative AI systems rapidly replacing traditional search engines for building knowledge, policymakers and Big Tech should partner to promote consumption of diverse content, combat confirmation bias, and foster social unity. Strategies could include integrating alternative perspectives into outputs or using design features like accordion menus, pop-up windows, and progressive disclosure buttons to enhance awareness of contextual information. A vast array of experimental designs from user experience (UX) testing (e.g., A/B testing; conversion rate optimization; multivariate testing) can help to identify decision architectures with positive versus negative impacts on critical thinking. Not all customization is harmful. It is essential to differentiate between using algorithmic customization approaches (e.g., unsupervised learning, latent pattern mining, anomaly detection, dimensionality reduction, etc.) to discover verifiable knowledge versus to adapt the delivery of knowledge to ensure palatability and user acceptance. This distinction (illustrated in Fig. 1) can help evaluate the rationale behind customization and align it with application goals. This figure presents distinctions between AI systems customized for information discovery and those for information delivery across several dimensions and highlights the impacts of these AI design approaches on human perception. Consider an AI-powered radiology tool that identifies relevant areas of a medical image and tailors outputs in line with a radiologist s preferences and past interactions with the system. The design choice to emphasize physician engagement is based on an appreciation that the uptake and utility of an AI tool depends not only on its performance but also on the degree to which users trust it. Emerging research suggests that user trust is partly influenced by whether an AI system considers a user s preferred variables and ways of receiving or visualizing outputs10,11. Leveraging these insights, novel AI systems for medical imaging analysis (e.g., Aidoc; PathAI) use customization to adapt workflows to mirror how individual radiologists prioritize cases, highlight abnormalities, and generate reports. Similarly, medical  scribes  that automate physician notetaking (e.g., DeepScribe) can be personalized to adapt to physicians  speech patterns, medical terminology preferences, and summarization styles that match a physician s preferred level of detail and prioritized information, and perform selective information retrieval (e.g., from electronic health records) based on a physician s regularly referenced information. Clinician-facing medical chatbots (e.g., OpenEvidence) designed to deliver clinically relevant literature, evidence-based guidelines, and decision support summaries at the point of care are using customization to align outputs with patterns in a clinician s queries and preferred sources. Other emerging AI tools in healthcare (e.g., for algorithmic risk prediction; patient scheduling; patient monitoring early alert systems) likewise feature customized information delivery. While tailoring outputs may lead to greater uptake of tools that ultimately enhance diagnostic efficiency and precision, it may cause physicians to miss critical information if preferences do not reflect variables with the highest predictive value for treating disease, or if preferred outputs constrain a physician s ability to consider the wide range of information necessary to make informed decisions. Further, outputs from AI-based medical scribes may embody a physician s unique expertise but also their biases. These biases may then be documented and perpetuated in a patient s medical record in ways that shape (and potentially constrain) the perspectives and understandings of future physicians. We should be designing AI tools for healthcare and other high stakes fields that work to offset, not perpetuate, human biases. Given the variability in how physicians interpret and respond to clinical information, hospital governance as well as developers and policymakers should establish limits on AI tool s accommodation of user preferences, balancing trust, engagement, and other objectives like accuracy or comprehensiveness. Further, hospitals should require regular audits of AI performance impacts on clinical decision making and health outcomes to ensure critical variables with high predictive value are not overlooked in favor of user preferences. These checks and balances are likewise critical in other sectors where fairness, standardization, or impartiality are principal considerations (e.g., national security; legal rulings; standardized testing). In these contexts, developers should prioritize design elements that enable users to access  ground truths accurate, verifiable information that reflects reality (e.g., the severity of a patient s disease, the full range of security threats). Depending on its aim, customization can either lead us closer to or further from these ground truths, wielding a power to constrain human perception as much as it can expand it. Customization thus requires caution and intentionality, supported by rationales that align with responsible uses of AI across different domains and settings. While all forms of information delivery, even books, guide our attention, the distinguishing aspect of AI that demands closer scrutiny is the degree to which reliance on AI over other sources of knowledge will influence human perspectives over longer timescales. The widespread integration of generative AI marks the first time that humans are relinquishing responsibility and control over generating and curating knowledge to a non-human entity, with uncertain impacts on human progress. Intentionally programming these systems to restrict the information we ingest must be done with utmost care. While we have ample evidence of the harmful impacts of content filtering in social media, we do not yet know the harms of taking this approach with generative AI. Now is the time to anticipate such consequences. Developers could be incentivized to disclose the potential consequences of customization. Modeled after  social impact statements  that prompt developers to consider the effects of AI systems on different groups or individuals,  design impact statements  could encourage developers to transparently evaluate how curating information for engagement, acceptability or other reasons might inadvertently exclude important information or perpetuate existing biases. Developers could also highlight supplementary design choices to ensure that users are made aware of domain-critical information, even if it does not align with user preferences or past interactions. Clear documentation of personalization parameters across use cases can also help to build an evidence base of impacts. Generative AI stands out from previous iterations of AI for its significant potential to expand the boundaries of human cognition and transcend conventional modes of thought. Despite being trained on existing knowledge, GenAI can produce entirely novel content. It does this by using two main deep learning techniques, generative adversarial networks (GANs) and transformers, which together enable AI to create unique material that is not just a copy or remix of existing data. Various forms of reinforcement learning can encourage GenAI to  think outside of the box.  Building  intrinsic rewards  into a system, like rewarding novelty- and surprise-seeking, can encourage GANs to explore new possibilities and deviate from existing expectations. Similarly, shaping rewards can help to gradually shift an algorithm s focus towards more complex or unexpected outcomes. Incorporating other motivation-like mechanisms indirectly through novel loss functions, regularization methods, or (e.g., curiosity-driven) training strategies can encourage diversity and exploration. These approaches are what afford these systems their unprecedented potential to expand the frontiers of human knowledge and creativity. However, the drive to outwardly expand human knowledge lies in direct contradiction to the impulse that underlies personalization, which is to narrowly channel information in line with existing inclinations. AI policies should actively promote the capacity of AI systems to help humans critically reflect, evaluate and act on information in new ways, not according to the old ways. Design choices should encourage us to engage in unconventional or divergent thinking, consider alternative perspectives, and pursue explainability to better understand causality. For instance, new research12 suggests that introducing certain forms of  prompt engineering  (crafting user query inputs to guide model outputs) can both enhance AI effectiveness and bolster users  capacity to assess and gauge system performance. These include prompt strategies that steer AI systems to decompose complex queries into stepwise instructions (instruction generation) and to deliver outputs with stepwise explanations (chain of thought generation). Offering these intermediate reasoning steps on both sides (input/output) of a user s query can enhance transparency and explainability and enable critical evaluation of outputs. Such an approach aligns with recent policy suggestions that AI systems provide metadata about system inputs and outputs that permit users to rigorously assess a system s performance accuracy, reliability, and fairness. Scholars have proposed solutions like putting  nutrition facts labels  on AI outputs13, or implementing design approaches that decelerate heuristic thinking or mitigate cognitive and emotional biases that inhibit users from dispassionate evaluation of facts (e.g., racial bias) or an over-reliance on system results (e.g., automation bias)14. Interactive design features mentioned earlier, including expandable elements, dynamic filtering or resorting, or translation of information to other multimodal formats (e.g., text-to-image; image-to-sound) may serve mutual goals of exposing alternative perspectives while igniting human creativity and imagination. AI policy and development should look beyond the most nefarious forms of AI-generated misinformation to address more nuanced forms of knowledge curation that limit rather than expand human outlooks across the wide variety of domains where AI is present, from entertainment and politics to medicine, engineering, and space exploration. Vast societal resources are being dedicated to creating AI systems that augment human intellect, ingenuity and creativity. This may be better accomplished through incremental advancement towards objective ground truths, rather than uncritical celebration of subjectivity, relativism and satisfaction of personal preferences. Both trajectories can exist for AI. This paper offers actionable suggestions to balance preference-sensitive with preference-agnostic designs where truth matters most. Bender, E. M., Gebru, T., McMillan-Major, A. & Shmitchell, S.  On the dangers of stochastic parrots: Can language models be too big?.  In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, 610 623, Virtual Event, Canada ACM. https://doi.org/10.1145/3442188.3445922 (2021). Munkhdalai, T., Faruqui, M. & Gopal, S. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv:2404.07143 (Preprint) (2024). Replika. http://replika.com/ (2024). Biden, J. R. Jr. Executive Order 14110.: Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. Signed 10/30/2023. Federal Register on November 1, 2023 under document number 2023-24283, vol. 88, p. 75191. European Parliament and Council. (2024). Regulation (EU) 2024/1689 of 13 June 2024 laying down harmonised rules on artificial intelligence and amending certain Union legislative acts (Artificial Intelligence Act). Official Journal of the European Union, L 168, 12 July 2024, pp. 1 60. Shavit, Y. et al.  Practices for governing agentic AI systems.  Research Paper, OpenAI (2023). Kitchens, B., Johnson, S. L. & Gray, P. Understanding Echo chambers and filter bubbles: the impact of social media on diversification and partisan shifts in news consumption. MIS Q. 44, 1619 1649 (2020). Article Google Scholar Sweller, J. Cognitive load theory and educational technology. Educ. Technol. Res. Dev. 68, 1 16 (2020). Article Google Scholar Gigerenzer, G.  What is bounded rationality?  in Routledge Handbook of Bounded Rationality (Taylor & Francis Group, 2020). Schrills, T. & Franke, T.  Color for Characters - Effects of visual explanations of AI on trust and observability  in Artificial Intelligence in HCI (Springer, 2020). Aidoc. (2022). Next Gen Radiology AI: The Journey from an Algorithm to a Clinical Solution. Retrieved from https://www.aidoc.com/wp-content/uploads/Aidoc-Next-Gen-Radiology-AI-Whitepaper.pdf. Nori, H. et al. Can generalist foundation models outcompete special-purpose tuning? Ca se Study in Medicine. arXiv preprint arXiv:2311.16452 [Preprint] (2023). Gerke, S.  Nutrition Facts Labels  for Artificial Intelligence/Machine Learning-Based Medical Devices-The Urgent Need for Labeling Standards. George Wash. Law Rev. 91, 79 (2023). Google Scholar Quenet, K. K. & Gerke, S. AI in the hands of imperfect users. NPJ Digit. Med. 5, 197 (2022). Article Google Scholar Download references Center for Medical Ethics and Health Policy, Baylor College of Medicine, Houston, TX, USA Kristin M. Kostick-Quenet Search author on:PubMed Google Scholar K.M.K-Q. is the sole contributor to the conceptualization and writing of this paper. Correspondence to Kristin M. Kostick-Quenet. The author declares no competing interests. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions Kostick-Quenet, K.M. A caution against customized AI in healthcare. npj Digit. Med. 8, 13 (2025). https://doi.org/10.1038/s41746-024-01415-y Download citation Received: 06 June 2024 Accepted: 22 December 2024 Published: 07 January 2025 DOI: https://doi.org/10.1038/s41746-024-01415-y Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Health Care Analysis (2025) Minds and Machines (2025) Advertisement npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",2
"How is AI influencing the field of medicine? - University of California, Riverside",https://news.google.com/rss/articles/CBMif0FVX3lxTFB2bG5nTWhMSk1lLUp3M3BkdHZzanJqcVMzdkh1d01wMlFlOGFUbm12QXYtbFE3M1pYOE90eXYwbDA5Q0wzMEhEcXltRjFOaTg1OVBQeFV3Q0dqNjZsbm5QejROMi1od3pubWtqY0dQaHR2b05tWm9wMWhpYWZyVVE?oc=5&hl=en-US&gl=US&ceid=US:en,"Follow US: Internal medicine physician at UC Riverside shares his thoughts The words  artificial intelligence  are now ubiquitous, their influence having risen dramatically in recent years and their impact being felt in healthcare and many other job sectors. In healthcare, artificial intelligence, or AI, has the potential to make drug discovery faster and cheaper and improve diagnoses and treatments. How should AI ensure patient privacy and how can it be used fairly? How might AI improve patient care, increase efficiency, and reduce costs? These are questions that need thoughtful discussion. Dr. Koushik Kasanagottu, a public health expert and assistant clinical professor in the Department of Internal Medicine at the UC Riverside School of Medicine, addresses questions pertinent to AI in medicine in the Q&A below. Before joining UCR as a community-based faculty member, Kasanagottu was a fellow at Harvard Medical School and the U.S. Senate, where he honed his expertise in healthcare policy and innovation. He is a founding member and physician advisor for DocAide, a healthcare AI startup focused on transforming clinical decision making. AI has had an exponential growth in the healthcare industry. It is significantly transforming healthcare by streamlining administrative tasks and improving documentation. In my own practice, I use an AI scribe daily to document patient visits. This allows me to spend more face-to-face time with patients rather than focusing on extensive notetaking, improving patient experience and the quality of the encounter. However, these tools still require clinician oversight to ensure accuracy and quality. Looking ahead, I envision AI tools augmenting decision making, supporting clinical reasoning, and reducing diagnostic errors. Doctors and medical staff are increasingly integrating AI tools into various aspects of care. This is just the beginning. As we learn more about the technology and its utilization, we will be able to integrate it into clinical practice as clinical decision support tools. However, there are risks with integration and it will require clinician oversight. As we continue to develop and refine AI-driven tools like scribing software and clinical decision support, we re also seeing AI systems that can analyze imaging, predict disease progression, and recommend treatment plans based on vast datasets. In the future, AI will likely become an integral part of both clinical decision making and patient management, supporting healthcare providers in real-time and reducing the healthcare burden, particularly in underserved areas. AI can have a significant role in reducing healthcare disparities by helping provide expert-level care in remote locations where specialists are scarce. However, there are risks. As AI is only as good as the data it is trained on, underserved and marginalized groups can be underrepresented in the dataset resulting in bias and inaccuracies. Regulating AI in medicine requires a balance between encouraging innovation and ensuring patient safety. It will be essential for regulatory bodies like the Food and Drug Administration (FDA) to establish clear, standardized guidelines for AI validation, ensuring that these technologies are thoroughly tested for accuracy and safety before deployment in clinical settings. We also need a set of standards that new AI products have to meet before they are integrated into clinical practice. The FDA is currently proposing several regulatory frameworks to ensure the safe deployment of AI products in clinical products. The ongoing monitoring of AI systems post-deployment will also be crucial to assess long-term effectiveness and mitigate any unforeseen risks. There needs to be robust testing and monitoring for accuracy throughout the AI s usage beyond patient and provider satisfaction scores. AI can play a pivotal role in both diagnosing diseases earlier and predicting patients  future health risks. For instance, AI algorithms can analyze patient data; identify patterns in lab results, imaging, and genetic information; and suggest early interventions for chronic diseases like diabetes or heart disease. There are several startups and technology companies innovating in this space to provide predictive and clinical decision tools. A study from the Lancet showed that AI-based tools are improving early cancer detection by analyzing medical imaging with remarkable accuracy, helping clinicians catch conditions in earlier, more treatable stages. While AI will not replace doctors, it will undoubtedly change how we practice medicine by handling repetitive tasks and augmenting clinical decision making. For example, AI can support diagnostic accuracy in imaging, help with routine data entry, and assist in treatment planning, which allows physicians to focus more on direct patient care. However, the human element of patient care, empathy, and clinical judgment will always be essential. There are significant ethical concerns regarding AI. I am deeply concerned about data privacy, algorithm bias especially in underserved populations, and how the reliance on this technology could worsen health disparities. In particular, AI systems may inadvertently perpetuate existing biases in the data they are trained on, which could lead to disparities in care. One of the ongoing ethical challenges is ensuring that AI tools are developed and tested in a way that represents diverse populations. Yes, there are potential patient safety risks and privacy concerns with the use of AI in healthcare. Safety risks might arise if AI systems make incorrect recommendations, potentially leading to misdiagnosis or delayed treatment. For example, an AI system might miss a rare condition or flag a benign issue as a serious problem, affecting clinical decision making. On the privacy side, AI relies on vast amounts of patient data, raising concerns about data breaches or misuse. It is critical to ensure that AI systems comply with strict privacy laws like HIPAA and that the data used to train these systems is anonymized and ethically sourced. Header image credit: Victor Perry, UC Riverside. 900 University Ave. Riverside, CA 92521 Tel: (951) 827-1012 900 University Ave. Riverside, CA 92521 tel: (951) 827-1012 email: webmaster@ucr.edu Find Us Follow US: This website uses cookies to enable essential site functionality, improve your experience, and analyze website traffic. By clicking ""Accept"", you agree to our website's standard and known use of collected information as described in our privacy statement.",2
Healthcare adopts AI with care - Healthcare IT News,https://news.google.com/rss/articles/CBMid0FVX3lxTFBEY3NPSDRzTG9wdm9yaV9DSHEtU2NXR2hMMzRobjB3d3A0RWtnZUM4RHNBeEFwOTR5TUxqM09PQnAzRnpFbTlNNTJiRTIzbzhETERzTDlyclJJVGppWnNsTzVOLWlKbzR4TUtsNGt2SkM4c3l1Vl9F?oc=5&hl=en-US&gl=US&ceid=US:en,"The global AI healthcare market, valued at USD 29.01 billion in 2024, is projected to reach USD 504.17 billion by 2032. In Europe alone, the market is expected to grow from USD 7.92 billion in 2024 to USD 143.02 billion by 2033, with an incredible 38% annual growth rate. The growing adoption underscores AI's significant potential across many areas of healthcare: It can enhance the accuracy and early detection of diseases, support personalised treatment plans, streamline administrative tasks such as billing and scheduling, and improve hospital resource management through predictive analytics. In clinical practice, AI is already showing impact in areas such as early detection of sepsis and improved breast cancer screening. As Antoine Tesni re, a professor of medicine at and managing director of PariSant  Campus, noted in an interview with HIMSS TV: ""AI is a true revolution for healthcare. AI tools allow us to understand that we will have super-precise, super-productive, super-preventive, super-personalised approaches in the very near future."" AI is advancing beyond merely assisting clinicians make decisions. ""The level of performance is approaching the human one as of today, but it will surpass the human level of performance, bringing new horizons for overall performance of healthcare,"" Tesni re said. Despite growing enthusiasm, significant concerns remain. ""Bias can impact clinical decision-making and patient care when we deploy algorithmic tools,"" said Dr. Jessica Morley, postdoctoral researcher at the Yale Digital Ethics Center, in a HIMSS TV interview. She points to current limitations in systems such as arrhythmia detection devices that typically do not work as well on people with darker skin and melanoma recognition algorithms failing across diverse populations. Morley also identifies a deeper systemic issue that she calls the ""inverse data quality law"": ""Where you have the greatest need, you often have the lowest availability of high-quality data."" This fundamental challenge means that creating equitable AI systems requires addressing both technical limitations and governance obstacles. Despite these obstacles, Morley remains optimistic that the right approaches can overcome the current challenges. She believes innovations like secure data environments offer a viable path forward: ""It is entirely possible to still protect individual patients' data and still leverage group-level population health benefits. You can have your cake and eat it, too,"" she claims. To address the AI challenges, the European Union has established two landmark regulatory frameworks to ensure healthcare AI development balances innovation with ethics, transparency, and respect for fundamental rights. The EU Data Act aims to improve access to data generated by connected medical devices, helping to create more diverse and representative datasets while reducing the risk of algorithmic bias. Meanwhile, the EU AI Act sets out clear requirements for high-risk AI systems in healthcare, introducing safeguards such as mandatory impact assessments, human oversight, explainable AI models and data verification. Together, these frameworks seek to support an environment where healthcare AI can deliver precise and personalised care while maintaining trust, fairness and accountability. Antoine Tesni re, managing director of PariSant  Campus, and Dr. Jessica Morley, postdoctoral researcher at the Yale Digital Ethics Center, will be speaking at HIMSS Europe 2025, taking place in Paris 10-12 June. Newsletter Signup Thank you! Your submission has been received.   current_year Healthcare IT News is a publication of HIMSS Media",2
Overcoming regulatory barriers to the implementation of AI agents in healthcare - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTE5EWl9BbmdLd3JfU21ZWWdkcnZLNk5WV3BabEt4MUExYzdIRHZic01ZMGFZb2lKaU1XSWEtbmVfaHE4M0pBemFLZW1OSkd5d1JMNkxWQ1JfbUYyc2NPRDVB?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement Nature Medicine (2025)Cite this article 629 Accesses 99 Altmetric Metrics details To facilitate the safe and effective implementation of autonomous artificial intelligence agents in healthcare, regulatory frameworks must evolve beyond static device paradigms to incorporate adaptive oversight and flexible pathways. This is a preview of subscription content, access via your institution Access Nature and 54 other Nature Portfolio journals Get Nature+, our best-value online-access subscription 27,99   / 30 days cancel any time Subscribe to this journal Receive 12 print issues and online access 195,33   per year only 16,28   per issue Buy this article Prices may be subject to local taxes which are calculated during checkout Freyer, O., Wiest, I. C. & Gilbert, S. Mayo Clin. Proc. Digit. Health 3, 100196 (2025). PubMed PubMed Central Google Scholar Zeng, D., Qin, Y., Sheng, B. & Wong, T. Y. JAMA 333, 1866 1869 (2025). PubMed Google Scholar Freyer, O., Wiest, I. C., Kather, J. N. & Gilbert, S. Lancet Digit. Health 6, e662 e672 (2024). CAS PubMed Google Scholar Moritz, M., Topol, E. & Rajpurkar, P. Nat. Biomed. Eng. 9, 432 438 (2025). PubMed Google Scholar Zou, J. & Topol, E. J. Lancet 405, 457 (2025). PubMed Google Scholar Anthropic. Building Effective AI Agents (2024); https://go.nature.com/44tZMYl OpenAI. A Practical Guide to Building Agents (2025); https://go.nature.com/4l8ugWL Gilbert, S., Dai, T. & Mathias, R. NPJ Digit. Med. 8, 165 (2025). PubMed PubMed Central Google Scholar Qiu, J. et al. Nat. Mach. Intell. 6, 1418 1420 (2024). Google Scholar Gilbert, S. et al. Sci. Adv. 11, eadv7719 (2025). PubMed PubMed Central Google Scholar Gottlieb, S. JAMA Health Forum 6, e250289 (2025). PubMed Google Scholar De Grandis, G., Brass, I. & Farid, S. S. Regul. Gov. 17, 810 832 (2023). Google Scholar DuPreez, J. A. & McDermott, O. Expert Rev. Med. Devices 22, 261 275 (2025). CAS PubMed Google Scholar Ayers, J. W., Desai, N. & Smith, D. M. JAMA 331, 639 640 (2024). PubMed Google Scholar Blumenthal, D. & Patel, B. NEJM AI 2, 7 (2024). Google Scholar Download references During the preparation of this work, the authors used DeepL (DeepL SE), Grammarly (Grammarly, Inc.) and ChatGPT (in version GPT-4o; OpenAI, Inc.) to improve the grammar, spelling and readability of the manuscript. After using these tools and services, the authors reviewed and edited the content as needed and take full responsibility for the content of the publication. J.N.K. is supported by the German Cancer Aid (DECADE, 70115166), the German Federal Ministry of Education and Research (PEARL, 01KD2104C; CAMINO, 01EO2101; TRANSFORM LIVER, 031L0312A; TANGERINE, 01KT2302 through ERA-NET Transcan; Come2Data, 16DKZ2044A; DEEP-HCC, 031L0315A; DECIPHER-M, 01KD2420A; NextBIG, 01ZU2402A), the German Academic Exchange Service (SECAI, 57616814), the German Federal Joint Committee (TransplantKI, 01VSF21048), the European Union s Horizon Europe research and innovation program (ODELIA, 101057091; GENIAL, 101096312), the European Research Council (ERC; NADIR, 101114631), the US National Institutes of Health (EPICO, R01 CA263318) and the National Institute for Health and Care Research (NIHR, NIHR203331) Leeds Biomedical Research Centre. The views expressed are those of the author(s) and not necessarily those of the National Health Service, the NIHR or the Department of Health and Social Care. This work was supported by the European Commission under the Horizon Europe Program, as part of the project CYMEDSEC (101094218), and by the European Union. The views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union. Neither the European Union, nor the granting authorities, can be held responsible for them. Responsibility for the information and views expressed therein lies entirely with the authors. This work was supported by the German Federal Ministry of Education and Research (BMBF) as part of the Zunkuftscluster SEMECO (03ZU1210BA). These authors contributed equally: Jakob N. Kather, Stephen Gilbert. Else Kr ner Fresenius Center for Digital Health, TUD Dresden University of Technology, Dresden, Germany Oscar Freyer, Sanddhya Jayabalan, Jakob N. Kather & Stephen Gilbert Department of Medicine I, University Hospital Dresden, Dresden, Germany Jakob N. Kather Department of Medical Oncology, National Center for Tumor Diseases (NCT), Heidelberg University Hospital, Heidelberg, Germany Jakob N. Kather Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Correspondence to Stephen Gilbert. O.F. has a leadership role and holds stock in WhalesDontFly GmbH, and has had consulting relationships with Prova Health Ltd. J.N.K. declares consulting services for Bioptimus, France; Panakeia, UK; AstraZeneca, UK; and MultiplexDx, Slovakia. Furthermore, he holds shares in StratifAI, Synagen and Ignition Lab (all in Germany); has received an institutional research grant by GSK; and has received honoraria by AstraZeneca, Bayer, Daiichi Sankyo, Eisai, Janssen, Merck, MSD, BMS, Roche, Pfizer and Fresenius. S.G. is an advisory group member of the Ernst & Young-coordinated  Study on Regulatory Governance and Innovation in the field of Medical Devices  conducted on behalf of the Directorate-General for Health and Food Safety of the European Commission. S.G. has or has had consulting relationships with Una Health GmbH, Lindus Health Ltd, Flo Ltd, Thymia Ltd, the FORUM Institut f r Management GmbH, High-Tech Gr nderfonds Management GmbH and Ada Health GmbH, and holds share options in Ada Health GmbH. Reprints and permissions Freyer, O., Jayabalan, S., Kather, J.N. et al. Overcoming regulatory barriers to the implementation of AI agents in healthcare. Nat Med (2025). https://doi.org/10.1038/s41591-025-03841-1 Download citation Published: 18 July 2025 DOI: https://doi.org/10.1038/s41591-025-03841-1 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement Nature Medicine (Nat Med) ISSN 1546-170X (online) ISSN 1078-8956 (print)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",2
AI is already touching nearly every corner of the medical field - Fortune,https://news.google.com/rss/articles/CBMiiAFBVV95cUxOYlg2Z0F6Z2ZCWEZUWmYtekVVUjFqT3NBcmdrUEgxamQ1UmN4d0VOQzN5dGdsUTh1blZQYWd6TXNBU0YxOXFNZjJQbGhrT0pqRXpoY2Z0Rk9YWnVPb1hsa1RraXJ4enZZc0YyOUFEY2FZZEFQY2U0TzJDY3BvZl94c2tBR3FjV1l5?oc=5&hl=en-US&gl=US&ceid=US:en,"Sage Lazzaro is a technology writer and editor focused on artificial intelligence, data, cloud, digital culture, and technology s impact on our society and culture. When ChatGPT entered the conversation a few years ago, Lloyd B. Minor, a surgeon and the dean of the Stanford School of Medicine, sought to test its ability by asking it about a rare inner ear condition called superior canal dehiscence syndrome. Minor had personally discovered the syndrome, wrote the first paper on it, and developed a surgical procedure to fix it, naturally making him an expert on the subject. The response, he says, was not only concise, accurate, and logical, but as good as anything he could have written.  The way it assimilated the information and presented it was really quite remarkable. And that s when I realized this is not just an incremental advance. [Large language models] are going to fundamentally change the way we access, learn, and embrace knowledge   and I think this is a fundamental shift for medicine,  Minor says. It s a sentiment felt widely across the medical field as AI starts to touch nearly every corner of medical research, physician training and processes, and patient care. The rollout of AI across health care is not without challenges, but in some cases, the impact is already being realized. Perhaps the most visible way AI is impacting the industry is in medical research. Machine learning model AlphaFold2, unveiled by Google DeepMind in 2020, made it possible to predict the structure of virtually all the 200 million proteins that researchers have identified, opening up avenues to design novel proteins and streamlining the first step in drug research. The model has since been used by more than 2 million people from 190 countries, earned its creators the Nobel Prize, and ignited a boom of new biotech startups. Last year, the first AI-targeted and AI-designed drug an idiopathic pulmonary fibrosis treatment developed by startup Insilico Medicine reached Phase II clinical trials, marking a new milestone for the technology. While no AI-developed drug has made it to patients just yet, the advancements in research processes are already making a material difference.  Recent advancements in artificial intelligence are revolutionizing active compound discovery by dramatically enhancing the efficiency, accuracy, and scalability previously challenged by traditional methods,  reads a paper exploring the impact AI is having on drug discovery, published last month in Nature by a dozen researchers. The research impacts aren t limited to drug discovery, however. An AI model developed by Cambridge researchers outperformed clinical tests in predicting the progression of Alzheimer s disease, in one example. Another out of Harvard was found to accurately detect multiple types of cancers, assess treatments, and predict survival rates. In June, Google DeepMind followed up its line of scientific research models with AlphaGenome, a model designed for a wider swath of biology research by predicting how single variants or mutations in human DNA sequences can impact biological processes regulating genes. The parts of the medical industry being most immediately impacted by AI, according to Wiljeana Glover, a researcher focused on health care innovation and improvement at Babson College s Kerry Murphy Healey Center for Health Innovation and Entrepreneurship, are the backend processes. For example, large language models are being used in emergency departments to quickly create synopses of complex medical records, and to record and transcribe conversations between providers and patients to generate medical documents.  Things that we re taking off of the clinicians  plate, that are more administrative, I think those are some of the places where we see AI moving really quickly,  she says. According to a 2020 study, physicians spend 33.4% of their work-related time on non-patient-facing health records and administrative tasks. They then spend an average of two hours per day on these tasks outside their official work hours including on weekends, making administrative tasks a leading cause of the burnout plaguing the medical field. Reducing this load not only can give providers more time to focus on direct patient care, but improve the patient experience as well for example, allowing physicians to make eye contact with patients during visits rather than typing notes. By improving access to information and thus treatment AI can have a notable impact on the physician and patient experience. Large language models allow physicians to more easily query databases containing assimilated knowledge to research potential treatment paths for complex cases. Physicians are also increasingly using AI-powered patient care apps alongside wearables to support ongoing treatment for chronic conditions like hypertension and diabetes, providing more information about the patient s health outside of in-person visits and increasing communication, Glover says. In some cases like radiology, health professionals are now regularly working alongside AI as a routine part of their work. Radiologists are using AI to sharpen images, identify abnormalities, and discern subtle changes in areas of concern essentially using it like a magnifying glass to uncover more information to then evaluate with their expertise. One AI model developed by Northwestern University researchers that can evaluate breast cancer tissue from digital images was even found to perform better at predicting the future course of a patient s disease than expert pathologists, which could help spare patients unnecessary chemotherapy and reduce the duration and intensity of treatments. Even prior to large language models, AI was starting to change what part of physicians  knowledge base they had to access in their daily work. Minor said that when he was in medical school, they had to memorize medication dosages and indications. Now, they use AI algorithms to optimize dosages based on patient and drug data. A paper in BMC Medical Education says AI has come to play  a crucial role in dose optimization and adverse drug event prediction, offering significant benefits in enhancing patient safety and improving treatment outcomes.  As AI increasingly impacts how physicians work, medical schools are updating how they train doctors. George Washington School of Medicine and Health Sciences will be offering a new course this fall on artificial intelligence applications in health care, and the Icahn School of Medicine at Mount Sinai said it will start giving all students access to ChatGPT Edu along with training on how to use it. The Stanford School of Medicine, Minor says, is in the early stages of reimagining its entire curriculum in light of the advances in generative AI, and the school has formally appointed one of its MD-PhDs in AI to lead its exploration. Minor emphasizes that he s not suggesting the field move away from requiring a base of medical knowledge, but that learning how to deploy generative AI to augment the ability of physicians to keep up-to-date and bring relevant knowledge to each individual patient needs to be explored educationally.  We want to be a pace-setter. We also don t abandon the essential elements of medical education. But I think defining what those essential elements are should be something we do in a thoughtful way, rather than just assuming that what everyone had to learn in the past, and how they have to learn it, is going to apply moving forward,  he says. AI is also emerging as a useful training tool in its own right. Researchers at New Jersey Institute of Technology, Robert Wood Johnson Medical School, and software company Robust AI developed an AI-powered program that allows students to simulate the basic movements of laparoscopic surgery and provides feedback so they can continuously improve. A recent study from this year showed the program is as good and even slightly better than faculty human evaluators at rating surgical skills. The wide variety of uses for AI within the medical industry isn t without challenges, drawbacks, and issues that need to be solved. Even with all the potential for research breakthroughs, drugs being developed with AI still need to be tested in wet labs using physical samples, go through clinical trials, and gain FDA approval. The medical documentation systems being popularized by companies like Augmedix, Abridge, and Microsoft-backed Nuance bring the risk of error and hallucination (though it s worth pointing out that humans especially ones who are burned out make errors, too). Algorithms designed to detect illness and injury, and make care decisions, have historically underperformed with women and people of color, largely because of these groups  underrepresentation in the datasets used to train such AI systems. There are also privacy concerns surrounding the training of AI models with patient data.  I think we re moving cautiously, and I think the caution is ideal,  says Glover.  But absolutely, the way that patients seek, reach, and receive care will be impacted by AI. It s happening. It s here.  Read more about AI s Long Reach Across New Industries, in the latest Fortune AIQ special report, a collection of stories detailing how businesses across virtually every industry are putting AI to work and how their particular field is changing as a result of the technology. Sage Lazzaro is a technology writer and editor focused on artificial intelligence, data, cloud, digital culture, and technology s impact on our society and culture.   2025 Fortune Media IP Limited. All Rights Reserved. Use of this site constitutes acceptance of our Terms of Use and Privacy Policy | CA Notice at Collection and Privacy Notice | Do Not Sell/Share My Personal Information FORTUNE is a trademark of Fortune Media IP Limited, registered in the U.S. and other countries. FORTUNE may receive compensation for some links to products and services on this website. Offers may be subject to change without notice. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work. You may exercise your right to opt out of the sale or sharing of personal information by using this toggle switch. We collect this data for analytics and to personalize your experience, allowing us to show you tailored offers, ads or content. Note that if you opt out, this may affect our ability to personalize ads according to your preferences. You may still see ads, but they may not be personalized to you. These cookies are set by a range of social media services that we have added to the site to enable you to share our content with your friends and networks. They are capable of tracking your browser across other sites and building up a profile of your interests. This may impact the content and messages you see on other websites you visit. If you do not allow these cookies you may not be able to use or see these sharing tools. Cookies of this category may be set on our website and are used to display personalized content to you that is believed to be in line with your interests. They are based on uniquely identifying your browser and internet device. For example, these cookies help us to provide information to you that is especially relevant to you. These cookies allow us to count visits and traffic sources so we can measure and improve the performance of our site. They help us to know which pages are the most and least popular and see how visitors move around the site. If you do not allow these cookies, we will not know when you have visited our site and will not be able to monitor its performance.",2
4 Ways Artificial Intelligence is Poised to Transform Medicine - UC San Francisco,https://news.google.com/rss/articles/CBMingFBVV95cUxOTUMtQlFWaDdxV0V5OEtwTlJzLWJqRk9XQks0OXFwSjdnS1BUZ2ZjRFM0TWVKd3JHNXJhaWk0NjJIZFJwbU5nRUJ4UUpsOXpBRkxtSXBERUxINFBhNUdKTnBCVU9CRnhNQTRmekFXdmpKNmNLVURVemc3dnlvdEtoSDY0SkstWTQwUThiRmo4R09vQllCYkh0LXFhcUNQUQ?oc=5&hl=en-US&gl=US&ceid=US:en,"University of California San Francisco By Laura L pez Gonz lez The radiologist was dead. Or at least that s what artificial intelligence (AI) experts prophesized in 2016 when they said AI would outperform radiologists within the decade. Today, AI isn t replacing imaging specialists, but its use is leading health care providers to reimagine the field. That s why UC San Francisco was among the first U.S. universities to combine AI and machine learning with medical imaging in research and education by opening its Center for Intelligent Imaging. Take a look at how UCSF researchers are pioneering human-centered AI solutions to some of medicine s biggest challenges. Coronary artery disease is among the leading causes of adult deaths worldwide. Caused by a build up of fatty deposits in the arteries, the disease is a common cause of heart attacks. Physicians commonly use a test called a coronary angiogram to diagnose the condition. As part of angiograms, physicians inject a special dye into the main vessels that feed the heart to see how blood flows using x-rays. The heart s left ventricle is the heart s main pumping chamber but coronary artery disease can damage it. Patients with suspected severe coronary artery disease undergo angiograms, but may also need additional testing with even more dye that can harm the kidneys. New research by UCSF cardiologist Geoff Tison, MD, MPH, and team is among the first to successfully use machine learning to estimate how well the left ventricle is pumping by analyzing standard angiogram videos that are already obtained from the coronary angiogram procedure. This provides information about the heart s function without requiring additional procedures or risk. The research, supported by the National Institutes of Health (NIH), could eventually give physicians and patients a quicker and less dangerous way to diagnose damage to the left ventricle. Tison and team trained a type of AI model called a deep neural network on anonymized angiogram videos recorded at UCSF. Deep neural networks are able to learn complex patterns in data such as images and videos, some of which are not readily apparent to humans. The team s model, dubbed CathEF, accurately predicted how well the left ventricle pumped when researchers compared the results to measurements of pump function taken from ultrasound. CathEF performed just as well when the team later tested it outside the lab, in a Canadian hospital.  CathEF offers a novel approach that leverages data routinely collected during every angiogram to provide information that is not currently available to clinicians,  said Tison.  Our model effectively expands the utility of medical data with AI with real-time information to inform clinical decision-making.  Tens of thousands of Americans suffer pneumothoraces, a type of collapsed lung, annually. The condition is caused by trauma or lung disease   and serious cases can be deadly if diagnosed late or left untreated. This type of collapsed lung is difficult to identify: The illness can mimic others both in symptoms and in x-rays, in which only subtle clues may indicate its presence. Meanwhile, radiologists must interpret hundreds of images daily, and some hospitals do not have around-the-clock radiologists. UCSF researchers created the first AI bedside program to help flag potential cases to radiologists. In 2019, the tool was the first AI innovation of its kind to be licensed by the U.S. Food and Drug Administration. Today, it s used in thousands of GE Healthcare machines around the world. Researchers from the Department of Radiology and Biomedical Imaging created a database of thousands of anonymous chest X-rays. Some of these images showed cases of collapsed lungs and others not. Next, researchers trained the AI tool on this database before testing it on thousands of other images to ensure it could flag potential cases accurately. The AI screener works with portable X-ray machines, so doctors can use it right at a patient s bedside without making major infrastructure investments.  I think of this as an additional safety check that can deliver diagnoses and patient care sooner,  explained Associate Chair for Translational Informatics John Mongan, MD, PhD, who codeveloped the AI algorithm with Radiology Professor Andrew Taylor, MD, PhD. Mongan is also a director of the Center for Intelligent Imaging. Magnetic Resonance Imaging (MRI) is particularly useful for studying the soft tissues that make up our livers, hearts and brains. Unlike X-rays, MRIs can produce finely detailed images of these organs and, in the case of the brain, help physicians detect tumors, subtle signs of strokes and changes over time. Most MRIs in the U.S. are performed with lower resolution 1.5T (Tesla) or 3T MRI systems that may miss signs and symptoms of conditions like multiple sclerosis and traumatic brain injury. Stronger 7T machines, which produce higher resolution images, could help, but their high cost is why only about 110 were in use globally as of 2022. The solution: UCSF Assistant Professor of Neurology Reza Abbasi-Asl, PhD, led a team that used a form of AI to enhance the resolution of standard MRIs featuring traumatic brain injuries. The technique dramatically improved 3T MRI images, putting them roughly on par with 7T images, while outperforming other types of AI-enhanced MRIs. These results may, one day, help improve care for those with traumatic brain injuries and other neurological conditions. Abbasi-Asl and team constructed small, anonymous databases of pairs of traumatic brain injury MRIs. Each pair contained MRIs of the same injury: one low-resolution, 3T version, and another high-resolution, 7T version. The team created machine learning models that connect bits of information based on data patterns to boost low-resolution images before comparing them to their high-resolution partners. The outcomes of these models identified patterns and features that were hard to detect for the human eye in 3T MRIs, using them to understand how to improve image quality   boosting specific details while minimizing  noise  like grainy specks.  Our findings highlight the promise of AI and machine learning to improve the quality of medical images captured by less advanced imaging systems,  Abbasi-Asl said. As many as one million Americans live with Parkinson s Disease, a degenerative nervous system disorder that affects movement, causing symptoms like tremors, stiffness and poor balance. To make the best treatment decisions, physicians need to understand how a patient s symptoms progress. Currently, doctors struggle with a gap in this data, relying on patients  accounts and observed changes between spread-out appointments to detect subtle changes in walking or the ability to tap a finger. Associate Neurology Professor Simon Little, MBBS, PhD, and Assistant Neurology Professor Reza Abbasi-Asl, PhD, used machine learning to build a system that could capture changes in patients  gaits and hand movements from smartphone and digital camera recordings. Although still in early development, the research could, in the future, allow physicians to monitor patients with a range of neurodegenerative diseases at home, providing more precise data for tailored treatments. It may also reveal new insights into how movement changes may predict the course of a disease. As part of their trial, the team recruited volunteers with Parkinson's Disease from the UCSF Movement Disorder and Neuromodulation Center. Using digital cameras, researchers filmed the participants walking and tapping their index finger, common clinical exam techniques. Machine learning programs processed the videos, identifying the most clinically relevant features like the speed of a finger tap that might indicate a more severe disease stage.  We ve kind of been conducting some areas of medicine, broadly, in the same way for the last 100 years: We see patients and talk to them. We do an examination in clinic and then we try and make an adjustment of some of their treatments,  Little explained.  We are at this transformational point, moving from old-fashioned, subjective views of patients to a digital transformation. I m hopeful that, within five years, this type of approach will be more common in clinical practice.  Subscribe to UCSF News Visit the Media Center   2025 The Regents of The University of California",1
New AI Class Helps Medical Students Dissect Artificial Intelligence - University of Miami,https://news.google.com/rss/articles/CBMimwFBVV95cUxNMDhqNWh4R0VXSVpSMm5ua0oyUFY2aDJFOUxzcVBJNmNCX3daVFdiaEo2Q3czVlU0THF2bnJuWF9BWmJGT3V1dmxzb0dzdlVHdzNIWHV3d1JTQUoxQjFUQktiWWxrb3lxUmFJeFBQRHZyZ295NC1PMjl5aDB0dWJia1ltN0lBRElMOGwtSUVCTzFuU3EwQU8ydktucw?oc=5&hl=en-US&gl=US&ceid=US:en,"Home / News / Medical Education / Medical Education Artificial intelligence (AI) is a promising technology that is already having a major impact on medicine. Advanced AI algorithms are being used to detect cancer, take notes during patient visits, summarize the latest medical literature and much more. To help students keep up with this rapidly evolving tool, the University of Miami Miller School of Medicine recently added an AI elective to its combined M.D./M.P.H. program. The course,  Introduction to AI in Medicine and Public Health,  offers a comprehensive look at AI s potential, as well as its limitations.  Seeing what s happening with AI, especially since the onslaught of generative AI, especially large language models, it s really exploded into people s consciousnesses,  said Shirin Shafazand, M.D., professor of medicine and director of the M.D./M.P.H. program at the Miller School.  We need to ensure that our students have the appropriate skills to use AI and understand what these tools actually do.  Dr. Shafazand designed the course to give M.D./M.P.H. students and faculty better information to effectively use AI and understand its inner workings. The course introduces students to numerous AI concepts, including:   Machine learning (which underpins prediction and prognosis models in medicine)   Deep learning (a type of machine learning)   Neural networks (AI inspired by the human brain)   Natural language processing (which allows computers to  read  and understand human language and generate appropriate responses)   Prompt engineering (which designs AI queries to retrieve the best responses) The class dove into ethics, cybersecurity, government regulation and much more. Students learned from experts in academia and industry at the forefront of AI innovation. To cap it off, students were asked to complete a project, such as developing their own AI agent   a tool that can perform specific tasks without human intervention.  The students came up with some really cool AI agents,  said Dr. Shafazand.  Some were very practical and reflected their interests at this stage of their careers. Many focused on patient education and enhancing medical knowledge. It was nice to see how the students were thinking.  For many of the students, the class was more than gathering knowledge. It was an ethical imperative.  AI is only going to become further integrated into society and our practice, and I feel like it s my responsibility, as a future physician, to get ahead of the curve, see the landscape and understand the ethical implications,  said Alex Pedowitz, a member of the Miller School s Class of 2026.  How is it already being used in clinical practice, and what can I do as an individual provider to advocate for better and responsible patient care using this technology?  The class provided unique opportunities to see under the AI hood. While they had all heard of these tools, and maybe dabbled in ChatGPT, this deep dive was eye-opening.  We started out not knowing how the information was stored or how the data was utilized,  said Chase DeLong, also part of the Class of 2026.  By the end of the course, we understood how difficult it is to create these systems, train them and get one to an operational level while creating the necessary safeguards to protect patient information and identity.  Protecting patient data is always a top priority, but making a privacy-compliant AI tool is no easy task. AI can seem incredibly smart, but algorithms lack intuition and can make mistakes. For example, AI can have trouble differentiating between telephone numbers and Social Security numbers, a privacy nightmare. Still, the technology offers many advantages, and it makes sense for medicine and industry to do the heavy lifting to overcome these barriers. AI-powered notetaking apps can automate electronic health record entries, giving physicians more face time with patients.  The chief medical informatics officer for UHealth, Maritza Suarez, M.D.  04, was one of the guest speakers and demonstrated an ambient scribe, which transcribes and summarizes conversations, live in class,  said Haikel Haile of the Miller School Class of 2026.  It was really interesting to see it take an eight-minute conversation and actually make it into a good note.  In addition to hearing about AI s inner workings and potential downsides, the students learned better ways to use the technology. Prompt engineering was a particularly a popular session.  Prompt engineering can be a really helpful tool to automate tasks and get more streamlined results,  said Pedowitz.  That s something I ll definitely use in the immediate future to practice for residency interviews and articulate my thoughts. Later on, as my classmates have already demonstrated with their projects, it can be really useful for patient care.  The students came out of the class feeling both empowered and forewarned. They know that AI is the future, but it should also be treated with extreme caution.  We cannot let our human judgment be replaced by AI,  said Dr. Shafazand.  Critical appraisal must always be in play. I want that to be a key takeaway for our M.D./M.P.H. students. Ask the questions and look at AI implementation from all angles. How will it impact society, patients, the health care system and costs? Develop a framework for evaluation and change management and then implement that AI tool into the health care system.  Tags: AI, artificial intelligence, Department of Medical Education, Dr. Shirin Shafazand, M.D./M.P.H., medical education, medical students, technology This article was printed from The Miller School of Medicine Medical News at the following URL: https://news.med.miami.edu/new-ai-class-helps-medical-students-dissect-artificial-intelligence/ Copyright   2025 University of Miami Health System Notifications",2
Artificial intelligence in medicine and everyday clinical practice - Technical University of Munich,https://news.google.com/rss/articles/CBMi0gFBVV95cUxNQmVfR0VRclVpLW1RbmlzLXkyRVJudmZBTmV0S3dnQ2E5TVFSeFVHVzBqNHdHZzNYMUV2UGNoVXlzMzRtZ3VveTc2aV9FVlVYa2YxM2RCUWpUMk5LR0J3NEpOR1dmLXdTUV82Q001UlZFZ1RVZVNtWlNQaWdRS2tza2N4RXAyUkNtUkFfODNlLVc2STE3dW9WbWpWQjZDZFFWazIzZUNCVEFRWUdLeHE4T2Y5LVVCbkoxU3JkRWNpTWllYnI4c1ZteWkyTTRSamNTN1E?oc=5&hl=en-US&gl=US&ceid=US:en,"If you use one of the color modes, the TUM website and its elements will be displayed in either dark or light. The settings are stored on your computer and not transferred to the server. Artificial intelligence in medicine and everyday clinical practice Artificial intelligence (AI) is also used in the medical field, to evaluate data or search for patterns in large amounts of data. Researchers at Technical University of Munich (TUM) are working on making these human-AI collaborations safe, reliable and efficient. AI doctors who make independent diagnoses are still a vision of the future, but AI applications for medicine can already bring a wide range of opportunities: Diagnoses could be made faster, easier and more error-free or therapies could be more targeted. In addition, more efficient processes in day-to-day clinical practice would give staff more time for patient care. Daniel R ckert is Professor of AI in Healthcare and Medicine at TUM and head of the new Center for Digital Medicine and Health. He and his team have been developing such AI solutions for years: ""AI is already close to humans as support and assistance systems, but there are still too many risks for autonomous AI decisions,"" R ckert says. He sees the greatest difficulties in finding a good compromise between regulations, patient protection and exploiting the opportunities offered by AI. Therefore, among other things, his research also focuses on the security of medical data. In order to make reliable statements in medical research, for example, about how successful therapies are, large amounts of data are required. These are generated daily at clinics, but they are distributed throughout Germany. Sending such sensitive data to be bundled and analyzed at a specific location is risky. Daniel R ckert's team has developed an AI model for this purpose, which, instead of the data, is sent from clinic to clinic. It uses the principle of federated learning. In concrete terms, it works as follows: The AI model is given to a clinic, which trains the model with its own patient data and sends it on to the next clinic. In the end, the model has learned with all data sets without the necessity to distribute the sensitive data. However, an AI is able to remember certain data records. This is problematic when it comes to protecting the privacy of patients. The researchers also found a solution to this problem: they added ""noise"" to the data, i.e., a useless portion of data that makes it impossible for the AI to distinguish between the individual data sets. The research group combined these two principles in one system for the first time and thus created a very secure use of data. ""One of our goals is to obtain new and better information from medical image data with the help of AI, so that unnecessary or lengthy examinations are no longer necessary,"" says Daniel R ckert. PhD candidate Vasiliki Sideri-Lampretsa works in this area of image analysis and evaluation. The Greek scientist has been in his group since 2020 and examines image data from magnetic resonance or computer tomography images (MRI and CT) such as lung movements during inhalation and exhalation or brains. Her task is to use algorithms to detect deviations from the norm and thus potentially disease-related changes by comparing images at specific time points and from different persons. However, for an AI to be able to use these large amounts of data from the clinic, they have to be processed, classified, sorted and analyzed differently depending on the issue and the examined image details. Sideri-Lampretsa is taking on this task in her doctoral thesis. She is working closely with clinicians and medical students from the TUM University Hospital to understand the images correctly. ""We can't do the doctors' work, but we can help them to recognize abnormalities that would otherwise be invisible,"" she explains. Computing principles developed by Daniel R ckert and his team can already be found in numerous MRI and CT devices. Thanks to these technical changes, far fewer measurements are required to generate a reliable image of the human anatomy or physiology. As a result, patients have to stay in the devices for much shorter periods of time, which makes more examinations possible. ""Our research is so successful because hospital staff and doctors do not have to deal with AI in their daily routine, and so are not restricted in their workflow. And the machine generates an image that the doctor can really rely on,"" explains R ckert. At this point content of an external provider (source: www.youtube-nocookie.com) is integrated. When displaying, data may be transferred to third parties or cookies may be stored, therefore your consent is required. You can find more information and the possibility to revoke your consent at www.tum.de/datenschutz. The article was also published on the  Research in Bavaria  website of the Bavarian State Ministry of Science and the Arts: https://www.research-in-bavaria.de/artificial-intelligence-in-medicine-and-healthcare/ Technical University of Munich Corporate Communications Center New infrastructure for pioneering research at the TUM University Hospital Algorithm for particularly precise assessment of brain damage Diagnostic capabilities of large language models tested Technical University of Munich Arcisstra e 21 80333 M nchen Phone: +49 (0)89-289-01 Fax: +49 (0)89-289-22000 Airbus   Altana   Audi   Bayerischer Bauindustrieverband   BMW   Bosch   Busch Vacuum   Clariant   Dr xlmaier   Evonik Industries   Google   Herrenknecht   HUAWEI   Infineon   Linde   MAN   Nestl    Rohde & Schwarz   RWE   SAP   SGL Carbon   Siemens   TRUMPF   T V-S d   Vereinigung der Bayerischen Wirtschaft   Volkswagen   Wacker Chemie",2
Mapping the application of artificial intelligence in traditional medicine: technical brief - World Health Organization (WHO),https://news.google.com/rss/articles/CBMiY0FVX3lxTE5velRHaEs1RzFCRzVySmN0YjRpNlRoQVIxQndYa1laUXd0Ykh1b1FEWlBvVFQ5d2M0QXJESTd3bnFOc1dyQUtKcXlxQ29PM29rVG16Q3c3MUVpbkcwbGEwTzNiMA?oc=5&hl=en-US&gl=US&ceid=US:en,Artificial Intelligence (AI) refers to the capability of algorithms integrated into systems and tools to learn from data so that they can perform automated tasks without explicit programming of every step by a human. This technical brief offers insight into the rapidly evolving AI for health landscape and how it might be utilized in Traditional Medicine (TM). Regional and global examples are presented to show how AI is currently being used in TM to support evidence-informed decision-making and policy-making to improve health systems and universal health coverage (UHC). The document was developed by leveraging the findings of a literature review and supplementing this with knowledge and inputs captured during the conceptualization process with experts from the Topic Group on AI and Traditional Medicine (TG-TM) under the ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H).,0
International partnership for governing generative artificial intelligence models in medicine - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTFBKMlBqNkhQUzN0Ml80a0RjVEVrQXB3SkVOM3FUSm96OEpPVXZacXNQbXR6TjBfX1FmalBNcFhXQmczVXhVWHNWeWU1U3BFUWdNRDBhb2tEM0dkTUswY0RN?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement Nature Medicine (2025)Cite this article 1097 Accesses 19 Altmetric Metrics details Generative artificial intelligence (GenAI) models, such as generative adversarial networks (GANs) and transformer-based large language models (LLMs), are developing at an accelerated pace and positioned to be integrated into clinical workflows and healthcare systems across the world. However, this rapid rise of GenAI in medicine and healthcare presents not just unprecedented opportunities, but also systemic risks in the integration of this new technology and critical vulnerabilities in terms of safety, governance and regulatory oversight. GenAI and LLMs are non-deterministic in nature, possess broad generalist functionalities, and display evolving capabilities1. These characteristics challenge conventional regulatory frameworks designed for deterministic, task-specific artificial intelligence (AI) models, such as those for Software as a Medical Device (SaMD). Some of the fundamental risks associated with GenAI and LLMs applications in healthcare are clear but yet to be fully addressed by current regulatory framework ( known unknowns ), whereas other risks and challenges have not yet even surfaced ( unknown unknowns ). Known unknowns include a lack of transparency in training data (including the possible use of synthetic data for training2), susceptibility to bias, hallucination of incorrect medical content, and potential misuse in high-stakes clinical settings1 (Box 1). This is a preview of subscription content, access via your institution Access Nature and 54 other Nature Portfolio journals Get Nature+, our best-value online-access subscription 27,99   / 30 days cancel any time Subscribe to this journal Receive 12 print issues and online access 195,33   per year only 16,28   per issue Buy this article Prices may be subject to local taxes which are calculated during checkout Ong, J. et al. Lancet Digit. Health 6, e428 e432 (2024). Article CAS PubMed Google Scholar Sheng, B. et al. Nat. Biomed. Eng 9, 443 444 (2025). Article PubMed Google Scholar Gallifant, J. et al. Nat. Med. 31, 60 69 (2025). Article CAS PubMed PubMed Central Google Scholar Ning, Y. et al. Lancet Digit. Health 6, e848 e856 (2024). Article CAS PubMed PubMed Central Google Scholar Lekadir, K. et al. BMJ 388, e081554 (2025). Article PubMed PubMed Central Google Scholar Callahan, A. et al. NEJM Catal. 5, CAT.24.0131 (2024). Freyer, O. et al. Lancet Digit. Health 6, e662 e672 (2024). Article CAS PubMed Google Scholar Christopher, A. L. et al. NEJM AI 1(8) (2024) Booth, A. et al. BMJ Glob. Health 4, e001107 (2019). Article PubMed PubMed Central Google Scholar Download references This work was supported by the Duke-NUS Signature Research Programme funded by the Ministry of Health, Singapore. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not reflect the views of the Ministry of Health; the European Commission under the Horizon Europe Program as part of the project CYMEDSEC (101094218) and by the European Union. The views and opinions expressed are those of the authors only and do not necessarily reflect those of the European Union. Neither the European Union, nor the granting authorities, can be held responsible for them. Responsibility for the information and views expressed therein lies entirely with the authors; German Federal Ministry of Education and Research (Bundesministerium f r Bildung und Forschung, BMBF) through the European Union-financed NextGenerationEU programme under grant number 16KISA100K, project PATH Personal Mastery of Health and Wellness Data; NIH R01CA294033, NIH U54CA274516-01A1, American Cancer Society and American Society for Radiation Oncology, ASTRO-CSDG-24-1244514-01-CTPS. Division of Pharmacy, Singapore General Hospital, Singapore, Singapore Jasmine Chiat Ling Ong Duke-NUS AI + Medical Sciences Initiative, Duke-NUS Medical School, Singapore, Singapore Jasmine Chiat Ling Ong, Yilin Ning, Mingxuan Liu, Yian Ma & Nan Liu Centre for Quantitative Medicine, Duke-NUS Medical School, Singapore, Singapore Yilin Ning, Mingxuan Liu, Yian Ma & Nan Liu UK EQUATOR Centre, Centre for Statistics in Medicine, University of Oxford, Oxford, UK Gary S. Collins Artificial Intelligence in Medicine (AIM) Program, Mass General Brigham, Harvard Medical School, Boston, MA, USA Danielle S. Bitterman Department of Radiation Oncology, Brigham and Women s Hospital/Dana-Farber Cancer Institute, Boston, MA, USA Danielle S. Bitterman Department of Medicine, Division of Cardiology, Weill Cornell Medicine, New York, NY, USA Ashley N. Beecy Information Technology Data Science, New York-Presbyterian Hospital, New York, NY, USA Ashley N. Beecy Stanford University School of Medicine, Palo Alto, CA, USA Robert T. Chang, Kuldev Singh & Nigam H. Shah Byers Eye Institute, Stanford University, Palo Alto, CA, USA Robert T. Chang, Kuldev Singh & Daniel Shu Wei Ting College of Medicine and Health, University of Birmingham, Birmingham, UK Alastair K. Denniston & Xiaoxuan Liu University Hospitals Birmingham NHS Foundation Trust, Birmingham, UK Alastair K. Denniston Else Kr ner Fresenius Center for Digital Health, TUD Dresden University of Technology, Dresden, Germany Oscar Freyer & Stephen Gilbert Julius Centre for Health Sciences and Primary Care, University Medical Centre Utrecht, Utrecht University, Utrecht, Netherlands Anne de Hond, Artuur M. Leeuwenberg & Karel G. M. Moons Department of Pharmacy, University of California San Francisco, San Francisco, CA, USA Liang Zhao Centre of Regulatory Excellence, Duke-NUS Medical School, Singapore, Singapore John C. W. Lim & Silke Vogel SingHealth Duke-NUS Global Health Institute, Duke-NUS Medical School, Singapore, Singapore John C. W. Lim Department of Medicine, University of California, San Diego, La Jolla, CA, USA Christopher A. Longhurst Department of Pediatrics, University of California, San Diego, La Jolla, CA, USA Christopher A. Longhurst Tsinghua Medicine, Tsinghua University, Beijing, China Yue Qiu & Tien Yin Wong The Lancet Group, London, UK Rupa Sarkar Department of Computer Science and Engineering, School of Electronic, Information, and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China Bin Sheng MOE Key Laboratory of AI, School of Electronic, Information, and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China Bin Sheng Artificial Intelligence Office, Singapore Health Services, Singapore, Singapore Iris Siu Kwan Tan & Daniel Shu Wei Ting Singapore Eye Research Institute, Singapore National Eye Centre, Singapore, Singapore Yih Chung Tham, Daniel Shu Wei Ting & Tien Yin Wong Department of Ophthalmology, Yong Loo Lin School of Medicine, National University of Singapore, Singapore, Singapore Yih Chung Tham Centre for Innovation and Precision Eye Health, Yong Loo Lin School of Medicine, National University of Singapore and National University Health System, Singapore, Singapore Yih Chung Tham Ophthalmology and Visual Science Academic Clinical Program (Eye ACP), Duke-NUS Medical School, Singapore, Singapore Yih Chung Tham Nuffield Department of Clinical Neurosciences, Medical Sciences Division, University of Oxford, Oxford, UK Arun J. Thirunavukarasu Graduate Studies Department, Office of Education, Duke-NUS medical School, Singapore, Singapore Silke Vogel Division of Computational Health Sciences, Department of Surgery, University of Minnesota, Minneapolis, MN, USA Rui Zhang Center for Learning Health System Sciences, University of Minnesota, Minneapolis, MN, USA Rui Zhang NEJM AI, Boston, MA, USA Jianfei Zhao Jiahui Medical Research and Education, Shanghai, China Jianfei Zhao Centre for Digital Transformation of Health, University of Melbourne, Melbourne, Victoria, Australia Wendy W. Chapman Department of Medicine, Stanford University, Palo Alto, CA, USA Nigam H. Shah Clinical Excellence Research Center, Stanford University, Palo Alto, CA, USA Nigam H. Shah Beijing Visual Science and Translational Eye Research Institute (BERI), School of Clinical Medicine, Beijing Tsinghua Changgung Hospital, Beijing, China Tien Yin Wong Pre-hospital & Emergency Research Centre, Health Services and Systems Research, Duke-NUS Medical School, Singapore, Singapore Nan Liu NUS Artificial Intelligence Institute, National University of Singapore, Singapore, Singapore Nan Liu Department of Biostatistics and Bioinformatics, Duke University, Durham, NC, USA Nan Liu Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Correspondence to Tien Yin Wong or Nan Liu. O.F. has a leadership role and holds stock in WhalesDontFly GmbH and has had consulting relationships with Prova Health Ltd; S.G. is an advisory group member of the Ernst & Young-coordinated  Study on Regulatory Governance and Innovation in the field of Medical Devices  conducted on behalf of the Directorate-General for Health and Food Safety of the European Commission. S.G. has or has had consulting relationships with Una Health GmbH, Lindus Health Ltd, Flo Ltd, Thymia Ltd, FORUM Institut f r Management GmbH, High-Tech Gr nderfonds Management GmbH, and Ada Health GmbH, and he holds share options in Ada Health GmbH; D.S.B. is an associate editor of Radiation Oncology, HemOnc.org (no financial compensation), associate editor at JCO Clinical Cancer Information, performs advisory and consulting work (unrelated to this paper) at MercurialAI. All other authors declare no relevant conflicts of interest. Reprints and permissions Ong, J.C.L., Ning, Y., Collins, G.S. et al. International partnership for governing generative artificial intelligence models in medicine. Nat Med (2025). https://doi.org/10.1038/s41591-025-03787-4 Download citation Published: 30 June 2025 DOI: https://doi.org/10.1038/s41591-025-03787-4 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement Nature Medicine (Nat Med) ISSN 1546-170X (online) ISSN 1078-8956 (print)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",2
Artificial intelligence tool development: what clinicians need to know? - BMC Medicine,https://news.google.com/rss/articles/CBMifkFVX3lxTE45MXFqYXRTc0p4My15bUxsNUZaajJoYlljb0tSZE5KN2tUTmVqc1I2T3V2Y1l4SHJ6RG5xeHNua3h3VDlXdEJqMWNUWVdxWHRlMVZ6aHFpaHVyZHlLOEVWU0xxS2ozT051WlhXa2oySVFJWWZ2RE9wUFAzWldpdw?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Advertisement BMC Medicine volume 23, Article number: 244 (2025) Cite this article 4192 Accesses 3 Citations 6 Altmetric Metrics details Digital medicine and smart healthcare will not be realised without the cognizant participation of clinicians. Artificial intelligence (AI) today primarily involves computers or machines designed to simulate aspects of human intelligence using mathematically designed neural networks, although early AI systems relied on a variety of non-neural network techniques. With the increased complexity of the neural layers, deep machine learning (ML) can self-learn and augment many human tasks that require decision-making on the basis of multiple sources of data. Clinicians are important stakeholders in the use of AI and ML tools. The review questions are as follows: What is the typical process of AI tool development in the full cycle? What are the important concepts and technical aspects of each step? This review synthesises a targeted literature review and reports and summarises online structured materials to present a succinct explanation of the whole development process of AI tools. The development of AI tools in healthcare involves a series of cyclical processes: (1) identifying clinical problems suitable for AI solutions, (2) forming project teams or collaborating with experts, (3) organising and curating relevant data, (4) establishing robust physical and virtual infrastructure, and computer systems  architecture that support subsequent stages, (5) exploring AI neural networks on open access platforms before making a new decision, (6) validating AI/ML models, (7) registration, (8) clinical deployment and continuous performance monitoring and (9) improving the AI ecosystem ensures its adaptability to evolving clinical needs. A sound understanding of this would help clinicians appreciate the development of AI tools and engage in codesigning, evaluating and monitoring the tools. This would facilitate broader use and closer regulation of AI/ML tools in healthcare settings. Peer Review reports The transformation and realisation of digital medicine [1] and smart healthcare [2] hinge upon the active and cognizant participation of clinicians in the entire development process and cycle [3, 4]. Clinicians educated in these aspects could provide invaluable insights during the design stage of digital health technologies, including artificial intelligence (AI)-enabled tools and systems [5]. This could ensure that these solutions meet their unique needs and preferences, emphasise patient safety and quality of care and facilitate seamless integration into clinical workflows [3]. Additionally, their endorsement and support foster adoption and acceptance among other healthcare providers, drive innovation and ultimately optimise clinical outcomes [6, 7]. AI is the cognitive ability of machines made possible by mathematically designed neural networks (see the Glossary in the Additional File 1). The electronic neural networks are built to mimic the human neuronal plexus and are programmed to manage a myriad of data according to their categories and to assign different factors of data different weights. The weights are decided from given data on specified outcomes, and this process is continuously being improved with ongoing receival of data. When vast amounts of data are interconnected, it enables new discoveries and creates opportunities that can transform personal experiences and advance science in nearly all aspects of human life [8]. For example, interconnected health data can lead to early detection of diseases through predictive analytics, while personalised education platforms use linked learning data to tailor teaching methods to individual needs. In science, combining datasets across disciplines can uncover patterns such as predicting climate change impacts or accelerating drug development through AI-driven simulations. The learning computer models are machine learning (ML) versions of AI, and deep learning (DL) is a version with multiple layers of neural networks. The predictive ability of such models is evaluated against the annotated or labelled outcome via weights applied to each variable when they are included in the models. The models are self-learning, improving their performance through repeated adjustments (iterations) using a method called backpropagation which optimises the model by minimising errors via a process known as stochastic gradient descent. This process in gauging the best weights for variables in the model as they progress through their different levels of complexity at the different neural layers. Other ML models include natural language processing and computer vision. Transformers are present when DL models differentially weigh the importance of each part of the input data and make natural-language processing possible (ChatGPT stands for the Chat Generative Pretrained Transformer). Augmented intelligence (AugI) is an AI that supplements and enhances humans  ability instead of substituting it. There are FDA-approved software, applications, programmes and devices that use AI to interpret a broad range of imaging modalities and diagnostic and prognostic assistance and help outline possible treatments for clinicians [9, 10]. It is important to discern between programmed computer systems and applications that mimic AI tools and systems but are not considered true AI. Table 1 shows examples of similar tools in these two categories in the healthcare industry. The integration of AI into routine clinical care is accelerating, which is now reviewing patient histories, drafting physician notes, offering patient instructions, and not just reading X-rays and histopathological images [11,12,13,14]. The appropriate use of AI technology in healthcare defined as ethical, clinically validated and seamlessly integrated applications that enhance patient care and efficiency which is also potentially cost-effective [15]. This is when improved quality care by reducing variation, being safe and expedient [16, 17], transforming reactive healthcare to a more proactive approach, and focusing on health promotion, disease prevention and health management rather than disease treatment, resulting in fewer hospitalisations, fewer doctor visits and fewer treatments [18]. AI applications are projected to reduce annual healthcare expenditures in the USA by USD 150 billion by 2026, primarily through increased efficiency, improved diagnostics, and optimised care delivery [18]. However, all this AI advancement is not without great challenges from development to deployment [19,20,21], integration in clinical workflows [22, 23] and influences on doctor patient consultation [24, 25]. Persistent concerns about integrating AI systems into existing clinical consultations include alert fatigue [26, 27], data quality, data security, transparency and accountability, alignment with standards and guidelines and unintended consequences along with model design requirements, and retention of clinician autonomy [28]. The human factors and AI systems that may affect medical professionals  interactions with technology could be related to perceptions of training data quality, performance of AI systems, explainability, adaptability, medical expertise (young versus experienced clinicians), technological expertise, personality, cognitive biases (proper understanding and use of AI outputs) and trust in the whole ecosystem [29]. Table 2 shows the real present challenges of AI technology in healthcare and its more certain progress in the near future. Globally, the AI healthcare market was valued at USD 20.9 billion in 2024 and is anticipated to grow at a compound annual growth rate of 48.1% reaching an estimated USD 148.4 billion by 2029. This growth reflects expanding investments in AI-driven technologies and services across the healthcare sector [30]. This review explains the usual path for new AI tool development and deployment in healthcare and clinical services. It concurs with other evaluation frameworks [31, 32] (Table 3) and could be extended to include assessments of health economic benefits [33]. When selecting an evaluation framework, users should consider the specific objectives of their study as some frameworks focus on quality evaluation, transparent reporting or risk of bias, while others address specific stages, designs or disciplines. Depending on the purpose, a single framework or a combination can guide study planning, implementation and reporting to ensure robust and impactful outcomes. However, articles with sufficient and clear technical explanations of the AI development process for clinicians are scarce [31, 32, 34]. Clinicians with a sound understanding of the whole development process of AI tools would help them engage in codesign, effective collaboration [35], evaluation and monitoring of the tools, and further facilitate broader use and closer regulation of these tools in healthcare settings [36]. Some reporting guidelines are study design specific (TRIPOD-AI for prognostic and diagnostic studies, STARD-AI for diagnostic test studies, SPIRIT/CONSORT and SPIRIT/CONSORT-AI for clinical trials), stage specific (DECIDE-AI for early clinical studies) or discipline specific (CHEERS-AI for health economy, IDEAL for surgery, and CLAIM and FUTURE-AI for radiology) [39]. This focused integrative review attempts to update and delineate practical knowledge on AI tools or model development throughout the whole process for clinicians. The review questions are as follows: What is the typical process of AI tool development in the full cycle? What are the important concepts and technical aspects of each step? The approach includes a targeted literature review and synthesised summaries from online courses, including but not limited to the AI for healthcare by the National University of Singapore (https://nusmed.emeritus.org/ai-for-healthcare), the No Code AI and Machine Learning: Building Data Science Solutions by the Massachusetts Institute of Technology (https://professionalonline2.mit.edu/no-code-artificial-intelligence-machine-learning-program) and the European Information Technologies Certification Academy (EITCA) Artificial Intelligence Academy (https://eitca.org/certification/eitca-ai-artificial-intelligence-academy/). It strives to provide adequate technical knowledge that is immediately useful for clinicians to appreciate the development of AI tools and is able to engage with developers, vendors and researchers when considering clinical adoption and codesigning a new tool. ChatGPT 3.5, 4o and o1 (OpenAI, San Francisco, CA, USA) were used to assist in drafting and language editing of portions of this review. The authors have reviewed and edited the content produced by ChatGPT for accuracy and integrity, and accept full responsibility for the final version of the manuscript. The development of AI in healthcare involves a series of cyclical processes (Fig. 1). It begins by identifying clinical problems suitable for AI solutions, forming project teams or collaborating with experts, and organising and curating relevant data. The establishment of robust infrastructure and architecture supports subsequent stages, including the exploration of AI neural networks on open access platforms and the validation of AI/ML models. Following registration procedures, clinical deployment and continuous performance monitoring occur. Finally, a commitment to improving the AI ecosystem ensures its adaptability to evolving clinical needs. AL/ML tool development process This first step is the most important starting point for the rest of the development process (see Fig. 2). Some clinical and biomedical problems in healthcare services could be best resolved with the help of an automated solution. These are problems or challenges that are technically factual, mechanical, repetitive and complex in nature owing to the need to process multiple aspects of healthcare services, people in the health system or patient characteristics (see tips and examples in Table 4). DL/AugI/ML does not help address personal values, health beliefs or emotions that change until these constructs are measured in certain ways. Identifying the problem includes deciding on the level of the problem for the AI technology to solve. This method is descriptive, diagnostic, predictive or prescriptive and uses either assistive or autonomous AI algorithms (Table 4). Descriptive AI models are about estimating the quantity of a certain condition, diagnostic models are about the probability of occurrence of certain conditions, prognostic models are predicting certain outcomes, and prescriptive models suggest the most likely efficacious treatment. The order denotes an incremental level of value and complexity to be expected in the development of the tool. Be as specific and clearly defined as possible with all the variables, especially the outcome variable (supervised learning models). The process of identifying AI tools for clinical problems A successful team must consist of individuals with the right skillsets. This includes data scientists for data validation, transformation, curation, and visualisation for AI/ML models; data engineers to implement data workflows, such as storage; data architects to design the system architecture for data repositories; and a chief data officer to establish the data governance structure and policies. Clinicians, healthcare administrators and relevant stakeholders, including patients and public groups, are essential for the planning, development, deployment and sustained use of the AI/ML tool. Additionally, health informatics professionals and business or industry partners should be considered. To secure funding, the project must address the tool s ethical aspects, ensuring professional integrity, a clear balance of benefits over harm, justice and trustworthiness, with designated accountability for its implementation. Relevant real-world data sources must be explored, annotated and preprocessed (Table 5). The availability of high-quality and sufficient variables in the target population is crucial for AI solutions to address clinical problems. These data must be diverse and representative [58], properly labelled and curated to minimise bias and errors. Data will need to go through several stages before becoming useful for AI algorithmic models. These stages include standardisation (coding structured and unstructured data) for interoperability, cataloguing, deidentification (pseudo or anonymisation), cleaning/transformation (validation), and linking and combining different sources into a single dataset. Managing a large amount of quality data within credible data governance structures remains a significant challenge [59]. An adequate capacity of computers and servers and accessories for operating large amounts of data at the optimal speed are needed (Table 6). Intelligence processing units (IPUs) are rarer, especially on certain clouds, and are best used in graph-based AI algorithms. In scenarios where high-performance, energy-efficient hardware acceleration is required to handle demanding AI workloads, enabling faster training, inference and deployment of AI models across various applications and industries. Another specialised hardware accelerator developed by Google is the tensor processing unit (TPU). It is specific to ML tasks, particularly those involving TensorFlow and Google s open-source ML framework. Compared with traditional CPUs and GPUs, they offer significant speedups and cost savings, particularly for large-scale AI workloads running on TensorFlow-based frameworks. The operations of these data servers include strong cybersecurity (data encryption), data privacy, controlled access and updated regulatory policies on the proper use of the data, and supervised incremental learning of the AI/ML tools. In addition to security and proper governance, the ease and speed of access to different users are paramount. The physical and virtual infrastructure, and computer systems  architecture must be scalable to meet the increasing needs and demands of the tools. Alternatively, cloud-based infrastructures offer more feasible services in AI tool development from algorithm building to deployment and scale AI applications by providing access to a rich ecosystem of resources and tools. The three main cloud service providers are Amazon Web Services (AWS), the Google Cloud Platform (GCP) and Microsoft Azure (Table 7). In addition to providing scalable infrastructure, it also provides robust data storage, management solutions and a wide range of AI development tools and frameworks, such as TensorFlow and Azure Machine Learning, and application programming interfaces (APIs) to streamline the development workflow. Cloud services also facilitate collaboration among team members and simplify the deployment of AI models in production environments. Additionally, they offer monitoring and optimisation tools to ensure the optimal performance of AI applications. Many AI algorithms are readily available on open-access platforms (Table 8), with similar algorithms often already developed and tested. Choosing appropriate AI/ML models and methods is essential for resolving clinical challenges. The model selection framework should balance performance requirements with cost, risk, deployment needs and stakeholder expectations [60]. The choice of algorithm depends on the input type, whether speech, language, vision, decision-making, or a combination of these. For example, convolutional neural networks are ideal for image data, whereas recurrent neural networks are best suited for text and numerical data [61]. The development of new AI neural networks requires data scientists with advanced skill sets and is time-consuming. The selected or newly developed algorithm must undergo training, validation and testing on a curated dataset (Table 5). Its performance should be evaluated and compared with that of the baseline model or standard of care before external validation, especially if the model is applied in different settings from where it was developed and then deployed in practice [37]. Table 9 shows the classification tasks and ML strategies on data [60, 61]. Both nonclinical and clinical validation are essential to establish its performance, ensuring its integration into routine clinical workflows, usability and positive effects on clinical outcomes. Properly designed clinical research, including clinical trials, may be necessary to assess its real-world clinical impact. Table 3 outlines recommendations for evaluating AI tools in clinical settings, whether as diagnostic or prognostic tools. Once finalised, the results are published for broad dissemination and peer scrutiny. It is also critical to explore and address any ethical and legal implications associated with using these tools in healthcare, as liability risks may arise from sources of error, error identification, potential harm and legal redress [62]. The registration of tools with relevant authorities is typically carried out by the manufacturer, developer or the organisation responsible for the AI tool. In many cases, this involves collaboration between technical and regulatory teams within the organisation to ensure compliance with the regulatory requirements of the target market. Tools registration with relevant authorities such as medical device authorities could increase the likelihood of successful implementation and deployment in the real world [63]. The evaluation criteria differ across countries, which may include an effectiveness trial [32]. AI tools in the UK are classified as medical devices and therefore require Medicines and Healthcare products Regulatory Agency (MHRA) approval bearing the  United Kingdom Conformity Assessed  (UKCA) logo. However, AI tools in Europe are regulated by the EU Medical Device Regulation (EU MDR) and bearing the  Conformit  Europ enne  logo to be marketed in Europe. In the USA, AI tools are regulated by the Food & Drug Administration (FDA) (https://www.fda.gov/science-research/science-and-research-special-topics/artificial-intelligence-and-medical-products). FDA classifies AI tools based on their risk level and intended use following pathways such as 510(k) premarket notification, De Novo classification or premarket approval. Many AI tools are categorised as Software as a Medical Device and must meet rigorous criteria for safety, effectiveness and transparency including Good Machine Learning Practices. Post-market surveillance is often required to monitor real-world performance while labelling must clearly define intended use, performance metrics and limitations. European AI Act [64] prohibits AI systems that collect sensitive personal information that causes discrimination, manipulates human behaviour or exploits vulnerabilities of certain groups of people at all social places. The core principle is that AI   should be a human-centric technology. It should serve as a tool for people, with the ultimate aim of increasing human well-being.  Clinicians play a vital role in evaluating AI tools  suitability for their practice and ensuring their safe and effective use. While regulatory frameworks vary across regions, a unifying principle among global authorities is the emphasis on ensuring that AI tools align with ethical standards, prioritising human well-being, fairness and accountability. These principles not only guide the evaluation and approval processes but also ensure that the implementation of AI tools remains consistent with societal values and promotes trust among users and stakeholders. The ethical principles of nonmaleficence, beneficence, autonomy and justice with added governance and associated principles of privacy, diversity, inclusiveness, transparency, reliability, fairness, social good, well-being, sustainability, auditability, explicability, interpretability and quality data are referred to in high-level policy documents [65,66,67,68] (see Fig. 3, and Additional File 2: AI Ethics and Policy Frameworks from the United Nations Educational, Scientific and Cultural Organization 2022 [7, 65, 69], UN Resolution on AI 2024 [66], International Scientific Report on the Safety of Advanced AI: Interim Report 2024 [70], Diversity, INclusivity and Generalisability: STANDING Together project team 2023 [58], US Executive Order on AI 2023 [67], Artificial Intelligence Act European Parliament/2024 [68], Harmonised Standards for the European AI Act: European Parliament 2024 [71], Ethics and governance of artificial intelligence for health: World Health Organization guidance 2021 [72], Organization for Economic Cooperation and Development AI Principles 2019 [73], Universal Guidelines for AI: Center for AI and Digital Policy 2018 [74], Asilomar AI Principles: Future of Life Institute 2017[75]). These principles, in the form of typology according to the different stages of the AI life cycle and sources, are available here (https://ricardo-ob.github.io/tools4responsibleai/#title-cite) [76] and foster the development of responsible AI tools and systems by technical and nontechnical persons, balancing the risk and benefits to the public [77]. AI tools and systems are prohibited by the European Union s AI Act if they manipulate cognitive behaviours, classify the traits and status of people through facial or emotion recognition and collect sociobiological characteristics such as sexual orientation or religious beliefs into various forms of social scoring or biometric categorisation [68]. AI ethics that may determine progressive or regressive outcomes. *Different age groups, cultural systems and language groups, persons with disabilities, girls and women, and disadvantaged, marginalised and vulnerable people or people in vulnerable situations. SDG = Sustainable development goals. Deployment is the method by which the tested AI tools are integrated into an existing clinical workflow to make practical healthcare decisions (outputs) on the basis of data (input). The best deployment strategy would consider the software systems or applications environment where the AI tools are to be deployed. If this system is a web service or electronic health records system, then it will require an API to enable data pipeline integration where the input and output could be executed. The easier the deployment process, which includes having the same API endpoint references, the faster the model improvements are. The design of the user interfaces must allow alerts or notifications to be displayed noninterruptively but effectively to achieve practice efficiency and provider acceptance and adoption. This could be tested via  silent  or  shadow  deployment, which is deployed in the actual environment but not fully for routine use. Another important step before deployment to production is the quality testing of scalability and performance optimisation in scenarios when high data flow occurs. The final deployment approach is likely a decision between the budget, availability of the infrastructures and the required performance of the AI tools. Another important task is to train clinicians and team members in healthcare facilities where tools are used or integrated into the electronic medical records system. Promoting human-AI teaming would augment performance and safeguard autonomy but require calibrated design, support and monitoring [34]. Be prepared to explain the decision process of the AI/ML tools and be present to support their use. This responsibility often lies with a collaborative effort between the developers and the clinical staff, with oversight from regulatory bodies. Neural networks in AI are often called  black boxes  because their internal workings are complex and not easily understood. This lack of transparency can be problematic in healthcare where understanding how decisions are made is crucial. To address this, techniques like SHAP (Shapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations) help identify which input features most influence the AI s decisions. Visualisation tools such as Grad-CAM (Gradient-weighted Class Activation Mapping) can highlight areas in medical images that the AI focuses on, providing insights into its decision-making process [78]. The tool s effect in clinical care should be measured, and the incremental learning of the tool should be supervised. Additionally, monitoring should continue for any unwanted effects arising from its use including data integrity, cybersecurity and impact mitigation in cases of breach and functional recovery [79]. A medical algorithmic audit could and should be conducted if it was not performed before real-world deployment [80]. Changes in the clinical practices such as changing guidelines, treatments or diagnostic protocols can render previously trained models less effective over time. Be vigilant and prepared to retrain the AI/ML algorithm if the performance has drifted to below expectations. Clinicians, developers and IT teams must remain vigilant in monitoring AI performance for signs of drift, with clinicians reporting inconsistencies and developers tracking key metrics. This includes prediction accuracy, response time and resource utilisation. Developers are primarily responsible for retraining models, using updated data and collaborating with clinical staff to ensure relevance, while compliance teams ensure adherence to regulatory standards. Retraining involves collecting new data, refining the model, validating updates and carefully redeploying the system with ongoing performance monitoring. Any update done to the tools may require a notification to the regulatory body, and it is essential to consult specific guidelines and maintain open communication with the relevant regulatory body. Actively engage stakeholders and the public with the tool throughout the development-deployment-monitoring process. This could improve confidence and progress the AI ecosystem in the country [17]. This approach aims to improve broader and better perceptions of AI/ML technologies, invite more keen interest and training from experts, and establish central governance, trusted custodian, ethical value and proper regulation. This may increase the investment and uptake of AI technologies in healthcare and research that are supported by sufficient funding and infrastructure to allow freedom to innovate and implement more AI/ML tools. Ethical considerations around data privacy and patient safety are well-known challenges that must be addressed [81, 82]. Similarly, traditional medical principles remain crucial as they uphold patient dignity and foster mutual trust among doctors, patients and society. Establishing a successful partnership between technology companies, which provide technological expertise, and healthcare facilities which offer data and expert input, is essential. This partnership must be both regulatory compliant and economically beneficial to ensure the effective implementation and deployment of algorithms [83]. This research shares the experience and valuable lessons of the National University Health System, Singapore (NUHS), in obtaining AI tools for production in healthcare services. NUHS s experience in implementing AI-driven healthcare systems offers valuable insights for institutions pursuing similar transformations. Success in AI implementation extends beyond the technology itself, requiring four critical elements: (1) establishing robust data infrastructure, (2) building organisational trust, (3) ensuring continuous human oversight through committees and (4) committing to long-term engagement with AI technology [84]. They developed the ENDEAVOUR AI platform, a comprehensive AI system that integrates various tools to streamline operations [85]. Additionally, they established DISCOVERY AI, a private AI training cloud featuring NVIDIA DGX A100 s to support the development of AI models, which functions as both a production system and a research sandbox for modular machine learning tools [16, 85]. It adheres to local and international regulatory guidelines, with data anonymized by removing identifiers such as names, addresses and identification numbers. A robust master governance structure ensures equitable data access, centralised anonymisation and differential data linkage. Data access and sharing are overseen by custodians of specific databases and a dedicated committee. This governance framework also manages research administration, including institutional review board processes and collaborative agreements. Integrated with the electronic health record system, the platform leverages multiple clinical data and research databases through embedded algorithms to enable many AI predictions. With both the ENDEAVOUR AI platform and DISCOVERY AI, a series of AI tools have been developed and successfully deployed for clinical care, while some have undergone internal validation within the institution and are pending full peer-reviewed publication [84, 85]. An AI-driven system, the Pathfinder Dashboard (Additional File 3 shares the experience of Pathfinder Dashboard AI tools development and challenges according to the nine steps mentioned in this review) predicts patient wait times and manages patient inflows at the emergency department, enhancing care quality and patient satisfaction. Should patients have to be admitted for inpatient care, the estimated length of stay model predicts patient hospital stays, ensuring timely and appropriate care and thus optimising the effective planning and allocation of resources [86]. When discharge is possible or decided upon, the 30-day readmission prediction model could personalise patient care to prevent readmission and reduce hospital costs. The Disease Progression Modelling tool enables earlier intervention by anticipating disease progression, particularly for chronic conditions, and the Pharmacogenomics Alerts System tailors medication recommendations on the basis of genetic profiles, enhancing precision medicine and reducing adverse drug reactions. NUHS has enhanced patient communication with various chatbot systems, including RUSSELL-GPT [87], which provides instant responses and personalised health information. These chatbots use advanced GPT models to cater to both patients and researchers while maintaining data security. For managing chronic diseases, all AI tools at NUHS are integrated with the Epic EMR System, providing a unified AI dashboard that offers comprehensive insights. This integration enhances decision-making and patient care by consolidating information and streamlining hospital operations. There is the CURATE.AI to optimise chemotherapy treatments for prostate cancer [88] and solid tumours [89]. It has also been applied to personalise dose selection [90] and to tailor immunosuppressant drug dosages for liver transplant patients to prevent organ rejection [91]. NUHS introduced the Chronic Disease Management Programme (CHAMP) Chatbot System, which engages patients with reminders and follow-ups via WhatsApp. Compared with similar programmes, this tool aims to improve patient adherence to treatment plans, leading to higher enrollment rates and lower dropout rates. Developing and translating AI innovations from research to clinical practice faces significant challenges often referred to as the  valley of death  [92]. These include the complexity of identifying the right pain point, clinical validation, regulatory hurdles and the need for robust evidence of efficacy and safety, registration with the regulatory body and communication with trust with healthcare stakeholders for integration into an existing clinical workflow [93, 94]. Additionally, the lack of standardised reporting and evaluation frameworks complicates the explainability and interpretability for the integration of AI tools into healthcare settings [95]. Clinicians who are more than aware of the full cycle of AI tools development delineated in this paper could facilitate the development, reporting, assessment and smoother transitions from bench to bedside of these tools [96, 97]. The most important step and challenge to tackle is the biases in training data. This could perpetuate healthcare disparities such as the underrepresentation of specific demographic groups or the reinforcement of historical biases in data collection [98]. Compounding this issue would be a dataset shift post-deployment where the model s operational environment differs from its training environment causes a degrade performance and compromise generalisability [99]. Mitigating these challenges requires careful dataset curation to have diverse and representative samples, along with the deployment of bias detection and mitigation strategies [100]. Rigorous external validation across varied populations and settings is essential to ensure the reproducibility and generalisability of AI models, both of which are foundational to achieving fairness, equity and clinical adoption [101]. Beside the strategies alluded to when faced with a lack of data quantity or poor data quality, there are several strategies generative AI can offer. This includes creating synthetic data based on the electronic health records, omics data and bioimages to train diagnostic and predictive models [102, 103]. This transformative potential alleviates data scarcity, enhances patient privacy and enables the simulation of rare or complex clinical scenarios. However, challenges remain in ensuring that synthetic data maintains the variability and complexity of real-world datasets to achieve reproducibility [104]. For AI models trained on synthetic data, rigorous testing and validation are necessary to confirm that they generalise accurately to diverse populations and clinical realities. Addressing these challenges allows generative AI to significantly enhance the robustness and utility of healthcare AI systems. Evaluating AI models against professional clinicians is crucial to understanding their clinical utility and assessing their algorithmic quality [105]. While some AI models achieve expert-level accuracy, a lack of rigorous study design often leads to overestimated performance claims. Comparative studies and standardised evaluation frameworks are critical for determining whether AI tools can complement or enhance human decision-making in healthcare [93]. Such evaluations are vital for building confidence among stakeholders and ensuring the safe and effective deployment of AI in clinical practice. Reproducibility and generalisability are critical pillars for ensuring the effective translation, application, and evaluation of AI models that has been developed for healthcare. Reproducibility demands consistent results through transparent documentation of data collection, preprocessing and model training, fostering trust and reliability [106]. Generalisability ensures that AI models perform accurately and equitably across diverse populations, clinical settings and evolving medical practices, addressing key challenges such as dataset biases and shifts [107]. These principles are essential for validating all AI tools and ensuring robust, scalable solutions. By integrating these considerations across the entire AI lifecycle, from development to deployment, clinicians and developers can create innovative, equitable and reliable tools that meet the demands of real-world healthcare. All AI systems, regardless of type, follow a similar life cycle as described in this review. The primary differences lie in complexity, scalability, interpretability, resource demands and training methodology. Traditional AI/ML models that utilise established statistical or rule-based techniques with manually chosen features are typically simpler, more interpretable and less computationally demanding. Neural networks and deep learning models that employ layered neuron-like architectures that learn patterns from raw data could scale well with large data but require more computational resources and are harder to interpret. Generative AI models implement advanced frameworks that generate original outputs by modelling data distributions do push these challenges further, often requiring massive data and compute resources, more complex training regimes (pretraining plus fine-tuning), and specialised evaluation and monitoring strategies. This review presents a straightforward explanation of the entire development process of AI tools, outlined in nine cyclical and iterative steps, which could enhance understanding among clinicians. More importantly, the presentation with many infographics and examples, combined with adequate technical details, has the potential to reach a broader audience, particularly in countries that face greater inequities in the health AI/ML literature [108, 109] and are at risk of health disparities from this technology [110, 111]. Notably, other great challenges include win win partnerships between technology companies such as technological know-how, health-care facilities, as data sources, and expert inputs to algorithms [112, 113]. This is to be regulatory, acceptable and economically rewarding to the two stakeholders [83, 114]. Robust AI tools are those that resolve real-world clinical problems, are developed by a team of relevant stakeholders, are trained on broad-based high-quality data; are validated externally, prospectively and in controlled trials or equivalently. They perform in real time, are unbiased, safe and trustworthy with acceptable human AI tool interactions [20], are quick in algorithm updates to cover emerging diseases, are controllable by human agents, are acceptable to target users who are either explainable or unexplainable [5, 115], and are ethically justifiable and legally compliant [116]. Challenges in attaining high-performing AI systems include having high-quality infrastructures in terms of computing power, memory and storage capacity, high-speed internet connectivity, low-latency networking, more energy efficient computing technology (quantum computing and optical computing) [117], and scalability and elasticity that are supported by ethics and regulatory compliance data governance [118]. Ultimately, AI/ML tools offer significant benefits by reducing systemic, sporadic medical errors and enhancing patient care quality. These tools streamline healthcare processes, integrate seamlessly into health systems and are continuously monitored to ensure safety and effectiveness. Having legal framework that ensure compliance with data security, protection and privacy policies, positive economic impacts or at least an oversight by an established data governance body including representation from the public and patients could further strengthens accountability and trust in their use [119]. Accordingly, success clinical integration and implementation of AI tools must include building trust and confidence among clinicians in the development process, having quality data, and risk levels are understood by all stakeholders and mitigated as a team with clinicians [120], satisfying fairness, equity, robustness, privacy, safety, transparency, explainability and accountability with assured benefits for patients, healthcare providers and the organisation involved [121]. As the field of AI is anticipated to evolve quickly with new technologies and algorithms, it is essential for all stakeholders including clinicians, to stay informed about new guidelines, reporting standards for AI tools and systems, and the application of AI in medicine (see Table 2 on some of the important organisations on AI-related matters in Additional File 1) [116]. No datasets were generated or analysed during the current study. Artificial Intelligence Assessment List for Trustworthy AI Application Programming Interface Application-Specific Integrated Circuit Receiver Operating Characteristic   Area Under Curve Amazon Web Services Bidirectional Encoder Representations from Transformers Gradient-weighted Class Activation Mapping Chronic Disease Management Programme Consolidated Health Economic Evaluation Reporting Standards Checklist for Artificial Intelligence in Medical Imaging Convolutional Neural Networks Clinical Trial Reports For Interventions Involving Artificial Intelligence Best practice checklist to report on the use of structured electronic healthcare records in clinical research Clinical Practice Statement Current Procedural Terminology Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence Deep Learning Electrocardiogram Electronic Health Record European Information Technologies Certification Academy Foundation for Advancing Family Medicine Artificial Intelligence Research (lab) Food & Drug Administration Field-Programmable Gate Array International Consensus Guideline for Trustworthy and Deployable Artificial Intelligence in Healthcare Google Cloud Platform Generative Pre-trained Transformer High-Performance Computing International Classification of Diseases Innovation, Development, Exploration, Assessment, and Long-term Framework Intelligence processing units Local Interpretable Model-Agnostic Explanations Medicines and Healthcare products Regulatory Agency Minimum information about clinical artificial intelligence modelling MINimum Information for Medical AI Reporting Machine Learning Natural Language Generation Natural Language Processing National University Health System Obesity Medicine Association Organisational PerspecTIve Checklist for AI solutions adoption Receiver Operating Characteristic Area Under Curve Shapley Additive Explanations Standard Protocol Items: Recommendations for Interventional Trials Artificial Intelligence Standards for Reporting of Diagnostic Accuracy Studies AI Extension Tensor Processing Unit Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis Elenko E, Underwood L, Zohar D. Defining digital medicine. Nat Biotechnol. 2015;33:456 61. Article CAS PubMed Google Scholar Tian S, Yang W, Grange JML, Wang P, Huang W, Ye Z. Smart healthcare: making medical care more intelligent. Glob Health J. 2019;3:62 5. Article Google Scholar Adler-Milstein J, Aggarwal N, Ahmed M, Castner J, Evans BJ, Gonzalez AA, et al. Meeting the moment: addressing barriers and facilitating clinical adoption of artificial intelligence in medical diagnosis. NAM Perspect. 2022:10.31478/202209c. Rosen R. How is technology changing clinician-patient relationships? BMJ. 2024;384:q574. Sauerbrei A, Kerasidou A, Lucivero F, Hallowell N. The impact of artificial intelligence on the person-centred, doctor-patient relationship: some problems and solutions. BMC Med Inform Decis Mak. 2023;23:73. Article PubMed PubMed Central Google Scholar Trinkley KE, An R, Maw AM, Glasgow RE, Brownson RC. Leveraging artificial intelligence to advance implementation science: potential opportunities and cautions. Implement Sci. 2024;19:17. Article PubMed PubMed Central Google Scholar UNESCO. Ethical impact assessment: a tool of the Recommendation on the Ethics of Artificial Intelligence. Paris: UNESCO; 2023. Available from: https://www.unesco.org/en/articles/ethical-impact-assessment-tool-recommendation-ethics-artificial-intelligence. Accessed 21 Apr 2025. Haug CJ, Drazen JM. Artificial Intelligence and Machine Learning in Clinical Medicine, 2023. N Engl J Med. 2023;388:1201 8. Article CAS PubMed Google Scholar U.S. Food and Drug Administration. Artificial Intelligence and Machine Learning (AI/ML)-Enabled Medical Devices. Silver Spring (MD): FDA; 2023. Available from: https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices. Accessed 21 Apr 2025. Muehlematter UJ, Bluethgen C, Vokinger KN. FDA-cleared artificial intelligence and machine learning-based medical devices and their 510(k) predicate networks. Lancet Digit Health. 2023;5:e618 26. Article CAS PubMed Google Scholar Mehta N, Pandit A, Shukla S. Transforming healthcare with big data analytics and artificial intelligence: A systematic mapping study. J Biomed Inform. 2019;100: 103311. Article PubMed Google Scholar Bays HE, Fitch A, Cuda S, Gonsahn-Bollie S, Rickey E, Hablutzel J, et al. Artificial intelligence and obesity management: An Obesity Medicine Association (OMA) Clinical Practice Statement (CPS) 2023. Obes Pillars. 2023;6: 100065. Article PubMed PubMed Central Google Scholar Mohaideen K, Negi A, Verma DK, Kumar N, Sennimalai K, Negi A. Applications of artificial intelligence and machine learning in orthognathic surgery: A scoping review. J Stomatol Oral Maxillofac Surg. 2022;123:e962 72. Article PubMed Google Scholar Yin J, Ngiam KY, Teo HH. Role of Artificial Intelligence Applications in Real-Life Clinical Practice: Systematic Review. J Med Internet Res. 2021;23: e25759. Article PubMed PubMed Central Google Scholar Ghaddaripouri K, Ghaddaripouri M, Mousavi AS, Mousavi Baigi SF, Rezaei Sarsari M, Dahmardeh Kemmak F, et al. The effect of machine learning algorithms in the prediction, and diagnosis of meningitis: A systematic review. Health Sci Rep. 2024;7: e1893. Article PubMed PubMed Central Google Scholar Ngiam KY, Khor IW. Big data and machine learning algorithms for health-care delivery. Lancet Oncol. 2019;20:e262 73. Article PubMed Google Scholar Lysaght T, Lim HY, Xafis V, Ngiam KY. AI-Assisted Decision-making in Healthcare: The Application of an Ethics Framework for Big Data in Health and Research. Asian Bioeth Rev. 2019;11:299 314. Article PubMed PubMed Central Google Scholar Bohr A, Memarzadeh K. The rise of artificial intelligence in healthcare applications. In: Artificial Intelligence in Healthcare. Elsevier; 2020. p. 25 60. Shah P, Kendall F, Khozin S, Goosen R, Hu J, Laramie J, et al. Artificial intelligence and machine learning in clinical development: a translational perspective. Npj Digit Med. 2019;2:69. Article PubMed PubMed Central Google Scholar Bach TA, Kristiansen JK, Babic A, Jacovi A. Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review. 2023. https://doi.org/10.48550/ARXIV.2310.03392. Fehr J, Citro B, Malpani R, Lippert C, Madai VI. A trustworthy AI reality-check: the lack of transparency of artificial intelligence products in healthcare. Front Digit Health. 2024;6:1267290. Article PubMed PubMed Central Google Scholar Yu K-H, Kohane IS. Framing the challenges of artificial intelligence in medicine. BMJ Qual Saf. 2019;28:238 41. Article PubMed Google Scholar Adler-Milstein J, Redelmeier DA, Wachter RM. The Limits of Clinician Vigilance as an AI Safety Bulwark. JAMA. 2024. https://doi.org/10.1001/jama.2024.3620. Article PubMed PubMed Central Google Scholar Robertson C, Woods A, Bergstrand K, Findley J, Balser C, Slepian MJ. Diverse patients  attitudes towards Artificial Intelligence (AI) in diagnosis. PLOS Digit Health. 2023;2: e0000237. Article PubMed PubMed Central Google Scholar Shaffer VA, Probst CA, Merkle EC, Arkes HR, Medow MA. Why Do Patients Derogate Physicians Who Use a Computer-Based Diagnostic Support System? Med Decis Making. 2013;33:108 18. Article PubMed Google Scholar Miller A, Moon B, Anders S, Walden R, Brown S, Montella D. Integrating computerized clinical decision support systems into clinical work: A meta-synthesis of qualitative research. Int J Med Inf. 2015;84:1009 18. Article Google Scholar Olakotan OO, Mohd YM. The appropriateness of clinical decision support systems alerts in supporting clinical workflows: A systematic review. Health Informatics J. 2021;27:146045822110075. Article Google Scholar Kennedy G, Gallego B. Clinical prediction rules: A systematic review of healthcare provider opinions and preferences. Int J Med Inf. 2019;123:1 10. Article Google Scholar Knop M, Weber S, Mueller M, Niehaves B. Human Factors and Technological Characteristics Influencing the Interaction of Medical Professionals With Artificial Intelligence-Enabled Clinical Decision Support Systems: Literature Review. JMIR Hum Factors. 2022;9: e28639. Article PubMed PubMed Central Google Scholar Artificial Intelligence (AI) in Healthcare Market (By Component: Software, Hardware, Services; By Application: Virtual Assistants, Diagnosis, Robot Assisted Surgery, Clinical Trials, Wearable, Others; By Technology: Machine Learning, Natural Language Processing, Context-aware Computing, Computer Vision; By End User) - Global Industry Analysis, Size, Share, Growth, Trends, Regional Outlook, and Forecast 2022   2030. Precedence Research; 2023. Hassan N, Slight R, Morgan G, Bates DW, Gallier S, Sapey E, et al. Road map for clinicians to develop and evaluate AI predictive models to inform clinical decision-making. BMJ Health Care Inform Online. 2023;30: e100784. Article Google Scholar Kwong JCC, Khondker A, Lajkosz K, McDermott MBA, Frigola XB, McCradden MD, et al. APPRAISE-AI Tool for Quantitative Evaluation of AI Studies for Clinical Decision Support. JAMA Netw Open. 2023;6: e2335377. Article PubMed PubMed Central Google Scholar Bakker L, Aarts J, Uyl-de Groot C, Redekop K. How can we discover the most valuable types of big data and artificial intelligence-based solutions? A methodology for the efficient development of the underlying analytics that improve care. BMC Med Inform Decis Mak. 2021;21:336. Article PubMed PubMed Central Google Scholar Committee on Human-System Integration Research Topics for the 711th Human Performance Wing of the Air Force Research Laboratory, Board on Human-Systems Integration, Division of Behavioral and Social Sciences and Education, National Academies of Sciences, Engineering, and Medicine. Human-AI Teaming: State-of-the-Art and Research Needs. Washington, D.C.: National Academies Press; 2022. Tan M, Lee H, Wang D, Subramonyam H. Is a Seat at the Table Enough? Engaging Teachers and Students in Dataset Specification for ML in Education. 2023. https://doi.org/10.48550/ARXIV.2311.05792. Ng FYC, Thirunavukarasu AJ, Cheng H, Tan TF, Gutierrez L, Lan Y, et al. Artificial intelligence education: An evidence-based medicine approach for consumers, translators, and developers. Cell Rep Med. 2023;4: 101230. Article PubMed PubMed Central Google Scholar Norgeot B, Quer G, Beaulieu-Jones BK, Torkamani A, Dias R, Gianfrancesco M, et al. Minimum information about clinical artificial intelligence modeling: the MI-CLAIM checklist. Nat Med. 2020;26:1320 4. Article CAS PubMed PubMed Central Google Scholar Kotecha D, Asselbergs FW, Achenbach S, Anker SD, Atar D, Baigent C, et al. CODE-EHR best-practice framework for the use of structured electronic health-care records in clinical research. Lancet Digit Health. 2022;4:e757 64. Article CAS PubMed Google Scholar Vasey B, Nagendran M, Campbell B, Clifton DA, Collins GS, Denaxas S, et al. Reporting guideline for the early-stage clinical evaluation of decision support systems driven by artificial intelligence: DECIDE-AI. Nat Med. 2022;28:924 33. Article CAS PubMed Google Scholar Cruz Rivera S, Liu X, Chan A-W, Denniston AK, Calvert MJ, The SPIRIT-AI and CONSORT-AI Working Group, et al. Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension. Nat Med. 2020;26:1351 63. Liu X, Rivera SC, Moher D, Calvert MJ, Denniston AK. Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI Extension. BMJ. 2020;370:m3164. Collins GS, Reitsma JB, Altman DG, Moons KGM. Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD statement. BMJ. 2015;350 jan07 4:g7594 g7594. Collins GS, Moons KGM, Dhiman P, Riley RD, Beam AL, Van Calster B, et al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods. BMJ. 2024;385:e078378. Moons KGM, Wolff RF, Riley RD, Whiting PF, Westwood M, Collins GS, et al. PROBAST: A Tool to Assess Risk of Bias and Applicability of Prediction Model Studies: Explanation and Elaboration. Ann Intern Med. 2019;170:W1. Article PubMed Google Scholar Collins GS, Dhiman P, Andaur Navarro CL, Ma J, Hooft L, Reitsma JB, et al. Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence. BMJ Open. 2021;11: e048008. Article PubMed PubMed Central Google Scholar Moons KGM, Damen JAA, Kaul T, Hooft L, Andaur Navarro C, Dhiman P, et al. PROBAST+AI: an updated quality, risk of bias, and applicability assessment tool for prediction models using regression or artificial intelligence methods. BMJ. 2025;388:e082505. Sounderajah V, Ashrafian H, Golub RM, Shetty S, De Fauw J, Hooft L, et al. Developing a reporting guideline for artificial intelligence-centred diagnostic test accuracy studies: the STARD-AI protocol. BMJ Open. 2021;11: e047709. Article PubMed PubMed Central Google Scholar Hernandez-Boussard T, Bozkurt S, Ioannidis JPA, Shah NH. MINIMAR (MINimum Information for Medical AI Reporting): Developing reporting standards for artificial intelligence in health care. J Am Med Inform Assoc. 2020;27:2011 5. Article PubMed PubMed Central Google Scholar Mongan J, Moy L, Kahn CE. Checklist for Artificial Intelligence in Medical Imaging (CLAIM): A Guide for Authors and Reviewers. Radiol Artif Intell. 2020;2: e200029. Article PubMed PubMed Central Google Scholar Hawksworth C, Elvidge J, Knies S, Zemplenyi A, Petyk  Z, Siirtola P, et al. Protocol for the development of an artificial intelligence extension to the Consolidated Health Economic Evaluation Reporting Standards (CHEERS) 2022. medRxiv. 2023. https://doi.org/10.1101/2023.05.31.23290788. Elvidge J, Hawksworth C, Av ar TS, Zemplenyi A, Chalkidou A, Petrou S, et al. Consolidated Health Economic Evaluation Reporting Standards for Interventions that use Artificial Intelligence (CHEERS-AI). Value Health. 2024;:S1098301524023660. Bilbro NA, Hirst A, Paez A, Vasey B, Pufulete M, Sedrakyan A, et al. The IDEAL Reporting Guidelines: A Delphi Consensus Statement Stage Specific Recommendations for Reporting the Evaluation of Surgical Innovation. Ann Surg. 2021;273:82 5. Article PubMed Google Scholar McCulloch P, Altman DG, Campbell WB, Flum DR, Glasziou P, Marshall JC, et al. No surgical innovation without evaluation: the IDEAL recommendations. The Lancet. 2009;374:1105 12. Article Google Scholar Marcus HJ, Bennett A, Chari A, Day T, Hirst A, Hughes-Hallett A, et al. IDEAL-D Framework for Device Innovation: A Consensus Statement on the Preclinical Stage. Ann Surg. 2022;275:73 9. Article PubMed Google Scholar Lekadir K, Osuala R, Gallin C, Lazrak N, Kushibar K, Tsakou G, et al. FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. 2021. https://doi.org/10.48550/ARXIV.2109.09658. Dagan N, Devons-Sberro S, Paz Z, Zoller L, Sommer A, Shaham G, et al. Evaluation of AI Solutions in Health Care Organizations   The OPTICA Tool. NEJM AI. 2024;1(1):e2300269. European Commission. Ethics guidelines for trustworthy AI. Brussels: European Commission; 2019. Available from: https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai. Accessed 21 Apr 2025. The STANDING Together collaboration. Recommendations for Diversity, Inclusivity, and Generalisability in Artificial Intelligence Health technologies and Health Datasets. 2023. Available from: https://zenodo.org/records/10048356. Whicher D, Rapp T. The Value of Artificial Intelligence for Healthcare Decision Making Lessons Learned. Value Health. 2022;25:328 30. Article PubMed Google Scholar Simon GJ, Aliferis C, editors. Artificial Intelligence and Machine Learning in Health Care and Medical Sciences: Best Practices and Pitfalls. Cham: Springer International Publishing; 2024. Google Scholar Das S, Nayak SP, Sahoo B, Nayak SC. Machine Learning in Healthcare Analytics: A State-of-the-Art Review. Arch Comput Methods Eng. 2024. https://doi.org/10.1007/s11831-024-10098-3. Article Google Scholar Mello MM, Guha N. Understanding Liability Risk from Using Health Care Artificial Intelligence Tools. N Engl J Med. 2024;390:271 8. Article PubMed Google Scholar Markowetz F. All models are wrong and yours are useless: making clinical prediction models impactful for patients. Npj Precis Oncol. 2024;8:54. Article PubMed PubMed Central Google Scholar Council of the European Union. Proposal for a regulation of the European Parliament and of the Council laying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain Union legislative acts analysis of the final compromise text with a view to agreement. Brussels: Council of the European Union; 2024. Available from: https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex%3A52021PC0206. Accessed 21 Apr 2025. Recommendation on the Ethics of Artificial Intelligence. programme and meeting document [184681]. France: UNESCO; 2022. Google Scholar United Nations. Seizing the opportunities of safe, secure and trustworthy artificial intelligence systems for sustainable development. United Nations; 2024. Available from: https://digitallibrary.un.org/record/4040897. Accessed 21 Apr 2025. Biden Jr JR. Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence. 2023. https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/. Accessed 22 Apr 2024. European Parliament. Artificial Intelligence Act. Brussels: European Parliament; 2024. Available from: https://eur-lex.europa.eu/eli/reg/2024/1689/oj. UNESCO. Readiness assessment methodology: a tool of the Recommendation on the Ethics of Artificial Intelligence. Paris: UNESCO; 2023. Available from: https://www.unesco.org/en/articles/readiness-assessment-methodology-tool-recommendation-ethics-artificial-intelligence. Accessed 21 Apr 2025. UK Government. International scientific report on the safety of advanced AI: interim report. London: UK Government; 2024. Josep SG, Sarah DN, Elias B, Ignacio S, Tatjana E, Andr  A-A, et al. Harmonised Standards for the European AI Act. Seville: European Commission; 2024. Available from: https://publications.jrc.ec.europa.eu/repository/handle/JRC139430. Accessed 21 Apr 2025. Ethics and governance of artificial intelligence for health. WHO guidance. Geneva: World Health Organization; 2021. Google Scholar OECD AI. OECD AI Principles overview. https://oecd.ai/en/ai-principles. Accessed 24 Apr 2024. CAIDP. Center for AI and Digital Policy (CAIDP). Universal Guidelines for AI. 2018. https://www.caidp.org/universal-guidelines-for-ai/. Accessed 24 Apr 2024. Asilomar AI Principles. Open Letters. 2017. https://futureoflife.org/open-letter/ai-principles/. Accessed 12 Oct 2024. Ortega-Bola os R, Bernal-Salcedo J, Germ n Ortiz M, Galeano Sarmiento J, Ruz GA, Tabares-Soto R. Applying the ethics of AI: a systematic review of tools for developing and assessing AI-based systems. Artif Intell Rev. 2024;57:110. Article Google Scholar Prainsack B, Forg  N. New AI regulation in the EU seeks to reduce risk without assessing public benefit. Nat Med. 2024. https://doi.org/10.1038/s41591-024-02874-2. Article PubMed Google Scholar Chaddad A, Peng J, Xu J, Bouridane A. Survey of Explainable AI Techniques in Healthcare. Sensors. 2023;23:634. Article PubMed PubMed Central Google Scholar Cohen IG, Evgeniou T, Gerke S, Minssen T. The European artificial intelligence strategy: implications and challenges for digital health. Lancet Digit Health. 2020;2:e376 9. Article PubMed Google Scholar Liu X, Glocker B, McCradden MM, Ghassemi M, Denniston AK, Oakden-Rayner L. The medical algorithmic audit. Lancet Digit Health. 2022;4:e384 97. Article CAS PubMed Google Scholar Vayena E, Blasimme A, Cohen IG. Machine learning in medicine: Addressing ethical challenges. PLOS Med. 2018;15: e1002689. Article PubMed PubMed Central Google Scholar Klonoff DC. The Current Status of mHealth for Diabetes: Will it Be the Next Big Thing? J Diabetes Sci Technol. 2013;7:749 58. Article PubMed PubMed Central Google Scholar Zeitoun J-D, Ravaud P. Artificial intelligence in health care: value for whom? Lancet Digit Health. 2020;2:e338 9. Article PubMed Google Scholar Boulais W. Transforming Healthcare with AI: The NUHS Model and Its Global Implications. 2024. https://www.linkedin.com/pulse/transforming-healthcare-ai-nuhs-model-its-global-wayne-boulais-yukac/. Accessed 5 Sep 2024. Meet the doctor whose healthcare innovations are  out of this world.  NUHS+ Health Inside Out. 2024. https://nuhsplus.edu.sg/article/meet-the-doctor-whose-healthcare-innovations-are--out-of-this-world. Accessed 6 Sep 2024. Shorter hospital waiting times with artificial intelligence. NUHS+ Health Inside Out. 2023. https://nuhsplus.edu.sg/article/shorter-hospital-waiting-times-with-artificial-intelligence. Accessed 6 Sep 2024. Chua CE, Lee Ying Clara N, Furqan MS, Lee Wai Kit J, Makmur A, Tham YC, et al. Integration of customised LLM for discharge summary generation in real-world clinical settings: a pilot study on RUSSELL GPT. Lancet Reg Health - West Pac. 2024;51:101211. Pantuck AJ, Lee D, Kee T, Wang P, Lakhotia S, Silverman MH, et al. Modulating BET Bromodomain Inhibitor ZEN 3694 and Enzalutamide Combination Dosing in a Metastatic Prostate Cancer Patient Using CURATE.AI, an Artificial Intelligence Platform. Adv Ther. 2018;1:1800104. Blasiak A, Truong A, Tan WJ Lester, Kumar KS, Tan SB, Teo CB, et al. PRECISE CURATE.AI: A prospective feasibility trial to dynamically modulate personalized chemotherapy dose with artificial intelligence. J Clin Oncol. 2022;40 16_suppl:1574 1574. Blasiak A, Tan LWJ, Chong LM, Tadeo X, Truong ATL, Senthil Kumar K, et al. Personalized dose selection for the first Waldenstr m macroglobulinemia patient on the PRECISE CURATE.AI trial. Npj Digit Med. 2024;7:223. Zarrinpar A, Lee D-K, Silva A, Datta N, Kee T, Eriksen C, et al. Individualizing liver transplant immunosuppression using a phenotypic personalized medicine platform. Sci Transl Med. 2016;8. Seyhan AA. Lost in translation: the valley of death across preclinical and clinical divide   identification of problems and overcoming obstacles. Transl Med Commun. 2019;4(1):18. https://doi.org/10.1186/s41231-019-0050-7. Article Google Scholar Nagendran M, Chen Y, Lovejoy CA, Gordon AC, Komorowski M, Harvey H, et al. Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies. BMJ. 2020;368:m689. https://doi.org/10.1136/bmj.m689. Rajagopal A, Ayanian S, Ryu AJ, Qian R, Legler SR, Peeler EA, et al. Machine Learning Operations in Health Care: A Scoping Review. Mayo Clin Proc Digit Health. 2024;2:421 37. Article PubMed PubMed Central Google Scholar Ahmed MI, Spooner B, Isherwood J, Lane M, Orrock E, Dennison A. A Systematic Review of the Barriers to the Implementation of Artificial Intelligence in Healthcare. Cureus. 2023. https://doi.org/10.7759/cureus.46454. Article PubMed PubMed Central Google Scholar Teo ZL, Kwee A, Lim JC, Lam CS, Ho D, Maurer-Stroh S, et al. Artificial intelligence innovation in healthcare: Relevance of reporting guidelines for clinical translation from bench to bedside. Ann Acad Med Singapore. 2023;52:199 212. Article PubMed Google Scholar Ayorinde A, Mensah DO, Walsh J, Ghosh I, Ibrahim SA, Hogg J, et al. Health Care Professionals  Experience of Using AI: Systematic Review With Narrative Synthesis. J Med Internet Res. 2024;26: e55766. Article PubMed PubMed Central Google Scholar Celi LA, Cellini J, Charpignon M-L, Dee EC, Dernoncourt F, Eber R, et al. Sources of bias in artificial intelligence that perpetuate healthcare disparities A global review. PLOS Digit Health. 2022;1: e0000022. Article PubMed PubMed Central Google Scholar Subbaswamy A, Saria S. From development to deployment: dataset shift, causality, and shift-stable models in health AI. Biostatistics. 2020;21(2):345 52. https://doi.org/10.1093/biostatistics/kxz041. Ganapathi S, Palmer J, Alderman JE, Calvert M, Espinoza C, Gath J, et al. Tackling bias in AI health datasets through the STANDING Together initiative. Nat Med. 2022;28:2232 3. Article CAS PubMed Google Scholar Ratwani RM, Sutton K, Galarraga JE. Addressing AI Algorithmic Bias in Health Care. JAMA. 2024;332:1051. Article PubMed Google Scholar Chen F, Wang L, Hong J, Jiang J, Zhou L. Unmasking bias in artificial intelligence: a systematic review of bias detection and mitigation strategies in electronic health record-based models. J Am Med Inform Assoc. 2024;31:1172 83. Article PubMed PubMed Central Google Scholar Ghebrehiwet I, Zaki N, Damseh R, Mohamad MS. Revolutionizing personalized medicine with generative AI: a systematic review. Artif Intell Rev. 2024;57:128. Article Google Scholar Ibrahim M, Khalil YA, Amirrajab S, Sun C, Breeuwer M, Pluim J, et al. Generative AI for synthetic data across multiple medical modalities: a systematic review of recent developments and challenges. arXiv. 2024. https://doi.org/10.48550/arXiv.2407.00116. Takita H, Kabata D, Walston SL, Tatekawa H, Saito K, Tsujimoto Y, et al. Diagnostic performance comparison between generative AI and physicians: a systematic review and meta-analysis. medRxiv. 2024. https://doi.org/10.1101/2024.01.20.24301563. Beam AL, Manrai AK, Ghassemi M. Challenges to the Reproducibility of Machine Learning Models in Health Care. JAMA. 2020;323:305. Article PubMed PubMed Central Google Scholar Azad TD, Ehresman J, Ahmed AK, Staartjes VE, Lubelski D, Stienen MN, et al. Fostering reproducibility and generalizability in machine learning for clinical prediction modeling in spine surgery. Spine J. 2021;21:1610 6. Article PubMed Google Scholar Alberto IRI, Alberto NRI, Altinel Y, Blacker S, Binotti WW, Celi LA, et al. A scientometric analysis of fairness in health AI literature. PLOS Glob Public Health. 2024;4: e0002513. Article PubMed PubMed Central Google Scholar Yang R, Nair SV, Ke Y, D Agostino D, Liu M, Ning Y, et al. Disparities in clinical studies of AI enabled applications from a global perspective. Npj Digit Med. 2024;7:209. Article PubMed PubMed Central Google Scholar Serra-Burriel M, Locher L, Development VKN, Pipeline and Geographic Representation of trials for artificial intelligence, machine learning-enabled medical devices (2010 to 2023). NEJM AI. 2024;1(1):AIpc2300038. https://doi.org/10.1056/AIpc2300038. Google Scholar Liu M, Ning Y, Teixayavong S, Mertens M, Xu J, Ting DSW, et al. A translational perspective towards clinical AI fairness. Npj Digit Med. 2023;6:172. Article PubMed PubMed Central Google Scholar Susanto AP, Lyell D, Widyantoro B, Berkovsky S, Magrabi F. Effects of machine learning-based clinical decision support systems on decision-making, care delivery, and patient outcomes: a scoping review. J Am Med Inform Assoc. 2023;30:2050 63. Article PubMed PubMed Central Google Scholar Romero-Brufau S, Wyatt KD, Boyum P, Mickelson M, Moore M, Cognetta-Rieke C. A lesson in implementation: A pre-post study of providers  experience with artificial intelligence-based clinical decision support. Int J Med Inf. 2020;137: 104072. Article Google Scholar Van De Sande D, Van Genderen ME, Smit JM, Huiskens J, Visser JJ, Veen RER, et al. Developing, implementing and governing artificial intelligence in medicine: a step-by-step approach to prevent an artificial intelligence winter. BMJ Health Care Inform. 2022;29: e100495. Article PubMed PubMed Central Google Scholar Katharine M. Should AI Models Be Explainable? That depends. 2021. https://hai.stanford.edu/news/should-ai-models-be-explainable-depends. Accessed 20 Mar 2024. Rajpurkar P, Chen E, Banerjee O, Topol EJ. AI in health and medicine. Nat Med. 2022;28:31 8. Article CAS PubMed Google Scholar Stiefel KM, Coggan JS. The energy challenges of artificial superintelligence. Front Artif Intell. 2023;6:1240653. Article PubMed PubMed Central Google Scholar Jovanovic M, Mitrov G, Zdravevski E, Lameski P, Colantonio S, Kampel M, et al. Ambient Assisted Living: Scoping Review of Artificial Intelligence Models, Domains, Technology, and Concerns. J Med Internet Res. 2022;24: e36553. Article PubMed PubMed Central Google Scholar Wolff J, Pauling J, Keck A, Baumbach J. Systematic Review of Economic Impact Studies of Artificial Intelligence in Health Care. J Med Internet Res. 2020;22: e16866. Article PubMed PubMed Central Google Scholar Wolff J, Pauling J, Keck A, Baumbach J. Success Factors of Artificial Intelligence Implementation in Healthcare. Front Digit Health. 2021;3: 594971. Article PubMed PubMed Central Google Scholar Saenz AD, Mass General Brigham AI Governance Committee, McCoy T, Mantha AB, Martin R, Damiano R, et al. Establishing responsible use of AI guidelines: a comprehensive case study for healthcare institutions. Npj Digit Med. 2024;7:348. Download references We would like to thank Ms. Lin Jing, a data scientist in the Department of Biomedical Informatics, Yong Loo Lin School of Medicine, National University of Singapore who assisted the preparation of Additional File 3 on the experience of Pathfinder Dashboard AI tools development and challenges in National University Health System (NUHS), Singapore. I am grateful to Dr. Judice, Dr. Mohammad Shaheryar Furqan and Professor Dr. Ngiam Kee Yuan for their hospitality when hosting me at NUS, where this work was completed. I was supported and sponsored by UPM to attend some of the course and for my sabbatical leave to NUS for a period of nine months when a few other works were also produced beside this one. I would like to express my appreciation to the Foundation for Advancing Family Medicine (FAFM) for the grant of the Besrour Centre Family Medicine Early Career Researcher Award 2022 that supported my attendance to the online courses of the EITCA/Artificial Intelligence Academy. I greatly appreciate my colleagues in the Department of Family Medicine, UPM, who were kind to spare me and to stand in for me to make my sabbatical and research leaves possible. Lastly, the authors acknowledge the use of ChatGPT 3.5, 4o and o1 (OpenAI, San Francisco, CA, USA) to assist in drafting and language editing of portions of this manuscript. The authors have reviewed and edited the content produced by ChatGPT for accuracy and integrity, and accept full responsibility for the final version of the manuscript. This research did not receive any specific grant from funding agencies in the public, commercial or not-for-profit sectors. Department of Biomedical Informatics, Yong Loo Lin School of Medicine, National University of Singapore C/O NUHS Tower Block, Level 8, 1E Kent Ridge Road, Singapore, 119228, Singapore Boon-How Chew & Kee Yuan Ngiam Department of Family Medicine, Faculty of Medicine and Health Sciences, Universiti Putra Malaysia, Serdang, Selangor, 43400, Malaysia Boon-How Chew Department of Surgery, Division of General Surgery (Thyroid and Endocrine Surgery), National University of Singapore, University Surgical Cluster, National University Hospital National University Health System Corporate Office, Singapore, Singapore Kee Yuan Ngiam Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar BHC conceived the work, completed the acquisition and analysis of data, wrote and drafted the manuscript. BHC and KYN involved in the interpretation of data, read and approved the final version of the manuscript. BHC: https://twitter.com/chewboonhow. Correspondence to Boon-How Chew. Not applicable. Not applicable. The authors declare no competing interests. Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Additional file 1. Table 1 2, and Fig. 1. Table 1- Glossary. Figure 1- AI Landscape. Table 2- Important organisations on AI-related matters. Additional file 3. Table 1: The Experience of Pathfinder Dashboard AI tools development and challenges in National University Health System (NUHS), Singapore. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions Chew, BH., Ngiam, K.Y. Artificial intelligence tool development: what clinicians need to know?. BMC Med 23, 244 (2025). https://doi.org/10.1186/s12916-025-04076-0 Download citation Received: 16 September 2024 Accepted: 11 April 2025 Published: 24 April 2025 DOI: https://doi.org/10.1186/s12916-025-04076-0 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement ISSN: 1741-7015 By using this website, you agree to our Terms and Conditions, Your US state privacy rights, Privacy statement and Cookies policy. Your privacy choices/Manage cookies we use in the preference centre.   2025 BioMed Central Ltd unless otherwise stated. Part of Springer Nature.",2
A bibliometric analysis of the advance of artificial intelligence in medicine - Frontiers,https://news.google.com/rss/articles/CBMijgFBVV95cUxOeGliRzdkcm55ajcxU04xS1FLTnE2b0RwYUNObl9yMS1KWl9ndlcwbkJvZ3cxc3NPcnN5d2hKVXZHUFZsQ1lfdHFJZ2ZPalRLdjVsSUNGTE96bU5RUENGYUp0OHJyQnhWNFJzMHZ5anE1WVZmZlZlTzVRN0NrdmdGSlBVMTJYZUg0Z0tWNWRB?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better ORIGINAL RESEARCH article Front. Med., 21 February 2025 Sec. Precision Medicine Volume 12 - 2025 | https://doi.org/10.3389/fmed.2025.1504428 This article is part of the Research TopicPioneers & Pathfinders: 10 Years of Frontiers in MedicineView all 17 articles Introduction: The integration of artificial intelligence (AI) into medicine has ushered an era of unprecedented innovation, with substantial impacts on healthcare delivery and patient outcomes. Understanding the current development, primary research focuses, and key contributors in AI applications in medicine through bibliometric analysis is essential. Methods: For this research, we utilized the Web of Science Core Collection as our main database and performed a review of literature covering the period from January 2019 to December 2023. VOSviewer and R-bibliometrix were performed to conduct bibliometric analysis and network visualization, including the number of publications, countries, journals, citations, authors, and keywords. Results: A total of 1,811 publications on research for AI in medicine were released across 565 journals by 12,376 authors affiliated with 3,583 institutions from 97 countries. The United States became the foremost producer of scholarly works, significantly impacting the field. Harvard Medical School exhibited the highest publication count among all institutions. The Journal of Medical Internet Research achieved the highest H-index (19), publication count (76), and total citations (1,495). Four keyword clusters were identified, covering AI applications in digital health, COVID-19 and ChatGPT, precision medicine, and public health epidemiology.  Outcomes  and  Risk  demonstrated a notable upward trend, indicating the utilization of AI in engaging with clinicians and patients to discuss patients  health condition risks, foreshadowing future research focal points. Conclusion: Analyzing our bibliometric data allowed us to identify progress, focus areas, and emerging fields in AI for medicine, pointing to potential future research directions. Since 2019, there has been a steady rise in publications related to AI in medicine, indicating its rapid growth. In addition, we reviewed journals and significant publications to pinpoint prominent countries, institutions, and academics. Researchers will gain important insights into the current landscape, collaborative frameworks, and key research topics in the field from this study. The findings suggest directions for future research. Artificial intelligence (AI) involves interpreting information and analyzing the application of algorithms. Advanced computer algorithms are utilized in AI to perform tasks such as decision-making and data interpretation, similar to humans. AI offers diverse options for identifying and solving various problems. Like humans, AI machines have the capacity for critical thinking. AI operates through multiple pathways, enabling systems to detect new patterns and derive different formulations from given data. Ongoing AI development has transformed medical practice significantly, transitioning from traditional methods to digital healthcare. Leveraging its advanced algorithms and deep learning capabilities, AI has become a valuable tool for physicians and healthcare providers, aiding in various aspects such as health information management, geolocation of health data, disease surveillance, predictive analytics, decision-making support, and medical imaging (1 4). As a significant cause of death and disability around the world, stroke presents a considerable threat to public health. Neuroimaging plays a significant role in stroke research, and CT scans are the usual choice for examining patients suspected of having a stroke. In a study involving 477 patients, CT angiography was processed using an automatic detection algorithm, obtaining a diagnostic sensitivity of 94% and a negative predictive value of 98% in merely 5 min (5). So, AI markedly improved diagnostic efficiency, especially for patients who required transfer to comprehensive stroke centers for thrombectomy, thus reducing further brain damage and enhancing their prognosis. Furthermore, The ability of AI to detect infarction regions was enhanced with a 1 4 h interval from symptom onset to imaging, emphasizing its potential for early intervention (6). This development is being applied to the entire cardiovascular medicine sector, which is increasingly using AI technologies. Machine learning (ML) represents a vital component of AI, facilitating autonomous learning from data by algorithms. Machine learning includes methods like linear regression, logistic regression, support vector machines (SVM), and decision trees (7). ML is applied in numerous sectors, including the analysis of medical images, the prediction of patient prognoses, and the formulation of personalized treatment plans (8). Tasks that are repetitive or manually intensive, like validating general chemistry test results or analyzing blood cells and urine cultures, have seen improved efficiency due to ML. Research involving continuous glucose monitoring (CGM) data for blood glucose prediction has indicated that ML can predict type 1 diabetes with accuracy rates surpassing 90% (9, 10) and inaccuracies in clinical laboratory test outcomes, like blood placed in incorrect tubes, mislabeled samples, and contamination, have been detected using ML during clinical lab testing (11). Thus, in laboratory medicine, ML has been researched to enhance the precision and dependability of test outcomes. Besides, AI systems can offer healthcare professionals continuous, and potentially instantaneous, access to medical updates from a multitude of sources, including academic journals, medical texts, clinical experiences, and patient data (12). This facilitates informed clinical decision-making, enables accurate forecasting of health outcomes, and enables accurate health risk alerts and outcome predictions (13). Medical research shows that AI holds more promise than other fields when it comes to output-input ratio (14). Bibliometric analysis examines its structure, quantity, and impact. Researchers, institutions, countries, or specific fields of research may be analyzed. It employs mathematical and probabilistic methods to retrieve and study information from academic journals. Bibliometric endeavors to discover trends, patterns, and developments in research literature. A significant impact of this analysis is in relation to the appraisal of academic performance, research productivity, and distribution of resources (15, 16). Lately, numerous global bibliometric studies have been conducted with the help of CiteSpace and VOSviewer. The analyses have concentrated on the overall rehabilitation statuses and research trends concerning diseases such as cancer, ankylosing spondylitis, motor and neuropathic pain, and osteoarthritis (17, 18). As AI becomes a crucial tool in medicine, it is important to comprehend its influence and evolution in the scientific domain. Nonetheless, the significant research and development in this field pose a challenge: the importance of systematically reviewing and measuring the rise of scientific literature on AI in healthcare. So our research investigates article characteristics on AI in medicine over the past 5 years, reflecting a growing interest in this domain. This surge in interest aligns with the heightened awareness of AI s significance in risk assessment, diagnosis, treatment, and prevention of diseases. Comparing to previous studies, the value of this research is found in its methodical review and integration of existing literature, effectively charting the complex network of research endeavors and collaborative efforts. By delineating the current knowledge landscape, this mapping creates a foundation for subsequent research and acts as a pivotal reference for researchers, medical practitioners, and policymakers, steering their initiatives toward the integration of AI in medicine, construction a knowledge graph in this area to deliver valuable insights for future investigations. A comprehensive search methodology was employed to search for works on the subject of AI in medicine from the Web of Science Core Collection (WoSCC) database spanning from January 2019 to December 2023. More than 21,000 journals in science, social sciences, and humanities can be accessed via the WoSCC (19). We chose this database for the multidisciplinary nature and citation tracking, which aids in identifying the most powerfull AI in medicine publications. It was possible to locate publications pertaining to the incidence, causes, genetic aspects, symptoms, identification, and treatment of this disorder by accessing this database. WoSCC stands out as a significant online database and is viewed as the most ideal for bibliometric analysis (19). The search strategy was: TI = ( artificial intelligence  OR  machine learning  OR  deep learning  AND  medicine ). It was retrieved between January 2019 and December 2023. Our search was limited to  article  documents. Only English-language papers were included in the search. The diagram outlining the process of choosing publications are show in Figure 1. Figure 1. Flowchart depicting the inclusion and exclusion process for literary research. Sources and data extracted from WoSCC files, in txt or BibTeX format, were imported into VOSviewer 1.6.18 and R 4.0.2 for analysis and visualization. The extracted data include information on authorship, institutions, publications, keywords, and other relevant details essential for bibliometric analysis. This data extraction methodology ensures the acquisition of current and relevant information in AI applied to medicine, enabling a comprehensive and representative analysis. The WoSCC database yielded 1,811 articles centered on AI in the medical field. These reports were collectively authored by 12,376 individuals representing 3,583 organizations across 97 countries. A total of 565 journals published these works, which collectively referenced 73,095 citations from 15,180 journals. Over the previous 5 years, there has been a rise in the number of publications, which has grown by 28.4% each year, indicating the field s developmental trend. This growth is visually depicted in Figure 2, illustrating a consistent year-on-year increase in publication output. Figure 2. The trend of publications about AI in medicine. The leading 10 countries in AI for medical research are outlined in Table 1, detailing their publication count (NP), total citations (NC), and average citations (AC). The United States ranked first in both publications and citations, contributing 709 papers (39.09%) and 14,764 citations. China followed with 371 papers (31.41%), and England with 189 papers (16.00%). It is noteworthy that despite China ranking second in NP, but the AC was relatively lower compared to other top 10 productive countries. Table 1. Displays the breakdown of the leading 10 countries. The study of scientific cooperation involves scholars working collectively to generate new scientific knowledge. Figure 3 presents the co-authorship analysis, with node size representing the number of articles published by each country. There is a close collaboration between nodes, with a wider line indicating more intensity. With 624 links and a total link strength of 2,653, the co-authorship network analysis of AI in medicine shows an organized structure with five clusters and strong global collaboration. Leading this collaboration are the United States, China, England, Germany, and Italy. Based on Table 2, the foremost five authors and co-cited authors have played a crucial role in contributing to AI research in medicine. The author with the most articles (15) is Li J, and the author with the most citations (20) is Ho MT. Table 2. An overview of the top five authors and their co-citations. Figure 3. Visualization of countries  relationships. The wide range of geographical locations represented in the network highlights the strong global cooperation in the industry, which promotes a variety of viewpoints and expertise in research. These results give significant viewpoints on the patterns in AI research in the medical field and emphasize the potential for increased cooperation among specific nations. As presented in Table 3, we collected the NC and publication output from the top 10 sources with the highest H-index in the area of AI in medicine. Notably, the Journal of Medical Internet Research exhibited the highest H-index (H-index = 19) and publication count (NP = 76). Applying Bradford s law to the 1,495 sources, we identified 13 journals, including Journal of Medical, Cancers, BMJ Open, JMIR Medical Informatics, and Journal of Personalized Medicine, as core journals as a result of t their relatively high publication output (Figure 4). Table 3. Leading 10 journals and their co-cited counterparts. Figure 4. Bradford s law identifies primary sources. To evaluate institutional importance and collaboration, we developed a network plot (Figure 5). Institutions are ranked by the size of their nodes and the level of their activity, and collaboration between them is measured by the thickness of their lines. Research institutions in AI medicine were grouped into 12 primary clusters. Institutions with the greatest prominence and activity were Harvard Medical School, followed by Stanford University, University of Toronto, Mayo Clinic, and Johns Hopkins University. Among the top five institutions, four are located in the United States, with the exception of the University of Toronto in Canada. Harvard Medical School exhibited the highest level of collaboration. Shanghai Jiao Tong University had the highest centrality in China, followed by Sichuan University and South China University. Furthermore, these institutions were central to facilitating collaboration within the same groups. Additionally, Stanford University and Harvard Medical School showed significant disparities in their arrangement and distribution within the co-occurrence network. With a clustering coefficient of 12, research demonstrates unique characteristics, advantages, or distinct directions. Figure 5. Scientific collaboration between institutions. Table 4 shows the 10 papers that are most frequently cited in the field of AI in medicine, representing significant contributions and perhaps the most renowned works in this domain. Normalized Local Citations (NLC) serve to mitigate variations in citation counts due to differences in academic disciplines and publication dates. The article  Key challenges for delivering clinical impact with artificial intelligence,  authored by Kelly et al. (5) from Google Health, London, United Kingdom, and published in BMC Medicine in 2019, holds the highest NLC (16.98). This article emphasizes the exciting opportunity presented by AI to enhance healthcare. It stresses the essential need for robust, prospective clinical evaluation to ensure the safety and effectiveness of AI systems. Such evaluation should employ clinically applicable performance metrics that extend beyond technical accuracy to encompass the impact of AI on care quality, healthcare professionals  variability, clinical practice efficiency and productivity, and patient outcomes (5). Closely following in the NLC ranking is the article  The state of artificial intelligence-based FDA-approved medical devices and algorithms: an online database,  authored by Benjamens et al. (21) from the Netherlands in 2020. This article examines FDA-approved AI-based medical devices and algorithms. Table 4. The 10 most frequently cited local papers on the topic. Document co-citation identifies literature that is frequently cited together by different authors. This method visualizes citation co-occurrence between two publications to assess their connection (22). Figure 6 illustrates a co-citation reference map for AI in medicine. In 2019, the paper that received the most citations, totaling 110, was  High-performance medicine: The convergence of human and artificial intelligence  by Topol (23). This article highlighted AI s influence in medicine on three fronts: aiding clinicians with swift and precise image analysis; improving health systems by streamlining processes and minimizing medical mistakes; and empowering patients with self-data processing. The second-ranked paper illustrates skin lesion classification utilizing Deep Convolutional Neural Networks (CNNs) (24). With a dataset of 129,450 clinical images and 2,032 diseases, the authors trained a CNN that is much larger than previous datasets. The CNN s classification of skin cancer matches the performance of dermatologists in both tasks. Figure 6. The analysis of co-cited documents. Figure 7 illustrates the most frequently used keywords, in Figure 8, the use patterns and trends of author keywords are summarized. The keyword  Classification  emerged as the most frequently used term after 2019, but has experienced a slight decline since 2021. Its usage surged significantly, reflecting an increased research focus in this area. Furthermore, keywords such as  Cancer,   Diagnosis,  and  Prediction  gained high popularity from 2019 to 2022. Since 2022, the keywords  Artificial intelligence,   Outcomes,  and  Risk  have exhibited a notable increase in usage, indicating their promise as developing focal points in research. Figure 7. Tree Map of the keyword. Figure 8. Time series of author keywords. A three-field plot in Figure 9 presents the co-occurrence relationships among authors, keywords, and publication sources. The size of each node signified its frequency or importance, and the connections between nodes represented co-occurrence relationships, with the thickness of the lines indicating how often or strongly these co-occurred. The three categories, listed from left to right, are authors, keywords, and associated journals. The most notable co-occurrence was between  machine learning  and the Journal of Medical Internet Research. The keyword  Artificial intelligence  is mainly associated with the Journal of Medical Internet Research, whereas  deep learning  is primarily linked to JMIR Medical Informatics. Prominent authors demonstrate a robust co-occurrence with the keyword  machine learning.  It is noteworthy that certain authors co-occurred with specific keywords significantly: Wang J and Li J with  machine learning,  Liu Y with  artificial intelligence,  and Lee S with  deep learning.  Based on these observations, we highlight key authors and teams in this area of research, revealing significant co-occurrence with the keyword  machine learning  and suggesting potential future research directions. Keywords co-occurred heavily with certain authors: Wang J and Li J with  machine learning,  Liu Y with  artificial intelligence,  and Lee S with  deep learning.  Research directions can be derived from these observations, which highlight key authors and research teams in the research domain. Figure 9. A three-field plot connecting authors (AU), keywords (DE), and sources (SO). By analyzing keyword co-occurrences, we were able to identify popular topics and assist scholars in gaining a deeper understanding of current scientific concerns. Developing a co-occurrence network from authors  keywords helps to pinpoint semantic similarities between terms and reveals the knowledge structures in the relevant field (20). The parameters mentioned in  2 Materials and methods  section served to explain the network illustrated in Figure 10. The authors  keywords (nodes) are depicted in Figure 10, categorized into four communities (clusters or subdomains). The grouping of terms within a cluster reflects their contextual similarity and relationship. Within the network, edges signify terms that co-occur in the same document. In the red cluster, there are nine terms with primary keywords including  digital health,   COVID-19,  and  AI.  This cluster explores AI applications in digital health, COVID-19, and ChatGPT. The blue cluster, comprising 37 nodes, prominently features keywords like ML, AI, precision medicine, and deep learning, focusing on AI applications in precision medicine. The green cluster explores AI applications in epidemiology, whereas the fourth cluster focuses on AI applications in public health. Figure 10. Co-occurrences of keywords were used to cluster research topics. In this research, we performed a bibliometric analysis of relevant literature concerning the application of AI in medicine spanning from 2019 to 2023. Trends over the specified time frame were discerned by tracking publications per year. Articles with high citation counts, identified as those in the top percentile, were recorded because they frequently signify significant scientific advancements. The productivity of countries was assessed based on the volume of AI in medicine publications across the time frame. Keyword analysis was used to identify current research themes and topics of interest. Tracking these bibliometric parameters over time allowed for the visualization of the growth of global AI in medicine research. As represented in Figure 2, there were only 144 articles released in 2019, which suggests a nascent stage of understanding among researchers. The volume of research papers has consistently increased, averaging an annual growth rate of 28.4%. This trend indicates a mounting scholarly interest in the study of AI in medicine, solidifying its status as a popular and enduring research topic. The timeframe includes the emergence of key applications of AI in medicine, such as digital health, precision medicine, and the response to the COVID-19 pandemic, which have significantly impacted healthcare delivery and research priorities. Based on the review of nations, the United States played the most crucial role in NP and NC, indicating its leadership role in AI. United States President Donald Trump issued the executive order  Maintaining American Leadership in Artificial Intelligence  on 11 February 2019, directing federal agencies to accelerate AI research and development (25). With USD 10,202 spent on healthcare per person, the United States outspends any other nation (26), which may help to explain why there are more publications there. Given the United States has significantly impacted on this academic field and because the majority of partnerships on AI in medicine, there is a clear need for increased international collaboration with the United States. Because of the country s degree of economic development and the support of national policy, the majority of the pertinent publications were published by China. While China s AC was relatively low. These findings indicate a disparity in China s output quantity and quality. Researchers should, therefore, direct their attention to this matter. This challenge requires enhanced collaboration with different nations, including the United States, England, and Germany, while actively tracking scientific progress and performing thorough research. Another implication involves the factors influencing research output in medical AI. Empirical data indicate that influential authors, identified by total and per-paper citations, are typically those who either lead a field with sustained productivity or develop widely applicable methodologies (27). The productivity of junior and inexperienced authors is also significantly improved by senior and productive authors (28). At the institutional level, Harvard Medical School has been recognized as the leading institution due to its high volume of published articles in this field, signifying its paramount importance and activity in this domain. In China, Shanghai Jiao Tong University, Sichuan University, and South China University have achieved notable prominence, despite having limited ties to the United States. Research institutions worldwide must collaborate extensively to advance global research in this field. This collaboration will help understand the similarities, differences, and correlations with medicine among various racial groups. Based on the journal analysis, research on AI in medicine tends to be prominent in large-scale journals related to comprehensive medical topics and internet-related issues. An NC metric measures the influence of a journal, whereas an H metric measures both publication volume and citations (29). The Journal of Medical achieved the highest H-index. In this journal, discussions focus on novel systems techniques and methods for managing hospitals and clinics, pathology, radiology, pharmaceutical delivery, medical records, and patient support systems. It delivers insightful essays, articles, and studies on a variety of medical systems, from large-scale hospital programs to inventive small-scale services. NPJ Digital Medicine, a peer-reviewed open-access journal by Nature Partner Journals (Nature Publishing Group), focuses on digital medicine research. It covers medical information technology, telemedicine, medical sensor technology, health data analysis, and AI applications in healthcare. The journal seeks to promote innovation in digital medicine and offers a platform for researchers to share original research and review articles to advance medical science. Bradford s law identifies the Journal of Medical, Cancers, BMJ Open, JMIR Medical Informatics, and Journal of Personalized Medicine as key resources for future research, providing valuable references for upcoming scholars. Nevertheless, this legislation focuses solely on the number of publications in pertinent categories, disregarding the quality and impact of the research (30). As a result, it is important to merge research results from various publications to deepen insights into AI applications in medicine. Keywords provide a concise summary of an article s core ideas and are key indicators of research directions and hotspots in a certain field. A shift in keywords as time progresses often indicates the evolution of research hot spots and can be used to guide future research. Figure 8 demonstrates a gradual transition in keywords from AI-based classification and its application in cancer diagnosis and disease prediction to outcomes and risk.  Classification  emerged as the most frequently used keyword. Sidey-Gibbons and Sidey-Gibbons (31) developed three ML models to classify breast cancer. The algorithms included Regularized General Linear Model regression (GLM), SVM with radial basis function kernels, and single-layer Artificial Neural Networks. The authors trained these algorithms on the evaluation sample data and then used them to predict diagnostic outcomes in the validation data set. They compared the model predictions on validation datasets with actual diagnostic decisions to calculate accuracy, sensitivity, and specificity. The integration of advanced technologies such as AI in cancer detection and prediction has led to significant advances and diversified applications. In dermatology, combining conventional neural networks with transfer learning techniques has effectively classified skin lesions, providing a valuable tool for early skin cancer detection (32). AI applications in abdominal cancers now include cancer detection, diagnosis, classification, genomics, genetic alteration detection, tumor micro-environment analysis, predictive biomarker identification, and follow-up (33). By analyzing histopathology images, ML can accurately predict patient prognosis in the context of lung cancer, contributing to precision oncology (34). Furthermore, the application of AI has achieved significant success in medical image-based cancer diagnosis, demonstrating favorable results in image classification, reconstruction, detection, segmentation, registration, and synthesis (35). Since 2022, there has been a significant rise in the use of keywords like  Outcomes  and  Risk,  indicating of the integration of AI in medicine. This integration aims to enhance the dialogue between healthcare providers and patients regarding potential health risks and probable outcomes. Identifying keywords and their evolution in various articles through co-occurrence analysis aids in exploring the focal points within the research field (36). The red cluster explores AI applications in digital health and COVID-19. Digital health refers to the utilization of digital technologies to enhance human health (37). Evidence suggests that AI can utilize electrocardiograph signals to predict atrial fibrillation and select patients for intervention (38). Consequently, this approach allows for targeted therapies addressing actual necessary conditions and diseases. Explainable AI can predict Alzheimer s disease in patients with mild impairments by using interpretable machine-learning algorithms to elucidate complex patterns for individual patient predictions (39). The focus of digital health has shifted beyond the mere diagnosis and treatment of diseases to encompass early prevention, precision intervention, and health management with the citizen at the center. Innovative AI technology can advance intelligent telemedicine and support the creation of a comprehensive digital health platform, potentially guiding future research. The COVID-19 pandemic has profoundly impacted the world in unprecedented ways. Effectively managing the pandemic necessitates accurate and timely information regarding the spread of the SARS-CoV-2 virus, the efficacy of mitigation interventions, and its impact on diverse populations. Numerous previous studies have explored the use of AI in combating COVID-19. Hussain et al. (40) found AI showed to be a powerful tool for predicting, detecting, and reducing infectious disease outbreaks in the course of the COVID-19 pandemic. Additionally, Pham et al. (41) and Nguyen et al. (42) discussed the use of AI in vaccine and drug development. During the COVID-19 pandemic, AI has been widely employed to facilitate various tasks. Robots have been utilized for the efficient distribution of essential food items and for disinfecting areas using ultraviolet rays, thereby reducing human exposure to the virus (43). In hospitals, robots have taken over tasks traditionally performed by healthcare workers, thereby alleviating the burden on medical staff. Furthermore, hospitals have been equipped with 5G-powered temperature-detecting devices, and wearable accessories such as wristbands have been utilized for monitoring heart rates and detecting oxygen levels (44). Additionally, robots have assisted patients during quarantine, enhancing the overall experience. Robots have been utilized for patient health monitoring, conducting scans, and sharing data with researchers via cloud services (44). Their immunity to disease and ease of disinfection makes them effective in laboratory testing and clinical trials. Moreover, robots have served as intermediaries between patients and doctors, thereby minimizing the risk of virus exposure to healthcare professionals (45). ChatGPT, created by OpenAI, is a large language model capable of analyzing and generating text in a way that resembles human intelligence (46). Merely a few months following its release, ChatGPT has already made a significant impact in the field of medical science, embracing scientific research, medical education, medical writing, and diagnostic decision-making (47, 48). ChatGPT is capable of generating scientific articles using suitable vocabulary and varying tones from informal to highly technical (49). By providing accurate differential diagnoses and insights for cancer screening, ChatGPT aids physicians in clinical decision-making (50 52). In the past decade, AI research has significantly improved the forecasting, identification, diagnosis, categorization, treatment, and survival forecasting of diseases (53, 54), fostering medical innovation and promoting a sustainable approach to precision medicine. Biomarker tests or tools indicate normal biological processes, pathogenic processes, or predictive biomarkers are measured once to forecast future events, while monitoring, response, and safety biomarkers are measured over time (55). AI applications have exponentially grown across various fields, with the earliest recorded automated pattern recognition appearing in a 1960 report in The Lancet (56). Precision medicine approaches are already being implemented in the context of cancer, encompassing both diagnosis and treatment. In the realm of cancer diagnosis, current literature (57 59) showcases numerous studies delving into AI s potential, comparing its results to manual detection by pathologists. AI exhibits superior accuracy compared to human pathologists in diagnosing certain cancer types (60 62). The precision medicine approach tailor s cancer treatment plans by considering tumor-associated and inherited genetic variations, environmental exposures, lifestyle, general health, and medical history. AI is revolutionizing drug discovery, target identification and clinical trials. Traditional methods used in drug discovery are often expensive and time-consuming, and they may not consistently offer accurate forecasts of a drug s efficacy and safety. The use of AI algorithms, particularly ML models, has significantly advanced drug discovery through better predictive analytics and target identification. By analyzing vast amounts of data, these algorithms hasten the early stages of drug development by detecting patterns and predicting potential drug candidates (63). AI models, such as those in ML and deep learning, draw on data from genomics, proteomics, chemical structures, and clinical trials to pinpoint drug candidates, evaluate safety, and estimate effectiveness using historical data. By forecasting interactions, toxicity, and pharmacokinetics of compounds, AI allows researchers to prioritize drug candidates for further development (64). Moreover, AI simplifies the process of identifying targets by examining biological data and understanding disease mechanisms. AI-powered strategies, such as network analysis and building knowledge graphs, bring together diverse data sources to highlight promising therapeutic targets (65). AI also integrates multi-omics data effectively, offering a broad understanding of disease pathways and improving target identification accuracy (66). Furthermore, neural networks and other deep learning models rank drug targets by evaluating complex connections between molecular characteristics and disease pathways, aiding in therapeutic intervention (67). To find actionable therapeutic targets in amyotrophic lateral sclerosis (ALS), Pun et al. (68) integrated various bioinformatic and deep learning models, which were trained on disease-specific multitopic and text data to prioritize drug-gable genes, revealing 18 potential targets for ALS treatment. Additionally, West et al. (69) created a deep learning method with an innovative modular structure to pinpoint human genes connected to multiple age-related diseases by studying patterns derived from gene or protein attributes such as Gene Ontology terms, protein protein interactions, and biological pathways. Through automating the identification of suitable participants, AI also has the potential to improve patient recruitment and eligibility in clinical trials. AI algorithms process electronic health records, medical literature, and additional healthcare data to pinpoint potential candidates. Automated prescreening tools help to minimize manual tasks and increase efficiency in recognizing eligible participants (70). Predictive analytics improve recruitment by predicting enrollment rates, enabling trial sponsors to allocate resources efficiently using patient demographics and historical data (71). AI is also altering clinical trial approaches with the help of real-world evidence (RWE) and adaptive trial designs. AI algorithms examine data from various real-world sources to offer a more comprehensive insight into patient demographics, disease development, and treatment results (72). AI supports the division of patients into categories and the recognition of subgroups, targeting distinct patient profiles for adaptive trials, and predictive analytics help in anticipating recruitment rates and treatment responses (73). By adapting trial designs in real-time, protocols are optimized, leading to greater trial efficiency. In general, AI s adaptive trial designs increase flexibility, efficiency, and success. Machine learning, a branch of AI, enables computer algorithms to learn from data without being explicitly programmed to carry out tasks (74). As depicted in Figure 10, deep learning has the most significant impact on precision medicine. Deep learning represents the most advanced form of ML presently available and has emerged since 2010 as an AI technique facilitating the analysis of medical images and genomic studies (75, 76). Deep learning has recently used genomic data to identify two unique glioma subtypes, providing insights into their molecular mechanisms (77). Through deep learning, potential candidates can be efficiently screened, therefore reducing drug discovery costs (78). Without explicit instructions, the system discerned patterns from the data and autonomously learned what to seek and report. Based on the summary of the Public Health and Epidemiology Informatics section in the 2017 IMIA Yearbook (79), precision public/global health and digital epidemiology are still used in 2018 (80, 81). It entails providing the appropriate intervention to the suitable population at the optimal time (80). The latter phrase pertains to employing digital data, especially data not deliberately gathered, to answer epidemiological questions (81). The significant potential of Big Data in epidemiology was showcased by Deiner et al. s (82) innovative study, which demonstrated that monitoring social media for disease symptom queries can lead to early detection of epidemics. Pattern recognition and data analytics were employed to detect, identify, and categorize patterns of disease occurrence associated with conjunctivitis. Conversely, wearable technologies will enable the monitoring and collection of individual medical information and the refinement of the care process. The fusion of AI with virtual reality and augmented reality (83), will enable the creation of both virtual medical services that citizens can access easily and directly, as well as increasingly effective and safe applications for robotic surgery. It is noteworthy that the keyword  ethics  is depicted in Figure 10, indicating a growing focus on AI ethics in medicine. Regulatory laws and guidelines for medical AI are frequently formulated without engaging in dialogue among community members, clinicians, developers, and ethicists. This lack of collaboration may result in regulations that do not align with the experiences of community members as users of medical AI. Ethical concerns highlighted by policymakers and scholars may not match those of patients, providers, and developers, leading to a disconnect that makes ethical decision-making tools ineffective for AI users. The ethical issues identified by policymakers and scholars may not correspond with those of patients, providers, and developers, creating a disconnect that makes ethical decision-making tools ineffective for AI users (84). Analyzing empirical studies on the ethics of medical AI assists educators, researchers, and ethicists in understanding and addressing perceived ethical concerns (85). Understanding the ethical awareness of patients, families, and healthcare providers regarding AI in healthcare is essential for informing the progression and research of medical AI. Identifying stakeholders perceived ethical risks of medical AI allows for the development of practice protocols, organizational norms, and legal requirements to promote AI interventions guided by ethical considerations. Although AI has significantly benefited the healthcare system and advanced medicine, unethical use of this technology can endanger both patients and physicians. Establishing ethical standards for all stakeholders in healthcare and related fields is essential. Establishing global and national protocols to regularly review and validate AI products in clinical and practical settings is essential. Nonetheless, this study still has certain limitations. Firstly, this study only searched the WoSCC database, which is considered one of the most widely used large multidisciplinary abstract databases globally, but it may still have incomplete coverage and the search strategy is not perfect. Secondly, only English-language articles were included. But this limitation is unlikely to affect the study s stability significantly, as the WoSCC database predominantly features articles in English. Finally, there is a lag in the citation numbers for articles, which means that more recently published high-quality articles may have been under explored. Future studies are recommended to be updated accordingly and the words like these:  Computer Heuristics,   Expert Systems,   Fuzzy Logic,   Knowledge Bases,   Natural Language Processing,  and  Neural Networks, Computer  are related to the search strategy. The study analyzed papers from WoSCC published between 2019 and 2023 on the integration of AI and medicine. Our research shows a significant rise in yearly publications, suggesting growing interest in this subject. A bibliometric analysis shows that the United States leads the world both in the volume of publications and their central role, indicating its paramount importance and activity in this domain. Universities are the primary research institutions in this field. So, there remains a requirement for more effective cross-regional and international cooperation to further drive progress. Recent keyword clustering identifies  digital health,   COVID-19,   precision medicine,  and  epidemiology and public health  as emerging research frontiers. It is foreseeable that AI will increasingly play a crucial role in digital health and public health, and has a significantly improvement for the forecasting, identification, diagnosis, categorization, treatment, and survival forecasting of diseases to promote a sustainable approach for precision medicine. This bibliometric study aids researchers in identifying the present state and developing trends in medical AI and is beneficial for optimizing medical resource use and enhancing patients  quality of life. The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. ML: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Supervision, Writing   original draft, Writing   review & editing. LZL: Conceptualization, Data curation, Formal analysis, Investigation, Writing   original draft. LLL: Conceptualization, Data curation, Formal analysis, Investigation, Writing   original draft. ZL: Conceptualization, Investigation, Writing   original draft, Writing   review & editing. XY: Conceptualization, Data curation, Formal analysis, Investigation, Project administration, Writing   original draft, Writing   review & editing. The author(s) declare that no financial support was received for the research, authorship, and/or publication of this article. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The authors declare that no Generative AI was used in the creation of this manuscript. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. 1. Yu K, Beam A, Kohane I. Artificial intelligence in healthcare. Nat Biomed Eng. (2018) 2:719 31. doi: 10.1038/s41551-018-0305-z PubMed Abstract | Crossref Full Text | Google Scholar 2. Bonderman D. Artificial intelligence in cardiology. Wien Klin Wochenschr. (2017) 129:866 8. doi: 10.1007/s00508-017-1275-y PubMed Abstract | Crossref Full Text | Google Scholar 3. Shaban-Nejad A, Michalowski M, Buckeridge D. Health intelligence: How artificial intelligence transforms population and personalized health. NPJ Digit Med. (2018) 1:53. doi: 10.1038/s41746-018-0058-9 PubMed Abstract | Crossref Full Text | Google Scholar 4. Mukherjee J, Sharma R, Dutta P, Bhunia B. Artificial intelligence in healthcare: A mastery. Biotechnol Genet Eng. (2024) 40:1659 708. doi: 10.1080/02648725.2023.2196476 PubMed Abstract | Crossref Full Text | Google Scholar 5. Kelly C, Karthikesalingam A, Suleyman M, Corrado G, King D. Key challenges for delivering clinical impact with artificial intelligence. BMC Med. (2019) 17:195. doi: 10.1186/s12916-019-1426-2 PubMed Abstract | Crossref Full Text | Google Scholar 6. Xu F, Dai Z, Ye Y, Hu P, Cheng H. Bibliometric and visualized analysis of the application of artificial intelligence in stroke. Front Neurosci. (2024) 8:1411538. doi: 10.3389/fnins.2024.1411538 PubMed Abstract | Crossref Full Text | Google Scholar 7. Yedavalli V, Tong E, Martin D, Yeom K, Forkert N. Artificial intelligence in stroke imaging: current and future perspectives. Clin Imag. (2021) 69:246 54. doi: 10.1016/j.clinimag.2020.09.005 PubMed Abstract | Crossref Full Text | Google Scholar 8. Pantanowitz L, Pearce T, Abukhiran I, Hanna M, Wheeler S, Soong T, et al. Non-generative Artificial intelligence (Ai) in medicine: Advancements and applications in supervised and unsupervised machine learning. Modern Pathol. (2024) 8:100680. doi: 10.1016/j.modpat.2024.100680 PubMed Abstract | Crossref Full Text | Google Scholar 9. Xie J, Wang Q. Benchmarking machine learning algorithms on blood glucose prediction for type i diabetes in comparison with classical time-series models. IEEE T Bio-Med Eng. (2020) 67:3101 24. doi: 10.1109/TBME.2020.2975959 PubMed Abstract | Crossref Full Text | Google Scholar 10. Zhu T, Li K, Herrero P, Georgiou P. Personalized blood glucose prediction for type 1 diabetes using evidential deep learning and meta-learning. IEEE T Bio-Med Eng. (2023) 70:193 204. doi: 10.1109/TBME.2022.3187703 PubMed Abstract | Crossref Full Text | Google Scholar 11. Zhou R, Liang Y, Cheng H, Wang W, Huang D, Wang Z, et al. A highly accurate delta check method using deep learning for detection of sample mix-up in the clinical laboratory. Clin Chem Lab Med. (2022) 60:1984 92. doi: 10.1515/cclm-2021-1171 PubMed Abstract | Crossref Full Text | Google Scholar 12. Jiang F, Jiang Y, Zhi H, Dong Y, Li H, Ma S, et al. Artificial intelligence in healthcare: Past, present and future. Stroke Vasc Neurol. (2017) 2:230 43. doi: 10.1136/svn-2017-000101 PubMed Abstract | Crossref Full Text | Google Scholar 13. Buch V, Ahmed I, Maruthappu M. Artificial intelligence in Medicine: current trends and future possibilities. Brit J Gen Pract. (2018) 68:143 4. doi: 10.3399/bjgp18X695213 PubMed Abstract | Crossref Full Text | Google Scholar 14. Ren Y, Yang Y, Chen J, Zhou Y, Li J, Xia R, et al. A scoping review of deep learning in cancer nursing combined with augmented reality: The Era of intelligent nursing is coming. Asia-Pac J Oncol Nur. (2022) 9:100135. doi: 10.1016/j.apjon.2022.100135 PubMed Abstract | Crossref Full Text | Google Scholar 15. Wu K, Liu Y, Liu L, Peng Y, Pang H, Sun X, et al. Emerging trends and research foci in tumor microenvironment of pancreatic cancer: A bibliometric and visualized study. Front Oncol. (2022) 12:810774. doi: 10.3389/fonc.2022.810774 PubMed Abstract | Crossref Full Text | Google Scholar 16. Glanzel W, Chen C, Song M. Visualizing a field of research: A methodology of systematic scientometric reviews. PLoS One. (2019) 14:e0223994. doi: 10.1371/journal.pone.0223994 PubMed Abstract | Crossref Full Text | Google Scholar 17. Stout N, Alfano C, Belter C, Nitkin R, Cernich A, Lohmann Siegel K, et al. Bibliometric analysis of the landscape of cancer rehabilitation research (1992-2016). J Natl Cancer I. (2018) 110:815 24. doi: 10.1093/jnci/djy108 PubMed Abstract | Crossref Full Text | Google Scholar 18. Akyol A, Kocyigit B. Ankylosing spondylitis rehabilitation publications and the global productivity: A web of science-based bibliometric analysis (2000-2019). Rheumatol Int. (2021) 41:2007 14. doi: 10.1007/s00296-021-04836-0 PubMed Abstract | Crossref Full Text | Google Scholar 19. You Y, Li W, Liu J, Li X, Fu Y, Ma X. Bibliometric review to explore emerging high-intensity interval training in health promotion: A new century picture. Fronti Public Health. (2021) 9:697633. doi: 10.3389/fpubh.2021.697633 PubMed Abstract | Crossref Full Text | Google Scholar 20. Radhakrishnan S, Erbis S, Isaacs J, Kamarthi S. Novel keyword co-occurrence network-based methods to foster systematic reviews of scientific literature. PLoS One. (2017) 12:e0172778. doi: 10.1371/journal.pone.0172778 PubMed Abstract | Crossref Full Text | Google Scholar 21. Benjamens S, Dhunnoo P, Mesk  B. The state of artificial intelligence-based fda-approved medical devices and algorithms: An online database. NPJ Digit Med. (2020) 3:118. doi: 10.1038/s41746-020-00324-0 PubMed Abstract | Crossref Full Text | Google Scholar 22. Luo J, Shi Y, Wang X, Zhang R, Chen S, Yu W, et al. A 20-year research trend analysis of the influence of anesthesia on tumor prognosis using bibliometric methods. Front Oncol. (2021) 11:683232. doi: 10.3389/fonc.2021.683232 PubMed Abstract | Crossref Full Text | Google Scholar 23. Topol E. High-performance medicine: The convergence of human and Artificial intelligence. Nat Med. (2019) 25:44 56. doi: 10.1038/s41591-018-0300-7 PubMed Abstract | Crossref Full Text | Google Scholar 24. Esteva A, Kuprel B, Novoa R, Ko J, Swetter S, Blau H, et al. Dermatologist-level classification of skin cancer with deep neural networks. Nature. (2017) 542:115 8. doi: 10.1038/nature21056 PubMed Abstract | Crossref Full Text | Google Scholar 25. World Health Organization. Who Guidelines Approved by the Guidelines Review Committee. Who Guideline Recommendations on Digital Interventions for Health System Strengthening. Geneva: World Health Organization (2019). Google Scholar 26. El-Hajj V, Gharios M, Edstr m E, Elmi-Terander A. Artificial Intelligence in Neurosurgery: A Bibliometric Analysis. World Neurosurg. (2023) 171:152-8.e4. doi: 10.1016/j.wneu.2022.12.087 PubMed Abstract | Crossref Full Text | Google Scholar 27. Yuan W, Zhang J, Chen H, Yuan Y, Zhuang Y, Zhou H, et al. A bibliometric and visual analysis of cancer-associated fibroblasts. Front Immunol. (2023) 14:1323115. doi: 10.3389/fimmu.2023.1323115 PubMed Abstract | Crossref Full Text | Google Scholar 28. Duvvury N, Vara-Horna A, Chadha M. Development and validation of lost days of labor productivity scale to evaluate the business cost of intimate partner violence. J Interpers Violence. (2022) 37:N2912 43. doi: 10.1177/0886260520944532 PubMed Abstract | Crossref Full Text | Google Scholar 29. Zhang L, Peng C, Li J. Shedding light on dermographism: A narrative review. Int J Dermatol. (2024) 63:999 1006. doi: 10.1111/ijd.17102 PubMed Abstract | Crossref Full Text | Google Scholar 30. Weinstock M. Bradford s law. Nature. (1971) 233:434. doi: 10.1038/233434a0 PubMed Abstract | Crossref Full Text | Google Scholar 31. Sidey-Gibbons JAM, Sidey-Gibbons CJ. Machine learning in medicine: A practical introduction. BMC Med Res Methodol. (2019) 19:64. doi: 10.1186/s12874-019-0681-4 PubMed Abstract | Crossref Full Text | Google Scholar 32. Shi Z, Zhu J, Yu L, Li X, Li J, Chen H, et al. Two-stage end-to-end deep learning framework for pathologic examination in skin tumor diagnosis. Am J Pathol. (2023) 193:769 77. doi: 10.1016/j.ajpath.2023.02.008 PubMed Abstract | Crossref Full Text | Google Scholar 33. Barat M, Pellat A, Hoeffel C, Dohan A, Coriat R, Fishman E, et al. Ct and Mri of abdominal cancers: Current trends and perspectives in the era of radiomics and Artificial intelligence. Jpn J Radiol. (2024) 42:246 60. doi: 10.1007/s11604-023-01504-0 PubMed Abstract | Crossref Full Text | Google Scholar 34. Takamatsu A, Ueno M, Yoshida K, Kobayashi T, Kobayashi S, Gabata T. Performance of Artificial intelligence-based software for the automatic detection of lung lesions on chest radiographs of patients with suspected lung cancer. Jpn J Radiol. (2024) 42:291 9. doi: 10.1007/s11604-023-01503-1 PubMed Abstract | Crossref Full Text | Google Scholar 35. Misra S, Yoon C, Kim K, Managuli R, Barr R, Baek J, et al. Deep learning-based multimodal fusion network for segmentation and classification of breast cancers using B-Mode and Elastography ultrasound images. Bioeng Trans Med. (2023) 8:e10480. doi: 10.1002/btm2.10480 PubMed Abstract | Crossref Full Text | Google Scholar 36. Sun H, Bai W, Li X, Huang H, Cui X, Cheung T, et al. Schizophrenia and inflammation research: A bibliometric analysis. Front Immunol. (2022) 13:907851. doi: 10.3389/fimmu.2022.907851 PubMed Abstract | Crossref Full Text | Google Scholar 37. Jandoo T. Who guidance for digital health: What it means for researchers. Digital Health. (2020) 6:2055207619898984. doi: 10.1177/2055207619898984 PubMed Abstract | Crossref Full Text | Google Scholar 38. Attia Z, Noseworthy P, Lopez-Jimenez F, Asirvatham S, Deshmukh A, Gersh B, et al. An Artificial intelligence-enabled Ecg algorithm for the identification of patients with atrial fibrillation during sinus rhythm: A retrospective analysis of outcome prediction. Lancet. (2019) 394:861 7. doi: 10.1016/S0140-6736(19)31721-0 PubMed Abstract | Crossref Full Text | Google Scholar 39. Chun M, Park C, Kim J, Jeong J, Jang H, Kim K, et al. Prediction of conversion to dementia using interpretable machine learning in patients with amnestic mild cognitive impairment. Front Aging Neurosci. (2022) 14:898940. doi: 10.3389/fnagi.2022.898940 PubMed Abstract | Crossref Full Text | Google Scholar 40. Hussain A, Bouachir O, Al-Turjman F, Aloqaily M. Ai techniques for COVID-19. IEEE Access Pract Innov Open Solut. (2020) 8:128776 95. doi: 10.1109/ACCESS.2020.3007939 PubMed Abstract | Crossref Full Text | Google Scholar 41. Pham Q, Nguyen D, Huynh-The T, Hwang W, Pathirana P. Artificial intelligence (Ai) and big data for coronavirus (COVID-19) pandemic: A survey on the state-of-the-arts. IEEE Access Pract Innov Open Solut. (2020) 8:130820 39. doi: 10.1109/ACCESS.2020.3009328 PubMed Abstract | Crossref Full Text | Google Scholar 42. Nguyen D, Ding M, Pathirana P, Seneviratne A. Blockchain and Ai-based solutions to combat coronavirus (COVID-19)-like epidemics: A survey. IEEE Access Pract Innov Open Solut. (2021) 9:95730 53. doi: 10.1109/ACCESS.2021.3093633 PubMed Abstract | Crossref Full Text | Google Scholar 43. Sodhi G, Kaur S, Gaba G, Kansal L, Sharma A, Dhiman G. COVID-19: role of robotics, Artificial intelligence and machine learning during the pandemic. Curr Med Imaging. (2022) 18:124 34. doi: 10.2174/1573405617666210224115722 PubMed Abstract | Crossref Full Text | Google Scholar 44. Yang G, Nelson B, Murphy R, Choset H, Christensen H, Collins S, et al. Combating COVID-19-the role of robotics in managing public health and infectious diseases. Sci Robot. (2020) 5:eabb5589. doi: 10.1126/sciroboticsabb5589 Crossref Full Text | Google Scholar 45. Tukur M, Saad G, AlShagathrh F, Househ M, Agus M. Telehealth interventions during COVID-19 pandemic: A scoping review of applications, challenges, privacy and security issues. BMJ Health Care Inf. (2023) 30:e100676. doi: 10.1136/bmjhci-2022-100676 PubMed Abstract | Crossref Full Text | Google Scholar 46. Biswas S. Chatgpt and the future of medical writing. Radiology. (2023) 307:e223312. doi: 10.1148/radiol.223312 PubMed Abstract | Crossref Full Text | Google Scholar 47. Dahmen J, Kayaalp M, Ollivier M, Pareek A, Hirschmann M, Karlsson J, Winkler P. Artificial Intelligence bot chatgpt in medical research: The potential game changer as a double-edged sword. Knee Surg Sports Traumatol Arthroscopy. (2023) 31:1187 9. doi: 10.1007/s00167-023-07355-6 PubMed Abstract | Crossref Full Text | Google Scholar 48. Liu J, Wang C, Liu S. Utility of chatgpt in clinical practice. J Med Int Res. (2023) 25:e48568. doi: 10.2196/48568 PubMed Abstract | Crossref Full Text | Google Scholar 49. Gordijn B, Have H. Chatgpt: Evolution or revolution? Med Health Care Philos. (2023) 26:1 2. doi: 10.1007/s11019-023-10136-0 PubMed Abstract | Crossref Full Text | Google Scholar 50. Rao A, Pang M, Kim J, Kamineni M, Lie W, Prasad A, et al. Assessing the utility of chatgpt throughout the entire clinical workflow: Development and usability study. J Med Int Res. (2023) 25:e48659. doi: 10.2196/48659 PubMed Abstract | Crossref Full Text | Google Scholar 51. Hirosawa T, Harada Y, Yokose M, Sakamoto T, Kawamura R, Shimizu T. Diagnostic accuracy of differential-diagnosis lists generated by generative pretrained transformer 3 Chatbot for clinical vignettes with common chief complaints: A pilot study. Int J Environ Res Public Health. (2023) 20:3378. doi: 10.3390/ijerph20043378 PubMed Abstract | Crossref Full Text | Google Scholar 52. Liu S, Wright A, Patterson B, Wanderer J, Turer R, Nelson S, et al. Assessing the value of chatgpt for clinical decision support optimization. medRxiv [Preprint]. (2023) 2:23286254. doi: 10.1101/2023.02.21.23286254 PubMed Abstract | Crossref Full Text | Google Scholar 53. Leatherdale S, Lee J. Artificial intelligence (Ai) and cancer prevention: the potential application of Ai in cancer control programming needs to be explored in population laboratories such as compass. Cancer Causes Control. (2019) 30:671 5. doi: 10.1007/s10552-019-01182-2 PubMed Abstract | Crossref Full Text | Google Scholar 54. Abolfazl Zanghaei A, Zohreh Rostami Z, Ali Ameri A, Mahmood Salesi M, Ahmad Akhlaghi A, Ahmad Shalbaf A, et al. [*Prediction of Renal Transplantation Outcome Using Artificial Neural Networks and Investigating Important Risk Factors]. Urologiia. (2023):82 9. Google Scholar 55. FDA-NIH Biomarker Working Group. Best (Biomarkers, Endpoints, and Other Tools) Resource. Silver Spring, MD: Food and Drug Administration (2016). Google Scholar 56. Veiga-Pires J, Godfrey B. Robot Angiography. A Preliminary Report. Lancet. (1960) 2:542 4. doi: 10.1016/s0140-6736(60)91562-2 PubMed Abstract | Crossref Full Text | Google Scholar 57. Yamaguchi D, Shimoda R, Miyahara K, Yukimoto T, Sakata Y, Takamori A, et al. Impact of an Artificial Intelligence-Aided Endoscopic Diagnosis System on Improving Endoscopy Quality for Trainees in Colonoscopy: Prospective, Randomized, Multicenter Study. Digestive Endoscopy. (2024) 36:40 8. doi: 10.1111/den.14573 PubMed Abstract | Crossref Full Text | Google Scholar 58. Wallace M, Sharma P, Bhandari P, East J, Antonelli G, Lorenzetti R, et al. Impact of Artificial Intelligence on Miss Rate of Colorectal Neoplasia. Gastroenterology. (2022) 163:295-304.e5. doi: 10.1053/j.gastro.2022.03.007 PubMed Abstract | Crossref Full Text | Google Scholar 59. Lotter W, Hassett M, Schultz N, Kehl K, Van Allen E, Cerami E. Artificial Intelligence in Oncology: Current Landscape, Challenges, and Future Directions. Cancer Discov. (2024) 14:711 26. doi: 10.1158/2159-8290.CD-23-1199 PubMed Abstract | Crossref Full Text | Google Scholar 60. Xu J, Zeng B, Egger J, Wang C, Smedby  , Jiang X, et al. A Review on Ai-Based Medical Image Computing in Head and Neck Surgery. Phys Med Biol. (2022) 67: doi: 10.1088/1361-6560/ac840f PubMed Abstract | Crossref Full Text | Google Scholar 61. Wang Y, Gao S, Xiao Q, Li C, Grzegorzek M, Zhang Y, et al. Role of Artificial Intelligence in Digital Pathology for Gynecological Cancers. Comput Struct Biotechnol J. (2024) 24:205 12. doi: 10.1016/j.csbj.2024.03.007 PubMed Abstract | Crossref Full Text | Google Scholar 62. Marti-Bonmati L, Cerd -Alberich L, P rez-Girb s A, D az Beveridge R, Montalv  Or n E, P rez Rojas J, et al. Pancreatic Cancer, Radiomics and Artificial Intelligence. Br J Radiol. (2022) 95:20220072. doi: 10.1259/bjr.20220072 PubMed Abstract | Crossref Full Text | Google Scholar 63. Qi X, Zhao Y, Qi Z, Hou S, Chen J. Machine Learning Empowering Drug Discovery: Applications, Opportunities and Challenges. Molecules. (2024) 29:903. doi: 10.3390/molecules29040903 PubMed Abstract | Crossref Full Text | Google Scholar 64. Aliper A, Plis S, Artemov A, Ulloa A, Mamoshina P, Zhavoronkov A. Deep Learning Applications for Predicting Pharmacological Properties of Drugs and Drug Repurposing Using Transcriptomic Data. Mol Pharmaceut (2016) 13:2524 30. doi: 10.1021/acs.molpharmaceut.6b00248 PubMed Abstract | Crossref Full Text | Google Scholar 65. Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T. The Rise of Deep Learning in Drug Discovery. Drug Discov Today. (2018) 23:1241 50. doi: 10.1016/j.drudis.2018.01.039 PubMed Abstract | Crossref Full Text | Google Scholar 66. Chen H, Lu D, Xiao Z, Li S, Zhang W, Luan X, et al. Comprehensive Applications of the Artificial Intelligence Technology in New Drug Research and Development. Health Inf Sci Syst. (2024) 12:41. doi: 10.1007/s13755-024-00300-y PubMed Abstract | Crossref Full Text | Google Scholar 67. Sumathi S, Suganya K, Swathi K, Sudha B, Poornima A, Varghese C, et al. Review on Deep Learning-Driven Drug Discovery: Strategies, Tools and Applications. Curr Pharm Design. (2023) 29:1013 25. doi: 10.2174/1381612829666230412084137 PubMed Abstract | Crossref Full Text | Google Scholar 68. Pun F, Liu B, Long X, Leung H, Leung G, Mewborne Q, et al. Identification of Therapeutic Targets for Amyotrophic Lateral Sclerosis Using Pandaomics - an Ai-Enabled Biological Target Discovery Platform. Front Aging Neurosci. (2022) 14:914017. doi: 10.3389/fnagi.2022.914017 PubMed Abstract | Crossref Full Text | Google Scholar 69. West M, Labat I, Sternberg H, Larocca D, Nasonkin I, Chapman K, et al. Use of Deep Neural Network Ensembles to Identify Embryonic-Fetal Transition Markers: Repression of Cox7a1 in Embryonic and Cancer Cells. Oncotarget. (2018) 9:7796 811. doi: 10.18632/oncotarget.23748 PubMed Abstract | Crossref Full Text | Google Scholar 70. Harpaz R, DuMouchel W, Shah N, Madigan D, Ryan P, Friedman C. Novel Data-Mining Methodologies for Adverse Drug Event Discovery and Analysis. Clin Pharmacol Ther. (2012) 91:1010 21. doi: 10.1038/clpt.2012.50 PubMed Abstract | Crossref Full Text | Google Scholar 71. Ross J, Tu S, Carini S, Sim I. Analysis of Eligibility Criteria Complexity in Clinical Trials. Summit Transl Bioinform. (2010) 2010:46 50. Google Scholar 72. Si Y, Du J, Li Z, Jiang X, Miller T, Wang F, et al. Deep Representation Learning of Patient Data from Electronic Health Records (Ehr): A Systematic Review. J Biomed Inform. (2021) 115:103671. doi: 10.1016/j.jbi.2020.103671 PubMed Abstract | Crossref Full Text | Google Scholar 73. Obermeyer Z, Emanuel E. Predicting the Future - Big Data, Machine Learning, and Clinical Medicine. New Engl J Med. (2016) 375(13):1216 9. doi: 10.1056/NEJMp1606181 PubMed Abstract | Crossref Full Text | Google Scholar 74. Xu Z, Biswas B, Li L, Amzal B. Ai/Ml in Precision Medicine: A Look Beyond the Hype. Therapeutic Innov Regul Sci. (2023) 57:957 62. Google Scholar 75. Esteva A, Robicquet A, Ramsundar B, Kuleshov V, DePristo M, Chou K, et al. A Guide to Deep Learning in Healthcare. Nat Med. (2019) 25:24 9. doi: 10.1038/s41591-018-0316-z PubMed Abstract | Crossref Full Text | Google Scholar 76. Eraslan G, Avsec  , Gagneur J, Theis F. Deep Learning: New Computational Modelling Techniques for Genomics. Nat Rev Genet. (2019) 20:389 403. doi: 10.1038/s41576-019-0122-6 PubMed Abstract | Crossref Full Text | Google Scholar 77. Tian J, Zhu M, Ren Z, Zhao Q, Wang P, He C, et al. Deep Learning Algorithm Reveals Two Prognostic Subtypes in Patients with Gliomas. BMC Bioinformatics (2022) 23:417. doi: 10.1186/s12859-022-04970-x PubMed Abstract | Crossref Full Text | Google Scholar 78. Roy S, Meena T, Lim S. Demystifying Supervised Learning in Healthcare 4.0: A New Reality of Transforming Diagnostic Medicine. Diagnostics (Basel, Switzerland). (2022) 12:2549. doi: 10.3390/diagnostics12102549 PubMed Abstract | Crossref Full Text | Google Scholar 79. Thi baut R, Thiessard F. Public Health and Epidemiology Informatics. Yearbook Med Inf. (2017) 26:248 51. doi: 10.15265/IY-2017-036 PubMed Abstract | Crossref Full Text | Google Scholar 80. Flahault A, Geissbuhler A, Guessous I, Gu rin P, Bolon I, Salath  M, et al. Precision Global Health in the Digital Age. Swiss Med Weekly. (2017) 147:w14423. doi: 10.4414/smw.2017.14423 PubMed Abstract | Crossref Full Text | Google Scholar 81. Salath  M. Digital Epidemiology: What Is It, and Where Is It Going? Life Sci Soc Policy. (2018) 14:1. doi: 10.1186/s40504-017-0065-7 PubMed Abstract | Crossref Full Text | Google Scholar 82. Deiner M, Lietman T, McLeod S, Chodosh J, Porco T. Surveillance Tools Emerging from Search Engines and Social Media Data for Determining Eye Disease Patterns. JAMA Ophthalmol. (2016) 134:1024 30. doi: 10.1001/jamaophthalmol.2016.2267 PubMed Abstract | Crossref Full Text | Google Scholar 83. Ma M, Saha C, Poon S, Yiu R, Shih K, Chan Y. Virtual Reality and Augmented Reality- Emerging Screening and Diagnostic Techniques in Ophthalmology: A Systematic Review. Survey Ophthalmol. (2022) 67:1516 30. doi: 10.1016/j.survophthal.2022.02.001 PubMed Abstract | Crossref Full Text | Google Scholar 84. Crigger E, Khoury C. Making Policy on Augmented Intelligence in Health Care. AMA J Ethics. (2019) 21:E188 91. doi: 10.1001/amajethics.2019.188 PubMed Abstract | Crossref Full Text | Google Scholar 85. Kifle M, Teklemariam T, Teweldeberhan A, Tesfamariam E, Andegiorgish A, Azaria Kidane E. Malaria Risk Stratification and Modeling the Effect of Rainfall on Malaria Incidence in Eritrea. J Environ Public Health. (2019) 2019:7314129. doi: 10.1155/2019/7314129 PubMed Abstract | Crossref Full Text | Google Scholar Keywords: artificial intelligence, medicine, bibliometrics, VOSviewer, applications Citation: Lin M, Lin L, Lin L, Lin Z and Yan X (2025) A bibliometric analysis of the advance of artificial intelligence in medicine. Front. Med. 12:1504428. doi: 10.3389/fmed.2025.1504428 Received: 30 September 2024; Accepted: 10 February 2025;Published: 21 February 2025. Edited by: Reviewed by: Copyright   2025 Lin, Lin, Lin, Lin and Yan. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Xiaoxiao Yan, 13858753895@163.com; Zhengqiu Lin, dawn_ak@163.com  These authors have contributed equally to this work Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on",2
UF medical students explore artificial intelligence in research program - University of Florida,https://news.google.com/rss/articles/CBMiYkFVX3lxTE5lWU9VZWlZVXhsZ3I5TmlLRlVsV1I2eVJ1S19aN0I1OHFIQmtkVEpUOURVWlJNNXBpLUdjYTQ1UmZnYlhoYy13T01PTWd4dWY2SGJuZlVJV25YbW91R1RTT1lB?oc=5&hl=en-US&gl=US&ceid=US:en,"Emily Mavrakis June 6, 2025 Share When University of Florida medical student Taylor Edwards began her studies three years ago, she thought it would be interesting to embrace something groundbreaking in the field.  I knew how many AI resources the university had at hand, and I identified a big knowledge gap there for myself,  Edwards said.  So, I wanted to get involved.  Edwards joins more than 45 UF College of Medicine students who have participated in the Artificial Intelligence in Medicine track of the Research and Discovery Pathways Program over the last three years. The pathway enables UF medical students to conduct AI-related research projects, build connections with research faculty, hear presentations from guest presenters and gain access to premier tools and trainings. Medical students in the program have conducted AI-aided research in specialties ranging from obstetrics and gynecology to neurosurgery. Read more... University of Florida,Gainesville, FL 32611",2
Study: Physician views on artificial intelligence - KU News,https://news.google.com/rss/articles/CBMiwwFBVV95cUxNTmIzZkh1VmMtWUlJWFY5WlNiWDBMTEZPd3BNUjEtZzhtYXVfUENITlhtRmhPVTZPVWhIbGxZNUpsR1RCWElMcFFjX3lPbTBKZ28zMXJTSVRqQ3RWUGFHWWZkRVcwaDJVcTFPekpJajk2ajR4VmROMzBKRkQ3WmpJOGlvR09tTS0zREpFcEZ1SUFKLXVRaWdoQkpWQlBIcldveVpjaUM5RmpfZV9GUFNpM29OV3hHRkYyY0NTMU9WNWEzUDg?oc=5&hl=en-US&gl=US&ceid=US:en,"This website uses cookies and other technologies to improve the functionality and performance of this site and your experience. Learn more by reviewing our Privacy Policy. Jon Niccum LAWRENCE   Whether it s the Emergency Medical Hologram in  Star Trek  or the  MedPod  in the  Alien  films, the creators of science fiction have long considered the healing possibilities of artificial intelligence. Now physicians are also pondering the modern reality of such futuristic concepts.  It s easy to speculate about how medicine will change with the emergence of AI. But for this research, we were concerned with assessing how medical professionals are actually thinking about it in the present,  said John Symons, professor of philosophy at the University of Kansas. His new paper  Perceptions and attitudes toward artificial intelligence among frontline physicians and physicians  assistants in Kansas: a cross-sectional survey  suggests that perceptions of benefits, trust, risks, communication barriers, regulation and liability issues influence health care professionals  intention to use AI, regardless of their technological familiarity. The research appears in JAMIA Open. Co-written by Robert Badgett and Rosey Zackula of the University of Kansas School of Medicine-Wichita, Rajeev Seecheran of the University of New Mexico and Tanner Dean with Intermountain Health, Salt Lake City, the research found that the top concerns medical professionals have are liability-related and responsibility-related. But the respondents also shared a concern about how the practice of medicine and the satisfaction of interacting with patients might be reduced or eliminated.  Let s say I m a physician in Kansas, and I have 1,000 avatars of me out there on people s phones. These reflect my expertise, my bedside manner, etc.,  Symons said.  People would interact with these avatars during their day. They d say,  OK, doctor, I ve got this ailment. What do you think?  Obviously, the amount of time people can spend with these devices far exceeds the availability of a doctor. But how is my responsibility going to be distributed across 1,000 instances of me   or at least of my presence, of my image?  As alluded to in the title of the paper, both physicians and physician assistants  responses were nearly identical. While that proved somewhat predictable, what really surprised Symons were responses involving AI fluency.  We would expect that people who are more familiar with the technology would have more refined or different kinds of concerns,  Symons said.  But, typically, all the concerns echo an interest in the more social consequences of AI. That is broadly consonant with the kinds of research we re doing at the Center for Cyber-Social Dynamics, where we re seeing social transformation as becoming a more pressing concern than traditional concerns about privacy or security.  To obtain a comprehensive sample of participants, an email invitation was sent to all 12,290 actively licensed physicians and physician assistants of the Kansas State Board of Healing Arts (KBHA). The KBHA is the state's medical licensing and regulatory board for 16 different health professions. Of these active members, 532 responses were received. Currently, physicians are already using AI in limited capacities.  There are research applications of AI that are extremely prominent in the biomedical fields. There are office levels of paperwork applications of AI we can point to. You can also find many examples in imaging and diagnostics. But so far, day-to-day clinical use of AI isn t part of your normal internal medicine practice,  Symons said. What are some ways it might be used by physicians in the near future?  Likely applications of AI will involve avatars for doctors,  he said.  Your doctor will be present as an AI on your phone or computer. It s very likely that   assuming we can tackle the legislative problems, liability issues and the economics of such things   we will have broadened access to high-quality medical advice when we want it. And it would also be great if these artifacts were somehow attached to the authority and competence of a practicing doctor so they could write prescriptions and review the interaction you had with the AI.  A native of Cork, Ireland, Symons has been at KU since 2012. An expert in metaphysics and epistemology of science and philosophy of technology, he has written or edited a dozen books, including  The History and Philosophy of Materialism  (2024),  Formal Reasoning: A Guide to Critical Thinking  (2017) and  The Architecture of Cognition: Rethinking Fodor and Pylyshyn s Systematicity Challenge  (2014). He is the director of the Center for Cyber-Social Dynamics.  The use of AI will dramatically alter the way we value labor and expertise in the medical professions,  Symons said.  All of this is provisional, of course, on the kinds of institutional frameworks that insurance companies and large medical groups impose on us. While it s difficult to foresee how the economics of medicine will change in the United States, it s clear that something has to change. We re all dissatisfied with the current model of medical care, and this will be an opportunity for new economic models for health care to emerge.  Jon Niccum Jon Niccum KU News Service 785-864-7633 jniccum@ku.edu Nondiscrimination statement   2025 The University of Kansas The University of Kansas prohibits discrimination on the basis of race, color, ethnicity, religion, sex, national origin, age, ancestry, disability, status as a veteran, sexual orientation, marital status, parental status, gender identity, gender expression, and genetic information in the university's programs and activities. Retaliation is also prohibited by university policy. The following person has been designated to handle inquiries regarding the nondiscrimination policies and procedures and is the Title IX Coordinator for all KU and KUMC campuses: Associate Vice Chancellor for the Office of Civil Rights and Title IX, civilrights@ku.edu, Room 1082, Dole Human Development Center, 1000 Sunnyside Avenue, Lawrence, KS 66045, 785-864-6414, 711 TTY. Reports can be submitted by contacting the Title IX Coordinator as provided herein or using the Title IX online report form and complaints can be submitted with the Title IX Coordinator or using the Title IX online complaint form. The University of Kansas is a public institution governed by the Kansas Board of Regents.",2
Predicting outcomes following endovascular aortoiliac revascularization using machine learning | npj Digital Medicine - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTE1CalM2WVJXVGxmQWVXczVodHNwcVRDY3V5UXZ6UkpLNEV6dFZRMEF1MFh6REhVbVNFSUFIeWpWeUZjQ3IySk9kbldmNjlKdXp1WWtHUnl6UER1OTJFSGUw?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement npj Digital Medicine volume 8, Article number: 475 (2025) Cite this article Endovascular aortoiliac revascularization is a common treatment option for peripheral artery disease that carries non-negligible risks. Outcome prediction tools may support clinical decision-making but remain limited. We developed machine learning algorithms that predict 30-day post-procedural outcomes. The National Surgical Quality Improvement Program targeted vascular database was used to identify patients who underwent endovascular aortoiliac revascularization between 2011 2021. Input features included 37 pre-operative demographic/clinical variables. The primary outcome was 30-day post-procedural major adverse limb event (MALE) or death. Data were split into training (70%) and test (30%) sets. Using 10-fold cross-validation, 6 machine learning models were trained using pre-operative features. Overall, 6601 patients were included, and 30-day MALE/death occurred in 470 (7.1%) individuals. The best-performing model was XGBoost, achieving an AUROC (95% CI) of 0.94 (0.93 0.95). In comparison, logistic regression had an AUROC (95% CI) of 0.74 (0.73 0.76). The XGBoost model accurately predicted 30-day post-procedural outcomes, performing better than logistic regression. Peripheral artery disease (PAD) impacts more than 200 million people globally, leading to reduced quality of life, mounting health care costs, amputation, and death1,2,3,4. A distinct subset of PAD is aortoiliac occlusive disease (AIOD), which involves atherosclerosis of the infrarenal aorta and iliac arteries, affecting approximately 14% of the population in general5. Historically, advanced AIOD was treated with open revascularization6. Over the last few decades, endovascular intervention has become an increasingly common less invasive alternative7. Nevertheless, endovascular revascularization of AIOD carries a significant risk of adverse events, with a systematic review of 19 studies demonstrating mortality and complication rates up to 6.7% and 45%, respectively8. Consequently, careful assessment of perioperative risk is recommended by the Global Vascular Guidelines when patients are considered for revascularization9. There are currently no standardized tools to support prediction of complications following endovascular revascularization for AIOD. The Vascular Quality Initiative (VQI) Cardiac Risk Index (CRI) is limited to open revascularization10. Other tools such as the American College of Surgeons (ACS) National Surgical Quality Improvement Program (NSQIP) online surgical risk calculator11 rely on modeling techniques that require manual input of clinical variables, making them challenging to use in busy medical environments12. Therefore, there is an important need to develop better and more practical risk prediction tools for patients undergoing endovascular aortoiliac revascularization. Machine learning (ML) is a rapidly evolving technology that enables computers to learn from large amounts of data and make accurate predictions13. Using advanced analytics, ML can learn non-linear, complex relationships between user inputs, such as patient characteristics, and outputs, such as clinical outcomes13. The advantage of newer ML techniques over traditional statistical methods is that they can better model non-linear relationships between covariates and outcomes14, which is common in health care data15. Previously, we built a ML algorithm that can accurately predict 30-day complications following open revascularization for AIOD, which achieved better performance compared to logistic regression16. To complement this model and provide further guidance for the management of patients with advanced AIOD, NSQIP data were used to train ML algorithms that can predict 30-day outcomes following endovascular revascularization for AIOD. Overall, 7102 patients underwent endovascular revascularization for AIOD in the targeted NSQIP vascular database from 2011 to 2021. The following exclusions were made: intervention performed for aortoiliac aneurysm (n = 162), acute limb ischemia (n = 7), or dissection (n = 17), unreported symptom status (n = 132) or procedure type (n = 58), and concurrent major amputation (n = 7) or surgical bypass (n = 118). Overall, 6601 patients were included. The primary outcome of 30-day major adverse limb event (MALE) or death occurred in 470 (7.1%) individuals. The 30-day secondary endpoints occurred in this distribution: major vascular reintervention (n = 242 [3.7%]), untreated loss of patency (n = 63 [1.0%]), major amputation (n = 113 [1.7%]), death (n = 130 [2.0%]), major adverse cardiovascular event (MACE, n = 314 [4.8%]; composite of myocardial infarction (n = 205), stroke (n = 22), and death (n = 130)), wound complication (n = 236 [3.6%]), bleeding requiring transfusion or secondary procedure (n = 374 [5.7%]), other morbidity (n = 337 [5.1%]; composite of pneumonia (n = 75), unplanned reintubation (n = 67), pulmonary embolism (n = 7), failure to wean from ventilator (n = 47), acute kidney injury (n = 72), urinary tract infection (n = 42), cardiac arrest (n = 40), deep vein thrombosis (n = 30), sepsis (n = 51), septic shock (n = 42), Clostridium difficile infection (n = 14)), non-home discharge (n = 430 [6.5%]), and unplanned readmission (n = 507 [7.7%]). A flowchart summarizing patient selection and outcomes is reported in Fig. 1. Patients undergoing open revascularization were not included in this study. NSQIP National Surgical Quality Improvement Program. In comparison to patients who did not develop 30-day MALE or death, individuals with a primary outcome were older with a greater proportion being Black, residing in nursing homes or other facilities, or transferred from another hospital. A higher percentage of individuals with 30-day MALE or death had congestive heart failure (CHF), insulin-dependent diabetes, end stage renal disease (ESRD) requiring dialysis, and  1 high-risk physiologic factor, and were partially or totally dependent functionally. Despite having more cardiovascular comorbidities, a lower proportion of patients with an event received statins or antiplatelets. A greater proportion of patients with 30-day MALE or death had an ankle brachial index (ABI)   0.39 or no recorded ABI and pedal pulses that were non-palpable, as well as a prior surgical bypass involving the segment being treated. Individuals with a primary outcome were more likely to present with chronic limb threatening ischemia (CLTI), undergo more distal revascularization of the aortoiliac segment (i.e., external iliac rather than common iliac intervention), receive urgent or emergent intervention, and have an American Society of Anesthesiologists (ASA) classification  4. Patients who underwent intervention by vascular surgeons were less likely to develop a primary outcome compared to other specialists (Table 1). Six different ML models were trained and subsequently assessed on testing data for predicting 30-day MALE or death after endovascular revascularization for AIOD. Extreme Gradient Boosting (XGBoost) achieved the top performance with an area under the receiver operating characteristic curve [AUROC] (95% CI) of 0.94 (0.93 0.95) in comparison to random forest [0.92 (0.91 0.93)], radial basis function support vector machine [0.90 (0.89 0.91)], Na ve Bayes [0.85 (0.84 0.87)], multilayer perceptron artificial neural network [0.76 (0.74 0.77)], and logistic regression [0.74 (0.73 0.76)]. XGBoost attained the following secondary performance metrics: accuracy 0.86 (95% CI 0.85 0.88), specificity 0.87, sensitivity 0.86, negative predictive value 0.86, and positive predictive value 0.87 (Table 2). XGBoost obtained the following AUROC s (95% CI) for predicting 30-day secondary endpoints: major vascular reintervention [0.86 (0.85 0.87)], untreated loss of patency [0.95 (0.94 0.96)], major amputation [0.97 (0.86 0.98)], death [0.97 (0.96 0.98)], MACE [0.90 (0.89 0.91)], bleeding requiring transfusion or secondary procedure [0.95 (0.94 0.96)], wound complication [0.90 (0.89 0.91)], other morbidity [0.88 (0.87 0.89)], non-home discharge [0.97 (0.96 0.98)], and unplanned readmission [0.88 (0.87 0.89)] (Table 3). Logistic regression had worse performance compared to XGBoost across all primary and secondary outcomes, with AUROC s ranging from 0.70 0.78. Figure 2 illustrates the ROC curve for XGBoost in predicting 30-day MALE or death. The model was well-calibrated, achieving a Brier score of 0.08, which demonstrates excellent agreement between predicted/observed event probabilities (Fig. 3). For XGBoost, the most important predictors of 30-day MALE or death were: (1) CLTI, (2) functional status, (3)   1 high-risk physiologic factor, (4) pre-operative dialysis, (5) CHF, (6) transferred from another hospital, (7) urgency, (8) diabetes, (9) primary procedure (more distal revascularization of the aortoiliac segment), and (10) absence of pre-operative statin (Fig. 4). Subgroup analysis of feature importance based on symptom status demonstrated that eight of the ten most important features were identical for CLTI patients and individuals who were asymptomatic or claudicants, and the two most important features were functional status and  1 high-risk physiologic factor for both subgroups (Supplementary Fig. 1). AUROC area under the receiver operating characteristic curve, CI confidence interval. Extreme Gradient Boosting (XGBoost) model calibration for predicting 30-day major adverse limb event or death following endovascular aortoiliac revascularization. CLTI chronic limb threatening ischemia. Model performance was excellent across subgroups based on sex, age, race, ethnicity, procedure type, symptom status, urgency, and concurrent infrainguinal endovascular revascularization with a range of AUROC values from 0.92 to 0.95 and no significant differences were found between majority/minority subgroups (Supplementary Figs. 2 9). Using targeted NSQIP vascular data from 2011 to 2021, which comprised 6601 patients who received endovascular revascularization for AIOD, we built ML algorithms that pre-operatively predict 30-day MALE or death with excellent performance (AUROC 0.94). Our models also predicted 30-day untreated loss of patency, major vascular reintervention, major amputation, death, MACE, wound complication, bleeding, other morbidity, non-home discharge, and unplanned readmission with AUROC s ranging from 0.86 to 0.97. There were three other notable findings. First, individuals who suffer adverse events following endovascular aortoiliac revascularization are a high-risk cohort with predictive factors pre-operatively. They have more comorbidities and poorer functional status, with a greater proportion having high-risk anatomic and physiologic features. Furthermore, a greater proportion of patients with adverse events had CLTI, underwent more distal revascularization, and required urgent/emergent intervention. Despite these differences, they were less likely to receive optimal medical therapy including antiplatelets and statins. This represents an important opportunity to improve medical management of patients with PAD. Second, we evaluated six different ML models and XGBoost achieved the best predictive performance. Our algorithm was well-calibrated and remained robust across demographic/clinical subgroups. Third, we reported the most important predictors of 30-day MALE/death in our algorithms. These variables can help clinicians understand patient characteristics that lead to specific risk predictions, thereby supporting patient/procedure selection and pre-operative optimization. Risk prediction tools for patients undergoing endovascular aortoiliac revascularization are limited. Bertges et al.10 used multivariable logistic regression to develop the VQI CRI for predicting myocardial infarction after carotid endarterectomy, lower extremity bypass, and aortic aneurysm repair, achieving an overall AUROC of 0.7510. Notably, the model did not include endovascular aortoiliac revascularization, and therefore, our work fills an important gap in this tool10. Applying ML to a contemporary NSQIP cohort, we achieved an AUROC of 0.94 for predicting MALE or death at 30 days following endovascular revascularization for AIOD. This demonstrates the benefit of leveraging advanced data analytics to build procedure-specific risk prediction models using contemporary datasets. Our model provides accurate risk predictions for a patient population that has often not been included in existing tools10 and complements our previously described ML algorithm for predicting outcomes at 30 days after open revascularization for AIOD16. Bonde and colleagues (2021) built ML models on a sample of patients in the NSQIP database undergoing more than 2900 different procedures to predict post-operative complications, attaining AUROC s between 0.85 and 0.8817. Since individuals with PAD are a high-risk population with many vascular comorbidities, the ability for general risk prediction tools to perform well on this cohort could be limited18. By building ML models tailored to individuals receiving endovascular revascularization for AIOD, we attained AUROC s above 0.90. Our algorithms can also predict clinically relevant limb-related outcomes such as major vascular reintervention and major amputation, which are of importance to interventionalists and vascular surgeons. Consequently, developing ML algorithms specific to endovascular aortoiliac revascularization can increase performance and applicability. Our findings can be interpreted in several ways. First, individuals who suffer from adverse events after endovascular revascularization for AIOD are a high-risk group with many cardiovascular risk factors19. Multiple societal guidelines recommend statins and antiplatelets for all PAD patients9,20,21,22, yet individuals who had complications in our study were less likely to be taking these medications. The BEST-CLI trial corroborates the fact that the rates of best medical therapy are suboptimal in PAD patients23. Consequently, there remain critical opportunities to improve outcomes for PAD patients by accurately determining their perioperative risk and optimizing them medically before intervention. In our ML algorithm, we found that clinical presentation with CLTI was the most important predictor of 30-day MALE or death, which corroborates existing literature given that these patients generally have more extensive lesions and severe ischemia, increasing the risk of limb loss and adverse events24. Poor pre-operative functional status was also a strong predictive feature for worse outcomes, as it correlates well with the overall health status of the patient and their ability to tolerate physiologically demanding procedures25. Other important predictors included high-risk physiological factors, pre-operative dialysis, pre-operative CHF, among others, highlighting that the overall clinical status of the patient and their pre-existing conditions are important predictors of their post-operative outcomes26. Second, the ML models achieved better performance than existing tools for various reasons. In comparison to logistic regression, newer ML technology can better learn non-linear and complex relationships between covariates and outcomes27,28. This is particularly important in health care data because many demographic/clinical factors can influence patient outcomes29. XGBoost was the top-performing algorithm, which has important advantages over other ML approaches including relatively fewer issues with overfitting in addition to quicker computing while retaining precision30,31,32. Additionally, XGBoost performs well on structured data, which could be the reason it outperformed more complicated algorithms such as neural networks33. These findings are corroborated by previous work demonstrating that ML algorithms can achieve better discrimination than conventional statistical models34,35. Additionally, we used methodology that strengthens model validity including calibration measurement, adherence to reporting standards (TRIPOD + AI statement), and collaboration between computer scientists, biostatisticians, and clinicians34,35. Third, the performance of our algorithms remained excellent across demographic/clinical subpopulations. This is notable given that algorithm bias against underrepresented populations remains an important concern within ML research36. These biases were likely avoided because ACS NSQIP is a multi-national database that captures diverse patient populations with rich sociodemographic information37,38. Fourth, a small proportion of patients in our cohort underwent revascularization for asymptomatic disease (~5%). The reasons for these interventions are unclear from our dataset but may be related to treatment of hemodynamically significant stenoses of previous revascularization procedures, patient preference, poor adherence to guideline-directed revascularization, or coding errors39. Clinical decision-making can be supported by our ML algorithms in several ways. At the pre-operative stage, individuals predicted to be at elevated risk of adverse events could be assessed further in terms of both non-modifiable and modifiable factors26. Individuals with risks that are non-modifiable could be considered for alternative options including medical management alone, amputation, or close surveillance40,41,42. Those with modifiable risks could benefit from further evaluation and optimization including referrals to anesthesiologists, cardiologists, and/or internal medicine specialists when appropriate43,44. Conversely, low risk patients could be considered for open revascularization, which may be more durable45. Post-operatively, high risk patients could receive increased monitoring on the ward or intensive care unit46. Individuals at elevated risk for unplanned readmission or non-home discharge could receive increased support from allied health to ensure safe discharge planning47. These clinical decisions supported by the ML risk prediction tool may improve outcomes by mitigating complications related to endovascular revascularization for AIOD. The ML algorithm programming code is openly available on GitHub. This tool may be utilized by clinicians caring for patients under consideration for endovascular revascularization for AIOD. The algorithm could be implemented at the >700 NSQIP-participating centers globally. There is also potential to implement the model at non-NSQIP sites because the input variables are commonly captured for the routine care of vascular patients48. Given the challenges of deploying prediction models into clinical practice, consideration of implementation science principles is critical49. Our ML models have the advantage of providing automated risk predictions, thereby improving practicality in busy clinical settings compared with traditional risk predictors that generally require manual input of variables11. Specifically, our algorithm can autonomously extract a patient s NSQIP data to make risk predictions. The clinical applicability of our risk prediction tool stems from both its automated nature and highly accurate predictions that can guide vascular specialists in terms of patient/procedure selection, counseling, and peri-operative management to improve care for patients with AIOD being considered for endovascular revascularization. We advocate for dedicated health care data analytics teams at the institution level, as their benefits to patient care have been previously demonstrated and model implementation can be facilitated by these experts50. Our study has various limitations. First, the algorithms were built using data from NSQIP. Additional studies are needed to evaluate whether generalization of predictive performance can be made to non-NSQIP centers. Second, our dataset captured 30-day endpoints. Assessment of ML algorithms on datasets with longer follow-up periods could support prediction of long-term risk. Third, several variables that may contribute to risk predictions were not captured in our dataset, including lesion characteristics and use of low-dose rivaroxaban, drug eluting technology, post-procedural antiplatelet therapy, intravascular lithotripsy, covered stents, and intravascular ultrasound. Future predictive models developed on datasets that record these variables could improve accuracy. Additionally, some data points were unknown, not reported, or not documented; further refinement of datasets with more complete clinical information may improve model performance. Fourth, the sample size was lower than expected over a 10-year period likely because ACS NSQIP is primarily a surgical database, and procedures performed by interventional radiologists or other non-surgical specialists may be under-captured. Additional investigation of endovascular aortoiliac revascularization performed by non-surgical specialists may increase the sample size for analysis. Using the NSQIP targeted vascular database, we built ML algorithms that predict 30-day MALE/death and other clinically relevant outcomes following endovascular revascularization for AIOD with excellent performance using pre-operative data. Given that our ML algorithms performed better than existing tools and logistic regression, they have potential for important utility in the peri-operative management of patients being considered for endovascular aortoiliac revascularization to mitigate adverse outcomes. Prospective validation of our prediction models is warranted. This was an ML-based prognostic study and findings were reported based on the Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis + Artificial Intelligence (TRIPOD + AI) statement51. The methods were based on our previous work to demonstrate the robustness and reproducibility of our ML model development and evaluation process16. The ACS NSQIP database contains data on patients from >700 hospitals in ~15 countries worldwide52. Trained clinical reviewers prospectively collect data from electronic health records and ACS regularly audits the data for accuracy53. Targeted vascular registries in NSQIP were established in 2011 and comprise additional variables and outcomes specific to vascular procedures54. Research ethics board review was not required as the data source was a deidentified registry. All individuals in the NSQIP targeted vascular database who received endovascular revascularization for AIOD (angioplasty or stent of the aorta or iliac arteries) for PAD between 2011 and 2021 were included. Individuals who underwent the procedure for aortoiliac aneurysm, dissection, acute limb ischemia, malignancy, or trauma, patients with unreported symptom status or procedure type, and those who received concurrent major amputation or bypass were excluded. The input features for the ML models included 37 pre-operative variables. To maximize model performance, all pre-operative NSQIP variables were used, as ML methods excel at handling many inputs. Feature selection was not performed as it worsened model performance, and given the goal of automated predictions, reducing the number of features would decrease predictive performance without a significant change to model run time. Demographic features included age, body mass index, sex, race, and ethnicity. Comorbidities consisted of diabetes, hypertension, smoking status, chronic obstructive pulmonary disease, CHF, ESRD on dialysis, functional status, and high-risk physiologic factor [defined as  1 of: (1) ESRD, (2) age above 80, (3) CHF class III or IV (New York Heart Association), (4) ejection fraction below 30%, (5) unstable angina <30 days before intervention, or (6) myocardial infarction <30 days before intervention]. Medications consisted of statins, antiplatelets, and beta blockers. Labs included serum creatinine, blood urea nitrogen, serum sodium, albumin, hematocrit, white blood cell count, platelet count, partial thromboplastin time, and international normalized ratio. Limb hemodynamics based on ABI and toe pressure, and high-risk anatomic factors (defined as a prior bypass or endovascular intervention involving the currently treated segment), were noted. Concurrent procedures documented were minor amputation (below the ankle) and endovascular infrainguinal revascularization. Other characteristics were symptom status [CLTI (defined as tissue loss or rest pain), claudication, or asymptomatic], procedure type, urgency of intervention (elective, urgent, or emergent), ASA class, and specialty of the primary operator. Urgent and emergent interventions, generally performed within 24 48 h and <24 h of admission, respectively, were mostly performed for patients with severe ischemia resulting in CLTI. Isolated internal iliac artery interventions were included and generally performed on patients with lifestyle-limiting buttock claudication, rest pain, and/or tissue loss. Supplementary Table 1 provides a complete list of input features and definitions. The primary endpoint was 30-day post-procedural MALE (defined as a composite of major vascular reintervention, untreated loss of patency, or major amputation) or death. The definition of major vascular reintervention was thrombectomy/thrombolysis involving the treated segment or a new or revision aortoiliac bypass or interposition graft. The definition of untreated loss of patency was an occlusion of the treated segment with no subsequent revascularization. The definition of major amputation was a transtibial or more proximal amputation of the ipsilateral leg. The definition of death was all-cause mortality. These outcomes and definitions align with the Society for Vascular Surgery reporting standards55. The 30-day secondary outcomes consisted of individual components of MALE or death, MACE, bleeding requiring transfusion or secondary procedure, wound complication, other morbidity, non-home discharge, and unplanned readmission. MACE was a composite endpoint of myocardial infarction (ischemic changes on electrocardiogram, elevated troponin, or diagnosis by a physician or advanced provider), stroke (neurologic dysfunction exceeding 24 h in the setting of suspected stroke), or death. The definition of wound complication was a non-healing wound at the incision (if present), cellulitis, or dehiscence. Other morbidity was a composite endpoint of unplanned reintubation, failure to wean from the ventilator (cumulative time of ventilator-assisted respirations over 48 h), pneumonia, pulmonary embolism, acute kidney injury (rise in serum creatinine over 2 mg/dL from the pre-operative value or requirement of dialysis in a patient who was not on dialysis pre-operatively), cardiac arrest, urinary tract infection, Clostridium difficile infection, deep vein thrombosis requiring therapy, sepsis, or septic shock. The definition of non-home discharge was discharge to skilled care, rehabilitation, or other facility. These outcomes are defined by the ACS NSQIP data dictionary56. We trained 6 ML models to predict 30-day primary and secondary outcomes: Na ve Bayes classifier, XGBoost, radial basis function support vector machine, random forest, multilayer perceptron artificial neural network, and logistic regression. We selected these models because of their demonstrated efficacy in predicting postoperative outcomes57,58,59. We used logistic regression for baseline comparison because traditional risk predictors most commonly use this modeling technique60. We randomly split our data into two subsets: 70% training and 30% testing61. Testing data was reserved for model evaluation and not used for model training. We performed 10-fold cross-validation and grid search on training data to identify hyperparameters that were optimized for each ML model, including logistic regression62,63. The multivariable logistic regression model was developed and optimized with the same rigor as the advanced ML models with 10-fold cross-validation and grid search62,63. Preliminary analysis showed that the primary endpoint was rare, affecting 470/6601 (7.1%) of patients. To improve class balance, Random Over-Sample Examples (ROSE) was applied to training data64. ROSE uses smoothed bootstrapping to draw new samples from the feature space neighborhood around the minority class and is a commonly used method to facilitate predictive modeling of rare events64. The models were then assessed on test set data and ranked based on the primary discriminatory metric of AUROC. Our best performing model was XGBoost, which had the following optimized hyperparameters for our dataset: number of rounds = 200, maximum tree depth = 3, learning rate = 0.3, gamma = 0, column sample by tree = 0.6, minimum child weight = 1, and subsample = 0.9. The process for selecting these hyperparameters is described in Supplementary Table 2. Once the top-performing ML algorithm for the primary endpoint was identified, the same model was further trained to predict secondary endpoints. Baseline features were reported as means and standard deviations or numbers and proportions. Differences between patients with vs. without 30-day MALE or death were evaluated with independent t-test (continuous variables) or chi-square test (categorical variables). All continuous data were normally distributed. To account for multiple comparisons, Bonferroni correction was used to set statistical significance. AUROC was the primary model evaluation metric as it considers both sensitivity and specificity65. Secondary model evaluation metrics were specificity, sensitivity, negative predictive value, positive predictive value, and accuracy. Model robustness was evaluated using a calibration curve and Brier score, which measures the agreement between predicted/observed event probabilities66. Feature importance was assessed using the variable importance score (gain), which measures the influence of each covariate in contributing to a prediction67. We reported feature importance for 3 groups: the overall cohort, asymptomatic/claudication patients, and individuals with CLTI. Model performance was assessed across subgroups based on sex, age, race, ethnicity, procedure type, symptom status, urgency, and concurrent infrainguinal endovascular revascularization. Using a sample size calculator validated for prediction models, we determined that to achieve an AUROC over 0.8 with an outcome rate of approximately 7% and 37 input variables, 3815 patients with 268 events was the minimum cohort size required68,69. We included 6601 patients who had 470 primary events, which satisfied this condition. There was < 5% missing data for variables of interest; therefore, available case analysis was applied whereby only non-missing covariates for each patient were considered70,71. This is a valid method for analyzing datasets with minimal missing data70,71. Data were missing completely at random, and imputation was not performed to reflect modeling of real-world data, which generally has missing information that is not imputed72. All subjects included in the study (n = 6601) were used in all analyses. R version 4.3.073 was used for all analyses with the following packages: caret74, xgboost75, ranger76, naivebayes77, e107178, nnet79, and pROC80. The data used for this study comes from the American College of Surgeons National Surgical Quality Improvement Program (ACS NSQIP). Access to and use of the data requires approval through an application process available at https://www.facs.org/quality-programs/data-and-registries/acs-nsqip/participant-use-data-file/. The complete code used for model development and evaluation in this project is openly available on GitHub: https://github.com/benli12345/AIE-ML-NSQIP. Fowkes, F. G. R. et al. Comparison of global estimates of prevalence and risk factors for peripheral artery disease in 2000 and 2010: a systematic review and analysis. Lancet Lond. Engl. 382, 1329 1340 (2013). Google Scholar Agnelli, G., Belch, J. J. F., Baumgartner, I., Giovas, P. & Hoffmann, U. Morbidity and mortality associated with atherosclerotic peripheral artery disease: a systematic review. Atherosclerosis 293, 94 100 (2020). CAS PubMed Google Scholar Kim, M., Kim, Y., Ryu, G. W. & Choi, M. Functional status and health-related quality of life in patients with peripheral artery disease: a cross-sectional study. Int. J. Environ. Res. Public. Health 18, 10941 (2021). PubMed PubMed Central Google Scholar Kohn, C. G., Alberts, M. J., Peacock, W. F., Bunz, T. J. & Coleman, C. I. Cost and inpatient burden of peripheral artery disease: findings from the National Inpatient Sample. Atherosclerosis 286, 142 146 (2019). CAS PubMed Google Scholar Heaton, J. & Khan, Y. S. Aortoiliac Occlusive Disease. in StatPearls (StatPearls Publishing, Treasure Island (FL), 2022). Beckman, J. A., Schneider, P. A. & Conte, M. S. Advances in revascularization for peripheral artery disease: revascularization in PAD. Circ. Res. 128, 1885 1912 (2021). CAS PubMed Google Scholar Topfer, L.-A. & Spry, C. New technologies for the treatment of peripheral artery disease. in CADTH Issues in Emerging Health Technologies (Canadian Agency for Drugs and Technologies in Health, Ottawa (ON), 2016). Jongkind, V., Akkersdijk, G. J. M., Yeung, K. K. & Wisselink, W. A systematic review of endovascular treatment of extensive aortoiliac occlusive disease. J. Vasc. Surg. 52, 1376 1383 (2010). PubMed Google Scholar Conte, M. S. et al. Global vascular guidelines on the management of chronic limb-threatening ischemia. J. Vasc. Surg. 69, 3S 125S.e40 (2019). PubMed Google Scholar Bertges, D. J. et al. The Vascular Quality Initiative Cardiac Risk Index for prediction of myocardial infarction after vascular surgery. J. Vasc. Surg. 64, 1411 1421.e4 (2016). PubMed Google Scholar Bilimoria, K. Y. et al. Development and evaluation of the universal ACS NSQIP surgical risk calculator: a decision aid and informed consent tool for patients and surgeons. J. Am. Coll. Surg. 217, 833 842 (2013). PubMed PubMed Central Google Scholar Sharma, V. et al. Adoption of clinical risk prediction tools is limited by a lack of integration with electronic health records. BMJ Health Care Inf. 28, e100253 (2021). Google Scholar Ba tanlar, Y. &  zuysal, M. Introduction to machine learning. Methods Mol. Biol. 1107, 105 128 (2014). PubMed Google Scholar Ngiam, K. Y. & Khor, I. W. Big data and machine learning algorithms for health-care delivery. Lancet Oncol. 20, e262 e273 (2019). PubMed Google Scholar Liew, B. X. W., Kovacs, F. M., R gamer, D. & Royuela, A. Machine learning versus logistic regression for prognostic modelling in individuals with non-specific neck pain. Eur. Spine J. Off. Publ. Eur. Spine Soc. Eur. Spinal Deform. Soc. Eur. Sect. Cerv. Spine Res. Soc. 31, 2082 2091 (2022). Google Scholar Li, B. et al. Predicting outcomes following open revascularization for aortoiliac occlusive disease using machine learning. J. Vasc. Surg. S0741-5214(23)01614 2 https://doi.org/10.1016/j.jvs.2023.07.006 (2023). Bonde, A. et al. Assessing the utility of deep neural networks in predicting postoperative surgical complications: a retrospective study. Lancet Digit. Health 3, e471 e485 (2021). CAS PubMed Google Scholar Hers, T. M. et al. Inaccurate risk assessment by the ACS NSQIP risk calculator in aortic surgery. J. Clin. Med. 10, 5426 (2021). PubMed PubMed Central Google Scholar Bonaca, M. P. et al. Rivaroxaban in peripheral artery disease after revascularization. N. Engl. J. Med. 382, 1994 2004 (2020). CAS PubMed Google Scholar Conte, M. S. et al. Society for Vascular Surgery practice guidelines for atherosclerotic occlusive disease of the lower extremities: management of asymptomatic disease and claudication. J. Vasc. Surg. 61, 2S 41S.e1 (2015). PubMed Google Scholar Gerhard-Herman, M. D. et al. 2016 AHA/ACC guideline on the management of patients with lower extremity peripheral artery disease: executive summary: a report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines. Circulation 135, e686 e725 (2017). PubMed Google Scholar Aboyans, V. et al. Editor s Choice - 2017 ESC guidelines on the diagnosis and treatment of peripheral arterial diseases, in collaboration with the European Society for Vascular Surgery (ESVS). Eur. J. Vasc. Endovasc. Surg. J. Eur. Soc. Vasc. Surg. 55, 305 368 (2018). Google Scholar Farber, A. et al. Surgery or endovascular therapy for chronic limb-threatening ischemia. N. Engl. J. Med. https://doi.org/10.1056/NEJMoa2207899 (2022). Farber, A. Chronic limb-threatening ischemia. N. Engl. J. Med. 379, 171 180 (2018). PubMed Google Scholar Stewart, A. L. et al. Functional status and well-being of patients with chronic conditions. Results from the medical outcomes study. JAMA 262, 907 913 (1989). CAS PubMed Google Scholar Shaydakov, M. E. & Tuma, F. Operative Risk. in StatPearls (StatPearls Publishing, Treasure Island (FL), 2022). Stoltzfus, J. C. Logistic regression: a brief primer. Acad. Emerg. Med. J. Soc. Acad. Emerg. Med. 18, 1099 1104 (2011). Google Scholar Kia, B. et al. Nonlinear dynamics based machine learning: Utilizing dynamics-based flexibility of nonlinear circuits to implement different functions. PloS One 15, e0228534 (2020). CAS PubMed PubMed Central Google Scholar Chatterjee, P., Cymberknop, L. J. & Armentano, R. L. Nonlinear systems in healthcare towards intelligent disease prediction. Nonlinear Syst. Theor. Asp. Recent Appl. 1, e88163 (2019). Google Scholar Ravaut, M. et al. Predicting adverse outcomes due to diabetes complications with machine learning using administrative health data. Npj Digit. Med. 4, 1 12 (2021). Google Scholar Wang, R., Zhang, J., Shan, B., He, M. & Xu, J. XGBoost machine learning algorithm for prediction of outcome in aneurysmal subarachnoid hemorrhage. Neuropsychiatr. Dis. Treat. 18, 659 667 (2022). PubMed PubMed Central Google Scholar Fang, Z.-G., Yang, S.-Q., Lv, C.-X., An, S.-Y. & Wu, W. Application of a data-driven XGBoost model for the prediction of COVID-19 in the USA: a time-series study. BMJ Open 12, e056685 (2022). PubMed Google Scholar Viljanen, M., Meijerink, L., Zwakhals, L. & van de Kassteele, J. A machine learning approach to small area estimation: predicting the health, housing and well-being of the population of Netherlands. Int. J. Health Geogr. 21, 4 (2022). PubMed PubMed Central Google Scholar Shin, S. et al. Machine learning vs. conventional statistical models for predicting heart failure readmission and mortality. ESC Heart Fail 8, 106 115 (2021). PubMed Google Scholar Cho, S. M. et al. Machine learning compared with conventional statistical models for predicting myocardial infarction readmission and mortality: a systematic review. Can. J. Cardiol. 37, 1207 1214 (2021). PubMed Google Scholar Gianfrancesco, M. A., Tamang, S., Yazdany, J. & Schmajuk, G. Potential biases in machine learning algorithms using electronic health record data. JAMA Intern. Med. 178, 1544 1547 (2018). PubMed PubMed Central Google Scholar Mazmudar, A., Vitello, D., Chapman, M., Tomlinson, J. S. & Bentrem, D. J. Gender as a risk factor for adverse intraoperative and postoperative outcomes of elective pancreatectomy. J. Surg. Oncol. 115, 131 136 (2017). PubMed Google Scholar Halsey, J. N., Asti, L. & Kirschner, R. E. The impact of race and ethnicity on surgical risk and outcomes following palatoplasty: an analysis of the NSQIP pediatric database. Cleft Palate-Craniofacial J. Off. Publ. Am. Cleft Palate-Craniofacial Assoc. 10556656221078154 https://doi.org/10.1177/10556656221078154 (2022). R menapf, G., Morbach, S., Schmidt, A. & Sigl, M. Intermittent claudication and asymptomatic peripheral arterial disease. Dtsch.  rztebl. Int. 117, 188 193 (2020). PubMed PubMed Central Google Scholar Bevan, G. H. & White Solaru, K. T. Evidence-based medical management of peripheral artery disease. Arterioscler. Thromb. Vasc. Biol. 40, 541 553 (2020). CAS PubMed Google Scholar Barrows, R. J., Krumsdorf, U., Zankl, A., Katus, H. & Tiefenbacher, C. P. Significance of close surveillance of patients with peripheral arterial disease. Angiology 60, 462 467 (2009). PubMed Google Scholar Paulus, N., Jacobs, M. & Greiner, A. Primary and secondary amputation in critical limb ischemia patients: different aspects. Acta Chir. Belg. 112, 251 254 (2012). CAS PubMed Google Scholar O Connor, D. B. et al. An anaesthetic pre-operative assessment clinic reduces pre-operative inpatient stay in patients requiring major vascular surgery. Ir. J. Med. Sci. 180, 649 653 (2011). PubMed Google Scholar Davis, F. M. et al. The clinical impact of cardiology consultation prior to major vascular surgery. Ann. Surg. 267, 189 195 (2018). PubMed Google Scholar Premaratne, S., Newman, J., Hobbs, S., Garnham, A. & Wall, M. Meta-analysis of direct surgical versus endovascular revascularization for aortoiliac occlusive disease. J. Vasc. Surg. 72, 726 737 (2020). PubMed Google Scholar Gillies, M. A. et al. Intensive care utilization and outcomes after high-risk surgery in Scotland: a population-based cohort study. Br. J. Anaesth. 118, 123 131 (2017). CAS PubMed Google Scholar Patel, P. R. & Bechmann, S. Discharge Planning. in StatPearls (StatPearls Publishing, Treasure Island (FL), 2022). Nguyen, L. L. & Barshes, N. R. Analysis of large databases in vascular surgery. J. Vasc. Surg. 52, 768 774 (2010). PubMed Google Scholar Northridge, M. E. & Metcalf, S. S. Enhancing implementation science by applying best principles of systems science. Health Res. Policy Syst. 14, 74 (2016). PubMed PubMed Central Google Scholar Batko, K. &  l zak, A. The use of Big Data Analytics in healthcare. J. Big Data 9, 3 (2022). PubMed PubMed Central Google Scholar Collins, G. S. et al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods. BMJ 385, e078378 (2024). PubMed PubMed Central Google Scholar ACS NSQIP. ACS https://www.facs.org/quality-programs/data-and-registries/acs-nsqip/. Shiloach, M. et al. Toward robust information: data quality and inter-rater reliability in the American College of Surgeons National Surgical Quality Improvement Program. J. Am. Coll. Surg. 210, 6 16 (2010). PubMed Google Scholar Cohen, M. E. et al. Optimizing ACS NSQIP modeling for evaluation of surgical quality and risk: patient risk adjustment, procedure mix adjustment, shrinkage adjustment, and surgical focus. J. Am. Coll. Surg. 217, 336 346.e1 (2013). PubMed Google Scholar Stoner, M. C. et al. Reporting standards of the Society for Vascular Surgery for endovascular treatment of chronic lower extremity peripheral artery disease. J. Vasc. Surg. 64, e1 e21 (2016). PubMed Google Scholar ACS NSQIP Participant Use Data File. ACS https://www.facs.org/quality-programs/data-and-registries/acs-nsqip/participant-use-data-file/. Elfanagely, O. et al. Machine learning and surgical outcomes prediction: a systematic review. J. Surg. Res. 264, 346 361 (2021). PubMed Google Scholar Bekta , M., Tuynman, J. B., Costa Pereira, J., Burchell, G. L. & van der Peet, D. L. Machine learning algorithms for predicting surgical outcomes after colorectal surgery: a systematic review. World J. Surg. https://doi.org/10.1007/s00268-022-06728-1 (2022). Senders, J. T. et al. Machine learning and neurosurgical outcome prediction: a systematic review. World Neurosurg. 109, 476 486.e1 (2018). PubMed Google Scholar Shipe, M. E., Deppen, S. A., Farjah, F. & Grogan, E. L. Developing prediction models for clinical use using logistic regression: an overview. J. Thorac. Dis. 11, S574 S584 (2019). PubMed PubMed Central Google Scholar Dobbin, K. K. & Simon, R. M. Optimally splitting cases for training and testing high dimensional classifiers. BMC Med. Genom.4, 31 (2011). Google Scholar Jung, Y. & Hu, J. A K-fold averaging cross-validation procedure. J. Nonparametr. Stat. 27, 167 179 (2015). PubMed PubMed Central Google Scholar Adnan, M., Alarood, A. A. S., Uddin, M. I. & Ur Rehman, I. Utilizing grid search cross-validation with adaptive boosting for augmenting performance of machine learning models. PeerJ Comput. Sci. 8, e803 (2022). PubMed PubMed Central Google Scholar Wibowo, P. & Fatichah, C. Pruning-based oversampling technique with smoothed bootstrap resampling for imbalanced clinical dataset of COVID-19. J. King Saud. Univ. Comput. Inf. Sci. 34, 7830 7839 (2022). PubMed Google Scholar Hajian-Tilaki, K. Receiver Operating Characteristic (ROC) curve analysis for medical diagnostic test evaluation. Casp. J. Intern. Med. 4, 627 635 (2013). Google Scholar Redelmeier, D. A., Bloch, D. A. & Hickam, D. H. Assessing predictive accuracy: how to compare Brier scores. J. Clin. Epidemiol. 44, 1141 1146 (1991). CAS PubMed Google Scholar Loh, W.-Y. & Zhou, P. Variable importance scores. J. Data Sci. 19, 569 592 (2021). Google Scholar Riley, R. D. et al. Calculating the sample size required for developing a clinical prediction model. BMJ m441 https://doi.org/10.1136/bmj.m441 (2020). Ensor, J. pmsampsize: Sample Size for Development of a Prediction Model. The Comprehensive R Archive Network https://cran.r-project.org/package=pmsampsize (2023). Kang, H. The prevention and handling of the missing data. Korean J. Anesthesiol. 64, 402 406 (2013). PubMed PubMed Central Google Scholar Groenwold, R. H. H. et al. Missing covariate data in clinical research: when and when not to use the missing-indicator method for analysis. CMAJ Can. Med. Assoc. J. 184, 1265 1269 (2012). Google Scholar Hughes, R. A., Heron, J., Sterne, J. A. C. & Tilling, K. Accounting for missing data in statistical analyses: multiple imputation is not always the answer. Int. J. Epidemiol. 48, 1294 1304 (2019). PubMed PubMed Central Google Scholar Download R-4.3.0 for Windows. The R-project for statistical computing. https://cran.r-project.org/bin/windows/base/. Kuhn, M. et al. caret: Classification and Regression Training. The Comprehensive R Archive Network https://CRAN.R-project.org/package=caret (2024). Chen, T. & Guestrin, C. XGBoost: a scalable tree boosting system. Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min. KDD 16, 785 794 (2016). Google Scholar Wright, M. N., Wager, S. & Probst, P. ranger: A Fast Implementation of Random Forests. The Comprehensive R Archive Network https://cran.r-project.org/package=ranger (2024). naivebayes: High Performance Implementation of the Naive Bayes Algorithm version 0.9.7 from CRAN. https://rdrr.io/cran/naivebayes/. https://www.rdocumentation.org/packages/e1071/versions/1.7-11/topics/svm svm function - RDocumentation. Ripley, B. & Venables, W. nnet: Feed-Forward Neural Networks and Multinomial Log-Linear Models. The Comprehensive R Archive Network https://CRAN.R-project.org/package=nnet (2025). Robin, X. et al. pROC: an open-source package for R and S+ to analyze and compare ROC curves. BMC Bioinform. 12, 77 (2011). Google Scholar Download references This work was partially funded by the Canadian Institutes of Health Research, Ontario Ministry of Health, PSI Foundation, and Schwartz Reisman Institute for Technology and Society at the University of Toronto (Dr. Li). Dr. Hussain is funded by a Brigham and Women s Hospital Heart and Vascular Center Faculty Award. The funding sources did not play a role in the design or conduct of the research. The ACS NSQIP and the hospitals participating in the ACS NSQIP are the source of the data used herein; they have not verified, and are not responsible for, the statistical validity of the data analysis or the conclusions derived by the authors. Presented at the following scientific meetings: Society for Vascular Surgery 2024 Vascular Annual Meeting, June 19 22, 2024, Chicago, Illinois, United States. Canadian Society for Vascular Surgery 2024 Annual Meeting, September 13 14, 2024, St. John s, Newfoundland, Canada. 25th Congress of the Asian Society for Vascular Surgery, December 3-6, 2024, Bangkok, Thailand. These authors contributed equally: Ben Li, Badr Aljabri. Department of Surgery, University of Toronto, Toronto, ON, Canada Ben Li, Ori D. Rotstein, Charles de Mestral & Mohammed Al-Omran Division of Vascular Surgery, St. Michael s Hospital, Unity Health Toronto, University of Toronto, Toronto, ON, Canada Ben Li, Charles de Mestral & Mohammed Al-Omran Institute of Medical Science, University of Toronto, Toronto, ON, Canada Ben Li, Ori D. Rotstein, Muhammad Mamdani & Mohammed Al-Omran Temerty Centre for Artificial Intelligence Research and Education in Medicine (T-CAIREM), University of Toronto, Toronto, ON, Canada Ben Li, Muhammad Mamdani & Mohammed Al-Omran Department of Surgery, King Saud University, Riyadh, Saudi Arabia Badr Aljabri Data Science & Advanced Analytics, Unity Health Toronto, University of Toronto, Toronto, ON, Canada Derek Beaton & Muhammad Mamdani College of Medicine, Alfaisal University, Riyadh, Saudi Arabia Leen Al-Omran Division of Vascular and Endovascular Surgery and the Center for Surgery and Public Health, Brigham and Women s Hospital, Harvard Medical School, Boston, MA, USA Mohamad A. Hussain Division of Cardiology, Peter Munk Cardiac Centre, University Health Network, Toronto, ON, Canada Douglas S. Lee Institute of Health Policy, Management and Evaluation, University of Toronto, Toronto, ON, Canada Douglas S. Lee, Duminda N. Wijeysundera, Charles de Mestral & Muhammad Mamdani ICES, University of Toronto, Toronto, ON, Canada Douglas S. Lee, Duminda N. Wijeysundera, Charles de Mestral & Muhammad Mamdani Department of Anesthesia, St. Michael s Hospital, Unity Health Toronto, Toronto, ON, Canada Duminda N. Wijeysundera Li Ka Shing Knowledge Institute, St. Michael s Hospital, Unity Health Toronto, Toronto, ON, Canada Duminda N. Wijeysundera, Ori D. Rotstein, Charles de Mestral, Muhammad Mamdani & Mohammed Al-Omran Division of General Surgery, St. Michael s Hospital, Unity Health Toronto, Toronto, ON, Canada Ori D. Rotstein Leslie Dan Faculty of Pharmacy, University of Toronto, Toronto, ON, Canada Muhammad Mamdani Department of Surgery, King Faisal Specialist Hospital and Research Center, Riyadh, Saudi Arabia Mohammed Al-Omran Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar All authors meet all four criteria: (1) Substantial contributions to the conception or design of the work or the acquisition, analysis, or interpretation of the data, (2) Drafting the work or revising it critically for important intellectual content, (3) Final approval of the completed version, and (4) Accountability for all aspects of the work in ensuring that questions related to the accuracy or integrity of any part of the work are appropriately investigated and resolved. B.L. and M.A.O. had full access to and verified all the data in the study and take responsibility for the integrity of the data and accuracy of the data analysis. B.L., B.A., C.deM., M.M., and M.A.O. conceived of and designed the study. B.L. drafted the article, developed the models, and performed statistical analysis (with the support of data scientist D.B.). All authors (B.L., B.A., D.B., L.A.O., M.A.H., D.S.L., D.N.W., O.D.R., C.deM., M.M., and M.A.O.) acquired, analyzed, or interpreted data, and critically revised the article for important intellectual content. C.deM., M.M., and M.A.O. provided supervision. M.A.O. had the final responsibility for the decision to submit for publication. All authors (B.L., B.A., D.B., L.A.O., M.A.H., D.S.L., D.N.W., O.D.R., C.deM., M.M., and M.A.O.) had full access to the full data in the study, accept responsibility to submit for publication, and approved of the final manuscript. Correspondence to Mohammed Al-Omran. The authors declare no competing interests. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Presented at the following scientific meetings: Society for Vascular Surgery 2024 Vascular Annual Meeting, June 19-22, 2024, Chicago, Illinois, United States Canadian Society for Vascular Surgery 2024 Annual Meeting, September 13-14, 2024, St. John s, Newfoundland, Canada 25th Congress of the Asian Society for Vascular Surgery, December 3-6, 2024, Bangkok, Thailand. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions Li, B., Aljabri, B., Beaton, D. et al. Predicting outcomes following endovascular aortoiliac revascularization using machine learning. npj Digit. Med. 8, 475 (2025). https://doi.org/10.1038/s41746-025-01865-y Download citation Received: 08 April 2025 Accepted: 03 July 2025 Published: 24 July 2025 DOI: https://doi.org/10.1038/s41746-025-01865-y Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Collection Advertisement npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",1
Artificial intelligence in clinical medicine: a state-of-the-art overview of systematic reviews with methodological recommendations for improved reporting - Frontiers,https://news.google.com/rss/articles/CBMilwFBVV95cUxQazc2bFl2TkZfWE1sMkZRekh3aVg0RTUwLXdJdW90Q2N4b2hYSnhSVUhUY0luR01aS3ByekxiUlRNTkxZNVlaNVJtQkxsSEtTUVBBZVJhSmg0TjhMSW40LVNyX1RaQWZRUElpMmhIcE1XTkw3a0tCdjZvTFlWeUo0empTQkExVWxhUWo5YTFCUUhMS2NWMFZn?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better REVIEW article Front. Digit. Health, 05 March 2025 Sec. Health Technology Implementation Volume 7 - 2025 | https://doi.org/10.3389/fdgth.2025.1550731 This article is part of the Research TopicAdvances in Artificial Intelligence Transforming the Medical and Healthcare SectorsView all 14 articles Medicine has become increasingly receptive to the use of artificial intelligence (AI). This overview of systematic reviews (SRs) aims to categorise current evidence about it and identify the current methodological state of the art in the field proposing a classification of AI model (CLASMOD-AI) to improve future reporting. PubMed/MEDLINE, Scopus, Cochrane library, EMBASE and Epistemonikos databases were screened by four blinded reviewers and all SRs that investigated AI tools in clinical medicine were included. 1923 articles were found, and of these, 360 articles were examined via the full-text and 161 SRs met the inclusion criteria. The search strategy, methodological, medical and risk of bias information were extracted. The CLASMOD-AI was based on input, model, data training, and performance metric of AI tools. A considerable increase in the number of SRs was observed in the last five years. The most covered field was oncology accounting for 13.9% of the SRs, with diagnosis as the predominant objective in 44.4% of the cases). The risk of bias was assessed in 49.1% of included SRs, yet only 39.2% of these used tools with specific items to assess AI metrics. This overview highlights the need for improved reporting on AI metrics, particularly regarding the training of AI models and dataset quality, as both are essential for a comprehensive quality assessment and for mitigating the risk of bias using specialized evaluation tools. The advent of big data is radically changing every aspect of our civilization, and the medical field is among those most affected, creating opportunities for more comprehensive, data-driven clinical decision-making (1). Managing this wide amount of data should be possible only if automated, and AI has the potential to revolutionise and improve the efficiency and efficacy of men in many fields, including medicine. Large datasets are regularly generated in healthcare services, and AI can potentially utilise those datasets to improve most aspects of clinical medicine. Clinical medicine is the branch of medicine that focuses on the diagnosis, treatment, and care of patients. It involves the application of medical knowledge and skills to address the health needs of individuals (2). AI can be used for any step of clinical medicine: diagnosis, prognosis, clinical decision-making, treatment, patient education and even follow-up. Furthermore, thanks to its ability to analyse vast amounts of data, it could provide concrete help in applying the principles of personalised medicine, assisting the clinician in tailoring the best treatment specifically designed for each patient (3). On the other hand, while AI has the potential to revolutionise healthcare, its integration into clinical practice is still in the early stages (4). There is no clear quantification of the benefits of implementing AI-assisted tools in clinical medicine (5). AI can be a powerful tool, but, like any tool, clinicians need to be educated and trained in its use. Additionally, its limitations must be well-understood and carefully considered. For instance, AI inherently carries a risk of bias. These biases often stem from the data used to train AI models, which may reflect existing inequalities and inaccuracies in healthcare data (6). To mitigate these risks, standards for data diversity and continuous bias monitoring should be established (7, 8). The growing and enormous interest in the application of AI in medicine inevitably goes in two directions: evaluating the already existing applications in medicine and evaluating the possible role of future AI in every field of clinical medicine (9). We are still far from fully understanding and using AI to its full potential. A necessary step to reach this goal, which can change clinical medicine as we know it, is to have a clear picture of what is already done and understand as much as possible what is already established, what is working, and which research gaps are present. Furthermore, it is important to understand if current evidence reporting is sufficient to support researchers in building up from the present foundations. The number of guidelines for reporting studies on AI is increasing, a necessary step to accelerate knowledge growth in this area (10). Beyond reporting guidelines, reviewers have already made choices in different areas of clinical medicine that must be recognized to guide future research. We aim to produce a living systematic review of the AI applications clinically useful in rehabilitation, in collaboration and with the methodological supervision of Cochrane Rehabilitation according to their previous large experience with the rapid living systematic reviews on rehabilitation for COVID-19 (11). Creating a comprehensive map of the current understanding of AI applications in clinical medicine will allow us to develop a proper methodology and framework for comparison. An overview of reviews provides a comprehensive synthesis of existing systematic reviews, offering a broad perspective on a topic, identifying research gaps, and highlighting key findings to inform future studies and decision-making (12). For this reason, an overview of reviews on the use of AI in clinical medicine allows us to categorise current evidence about it and identify the current methodological state of the art in the field. On one side, we want to see where and how it has mostly been used, providing a complete overview of the current literature; on the other, we want to describe the methodological tools used. Thus, the primary aim of our overview of reviews is to verify the quality of reporting in SRs, to provide indications on qualitative and quantitative evaluation of AI to increase homogeneity and systematicity of future SRs and primary studies. In this way, we want to help future authors of secondary synthesis of the literature to build upon previous experiences; on the other, we want to contribute to the debate on how to better perform systematic reviews in the rapidly growing field of AI use in clinical medicine. To achieve this, considering the specific characteristics of AI, a tailored evaluation tool for reporting SRs in the AI field is needed. An evolution of the PRISMA reporting guidelines, called PRISMA-AI, is currently under development, but it is not yet ready for use (13). Therefore, it seems necessary to employ interim tools. Thus, a secondary goal of our work is to propose a fast and effective tool that can bridge the temporary gap while waiting for PRISMA-AI. The reporting of the overview followed the Preferred Reporting Items for Overviews of Reviews (PRIOR) Guidelines (14). The PubMed/MEDLINE, Scopus, Cochrane Library, EMBASE and Epistemonikos databases were screened from inception until 1 January 2024. Mixed, MeSH-terms and free-terms were used to perform the research. We used terms including artificial intelligence (i.e.,  artificial intelligence ,  machine learning ,  deep learning ) applied in the field of clinical medicine ( clinical medicine ). The search strategy was limited to systematic reviews and reviews (the complete search strategy is available in appendix A of the Supplementary Materials). The protocol was registered on the SFO database (https://doi.org/10.17605/OSF.IO/USB5J) (15). We included all systematic reviews, scoping reviews, and reviews with a structured search strategy that investigated the use of artificial intelligence in clinical medicine. Inclusion criteria were: (i) Systematic review, review, and scoping review with a structured search strategy; (ii) Investigation about the use of artificial intelligence or subgroups of AI; (iii) Application of AI tools in the field of clinical medicine, defined as reported in the introduction; (iv) English language; (v) Articles published in a peer-reviewed journal. Expert and narrative reviews and preprint manuscripts were excluded. All results were uploaded to Rayyan, a digital database for systematic reviews (16). Four blinded reviewers (AMC, LDA, AB, and RC) screened the results via title and abstract. After screening, in case of disagreement, the conflict was resolved with the support of two additional reviewers (FN, and GM). With the same procedure, the four reviewers analysed the full texts, and the conflicts were resolved with the intervention of the same two additional reviewers. The full texts were uploaded in PDF format (completed with Supplementary Materials) in a shared online folder. The information of each included study was extracted and compared from two reviewers in a synoptic table on a sheet. In case of disagreement, a third author was involved. The data extracted were divided into qualitative information and AI classification (Complete data extraction strategy is reported in Table 1). Qualitative information extracted included the first author's name, the year of publication, the country, the number of databases consulted, the studies included, the research period, the field of medicine and studied clinical processes. Moreover, methodological information on using PRISMA/PRIMA-ScR checklists and the risk of bias (ROB) tools utilised were also collected, both the generic ROB tools and those specific to AI studies. Table 1. Data extraction strategy. Lastly, we implemented a tool to evaluate the quality of reporting in SRs on AI (CLASMOD-AI). The tool aims to understand the classificatory categories used by systematic reviewers to group primary studies. Such a tool could be a reference for future authors when choosing how to classify AI studies in their reviews and will serve our scope within the field of rehabilitation. Starting from reporting tools already developed for other types of research studies involving AI (i.e., CONSORT-AI (17) and SPIRIT-AI (18), APPRAISE-AI (19) and TRIPOD-AI (20)), we analysed and extrapolated the key characteristics. An open discussion with the Board of the Italian Society of Artificial Intelligence in Medicine (SIIAM) produced the final proposal, which was purposefully limited to the most important items. The tool is not yet validated, but it is deemed necessary given the current lack of appropriate instruments. According to the current version of the tool CLASMOD-AI, we evaluated if the following classificatory items were reported in the SRs: (i) type of data (Text, Images, etc.), (ii) AI models (Machine Learning, Deep Learning), (iii) model training (Supervised, Unsupervised, Reinforcement), (iv) model performance metrics (i.e., Sensitivity, Specificity). After full-text screening, 161 reviews (The complete list is available in Appendix B of the Supplementary Materials) (151 systematic reviews and 10 scoping reviews) were included in the final synthesis (The search process is reported in Figure 1). Figure 1. Flow-diagram of inclusion process. The publication date of the included studies ranged from April 2005 to January 2024, with an average increase in the last three years of 10.4   2% for each year. The United Kingdom (12.4%), China (11.8%), and Italy (10.6%) are the countries most represented in terms of the total number of reviews published (Figure 2A). Figure 2. (A) Shows the number of publications per country (based on the affiliation of the first author). (B) Reports the number of publications of AI reviews for the year of publication. The cubic polynomial trendline shows the sprout of primary studies included in the reviews, specifically during the last five years. Panel C shows the number of SRs that performed a risk of bias (ROB) assessment for years of publication. The ROB tools were computed as  ROB  for conventional methodological quality assessment, and  ROB + AI metrics  for ROBs that evaluate specifically the AI metrics. The average (  standard deviation) number of databases screened for the research was 3.7   1.7. The research was conducted since inception in 43.5% of SRs, for a predefined time window in 52.2% of SRs, and in the remaining 4.3%, the information was missing. In the SRs that performed the research for predefined time windows, 14.3% searched for at least five years, 32.1% from 6 to 10 years, 23.8% from 11 to 20 years, 26.2% from 21 to 30, and 3.6% over 30 years, with an average of 13.2   8.4 years. The average number of primary studies included was 45.1   48.7. Comparing the number of primary studies included based on their year of publication, shows an increase, particularly over the last five years (Figure 2B). Except for one, all reviews reported the number of primary studies included in the synthesis, and in total, 7,672 studies were included. Seventy-seven percent of the studies reported according to the PRISMA (or PRISMA-ScR) guidelines and 49.1% of reviews performed a risk of bias (ROB) analysis of included studies. We found four approaches in the ROB analysis: (i) 62.0% of these reviews used a ROB tool for the assessment of methodological quality without specific items for AI metrics; (ii) 16.5% used only a tool with specific designed items for the assessment of quality of AI metrics; (iii) 13.9% used both approaches combining two different tools for the assessment of methodological quality and quality of AI metrics; and finally (iv) 7.6% used a single tool that combined items to assess both general methodological quality and quality of AI metrics. In general, 39.2% of the reviews analysed the quality of AI metrics alone or in combination with other tools. An increase in the use of ROB tools for assessing methodological quality was observed in recent years. However, concerning the use of dedicated tools for AI (or with specific items for AI), the increase is smaller and more recent (see Figure 2C). The most used ROB tool was the QUADAS-2 (41.8%), followed by the prediction model risk of bias assessment tool (PROBAST) (11.4%), and the radiomics quality score (RQS) (10.1%). The complete percentages of tools used for ROB assessment are reported in Table 2. Table 2. Description of risk of bias tools. We found SRs in 41 fields of clinical medicine, and the most frequent were oncology (13.9%), neurology (7.9%), radiology (7.9%), gastroenterology (6.0%), and dentistry (5.3%). We found that AI was used mostly for diagnosis (44.4%), prognosis/predictions (13.9%), and screening (9.3%) purposes. The oncology field used AI principally with the scope of diagnosis (57.1%) and prognosis/prediction (23.8%). The choice of AI models is primarily driven by the specific objective (i.e., diagnosis and prognosis/predictions) rather than the medical field itself. For instance, Convolutional Neural Networks (CNN) and Support Vector Machine (SVM) are frequently used across a wide range of fields, with algorithms like Random Forest (RF), Least Absolute Shrinkage and Selection Operator (LASSO), Deep Neural Network (DNN), and Logistic Regression (LR) often complementing them to enhance diagnostic accuracy and clinical decision-making. This widespread usage stems from the proven effectiveness of these algorithms in diagnostic tasks across diverse medical domains. In summary, these algorithms were used often in tandem to provide more accurate, efficient, and scalable solutions for medical diagnosis, enhancing the precision of disease detection and supporting clinical decision-making across a variety of healthcare applications. Medical fields with a low prevalence in our sample were collapsed into  other medical fields , but even in this case, the main objective of using AI remained diagnosis (35.3%). Furthermore, 13.2% of reviews investigated using AI for multiple purposes or with specific aims that were not yet categorizable (Figure 3). Figure 3. Number of revised papers for each medical field and each phase of the medical process. Oncology is the medical field most investigated, followed by neurology and radiology (including neuroradiology). Dentistry, gastroenterology and orthopaedics have also been widely studied. Among the medical processes, diagnosis is the most frequent scope of AI analyses. The most frequently reported information about AI metrics across the SRs was the category of AI models employed (84.5%). In contrast, there was a notable lack of reporting on model training categories, which was present in only 25.6% of cases. Only 15.5% of the SRs met all four items of the CLASMOD-AI, while 5.6% did not meet any item (Table 3). In SRs that satisfied just one item of CLASMOD-AI, 58.3% reported the AI model classification, while none reported the classification according to AI training. In SRs that satisfied two items, the most common combinations were items 2 and 4 (47.1%) and items 1 and 2 (33.3%), with no studies reporting any other combinations. Finally, in SRs that met three items, the most common combination was items 1, 2, and 4 (76.9%). In conclusion, item 3 (classification according to AI training) was generally the least reported, with a significant lack of information regarding AI training (e.g., supervised, unsupervised, generative AI). Moreover, the evaluation of the datasets was not conducted using dedicated measures such as ML-DQA, ROBUST-ML, MINIMAR, or the V7 Tool. These methodologies and tools are specifically designed to assess the quality, robustness, and reliability of AI datasets, ensuring that they meet predefined standards for accuracy, completeness, and bias mitigation. However, more than half of the SRs (63.3%) provide information on the dataset volume, either in terms of the number of acquisitions or the number of patients included. Table 3. Results of CLASMOD-AI. This overview of reviews aims to create a synthetic portrait of the use of AI in clinical medicine, merging the results of systematic reviews with a methodological focus on the reporting, risk of bias, and classification of AI tools used. In recent years, primary studies on AI in medicine have increased exponentially. SRs allow us to summarise on a large scale the major characteristics of the different fields of clinical medicine and the medical processes involved (i.e., diagnosis, prognosis, clinical decision-making and treatment). To our knowledge, this is the first overview of SRs on this emerging topic. The choice to use the overview of SRs design, despite some limitations discussed in the limitations section, provided us with a wider view on the current evidence regarding the clinical use of AI in medicine. This approach also made it possible to extract methodological features and related information reporting. We found 161 SRs, including about 7,500 primary studies, that investigate the use of AI for medical purposes in a large range of clinical medicine fields, most of which were published in the last five years. This finding confirms that AI is becoming more integrated into clinical medicine, as reflected by the broad range of medical fields covered in the included SRs (41 fields). This accounts for the phenomenon's complexity and the sprout of using AI in clinical medicine and other non-medical disciplines. The use of AI is more studied in some medical areas, such as oncology, neurology and radiology, but also in gastroenterology and dentistry. Despite the incredible proliferation of studies on the clinical use of AI, as evidenced by the works included in the reviews considered (7,672 studies), the overall methodological quality is disappointing. CLASMOD-AI, our newly developed tool for screening the quality of reporting in AI-related SRs, allowed us to highlight important issues in the included studies. The significance of CLASMOD-AI lies in the fact that, currently, there is no established guidance or tool available to help authors classify AI studies in SRs (for more information, see the appendix D of the Supplementary Materials). While PRISMA-AI guidelines are under development and will offer comprehensive reporting standards for SRs involving AI in healthcare (13), CLASMOD-AI addresses the immediate need for a practical, simplified framework. It serves as a valuable interim solution, offering a structured approach to address the challenges posed by the heterogeneity of AI studies in SRs. An important challenge in synthesising evidence from primary studies on AI in clinical medicine is their heterogeneity. To prevent confusion among readers, it is essential for every systematic review to clearly specify any specific constraints in the study's inclusion criteria. These constraints could relate to the type of input data, the category and type of AI models used, or the evaluation metrics considered. If no constraints limit the selection of studies to a particular instance within these categories (e.g., focusing only on studies using images as data), then these categories can serve as classification criteria to group the primary studies for clearer analysis and reporting. In our analysis, many authors opt to categorise studies based on the type of AI models employed, with approximately 85% of the SRs distinguishing primarily between machine learning and deep learning models. In contrast, only about one-quarter of these studies utilise the training category of AI models as a classification criterion. This is likely because most AI models in clinical medicine rely on supervised learning. Since supervised learning represents a single category, it often cannot be used as a classification criterion. Furthermore, the data type is a source of high heterogeneity among the primary studies, and it is often employed to group studies (around 63% of the SRs). Lastly, employing model evaluation metrics as criterion for grouping studies is crucial for quantitatively synthesising results and conducting meta-analyses (around 68% of the SRs). The classification patterns observed in our analysis highlight important considerations for future reviews. Researchers should consider the dominant classification schemes and their implications for synthesis and reporting. Specifically, distinguishing studies based on data types or evaluation metrics can provide more structured and insightful analyses. Ultimately, this approach will aid in harmonising evidence and improving the comprehensiveness of reviews in the evolving field of AI in clinical medicine. Furthermore, nearly a quarter of the studies did not report following PRISMA guidelines, and less than 50% conducted a risk of bias (ROB) analysis of the included studies. Naturally, the lack of use of these tools raises significant concerns about their quality (21). Furthermore, AI is a complex subject and also different and more technological from other issues we are used to studying in health care, which introduces additional methodological challenges. Many biases are particularly relevant or specific to AI studies, including inappropriate train-test split, lack of diversity within the input data, historical bias, representation bias, evaluation bias, aggregation bias, population bias and sampling bias (22). To address these challenges in evaluating the quality of AI studies, an adaptation of generic ROB tools to incorporate specific items for AI studies is required. AI-specific extensions and reporting guidelines have been created for randomised control trials (RCTs) in 2020 CONSORT-AI (Consolidated Standards of Reporting Trials AI) (17) and SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials-AI) (18) were published. However, only 19% of RCTs in the AI medical field published after the start of 2021 cited the CONSORT-AI guidelines (23). This highlights possible difficulties and delays in the adoption of these tools. To support this evidence, we identified only one SR reporting the use of CONSORT-AI in ophthalmology (24). Moreover, RCTs represent only a minority of primary studies in the medical AI field and extensions of ROB tools and reporting guidelines for other categories of AI studies were released very recently, such as APPRAISE-AI (19) and TRIPOD-AI20 for clinical decision support and prediction models in 2023 2024, or still under development, such as QUADAS-AI for diagnostic studies (25) and PROBAST-AI for prediction models (26), both announced in 2021. Given that over 50% of the reviews in our study that assessed the risk of bias used QUADAS-2 or PROBAST, it is crucial to expedite the release of the anticipated AI extensions of these tools. Considering potential delays in their adoption and the rapid pace of scientific production in this field, there is a risk that a growing number of SRs on AI across various medical fields may inadequately assess study quality. This could result in an expanding body of incomplete or potentially misleading evidence. Two different checklists developed specifically for imaging studies, RQS (27) and CLAIM (28), were widely used in reviews focusing on radiology. This alternative approach creating quality assessment tools tailored to the type of data rather than the study design could prove promising if similar tools are developed for text-based or structured data in the future. Moreover, the lack of dedicated dataset evaluations (29 31) suggests that potential data inconsistencies, biases, or other quality concerns may not have been systematically identified or addressed. In addition to the performance of the algorithms, it is crucial to understand certain characteristics of the datasets, as this information is essential for assessing their quality and, consequently, evaluating the quality of the AI model. As a result, any conclusions drawn from these datasets should be approached with caution, as underlying data quality issues could affect the overall performance of AI models. Regarding the medical processes, diagnosis is the most important purpose independently represented by the clinical medical fields. AI has demonstrated high accuracy and diagnostic potential across various areas, including early detection of cancers (32 34) and osteoarthritis (35), and predicting disease progression in conditions like Parkinson's (36) and otitis media (37). Radiomics, AI-enhanced imaging, and machine learning algorithms have improved diagnostic processes, with promising results in personalized care and tumor detection. However, while AI models show competitive performance compared to human experts in detecting specific conditions (e.g., prostate cancer, coeliac disease, and hip fractures) (34, 38, 39), challenges remain in their widespread clinical adoption. These include the need for more robust, diverse datasets and the resolution of methodological issues that impact model generalizability. However, AI is influencing all domains, including those where human judgement is typically expected to be decisive, such as therapy choice and clinical decision-making. Despite these advancements, their application is still in the early stages, showing promising results but facing significant challenges. In decision-making, AI algorithms enhance prediction accuracy, risk classification, and disease progression tracking. For instance, AI has shown higher sensitivity in lung nodule assessment (40), detecting vertical root fractures in endodontics (41), and clinical decision support for the hospital setting (42), often outperforming traditional methods. These models provide clinicians with data-driven insights that support dynamic care strategies, adaptive treatments, and precision medicine. AI is also applied in therapeutic support in areas like pediatric anesthesia (43), stroke treatment (44), and knee arthroplasty (45), aiding surgical planning, postoperative monitoring, and outcome prediction through deep learning models. It is worth noting that this last aspect will be promising, for example, in patients with multiple conditions, mitigating the problems of traditional clinical medicine as the hyperspecialization with a lack of integrated competencies. In this context, intelligent clinical decision support systems should be able to manage complexity and minimise iatrogenic risks (46). In medical education, AI allows for personalized, scalable learning, and it improves healthcare management by supporting decision systems for chronic disease and multimorbidity. Additionally, AI automates neonatal pain assessment (47), coordinates clinical nutrition research (48) for more personalized care, and optimizes outcomes in fields like ophthalmology and surgery. Furthermore, it contributes to public health by predicting and managing chronic disease outcomes, enhancing healthcare efficiency across diverse non-diagnostic areas. The SRs regarding AI in clinical medicine conducted in recent years have thus primarily focused on diagnosis, screening, and prognosis in oncology and radiology. The numbers are striking: despite identifying reviews across 41 different fields of clinical medicine, oncology, and radiology together account for  22% of the total, while studies on diagnosis, screening, and prognosis cover over 65%. AI can help bring order to complex decisions based on images or molecular markers using CNN, SVM, LASSO, RF, DNN and LR. However, we believe it is limiting to consider AI merely as a diagnostic tool. As abovementioned, AI can potentially be successfully employed in other processes, such as clinical decision-making (49), therapeutic applications (50), or patient education (51). In these domains of clinical medicine, research appears to still be in its early stages, and the potential of AI is largely untapped. We believe that our work can incentivise clinical researchers to identify new and exciting AI-based clinical approaches. The number of publications has increased, particularly since 2018, when we witnessed exponential growth. We can also see how, over the years, there has been increasing attention in SRs to using quality tools specifically built for evaluating medical research studies involving AI (26, 52). Despite ChatGPT emerging as a leader among AI systems utilising large language models (LLMs), it is important to note that we found no reviews investigating its use in clinical medicine despite including over 160 reviews. This is likely because, by its very nature, it is more suitable for use in healthcare for purposes other than clinical applications, such as education, research, or improving administrative processes (53). Another possible reason for the absence of SRs on LLMs is the time required to accumulate sufficient primary studies on the topic. Since LLMs are relatively new, there is a natural delay before enough research is available to conduct comprehensive reviews on specific areas of clinical medicine. Regarding geographical distribution, England, China, and Italy are the countries with the highest SR production. However, we found SRs published from all the major continents (Africa, the Americas, Asia, Australia, and Europe). The ubiquitous publication of studies is a potential positive indicator of AI diffusion worldwide. Although during the last 30 years, medical inequity in human resources for health was reduced, all-cause mortality was relatively higher in countries and territories with a limited health workforce (54). As declared by the WHO, it is important to develop equity-oriented health workforce policies, expanding health financing to achieve universal health coverage by 2030 (55). Undoubtedly, AI will play a fundamental role in this scenario. It is important to monitor these geographical aspects because, a great impact of AI on healthcare systems worldwide might create a unique opportunity to reduce inequity or pose a possible risk to augment the gap between high-income countries and the global south. Ethical considerations are essential to ensure that AI does not harm the patient and caregivers and that its application ensures a medical-social benefit. Developers economically interested in the development of AI may have prejudices for example about gender, origin, and the motor-cognitive functioning of the person. It is essential to create tools that also correctly analyze ethical aspects at every stage such as for development, validation, implementation, and surveillance of the AI Application in Healthcare. To date, SPIRIT-AI, for example, contemplates items related to ethics (56). While artificial intelligence represents a real possibility of improving the well-being and health of all people in every country, there is a real risk that it could increase the inequities between developed and developing countries and among and between different social classes of the same nations. In fact, artificial intelligence is part of a broader context of Digital Health that is supported by other technology domains that are considered as social determinants of health, such as big data analytics, the internet of things (IoT), next-generation networks (e.g., 5G), and privacy-preserving platforms, e.g., blockchain. The development and the healthcare use of these other technological domains is certainly not homogeneous worldwide (57). Our work is not without limitations. The use of an overview enables us to adopt a global approach to a broad topic, such as the application of AI in clinical medicine. However, this tool has an inherent limitation: it is more temporally delayed compared to a traditional systematic review, precisely because it involves secondary analysis. The topic considered is extremely broad. We aimed to come as close as possible to identifying all studies on the subject, as demonstrated by the significant screening effort (1,923 reviews, of which 161 were included). Nevertheless, the breadth of the topic leaves the possibility that our search string may have missed some studies. Another limitation of our study is that we did not conduct an analysis of how many primary studies are duplicated within our overview. However, since the methodological focus was on data reporting, we do not consider this a significant limitation for our purposes. Furthermore, the CLASMOD-AI tool has not yet been validated in a dedicated research paper. On the other hand, it has been developed through a structured internal consensus process by SIIAM. The proposed ad interim tool (CLASMOD-AI) does not consider the ethical aspects of AI implementation and reporting in systematic reviews. For future use, the part relating to ethical considerations must be implemented and validated. Moreover, it needs to be considered that, to our knowledge, there is still no reliable tool available for quality assessment of the reporting in SRs, probably because AI is a very novel topic and the correct methodology for research and study reports is still under investigation. Another limitation of our study is that our data extraction process did not fully capture the depth of information in the SRs. Due to the large and heterogeneous number of SRs, our focus was primarily on general and methodological considerations. Future research could benefit from a more targeted examination of specific clinical domains to extract more meaningful insights into AI's state of the art within those areas from SRs. Finally, a possible overlap in the primary studies included in the SRs was not investigated. However, this limit may be mitigated by the large number of fields of clinical medicine found across SRs. On the other hand, our overview has many strengths. To the best of our knowledge, it is the first study to conduct an overview of reviews on the use of AI in clinical medicine. We analysed over 160 reviews from a bibliographical, methodological, and content perspective and summarised the results. In our work, we introduced a quick and novel methodological assessment of the reporting in AI-related reviews, which future studies can use. The methodological quality of the analysed studies is unsatisfactory, as only a few of the included reviews utilize AI-specific tools for ROB analysis and dataset evaluation. Given that dataset quality is a critical factor in AI training, this lack of clarity may lead to challenges in validating the AI models used in research studies. Our newly developed tool to assess the quality of reporting of SRs regarding AI, CLASMOD-AI, allowed us to discover that an unsatisfactory percentage of SRs reported critical elements of primary studies. AI in clinical medicine is currently used primarily for diagnosis (44.4% of the studies considered) in oncology and radiology. Many countries from all major continents have conducted studies related to AI, with England, China, and Italy being the most prominent contributors. Most studies have been published in the last five years, with a continuously increasing trend year after year. Despite the proliferation of studies on the topic, the potential for AI to improve the work of clinicians worldwide remains largely untapped. Clinicians around the world must be aware of the potential risks associated with the use of AI: systematic reviews do not yet seem adequate to identify the ROB of primary studies effectively, ethical issues have not yet been resolved, and much research is still needed to identify all the domains and areas of medicine where AI can be effectively utilised. GM: Conceptualization, Writing   original draft. LD: Methodology, Writing   original draft. AM: Data curation, Investigation, Writing   review & editing. RC: Data curation, Investigation, Writing   review & editing. AB: Data curation, Investigation, Writing   review & editing. IC: Investigation, Supervision, Writing   review & editing. MI: Formal analysis, Methodology, Writing   original draft. SN: Conceptualization, Methodology, Supervision, Writing   review & editing. CK: Conceptualization, Methodology, Supervision, Writing   review & editing. FN: Conceptualization, Investigation, Methodology, Writing   original draft. The author(s) declare financial support was received for the research, authorship, and/or publication of this article. This study was supported and funded by the Department of Clinical Medicine, public health, life and environmental sciences, University of L Aquila (FFORIC24.31) and by the Italian Ministry of Health Ricerca Corrente. The funding source had no role in the design of this study and will not have any role during its execution, analyses, interpretation of the data, or decision to submit results. We thank the members of the methodological committee of Cochrane Rehabilitation for the support in writing the protocol. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The author(s) declared that they were an editorial board member of Frontiers, at the time of submission. This had no impact on the peer review process and the final decision. The author(s) declare that no Generative AI was used in the creation of this manuscript. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. The Supplementary Material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/fdgth.2025.1550731/full#supplementary-material 1. Secinaro S, Calandra D, Secinaro A, Muthurangu V, Biancone P. The role of artificial intelligence in healthcare: a structured literature review. BMC Med Inform Decis Mak. (2021) 21:125. doi: 10.1186/s12911-021-01488-9 PubMed Abstract | Crossref Full Text | Google Scholar 2. Law J, Martin E. Concise Colour Medical Dictionary. Oxford: Oxford University Press (2020). Google Scholar 3. Poalelungi DG, Musat CL, Fulga A, Neagu M, Neagu AI, Piraianu AI, et al. Advancing patient care: how artificial intelligence is transforming healthcare. J Pers Med. (2023) 13:1214. doi: 10.3390/jpm13081214 PubMed Abstract | Crossref Full Text | Google Scholar 4. He J, Baxter SL, Xu J, Xu J, Zhou X, Zhang K. The practical implementation of artificial intelligence technologies in medicine. Nat Med. (2019) 25:30 6. doi: 10.1038/s41591-018-0307-0 PubMed Abstract | Crossref Full Text | Google Scholar 5. Lam TYT, Cheung MFK, Munro YL, Lim KM, Shung D, Sung JJY. Randomized controlled trials of artificial intelligence in clinical practice: systematic review. J Med Internet Res. (2022) 24:e3718. doi: 10.2196/37188 Crossref Full Text | Google Scholar 6. Celi LA, Cellini J, Charpignon ML, Dee EC, Dernoncourt F, Eber R, et al. Sources of bias in artificial intelligence that perpetuate healthcare disparities a global review. PLOS Digit Health. (2022) 1:e0000022. doi: 10.1371/journal.pdig.0000022 PubMed Abstract | Crossref Full Text | Google Scholar 7. Mittermaier M, Raza MM, Kvedar JC. Bias in AI-based models for medical applications: challenges and mitigation strategies. NPJ Digit Med. (2023) 6:113. doi: 10.1038/s41746-023-00858-z PubMed Abstract | Crossref Full Text | Google Scholar 8. Ratwani RM, Sutton K, Galarraga JE. Addressing AI algorithmic bias in health care. JAMA. (2024) 332:1051 2. doi: 10.1001/jama.2024.13486 PubMed Abstract | Crossref Full Text | Google Scholar 9. Haug CJ, Drazen JM. Artificial intelligence and machine learning in clinical medicine, 2023. N Engl J Med. (2023) 388:1201 8. doi: 10.1056/NEJMra2302038 PubMed Abstract | Crossref Full Text | Google Scholar 10. Kolbinger FR, Veldhuizen GP, Zhu J, Truhn D, Kather JN. Reporting guidelines in medical artificial intelligence: a systematic review and meta-analysis. Commun Med (Lond). (2024) 4:71. doi: 10.1038/s43856-024-00492-0 PubMed Abstract | Crossref Full Text | Google Scholar 11. Arienti C, Lazzarini SG, Andrenelli E, Cordani C, Negrini F, Pollini E, et al. Rehabilitation and COVID-19: systematic review by cochrane rehabilitation. Eur J Phys Rehabil Med. (2023) 59:800 18. doi: 10.23736/S1973-9087.23.08331-4 PubMed Abstract | Crossref Full Text | Google Scholar 12. Pollock M, Fernandes RM, Becker LA, Pieper D, Hartling L. Chapter V: overviews of reviews. In: Higgins JPT, Thomas J, Chandler J, Cumpston M, Li T, Page MJ, Welch VA, editors. Cochrane Handbook for Systematic Reviews of Interventions Version 6.5. Cochrane (2024). Available online at: www.training.cochrane.org/handbook (accessed August 2023) Google Scholar 13. Cacciamani GE, Chu TN, Sanford DI, Abreu A, Duddalwar V, Oberai A, et al. PRISMA AI reporting guidelines for systematic reviews and meta-analyses on AI in healthcare. Nat Med. (2023) 29:14 5. doi: 10.1038/s41591-022-02139-w PubMed Abstract | Crossref Full Text | Google Scholar 14. Gates M, Gates A, Pieper D, Fernandes RM, Tricco AC, Moher D, et al. Reporting guideline for overviews of reviews of healthcare interventions: development of the PRIOR statement. Br Med J. (2022) 378:e070849. doi: 10.1136/bmj-2022-070849 PubMed Abstract | Crossref Full Text | Google Scholar 15. Morone G, De Angelis L, Cinnera AM, Carbonetti R, Bisirri A, Ciancarelli I, et al. Artificial intelligence in clinical medicine: a state-of-the-art overview of systematic reviews with methodological recommendations for improved reporting. OSF. (2024). https://osf.io/tvaew/ Google Scholar 16. Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan a web and mobile app for systematic reviews. Syst Rev. (2016) 5:210. doi: 10.1186/s13643-016-0384-4 PubMed Abstract | Crossref Full Text | Google Scholar 17. Ibrahim H, Liu X, Rivera SC, Moher D, Chan AW, Sydes MR, et al. Reporting guidelines for clinical trials of artificial intelligence interventions: the SPIRIT-AI and CONSORT-AI guidelines. Trials. (2021) 22:11. doi: 10.1186/s13063-020-04951-6 PubMed Abstract | Crossref Full Text | Google Scholar 18. Cruz Rivera S, Liu X, Chan AW, Denniston AK, Calvert MJ. Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension. Nat Med. (2020) 26:1351 63. doi: 10.1038/s41591-020-1037-7 PubMed Abstract | Crossref Full Text | Google Scholar 19. Kwong JCC, Khondker A, Lajkosz K, McDermott MBA, Frigola XB, McCradden MD. APPRAISE-AI tool for quantitative evaluation of AI studies for clinical decision support. JAMA Netw Open. (2023) 6:e2335377. doi: 10.1001/jamanetworkopen.2023.35377 PubMed Abstract | Crossref Full Text | Google Scholar 20. Collins GS, Moons KGM, Dhiman P, Riley RD, Beam AL, Van Calster B, et al. TRIPOD+AI statement: updated guidance for reporting clinical prediction models that use regression or machine learning methods. Br Med J. (2024) 385:e078378. Google Scholar 21. Ahmed MI, Spooner B, Isherwood J, Lane M, Orrock E, Dennison A. A systematic review of the barriers to implementing artificial intelligence in healthcare. Cureus. (2023) 15:e46454.37927664 PubMed Abstract | Google Scholar 22. Mehrabi N, Morstatter F, Saxena N, Lerman K, Galstyan A. A survey on bias and fairness in machine learning. ACM Comput. Surv. (2021) 54:115. Google Scholar 23. Han R, Acosta JN, Shakeri Z, Ioannidis JPA, Topol EJ, Rajpurkar P. Randomised controlled trials evaluating artificial intelligence in clinical practice: a scoping review. Lancet Digit Health. (2024) 6:e367 73. doi: 10.1016/S2589-7500(24)00047-5 PubMed Abstract | Crossref Full Text | Google Scholar 24. Pattathil N, Zhao JZL, Sam-Oyerinde O, Felfeli T. Adherence of randomised controlled trials using artificial intelligence in ophthalmology to CONSORT-AI guidelines: a systematic review and critical appraisal. BMJ Health Care Inform. (2023) 30:e100757. doi: 10.1136/bmjhci-2023-100757 PubMed Abstract | Crossref Full Text | Google Scholar 25. Sounderajah V, Ashrafian H, Rose S, Shah NH, Ghassemi M, Golub R, et al. A quality assessment tool for artificial intelligence-centered diagnostic test accuracy studies: QUADAS-AI. Nat. Med. (2021) 27:1663 5. doi: 10.1038/s41591-021-01517-0 PubMed Abstract | Crossref Full Text | Google Scholar 26. Collins GS, Dhiman P, Andaur Navarro CL, Ma J, Hooft L, Reitsma JB, et al. Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence. BMJ Open. (2021) 11:e048008.34244270 PubMed Abstract | Google Scholar 27. Lambin P, Leijenaar RTH, Deist TM, Peerlings J, de Jong EEC, van Timmeren J, et al. Radiomics: the bridge between medical imaging and personalized medicine. Nat Rev Clin Oncol. (2017) 14:749 62. doi: 10.1038/nrclinonc.2017.141 PubMed Abstract | Crossref Full Text | Google Scholar 28. Mongan J, Moy L, Kahn CE Jr. Checklist for artificial intelligence in medical imaging (CLAIM): a guide for authors and reviewers. Radiol Artif Intell. (2020) 2:e200029. doi: 10.1148/ryai.2020200029 PubMed Abstract | Crossref Full Text | Google Scholar 29. Sendak M, Sirdeshmukh G, Ochoa T, Premo H, Tang L, Niederhoffer K, et al. Development and validation of ML-DQA a machine learning data quality assurance framework for healthcare. Proc Machine Learn Res. (2022):741 59. Google Scholar 30. Al-Zaiti SS, Alghwiri AA, Hu X, Clermont G, Peace A, Macfarlane P, et al. A clinician s guide to understanding and critically appraising machine learning studies: a checklist for ruling out bias using standard tools in machine learning (ROBUST-ML). Eur Heart J Digit Health. (2022) 3(2):125 40. doi: 10.1093/ehjdh/ztac016 PubMed Abstract | Crossref Full Text | Google Scholar 31. Hernandez-Boussard T, Bozkurt S, Ioannidis JPA, Shah NH. MINIMAR (MINimum information for medical AI reporting): developing reporting standards for artificial intelligence in health care. J Am Med Inform Assoc. (2020) 27(12):2011 5. doi: 10.1093/jamia/ocaa088 PubMed Abstract | Crossref Full Text | Google Scholar 32. d Este S. H., Nielsen M. B., Hansen A. E. Visualizing glioma infiltration by the combination of multimodality imaging and artificial intelligence, a systematic review of the literature. Diagnostics. (2021) 11(4):592. doi: 10.3390/diagnostics11040592 Crossref Full Text | Google Scholar 33. Martinino A, Aloulou M, Chatterjee S, Scarano Pereira JP, Singhal S, Patel T, et al. Artificial intelligence in the diagnosis of hepatocellular carcinoma: a systematic review. J Clin Med. (2022) 11(21):6368. doi: 10.3390/jcm11216368 PubMed Abstract | Crossref Full Text | Google Scholar 34. Sushentsev N, Moreira Da Silva N, Yeung M, Barrett T, Sala E, Roberts M, et al. Comparative performance of fully-automated and semi-automated artificial intelligence methods for the detection of clinically significant prostate cancer on MRI: a systematic review. Insights Imaging. (2022) 13:59. doi: 10.1186/s13244-022-01199-3 PubMed Abstract | Crossref Full Text | Google Scholar 35. Binvignat M, Pedoia V, Butte AJ, Louati K, Klatzmann D, Berenbaum F, et al. Use of machine learning in osteoarthritis research: a systematic literature review. RMD open. (2022) 8(1):e001998. doi: 10.1136/rmdopen-2021-001998 PubMed Abstract | Crossref Full Text | Google Scholar 36. Bian J, Wang X, Hao W, Zhang G, Wang Y. The differential diagnosis value of radiomics-based machine learning in Parkinson s disease: a systematic review and meta-analysis. Front Aging Neurosci. (2023) 15:1199826. doi: 10.3389/fnagi.2023.1199826 PubMed Abstract | Crossref Full Text | Google Scholar 37. Song D, Kim T, Lee Y, Kim J. Image-based artificial intelligence technology for diagnosing middle ear diseases: a systematic review. J Clin Med. (2023) 12(18):5831. doi: 10.3390/jcm12185831 PubMed Abstract | Crossref Full Text | Google Scholar 38. Sharif K, David P, Omar M, Sharif Y, Patt YS, Klang E, et al. Deep learning in coeliac disease: a systematic review on novel diagnostic approaches to disease diagnosis. J Clin Med. (2023) 12(23):7386. doi: 10.3390/jcm12237386 PubMed Abstract | Crossref Full Text | Google Scholar 39. Korneev A, Lipina M, Lychagin A, Timashev P, Kon E, Telyshev D, et al. Systematic review of artificial intelligence tack in preventive orthopaedics: is the land coming soon? Int Orthop. (2023) 47(2):393 403. doi: 10.1007/s00264-022-05628-2 PubMed Abstract | Crossref Full Text | Google Scholar 40. Li D, Mikela Vilmun B, Frederik Carlsen J, Albrecht-Beste E, Ammitzb l Lauridsen C, Bachmann Nielsen M, et al. The performance of deep learning algorithms on automatic pulmonary nodule detection and classification tested on different datasets that are not derived from LIDC-IDRI: a systematic review. Diagnostics. (2019) 9(4):207. doi: 10.3390/diagnostics9040207 PubMed Abstract | Crossref Full Text | Google Scholar 41. Mure anu S, Alm an O, Hede iu M, Dio an L, Dinu C, Jacobs R. Artificial intelligence models for clinical usage in dentistry with a focus on dentomaxillofacial CBCT: a systematic review. Oral Radiol. (2023) 39(1):18 40. doi: 10.1007/s11282-022-00660-9 PubMed Abstract | Crossref Full Text | Google Scholar 42. Schwartz JM, Moy AJ, Rossetti SC, Elhadad N, Cato KD. Clinician involvement in research on machine learning-based predictive clinical decision support for the hospital setting: a scoping review. J Am Med Inform Assoc. (2021) 28(3):653 63. doi: 10.1093/jamia/ocaa296 PubMed Abstract | Crossref Full Text | Google Scholar 43. Lopes S, Rocha G, Guimar es-Pereira L. Artificial intelligence and its clinical application in anesthesiology: a systematic review. J Clin Monit Comput. (2024) 38:247 59. doi: 10.1007/s10877-023-01088-0 PubMed Abstract | Crossref Full Text | Google Scholar 44. Drago  HM, Stan A, Pintican R, Feier D, Lebovici A, Panaitescu P- , et al. MRI radiomics and predictive models in assessing ischemic stroke outcome a systematic review. Diagnostics. (2023) 13(5):857. doi: 10.3390/diagnostics13050857 Crossref Full Text | Google Scholar 45. Kurmis AP. A role for artificial intelligence applications inside and outside of the operating theatre: a review of contemporary use associated with total knee arthroplasty. Arthroplasty. (2023) 5:40. doi: 10.1186/s42836-023-00189-0 PubMed Abstract | Crossref Full Text | Google Scholar 46. Fraccaro P, Arguello Casteleiro M, Ainsworth J, Buchan I. Adoption of clinical decision support in multimorbidity: a systematic review. JMIR Med Inform. (2015) 3:e4. doi: 10.2196/medinform.3503 PubMed Abstract | Crossref Full Text | Google Scholar 47. Heiderich TM, Carlini LP, Buzuti LF, Balda RCX, Barros MCM, Guinsburg R, et al. Face-based automatic pain assessment: challenges and perspectives in neonatal intensive care units. J Pediatr (Rio J). (2023) 99(6):546 60. doi: 10.1016/j.jped.2023.05.005 PubMed Abstract | Crossref Full Text | Google Scholar 48. Sak J, Suchodolska M. Artificial intelligence in nutrients science research: a review. Nutrients. (2021) 13(2):322. doi: 10.3390/nu13020322 PubMed Abstract | Crossref Full Text | Google Scholar 49. Sutton RT, Pincock D, Baumgart DC, Sadowski DC, Fedorak RN, Kroeker KI. An overview of clinical decision support systems: benefits, risks, and strategies for success. NPJ Digit Med. (2020) 3:17. doi: 10.1038/s41746-020-0221-y PubMed Abstract | Crossref Full Text | Google Scholar 50. Fiske A, Henningsen P, Buyx A. Your robot therapist will see you now: ethical implications of embodied artificial intelligence in psychiatry, psychology, and psychotherapy. J Med Internet Res. (2019) 21:e13216. doi: 10.2196/13216 PubMed Abstract | Crossref Full Text | Google Scholar 51. Bhattad PB, Pacifico L. Empowering patients: promoting patient education and health literacy. Cureus. (2022) 14:e27336. doi: 10.7759/cureus.27336 PubMed Abstract | Crossref Full Text | Google Scholar 52. Plana D, Shung DL, Grimshaw AA, Saraf A, Sung JJY, Kann BH. Randomized clinical trials of machine learning interventions in health care: a systematic review. JAMA Netw. Open. (2022) 5:e2233946. doi: 10.1001/jamanetworkopen.2022.33946 PubMed Abstract | Crossref Full Text | Google Scholar 53. Sallam M. ChatGPT utility in healthcare education, research, and practice: systematic review on the promising perspectives and valid concerns. Healthcare (Basel). (2023) 11:887. doi: 10.3390/healthcare11060887 PubMed Abstract | Crossref Full Text | Google Scholar 54. Yan W, Qin C, Tao L, Guo X, Liu Q, Du M, et al. Association between inequalities in human resources for health and all-cause and cause-specific mortality in 172 countries and territories, 1990 2019: observational study. Br Med J. (2023) 381:e073043. Google Scholar 55. Boniol M, Kunjumen T, Nair TS, Siyam A, Campbell J, Diallo K. The global health workforce stock and distribution in 2020 and 2030: a threat to equity and  universal  health coverage? BMJ Glob Health. (2022) 7:e009316. doi: 10.1136/bmjgh-2022-009316 PubMed Abstract | Crossref Full Text | Google Scholar 56. Crossnohere NL, Elsaid M, Paskett J, Bose-Brill S, Bridges JFP. Guidelines for artificial intelligence in medicine: literature review and content analysis of frameworks. J Med Internet Res. (2022) 24(8):e36823. doi: 10.2196/36823 PubMed Abstract | Crossref Full Text | Google Scholar 57. Ong JCL, Seng BJJ, Law JZF, Low LL, Kwa ALH, Giacomini KM, et al. Artificial intelligence, ChatGPT, and other large language models for social determinants of health: current state and future directions. Cell Rep Med. (2024) 5(1):101356. doi: 10.1016/j.xcrm.2023.101356 PubMed Abstract | Crossref Full Text | Google Scholar Keywords: artificial intelligence, machine learning, natural language processing, clinical medicine, rehabilitation, neural networks, information science, digital health Citation: Morone G, De Angelis L, Martino Cinnera A, Carbonetti R, Bisirri A, Ciancarelli I, Iosa M, Negrini S, Kiekens C and Negrini F (2025) Artificial intelligence in clinical medicine: a state-of-the-art overview of systematic reviews with methodological recommendations for improved reporting. Front. Digit. Health 7:1550731. doi: 10.3389/fdgth.2025.1550731 Received: 23 December 2024; Accepted: 12 February 2025;Published: 5 March 2025. Edited by: Reviewed by: Copyright:   2025 Morone, De Angelis, Martino Cinnera, Carbonetti, Bisirri, Ciancarelli, Iosa, Negrini, Kiekens and Negrini. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Alex Martino Cinnera, a.martino@hsantalucia.it Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on Supplementary Material Data Sheet 1.xlsx",2
AI in medical education - American Medical Association,https://news.google.com/rss/articles/CBMihAFBVV95cUxOakxfNmV3QzRWckV6ak02VjgtMGh6czVhYzVCM0dQWUJMdjZFNUVSNXVwQUVzR0QteWhVdWZWM2FHeGJiMVk3SEEwdjN5VFc2TDU3clR6TU1rN0taUFVSeF9mOUR0aVhkNW9tRzNTRVhNcmFFUVdKMTNEVzAzRmdnM0VCSWI?oc=5&hl=en-US&gl=US&ceid=US:en,"Caffeine can be part of a healthy diet for most people, but too much may pose a danger to your health. Four physicians share what to keep in mind. The AMA Update covers a range of health care topics affecting the lives of physicians and patients. Learn more about cervical cancer and at-home screening tests. Strong physician representation is needed to ensure that health AI is transparent and has the right oversight so it works for patients and doctors. Get real answers from the AMA to common myths about documenting the time spent on each specific task associated with an outpatient visit. International medical graduates (IMGs) play a critical role in U.S. health care. Learn how the AMA works to help IMGs meet the nation s health needs. Residents share the responsibility to create an effective and respectful learning environment. The AMA has advice on how to make that happen. When writing your personal statement, veteran residency program directors said that authenticity will trump AI every time. ChatGPT agrees. Medical student research experience among residency applicants is common, but not always decisive. Learn about the physician specialties where it matters most. Most physicians practice where they completed residency, but not all. Learn which specialties and states are most likely to keep you local. After gaining footing as interns, second-year residents take on a new role and increased responsibility. These tips help new PGY-2s excel. Proposed 2026 Medicare physician payment rule would redistribute pay across specialties and practice types and more in the latest Medicare Payment Reform Advocacy Update. AMA expresses strong concern over proposed rulemaking on Medicaid provider tax reforms and more in the latest National Advocacy Update. This two-day boot camp, Sept. 17-18, 2025, will equip attendees with the time-saving tools and strategies to reform their organizations and enhance professional satisfaction. ChangeMedEd  is a national conference that brings together leaders and innovators to accelerate change in medical education across the continuum. Learn more. Review the agenda and schedule of events for the 2025 HOD Interim Meeting at the Gaylord National Resort and Convention Center in National Harbor, Maryland. Read the House of Delegates (HOD) speakers' updates for the 2025 HOD Interim Meeting. Download PDFs of reports organized by year for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA Interim and Annual Meetings. Download PDFs of reports on this topic for the Council on Ethical & Judicial Affairs (CEJA) presented during the AMA interim and annual meetings. Read current and past issues of WPS Members & News Highlights for the latest information on the work being done by WPS members and its leadership. An examination of ""kinkeeping"" and gender equity, and an introduction to the 2025-26 goals, the ""5 As:"" Action, Activism, Advancement, Advocacy and Achievement. In the news: FDA panel discusses risk of SSRIs during pregnancy, how the pandemic aged our brains, eating eggs may protect against Alzheimer s and more. AMA participates in health care conferences and events held throughout the U.S.A. as well as internationally. AI (artificial and augmented intelligence) is playing an increasingly important role at all stages of the medical education continuum, both as a tool for educators and learners and as a subject of study in and of itself. AI has the potential to transform the educational experience as a part of precision education and transform patient care as a part of precision health. Learn more about how AI can impact medical education. The AMA ChangeMedEd  Artificial Intelligence in Health Care Series is a seven-part online module series developed by the AMA ChangeMedEd  Initiative and the University of Michigan DATA-MD team. It introduces learners to critical AI and machine learning concepts in health care and is available at no cost on the AMA EdHub . Topics addressed in the series include navigating ethical and legal considerations of AI in health care, practical applications of AI in the health system and the use of AI in diagnosis. Targeted toward medical students, the introductory content is also suitable for residents, fellows, practicing physicians and other health professionals. It is also beneficial to medical educators looking to enhance their knowledge in this critical area. The Medical Educator s Guide to Projects Leveraging Artificial Intelligence and Learning Analytics (PDF, free download) outlines the core issues to consider when designing technological solutions to educational challenges. Medical educators are enthused about the potential to leverage artificial intelligence, learning analytics, and other emerging technologies to improve medical training and promote precision education across the continuum of medical school, residency, and practice. However, many educators lack experience in developing and deploying such technologies. This guide provides an overview of key points for discussion with potential technical partners, facilitating essential details in planning and implementation that give projects the greatest chance at success. Kim Lomis, MD, AMA vice president of Medical Education Innovations, and Margaret Lozovatsky, MD, AMA vice president of Digital Health Innovations discuss the future of AI in medicine. This AMA Future of Health report and AMA STEPS Forward  Innovation Academy webinar explores the applications of AI in health care. Join experts discussing the practical uses of AI in medical practice, including addressing the risks and exploring its transformational potential. Gain a better understanding of how physicians are navigating this new technology and leading the way in how AI is designed and implemented in health care today. Advancing AI in medical education through ethics, evidence and equity is a companion to the AMA policy for augmented intelligence (PDF) and leverages the same framework when considering the educational applications of AI. This document addresses the use of AI to facilitate the process of training health professionals but does not consider issues around the important goal of providing training in AI for health care professionals to facilitate their duties in providing care. The AMA promotes the art and science of medicine and the betterment of public health. The best in medicine, delivered to your mailbox We use third party technologies for analytics, personalization and marketing purposes. These technologies may be used by us or our partners to personalize the site or deliver relevant marketing to you on third party sites. These cookies are necessary for the website to function and cannot be switched off in our systems. They are usually only set in response to actions made by you which amount to a request for services, such as setting your privacy preferences, logging in or filling in forms. You can set your browser to block or alert you about these cookies, but some parts of the site will not then work.",0
Transforming diagnosis through artificial intelligence - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTFBPVl93aHl0VHBHTUZ0bHZBNUU1Q2JxZHI5bmtuTzBrNkpKTnM3SjF0d1NfWnE1eUpNNWdReEM3Mnc1dEViZXZmR3NUR2hreERzbWx1NXJKYjlPSDBTeTVz?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement npj Digital Medicine volume 8, Article number: 54 (2025) Cite this article 15k Accesses 1 Citations 12 Altmetric Metrics details Artificial intelligence (AI) is increasingly permeating the fabric of medicine, but getting full benefits will likely require fundamental changes in practice. Accepting this will be challenging for many clinicians. However, it may be necessary to ensure that AI s ambitious promises translate into real-life improvement. The diagnostic process is a particularly important target for clinical decision support in general, and for AI. Diagnoses are often incorrect but finding the correct diagnosis and doing so quickly are pivotal for delivering effective care. AI-driven algorithms are increasingly used in healthcare settings to support clinicians with diagnosis, treatment, and patient outcome prediction. Drawing on powerful techniques such as machine learning (ML) and deep learning (DL), these algorithms are designed to gain insights from clinical data to assist rapid, accurate diagnostic problem-solving and treatment decision-making. Despite the rapid diffusion of these potentially highly transformative technologies, however, we know little to date about how they may affect real-world diagnostic processes and their outcomes - a gap known as the  AI chasm 1. One area in which augmented diagnostic models are proving particularly useful is hyperacute stroke. This is a high-stakes context where the cost of errors can be clinically and reputationally very high, as well as holding fundamental moral and ethical implications for patients, clinicians and healthcare organizations. Many off-the-shelf stroke AI applications are being introduced into clinical research and practice which aim to generate improvements to the stroke diagnostic and treatment referral workflows. AI appears to be radically changing this process by providing a number of new capabilities including communication features, which can instantaneously distribute MRI/CT images to the stroke team s mobile phones, and prediction features, such as the indication of whether there is a suspected large vessel occlusion (the principal stroke marker) and of how much of the brain affected by the stroke may be salvageable through surgical intervention (i.e., mechanical thrombectomy)2. The implications for clinical work, however, are not yet fully understood. We are reporting the emergent findings from a five-year, in-depth, qualitative study investigating the adoption of leading-edge AI applications for stroke care at 3 major UK stroke hubs. Our observations focused on the  door-to-treatment  portion of the stroke pathway, starting with the patient scan, and ending with a treatment decision. Our evidence suggests that AI may trigger radical changes to the stroke diagnosis and treatment referral processes as conducted in practice by clinicians. Traditionally, the diagnostic process has begun with a physician examining the patient, gathering data, and constructing an intuitive assessment of their condition. Initial judgment is progressively refined through an iterative process of information gathering, information integration and interpretation. This refinement process, involving hypothesis generation, fine-tuning, and validation, usually culminates with a diagnostic decision, that attributes a classification label to inform treatment (e.g., thrombolysis or thrombectomy). As experts have noted, however, AI is not focused on supporting the diagnostic journey as it stands3. Predictive AI tools are generally designed to produce an answer, in the form of a diagnostic label, answering the binary question of whether a patient has a certain diagnosis. This approach has been challenged as  any tools that predicts your destination at the start of your journey isn t very helpful if it tells you nothing about how to get there 4. Nevertheless, clinicians do not seem discouraged. Our findings show that expert clinicians are adapting to AI by transforming, rather than replacing, the traditional diagnostic process. Instead of simply accepting AI s diagnostic output or  label , they are developing a new approach that begins with AI s suggestion and works backwards by assessing its validity against multiple verification steps. This includes cross-referencing patient records, validating against established medical standards, and consulting other experts  opinions. Through this evolution, clinicians are not only preserving their essential role but enhancing the diagnostic process by integrating AI s capabilities while maintaining clinical rigor and oversight. In the advanced stroke AI adoption settings we observed, the diagnostic journey starts with AI producing a recommendation (diagnostic label), based on processing MRI/CT imaging, which is seen simultaneously by the entire stroke team. This is different, both because of the speed with which this happens, and because the recommendations are broadly distributed. The AI  diagnosis , at least in the narrow sense of an algorithmic output, tends to be available in the initial phases of the diagnostic process, rather than just at the end (Fig. 1). A simplified, information-focused illustration of the complex, dynamic, longitudinal process clinicians embark on in formulating a diagnosis. It depicts how the diagnostic label, currently representing the endpoint of the diagnostic process becomes instead the starting point of the new, AI-mediated process. Our evidence shows that the automated AI diagnosis is produced, distributed, and read ahead of the clinician s diagnosis. In the case of stroke, AI might highlight the presence of a vessel occlusion, as well as suggesting the percentage of brain that might be affected by the stroke or identify bleeding. This initial (as yet unverified) claim is delivered by the software through a set of colored maps and 3D representations (including AI CT perfusion and AI CT angiography). From that point onwards, the clinical team s task becomes to verify the validity of the initial AI  judgment  against their own clinical findings, as well as against conventional imaging tools (e.g., CT brain, CT angiography). The early availability of the AI diagnosis in turn results in the triggering of a specific treatment pathway (e.g., the thrombectomy pathway in the case of a large vessel occlusion in a main cerebral artery, leading to interventional neuroradiology referral). The neuroradiologist/interventionist is usually alerted in advance of AI s prediction being fully verified against further evidence, and their role becomes to establish whether to recommend accepting or rejecting it. This scenario appears to suggest that the dial spanning the spectrum between human and machine agency may be subtly but firmly shifting one further notch towards the machine. The introduction of AI is likely to fundamentally impact stroke care processes with consequences for clinicians, patients and the healthcare system that warrant urgent further investigation5. Analyses should be done on the implications of AI-induced changes on the diagnostic process and its outcomes along several dimensions. First, a crucial question is whether and how the introduction of stroke AI and subsequent workflow changes increase diagnostic accuracy. For instance, will AI-augmented clinicians be better equipped to accurately detect large vessel occlusions compared to clinicians operating independently? Trial evidence shows that AI systems perform well compared to clinicians, but these initial findings have not yet been fully verified in real-world conditions6,7. Will this heightened accuracy, if validated, result in more patients receiving treatment than would have otherwise? Similar concerns relate to diagnostic speed. Will AI enable clinicians to reach a diagnosis faster than they could without it? While initial studies suggest this might be the case8, our evidence shows that potential speed gains must be verified against the need for additional verification practices that could paradoxically slow the diagnostic process. Second, research is urgently needed to understand how to make AI safe and ethical in the context of real-world clinical practices9. This includes understanding how clinicians may learn to deal in practice with AI limitations. While AI could reduce subjectivity and bias in problem-solving and decision-making relative to clinicians, it might also add new sources of bias10. How will clinicians and healthcare organizations be able to manage potential AI errors in the presence of algorithmic opacity? 11,12 How will they address issues of privacy, such as the secure transfer between imaging centers and AI? And how will patient consent practices have to change to reflect the complexity of explaining AI s role in decision-making? Our findings suggest that clinicians are developing new, dedicated practices and methods to address AI-generated risks and uncertainties. Third, appreciating AI s impact on expertise is fundamental. Will inexperienced or overworked clinicians be able to critically assess and, when needed, reject AI s opinion or will they be succumbing to  automation bias ?13 How will AI alter the tasks and roles of clinicians involved in stroke diagnosis and referral? For example, might stroke physicians develop imaging-based diagnostic expertise akin to radiologists and interventionists? Could this precipitate jurisdictional conflicts between stroke physicians and radiologists or interventionists? Another dimension of evolving expertise is the potential emergence of new roles focused on monitoring AI s clinical application over time. Will these new experts become embroiled in negotiations surrounding competing claims to control and authority over AI outputs, including their reliability and interpretation? Fourth, the implication of AI-supported diagnosis and referral for patients requires careful consideration. Will swift, easily sharable access to patient information/images via the centralized AI app take precedence over real-life patient presentation in informing diagnosis and referral? This already occurs in some cases like when patients get scanned at stroke spoke hospitals and then transferred to hubs, or after-hours when diagnosis happens via clinicians remotely accessing a screen. However, AI could normalise this scenario making it more prominent in the diagnostic process than an exception (Fig. 2a, b). a Simplified clinical workflow (stroke hub) with AI. b Simplified clinical workflow (stroke hub) without AI. The extent to which similar changes may occur outside hyperacute stroke deserves investigation. Initial evidence from automated urgent lung cancer triage suggests AI may similarly act as first reader for screening images requiring immediate follow-up, while clinicians analyze remainder cases for potential false negatives. This pattern could extend to other time-sensitive domains with  can t-miss diagnoses  like surgery (sepsis), pathology (acute leukemia), cardiology (acute myocardial infarction), and emergency care (pneumothorax). AI as a first reader could also add value in non-acute settings by managing high volumes of routine screenings and backlogs, maintaining consistent accuracy during repetitive tasks and long shifts, and optimizing specialist resources and geographic access. These applications warrant further study. In terms of generalisation, it is finally worth considering the global differences in AI implementation due to the variations in individual countries  underlying healthcare system structures. An interesting comparison here is between the US and the UK. It is possible, for example, that, while the highly standardized UK system may better coordinate the fundamental shift to AI-first diagnostics through concerted verification practices, it could benefit from US-style pragmatic approach to rapid innovation and process improvement. Conversely, the US could learn from the UK s systematic approach to standardization, risk management, and equitable implementation. New technologies are hard to grasp, and their effects even harder to envisage examples include the internet, smart phone, computer-aided decision systems and, in a medical/healthcare context, the x-ray, the MRI scanner14,15. AI is likely to represent a transformational change of similar magnitude. Despite the strong uncertainty around AI and its effects, one thing is becoming increasingly clear: realizing its benefits fully will require fundamental changes in how we practice. Topol, E. J. High-performance medicine: the convergence of human and artificial intelligence. Nat. Med. 25, 44 56 (2019). Article CAS PubMed Google Scholar Shafaat, O. et al. Leveraging artificial intelligence in ischemic stroke imaging. J. Neuroradiol. 49, 343 351 (2022). Article PubMed Google Scholar Adler-Milstein, J., Chen, J. H. & Dhaliwal, G. Next-Generation Artificial Intelligence for Diagnosis: From Predicting Diagnostic Labels to ""Wayfinding"". JAMA 326, 2469 2468 (2021). Article Google Scholar https://med.stanford.edu/content/dam/sm/healthcare-ai/documents/Unlocking-New-Opportunities-for-AI-enabled-Diagnosis-2-.pdf Recht, M. & Bryan, R. N. Artificial Intelligence: Threat or Boon to Radiologists? J. Am. Coll. Radiol. 14, 1476 1480 (2017). Article PubMed Google Scholar Soun, J. E. et al. Impact of an automated large vessel occlusion detection tool on clinical workflow and patient outcomes. Front. Neurol. 14, 1179250 (2023). Article PubMed PubMed Central Google Scholar Chandrabhatla, A. S. et al. Artificial intelligence and machine learning in the diagnosis and management of stroke: a narrative review of United States food and drug administration-approved technologies. J. Clin. Med. 12, 3755 (2023). Article PubMed PubMed Central Google Scholar Martinez-Gutierrez, J. C. et al. Automated large vessel occlusion detection software and thrombectomy treatment times: a cluster randomized clinical trial. JAMA Neurol. 80, 1182 1190 (2023). Article PubMed PubMed Central Google Scholar Bates, D. W. et al. The potential of artificial intelligence to improve patient safety. NPJ Digit. Med. 4, 54 (2023). Article Google Scholar Weidener, L. & Fischer, M. Role of ethics in developing AI-based applications in medicine: insights from expert interviews and discussion of implications. JMIR AI 3, e51204 (2024). Article PubMed PubMed Central Google Scholar Challen, R. et al. Artificial intelligence, bias and clinical safety. BMJ Qual. Saf. 28, 231 237 (2019). Article PubMed PubMed Central Google Scholar Reddy, S., Allan, S., Coghlan, S. & Cooper, P. A governance model for the application of AI in health care. J. Am. Med. Inform. Assoc 27, 491 497 (2020). Article PubMed Google Scholar Khera, R., Simon, M. A. & Ross, J. S. Automation bias and assistive AI: risk of harm from AI-driven clinical decision support. JAMA 330, 2255 2257 (2023). Article PubMed Google Scholar Lea, A. S. Digitizing Diagnosis: Medicine, Minds, and Machines in Twentieth-century America (Johns Hopkins University Press, 2023). Bates, D. W. An Artificial History of Natural Intelligence: Thinking with Machines from Descartes to the Digital Age (University of Chicago Press, 2024). Download references Dr. D Adderio gratefully acknowledges the Chief Scientist Office grant no. HIPS/22/15  The Impact Of Artificial Intelligence On Hyperacute Stroke Diagnostic And Treatment Pathways  (PI: D Adderio) and the Wellcome Leap SAVE Grant no. 133448282,  Global Surgery Health Technology Evaluation And Validation Consortium  (PI: Harrison). Dr D Adderio also acknowledges her Chancellor s Fellowship funding (PI: D Adderio). Centre for Medical Informatics, Usher Institute, The University of Edinburgh Medical School, Edinburgh, Scotland, UK Luciana D Adderio The Alan Turing Institute, London, UK Luciana D Adderio Clinical and Quality Analysis, Mass General Brigham, Somerville, MA, USA David W. Bates Division of General Internal Medicine, Brigham and Women s Hospital, Boston, MA, USA David W. Bates Harvard Medical School, Boston, MA, USA David W. Bates Harvard T.H. Chan School of Public Health, Boston, MA, USA David W. Bates Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar L.D.A. conceived of the idea for the manuscript. L.D.A. wrote the paper. D.W.B. provided feedback and revisions. Both authors have read and approved the manuscript. Correspondence to Luciana D Adderio. Dr D Adderio declares no competing interests. Dr Bates reports grants and personal fees from EarlySense, personal fees from CDI Negev, equity from ValeraHealth, equity from Clew, equity from MDClone, personal fees and equity from AESOP, personal fees and equity from FeelBetter, personal fees and equity from Guided Clinical Solutions and grants from IBM Watson Health, outside the submitted work. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions D Adderio, L., Bates, D.W. Transforming diagnosis through artificial intelligence. npj Digit. Med. 8, 54 (2025). https://doi.org/10.1038/s41746-025-01460-1 Download citation Received: 16 July 2024 Accepted: 15 January 2025 Published: 24 January 2025 DOI: https://doi.org/10.1038/s41746-025-01460-1 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Journal of Translational Medicine (2025) Advertisement npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing: Translational Research newsletter   top stories in biotechnology, drug discovery and pharma.",2
Evolution of artificial intelligence in healthcare: a 30-year bibliometric study - Frontiers,https://news.google.com/rss/articles/CBMijgFBVV95cUxORFVic2Rlb0hWbVlQTUtGcVZhdTEyM1FkODM5cG0xb0lXeUxDY0NOWW91ekQ3WGtXZzltLUpaY1RJZnJoaGxlWVZhb0ZUN3Q1VXFZVVF4VXFETmVVaC1hM0tvRzQ5MWhIU2ZycG5EYndLWUVuRUFHNVFWYlRoVjdTaURNQWxiOWpKaFNoT2p3?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better SYSTEMATIC REVIEW article Front. Med., 15 January 2025 Sec. Family Medicine and Primary Care Volume 11 - 2024 | https://doi.org/10.3389/fmed.2024.1505692 This article is part of the Research TopicArtificial Intelligence and Big Data for Value-Based Care - Volume IIIView all 8 articles Introduction: In recent years, the development of artificial intelligence (AI) technologies, including machine learning, deep learning, and large language models, has significantly supported clinical work. Concurrently, the integration of artificial intelligence with the medical field has garnered increasing attention from medical experts. This study undertakes a dynamic and longitudinal bibliometric analysis of AI publications within the healthcare sector over the past three decades to investigate the current status and trends of the fusion between medicine and artificial intelligence. Methods: Following a search on the Web of Science, researchers retrieved all reviews and original articles concerning artificial intelligence in healthcare published between January 1993 and December 2023. The analysis employed Bibliometrix, Biblioshiny, and Microsoft Excel, incorporating the bibliometrix R package for data mining and analysis, and visualized the observed trends in bibliometrics. Results: A total of 22,950 documents were collected in this study. From 1993 to 2023, there was a discernible upward trajectory in scientific output within bibliometrics. The United States and China emerged as primary contributors to medical artificial intelligence research, with Harvard University leading in publication volume among institutions. Notably, the rapid expansion of emerging topics such as COVID-19 and new drug discovery in recent years is noteworthy. Furthermore, the top five most cited papers in 2023 were all pertinent to the theme of ChatGPT. Conclusion: This study reveals a sustained explosive growth trend in AI technologies within the healthcare sector in recent years, with increasingly profound applications in medicine. Additionally, medical artificial intelligence research is dynamically evolving with the advent of new technologies. Moving forward, concerted efforts to bolster international collaboration and enhance comprehension and utilization of AI technologies are imperative for fostering novel innovations in healthcare. With the continuous advancement of science and technology, particularly in the fields of computer science, data processing, and machine learning, the application of artificial intelligence (AI) technology in healthcare has become increasingly widespread (1 3). Current research has demonstrated that the integration of healthcare and AI enhances patient care, improves efficiency, and reduces costs in the healthcare industry, leading to smarter, faster, and more efficient healthcare systems (4 7). This optimization spans from diagnosis to treatment planning, driving advancements in disease prediction, diagnosis, and therapeutic interventions, thereby providing substantial benefits for both patients and healthcare providers (8 10). Artificial intelligence has extensive applications in medicine, including risk assessment, triage, diagnosis, follow-up management, drug development, and therapies (11, 12). In risk assessment, AI algorithms are utilized to analyze patient data and identify individuals at high risk for developing specific conditions, allowing for early intervention and prevention strategies (13). Primary care, as the frontline of healthcare, plays a vital role in prevention, chronic disease management, and personalized guidance. AI-powered tools in primary care can assist in early screening for conditions such as diabetes, hypertension, and mental health disorders, enabling timely management and reducing the progression of these diseases (14). For chronic disease management, AI systems can provide personalized recommendations based on real-time patient data, helping healthcare providers create tailored lifestyle and treatment plans (15). ChatGPT is a sophisticated language model designed using deep learning algorithms to produce responses that closely resemble human conversation. As part of the Generative Pre-trained Transformer (GPT) series developed by OpenAI, it stands out as one of the largest and most accessible language models currently available (16). Utilizing a vast repository of textual data, ChatGPT excels in capturing the intricacies and subtleties of human language, allowing it to deliver highly relevant and context-aware responses across a diverse array of prompts. More recently, its functionality has expanded beyond text generation to include the creation of images and videos, significantly enhancing its multimodal capabilities and widening its applicability in areas such as healthcare, education, and creative industries (17). With their rapid development, these models are poised to take on a growing role in medical research, from facilitating systematic reviews to advancing personalized treatment approaches. As they continue to evolve, large language models have the potential to drive medical innovation forward, improving patient outcomes and enabling data-driven, precise decision-making in various areas of healthcare (18 20). To better understand the evolution and impact of AI in healthcare, bibliometric studies are essential for systematically mapping the research landscape and identifying emerging trends. This study aims to meticulously retrieve relevant literature from the Web of Science Core Collection database (WoSCC) from January 1, 1993, to December 31, 2023. Through quantitative and visual network analyses encompassing various parameters such as authors, institutions, countries/regions, and keywords, this analysis is expected to assist researchers in gaining comprehensive insights into AI-related research in healthcare and predicting future patterns and trends. The selection of the Web of Science Core Collection (WoSCC) was based on its ability to provide comprehensive data meeting the needs of bibliometric software, and its reputation as a prominent database in this research domain. Consequently, the bibliometric analysis yielded invaluable insights into the prevailing status of AI research in healthcare up to December 2023. Since the data are derived from publicly available databases, ethical approval for this study was considered unnecessary. To avoid bias, we decided not to include such specific algorithmic terms in the search. This study systematically identified relevant research by focusing on two primary categories: (1) AI technologies and (2) healthcare and medicine. The selection of search keywords was informed by an extensive preliminary literature review and consultations with researchers and domain experts. Keywords for AI technologies were chosen to represent a wide range of approaches and advancements in the field, while healthcare-related keywords ensured relevance to medical and healthcare domains. The finalized search formula was as follows: [TI = (""artificial intelligence"") OR TI = (""data learning"") OR TI = (""machine learning"") OR TI = (""expert systems"") OR TI = (""fuzzy logic"") OR TI = (""computer vision"") OR TI = (""automatic programming"") OR TI = (""speech understanding"") OR TI = (""autonomous robots"") OR TI = (""intelligent tutoring"") OR TI = (""intelligent agents"") OR TI = (""neural network"") OR TI = (""voice recognition"") OR TI = (""text mining"") OR TI = (""electronic health record"") OR TI = (""ChatGPT"") OR TI = (""large language models"")] AND [TS = (health) OR TS = (healthcare) OR TS = (medicine) OR TS = (mental health) OR TS = (behavioral health)]. To ensure high-quality and peer-reviewed content, the study included only publications classified as ""article"" or ""review article."" This excluded other document types such as conference abstracts, letters, expert opinions, editorial materials, corrections, retractions, and conference papers, which were considered less representative of the core focus of the study. The study assessed publications within the period from January 1, 1993, to December 31, 2023. This 30-year timeframe was selected to comprehensively capture the evolution of AI technologies and their applications in healthcare and medicine. The analysis was restricted to English-language publications to ensure consistency in the evaluation and accessibility of results. By adopting these eligibility criteria, this study aimed to identify and analyze literature trends, emerging technologies, and interdisciplinary applications of AI in healthcare, while maintaining a focus on reliability and academic rigor. Bibliometrix, Biblioshiny, and Microsoft Excel are used for data analysis and visualization of AI in health care article studies. Data were presented via descriptive statistics. Inferential bibliometric analyses included clustering of the selected parameters of keywords, keyword plus, titles, and abstracts; Bradford s law to evaluate core journals, and the Sankey diagrams to evaluate the flows between research themes over time. They can visualize the research results and has unique advantages in clustering technology and map display. A total of 22,950 documents were collected from 5,024 sources, with these sources specifically referring to academic journals. The dataset revealed an average annual growth rate of 26.97%, alongside an average document age of 3.41 years and an average of 17 citations per document. From 1993 to 2023, a notable upward trend has been observed in the annual scientific output within the field of bibliometrics. In the initial years, such as 1993 and 1994, the volume of scientific publications remained comparatively modest, with only 5 and 11 articles, respectively. Subsequent to 2010, this growth trajectory accelerated further, characterized by a marked escalation in the annual growth rate. Notably, the period spanning from 2019 to 2023 witnessed a particularly remarkable expansion in scientific productivity, yielding 1480, 2677, 4029, 5320, and 6450 articles respectively (Figure 1). Figure 1. Annual publication volume of artificial intelligence in healthcare. The scientific production of various countries offers valuable insights into their respective contributions to research. The United States leads with an impressive count of 28,663 articles, highlighting its significant influence and productivity within the scientific community. Following closely, China presents 12,740 articles, indicating its rapid growth and importance in scientific research. India ranks third, contributing 4,926 articles and establishing itself as a significant player in scientific inquiry. The United Kingdom maintains a robust presence with 4,821 articles, showcasing its active involvement and contributions to scientific endeavors. Canada follows suit with 3,567 articles, making substantial contributions to the global scientific landscape (Figure 2). Figure 2. Country-specific production. The color intensity is proportional to the number of documents. Higher blue intensity refers to greater number of documents (dark blue = high productivity; gray = no documents). The article counts and collaboration patterns based on the authors  countries reveal interesting trends. The United States leads in article count, having published a total of 6,409 articles, comprising 5,310 single corresponding author papers (SCP) and 1,099 multiple corresponding author papers (MCP). Despite its high MCP count, the US exhibits a comparatively lower MCP-to-total articles ratio of 0.171 compared to other countries. China closely follows with 3,774 articles, consisting of 2,882 SCPs and 892 MCPs, demonstrating a higher ratio of MCPs, indicating extensive collaborative research efforts. India contributes 1601 articles, with a higher proportion of SCPs than MCPs, aligning with its lower MCP frequency of 0.070. It is worth noting that contributions from the United Kingdom, South Korea, Canada, Australia, Italy, Germany, and Spain are also notable, with each country demonstrating a relatively high MCP ratio (Figure 3). These findings underscore the global distribution of research output in medical AI, highlighting diverse collaborative patterns among researchers from different countries. Figure 3. Corresponding author s countries. Countries of the top 20 most relevant corresponding authors  articles of artificial intelligence in health care in the Web of Science Core Collection. SCP, Single Country Publications; MCP, Multiple Country Publications. The top 20 research institutions in the field of medical AI, as indicated by publication volume, exhibit a concentration of academic powerhouses renowned for their research contributions. Leading the pack is Harvard University, with an impressive 1,690 articles, followed closely by the University of California System with 1,180 articles. Harvard Medical School, a prestigious institution affiliated with Harvard University, demonstrates a strong presence with 802 articles. The University of Toronto and the University of Pennsylvania also showcase significant engagement in this field, with 809 and 635 articles respectively. Other notable institutions include the University of Michigan, University of London, Stanford University, University System of Ohio, and the University of California San Francisco, each contributing substantially to the body of literature in medical AI. In the research publications, PLOS ONE leads with a publication count of 370 articles, possibly owing to its open access policy and broad research scope. Following closely is IEEE ACCESS, with 360 articles, showcasing the diversity and breadth of the field. SCIENTIFIC REPORTS ranks third with 358 articles, highlighting the intersection of medical AI. The JOURNAL OF MEDICAL INTERNET RESEARCH, with 347 articles, likely covers topics related to sensor technology and its applications in medical AI. Additionally, the JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION and SENSORS hold significant influence in the medical informatics field with 347 and 338 articles, respectively. The publication volume of these journals reflects the continued growth and interdisciplinary nature of the medical artificial intelligence field. Other journals such as APPLIED SCIENCES-BASEL, JMIR MEDICAL INFORMATICS, and BMC MEDICAL INFORMATICS AND DECISION MAKING also make substantial contributions, demonstrating the diversity and activity within the field of medical artificial intelligence. In 2023, bibliometric analysis of medical artificial intelligence research unveiled several highly cited global publications. Leading the list is the review article titled ""ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns,"" published in the journal ""Healthcare,"" which garnered 332 citations (21). Following closely is the Original Investigation article ""Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum,"" published in JAMA Internal Medicine, with 238 citations (22). Additionally, a review from Cureus titled ""Artificial Hallucinations in ChatGPT: Implications in Scientific Writing"" ranks as the third most cited (23), while an article from the Journal of Medical Systems titled ""Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios"" holds the fourth position (24). Notably, the top five most cited publications all center around topics related to ChatGPT. Table 1 presents an overview of the top ten most globally cited reviews and articles within the field of medical artificial intelligence. The most globally cited review, ""Machine learning: Trends, perspectives, and prospects,"" authored by M. I. Jordan from the University of California and T. M. Mitchell from Carnegie Mellon University, was published in the Science journal on July 17, 2015 (25). The top three most globally cited articles in the field of medical artificial intelligence literature are as follows: Guo L s article titled ""A multi-time scale approach to remaining useful life prediction in rolling bearing,"" published in Neurocomputing in 2017, investigates a multi-time scale modeling approach for predicting the remaining useful life (RUL) of rolling bearings. This study contributes to the advancement of predictive maintenance techniques in industrial applications (26). The second article, authored by A. Bate and titled ""A Bayesian neural network method for adverse drug reaction signal generation,"" was published in the European Journal of Clinical Pharmacology in 1998 and has accumulated 692 citations globally (27). This article suggests that Bayesian neural network models can effectively detect significant signals from adverse drug reaction data, particularly from the WHO Programme on International Drug Monitoring dataset. Lastly, Wang LD s research, titled ""COVID-Net: a tailored deep convolutional neural network design for the detection of COVID-19 cases from chest X-ray images,"" stands out with 690 citations worldwide (28). Table 1. Top most globally cited reviews and articles. From Figure 4, it can be seen that these Trend Topics were calculated using keywords plus from the literature. We observed several important themes. Among them, neural networks have received widespread attention in this field, appearing 11 times with a time span from 2001 to 2020, showing a gradually increasing trend. Clinical medicine is also one of the research hotspots, appearing 13 times, covering the period from 1998 to 2011, indicating long-term research attention. Research on medical records has also been highly regarded, appearing as many as 181 times, spanning from 2014 to 2021, and showing an increasing trend year by year, demonstrating the emphasis on medical information management. Decision support systems also play an important role in the field of medical artificial intelligence, appearing 52 times from 2012 to 2022, indicating sustained research interest. In addition, topics such as primary care, forecasting, models, and health have also received widespread attention, with research frequencies of 151, 1607, 1248, and 1230 times respectively, showing a continuous growth trend in recent years. Additionally, emerging topics such as COVID-19 and drug discovery have also attracted considerable research attention in a short period, indicating the rapid response and adaptability of the field of medical artificial intelligence to current important issues. Figure 4. Trend topic analysis of artificial intelligence in healthcare. The bibliometric analysis of national cooperation depicted in this graph illustrates the collaborative research endeavors among different countries. The size of nodes in the graph represents the research output (number of published papers) of each country, while the connections between nodes indicate collaborative relationships, with thicker lines denoting closer cooperation. From this graph, it can be observed that China stands out as the leading contributor in artificial intelligence in health care research, demonstrating both prolific research output and frequent collaboration with other countries, thus showcasing its prominent position in international research collaboration. Additionally, countries like the United States (unlabeled), Japan, and South Africa exhibit strong inclinations toward collaboration. The analysis of co-occurrence networks based on titles in bibliometrics is a method used to extract keywords from a large number of document titles and construct co-occurrence networks based on these keywords, revealing the associations and connections among documents. Our study indicates that after categorizing the extracted titles, they can be classified into five groups (Figure 5). Cluster 1 focuses on the application of neural networks in medical imaging, including terms like ""neural,"" ""network,"" ""based,"" ""detection,"" ""convolutional,"" ""deep,"" ""diagnosis,"" and ""classification."" This c With the continuous advancement of science and technology, particularly in the fields of computer science, data processing, and machine learning, the application of artificial intelligence (AI) technology in healthcare has become increasingly widespread (1 3). Current research has demonstrated that the integration of healthcare and AI enhances patient care, improves efficiency, and reduces costs in the healthcare industry, leading to smarter, faster, and more efficient healthcare systems (4 7). This optimization spans from diagnosis to treatment planning, driving advancements in disease prediction, diagnosis, and therapeutic interventions, thereby providing substantial benefits for both patients and healthcare providers (8 10). Figure 5. Co-occurrence network analysis of titles of artificial intelligence in healthcare. In 2022, OpenAI publicly introduced ChatGPT globally, marking the introduction of a new generation of powerful generative artificial intelligence tools (16, 17). This introduction has fundamentally altered the way people interact with AI technology and has sparked widespread interest and adoption across various fields, pushing the impact of AI on the medical field to new heights (18 20). This study aims to meticulously retrieve relevant literature from the Web of Science Core Collection database (WoSCC) from January 1, 1993, to December 31, 2023. Through quantitative and visual network analyses encompassing various parameters such as authors, institutions, countries/regions, and keywords, this analysis is expected to assist researchers in gaining comprehensive insights into AI-related research in healthcare and predicting future patterns and trends. The selection of the Web of Science Core Collection (WoSCC) was based on its ability to provide comprehensive data meeting the needs of bibliometric software, and its reputation as a prominent database in this research domain. Consequently, the bibliometric analysis yielded invaluable insights into the prevailing status of AI research in healthcare up to December 2023. Since the data are derived from publicly available databases, ethical approval for this study was considered unnecessary. Although the WoSCC predominantly includes English-language literature, it ensures high-quality and impactful sources, making it the most appropriate data source for a comprehensive, multilingual perspective. Given that the data are derived from publicly available databases, ethical approval for this study was deemed unnecessary. While PubMed is vital for biomedical and clinical research, its inclusion of non-peer-reviewed articles and conference abstracts may compromise the reliability of bibliometric analyses. Similarly, Scopus, despite its widespread use, exhibits variability in disciplinary coverage and journal quality, which could affect the precision of the analysis. Therefore, WoSCC is the most suitable database for our objectives. This study identified search keywords related to (1) AI technologies and (2) healthcare and medicine from preliminary literature reviews and consultation with researchers. Document types were limited to ""article"" and ""review article"". Conference abstracts, letters, expert views, editorial materials, corrections, retractions, and conference papers were excluded. (TI = (""artificial intelligence"") OR TI = (""data learning"") OR TI = (""machine learning"") OR TI = (""expert systems"") OR TI = (""fuzzy logic"") OR TI = (""computer vision"") OR TI = (""automatic programming"") OR TI = (""speech understanding"") OR TI = (""autonomous robots"") OR TI = (""intelligent tutoring"") OR TI = (""intelligent agents"") OR TI = (""neural network"") OR TI = (""voice recognition"") OR TI = (""text mining"") OR TI = (""electronic health record"") OR TI = (""ChatGPT"") OR TI = (""large language models"")) AND (TS = (health) OR TS = (healthcare) OR TS = (medicine) OR TS = (mental health) OR TS = (behavioral health)). The assessment period for published studies was from January 1, 1993, until December 31, 2023. with language restricted to English. Bibliometrix, Biblioshiny, and Microsoft Excel are used for data analysis and visualization of AI in health care article studies. Data were presented via descriptive statistics. Inferential bibliometric analyses included clustering of the selected parameters of keywords, keyword plus, titles, and abstracts; Bradford s law to evaluate core journals, and the Sankey diagrams to evaluate the flows between research themes over time. They can visualize the research results and has unique advantages in clustering technology and map display. Following a search strategy, a total of 22,950 documents were gathered from 5,024 distinct sources. The dataset revealed an average annual growth rate of 26.97%, alongside an average document age of 3.41 years and an average of 17 citations per document. From 1993 to 2023, a notable upward trend has been observed in the annual scientific output within the field of bibliometrics. In the initial years, such as 1993 and 1994, the volume of scientific publications remained comparatively modest, with only 5 and 11 articles respectively. Subsequent to 2010, this growth trajectory accelerated further, characterized by a marked escalation in the annual growth rate. Notably, the period spanning from 2019 to 2023 witnessed a particularly remarkable expansion in scientific productivity, yielding 1480, 2677, 4029, 5320, and 6450 articles respectively (Figure 1). The scientific production of various countries offers valuable insights into their respective contributions to research. The United States leads with an impressive count of 28663 articles, highlighting its significant influence and productivity within the scientific community. Following closely, China presents 12740 articles, indicating its rapid growth and importance in scientific research. India ranks third, contributing 4926 articles and establishing itself as a significant player in scientific inquiry. The United Kingdom maintains a robust presence with 4821 articles, showcasing its active involvement and contributions to scientific endeavors. Canada follows suit with 3567 articles, making substantial contributions to the global scientific landscape (Figure 2). The article counts and collaboration patterns based on the authors  countries reveal interesting trends. The United States leads in article count, having published a total of 6409 articles, comprising 5310 single corresponding author papers (SCP) and 1099 multiple corresponding author papers (MCP). Despite its high MCP count, the US exhibits a comparatively lower MCP-to-total articles ratio of 0.171 compared to other countries. China closely follows with 3774 articles, consisting of 2882 SCPs and 892 MCPs, demonstrating a higher ratio of MCPs, indicating extensive collaborative research efforts. India contributes 1601 articles, with a higher proportion of SCPs than MCPs, aligning with its lower MCP frequency of 0.070. It is worth noting that contributions from the United Kingdom, South Korea, Canada, Australia, Italy, Germany, and Spain are also notable, with each country demonstrating a relatively high MCP ratio (Figure 3). These findings underscore the global distribution of research output in medical AI, highlighting diverse collaborative patterns among researchers from different countries. The top 20 research institutions in the field of medical AI, as indicated by publication volume, exhibit a concentration of academic powerhouses renowned for their research contributions. Leading the pack is Harvard University, with an impressive 1690 articles, followed closely by the University of California System with 1180 articles. Harvard Medical School, a prestigious institution affiliated with Harvard University, demonstrates a strong presence with 802 articles. The University of Toronto and the University of Pennsylvania also showcase significant engagement in this field, with 809 and 635 articles respectively. Other notable institutions include the University of Michigan, University of London, Stanford University, University System of Ohio, and the University of California San Francisco, each contributing substantially to the body of literature in medical AI. In the research publications, PLOS ONE leads with a publication count of 370 articles, possibly owing to its open access policy and broad research scope. Following closely is IEEE ACCESS, with 360 articles, showcasing the diversity and breadth of the field. SCIENTIFIC REPORTS ranks third with 358 articles, highlighting the intersection of medical AI. The JOURNAL OF MEDICAL INTERNET RESEARCH, with 347 articles, likely covers topics related to sensor technology and its applications in medical AI. Additionally, the JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION and SENSORS hold significant influence in the medical informatics field with 347 and 338 articles, respectively. The publication volume of these journals reflects the continued growth and interdisciplinary nature of the medical artificial intelligence field. Other journals such as APPLIED SCIENCES-BASEL, JMIR MEDICAL INFORMATICS, and BMC MEDICAL INFORMATICS AND DECISION MAKING also make substantial contributions, demonstrating the diversity and activity within the field of medical artificial intelligence. In 2023, bibliometric analysis of medical artificial intelligence research unveiled several highly cited global publications. Leading the list is the review article titled ""ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on the Promising Perspectives and Valid Concerns,"" published in the journal ""Healthcare,"" which garnered 332 citations (21). Following closely is the Original Investigation article ""Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum,"" published in JAMA Internal Medicine, with 238 citations (22). Additionally, a review from Cureus titled ""Artificial Hallucinations in ChatGPT: Implications in Scientific Writing"" ranks as the third most cited (23), while an article from the Journal of Medical Systems titled ""Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios"" holds the fourth position (24). Notably, the top five most cited publications all center around topics related to ChatGPT. Table 1 presents an overview of the top ten most globally cited reviews and articles within the field of medical artificial intelligence. The most globally cited review, ""Machine learning: Trends, perspectives, and prospects,"" authored by M. I. Jordan from the University of California and T. M. Mitchell from Carnegie Mellon University, was published in the Science journal on July 17, 2015 (25). The top three most globally cited articles in the field of medical artificial intelligence literature are as follows: Guo L s article titled ""A multi-time scale approach to remaining useful life prediction in rolling bearing,"" published in Neurocomputing in 2017, investigates a multi-time scale modeling approach for predicting the remaining useful life (RUL) of rolling bearings. This study contributes to the advancement of predictive maintenance techniques in industrial applications (26). The second article, authored by A. Bate and titled ""A Bayesian neural network method for adverse drug reaction signal generation,"" was published in the European Journal of Clinical Pharmacology in 1998 and has accumulated 692 citations globally (27). This article suggests that Bayesian neural network models can effectively detect significant signals from adverse drug reaction data, particularly from the WHO Programme on International Drug Monitoring dataset. Lastly, Wang LD s research, titled ""COVID-Net: a tailored deep convolutional neural network design for the detection of COVID-19 cases from chest X-ray images,"" stands out with 690 citations worldwide (28). From Figure 4, it can be seen that these Trend Topics were calculated using keywords plus from the literature. We observed several important themes. Among them, neural networks have received widespread attention in this field, appearing 11 times with a time span from 2001 to 2020, showing a gradually increasing trend. Clinical medicine is also one of the research hotspots, appearing 13 times, covering the period from 1998 to 2011, indicating long-term research attention. Research on medical records has also been highly regarded, appearing as many as 181 times, spanning from 2014 to 2021, and showing an increasing trend year by year, demonstrating the emphasis on medical information management. Decision support systems also play an important role in the field of medical artificial intelligence, appearing 52 times from 2012 to 2022, indicating sustained research interest. In addition, topics such as primary care, forecasting, models, and health have also received widespread attention, with research frequencies of 151, 1607, 1248, and 1230 times respectively, showing a continuous growth trend in recent years. Additionally, emerging topics such as COVID-19 and drug discovery have also attracted considerable research attention in a short period, indicating the rapid response and adaptability of the field of medical artificial intelligence to current important issues. The bibliometric analysis of national cooperation depicted in this graph illustrates the collaborative research endeavors among different countries. The size of nodes in the graph represents the research output (number of published papers) of each country, while the connections between nodes indicate collaborative relationships, with thicker lines denoting closer cooperation. From this graph, it can be observed that China stands out as the leading contributor in artificial intelligence in health care research, demonstrating both prolific research output and frequent collaboration with other countries, thus showcasing its prominent position in international research collaboration. Additionally, countries like the United States (unlabeled), Japan, and South Africa exhibit strong inclinations toward collaboration. The analysis of co-occurrence networks based on titles in bibliometrics is a method used to extract keywords from a large number of document titles and construct co-occurrence networks based on these keywords, revealing the associations and connections among documents. Our study indicates that after categorizing the extracted titles, they can be classified into five groups (Figure 5). Cluster 1 focuses on the application of neural networks in medical imaging, including terms like ""neural,"" ""network,"" ""luster underscores the use of deep learning techniques for tasks such as disease detection and classification. Cluster 2 revolves around healthcare information systems and patient data management, featuring terms like ""health,"" ""electronic record,"" ""care,"" ""systems,"" and ""patient."" It highlights the significance of electronic health records and data systems in healthcare analytics and decision-making. Cluster 3 delves into machine learning, predictive modeling, and various medical applications, encompassing terms like ""machine learning,"" ""prediction,"" ""analysis,"" ""model,"" ""covid,"" ""disease,"" ""cancer,"" ""medical,"" ""application,"" and ""techniques."" This cluster indicates a broad exploration of machine learning methodologies for predictive modeling and analysis in healthcare settings. Cluster 4 centers on artificial intelligence and its applications in medicine, including terms like ""artificial intelligence,"" ""review,"" ""medicine,"" ""applications,"" and ""systematic."" It suggests a focus on systematic reviews and applications of artificial intelligence in healthcare research and practice. Cluster 5 covers clinical research, patient studies, and risk assessment, featuring terms like ""data,"" ""study,"" ""patients,"" ""clinical,"" ""system,"" ""risk,"" ""development,"" and ""assessment."" This cluster highlights the use of data-driven approaches for clinical research, patient management, and risk assessment in healthcare settings. Our analysis reveals the research landscape of the past 30 years in the field of medical artificial intelligence. The remarkable expansion of scientific productivity from 2019 to 2023 is particularly noteworthy, reflecting an unprecedented period of growth and innovation in the field of bibliometrics in recent years. In recent years, with the advancement of deep learning, machine learning algorithms, hardware capabilities, and databases, artificial intelligence technology has experienced a third wave of development, providing powerful assistance to clinical work (24). As evidenced by our research findings, there has been a growing trend in artificial intelligence research in the medical field, from only 15 publications in 1995 to 310 publications in 2015, and reaching 5,297 publications by 2023. The United States leads in publications, followed closely by China, with India ranking third. The United States holds a significant lead in medical artificial intelligence for several key reasons: 1) The U.S. healthcare industry generates a vast amount of diverse medical data, providing a rich environment for the application of artificial intelligence and machine learning technologies. 2) Major U.S. tech companies such as Google, IBM, and Amazon have been heavily investing in and applying their expertise in artificial intelligence to the healthcare sector, developing tools for medical imaging analysis, drug discovery, and virtual healthcare. 3) Despite ongoing developments in its regulatory environment, the United States exhibits a more lenient attitude toward the adoption of artificial intelligence in healthcare compared to other countries, leading to substantial private investments in medical artificial intelligence startups. In contrast, the impact of the EU AI Act on innovation is complex, involving legal, policy, and social dimensions, which makes it challenging to draw definitive conclusions within the scope of this study. As a result, existing literature on the topic is presented without an in-depth exploration of the Act s. 4) The U.S. government allocates funding for medical artificial intelligence research through various institutions such as the National Institutes of Health and the Food and Drug Administration, supporting research in areas such as medical imaging analysis, disease diagnosis, and drug discovery. Moreover, China, with a higher proportion of international collaboration than the United States, actively promotes the development and application of artificial intelligence through measures such as formulating development plans, driving technological innovation, strengthening patent applications, promoting industrial ecosystem construction, and proposing global governance initiatives. Patent applications, as an early indicator of technological trends and innovation, allow for timely identification of emerging topics and standardized cross-national comparisons, avoiding biases introduced by varying patent approval times across countries. China leads the world in the number of patent applications related to artificial intelligence, reflecting its innovative vitality in this field. From January 2018 to October 2022, China filed over 648,000 patent applications related to artificial intelligence, far surpassing the numbers from the United States and South Korea (29). As our research indicates, over the past 20 years, around 2002, the development and application of neural networks became one of the focal points in medical artificial intelligence research. At that time, the potential importance of artificial intelligence technologies, particularly neural networks, in providing innovative solutions for tasks such as medical imaging analysis, disease diagnosis, and personalized treatment recommendations, was already evident (30 34). Furthermore, the rapid growth of emerging topics such as COVID-19 in recent years is also worth noting. COVID-19, as a global public health crisis, has spurred interest in pandemic management and treatment within the medical artificial intelligence field. Researchers have utilized artificial intelligence technologies for COVID-19 prediction, virus tracing, drug discovery, and other aspects to address this challenge (35 39). Particularly in drug discovery, artificial intelligence technologies such as deep learning and machine learning have been widely applied, accelerating the drug development process (40 43). Additionally, the application of blockchain technology in the field of medical artificial intelligence has garnered significant attention (44, 45). The decentralized, tamper-proof, and secure characteristics of blockchain provide new solutions for medical data management, medical information sharing, and medical privacy protection (46 49). Therefore, the emergence of COVID-19, drug discovery, blockchain, and other emerging topics enriches the research landscape of medical artificial intelligence, expands its application scope, and holds significant implications for promoting medical innovation and addressing practical issues. When examining the most cited literature in our study, we surprisingly found that all five of the highest cited publications in 2023 were related to medical artificial intelligence research involving ChatGPT. This indicates the widespread attention and citation of ChatGPT s application and research in the medical field, demonstrating the importance and influence of artificial intelligence in medicine. Current research has shown that ChatGPT has a wide range of applications in medical research (19). This natural language processing-based artificial intelligence technology can assist physicians in better understanding patients  conditions, bringing innovation to the medical field. In medical writing, ChatGPT is used to generate initial drafts of documents, assist medical authors with writing support, and automate the review and editing process (23). Additionally, ChatGPT can be applied to medical education to help adjust teaching strategies, prevent inappropriate use, and promote the cultivation of students  critical thinking abilities (21). The rapid advancement of artificial intelligence (AI) holds great promise for medical applications. Personalized medicine, integrating genomic and other patient data, is poised to enable precise diagnosis and tailored treatment. Interdisciplinary collaboration is driving innovation in areas such as intelligent diagnostic devices and drug delivery systems. Strengthening ethical and regulatory frameworks is critical to safeguarding privacy, fostering trust, and ensuring patient-centered AI development. Moreover, international cooperation can expand AI applications to resource-limited regions, enhancing healthcare accessibility and reducing disparities. Notably, ChatGPT has emerged as a transformative tool, optimizing patient consultation workflows, streamlining medical documentation, boosting research efficiency, and supporting personalized learning and simulation in medical education. There are also limitations in this study. Firstly, many computer-related papers in medical artificial intelligence are often published in conference proceedings, which may lead to an underestimation of the impact of such research. Additionally, ChatGPT not only assists clinicians in better understanding patient conditions in clinical practice, but also provides new ideas and solutions in medical research. It is expected that significant changes will be observed in the bibliometric analysis of related literature in 2024. Overall, the development of artificial intelligence (AI) in healthcare is closely linked to advancements in foundational AI techniques, with applications like ChatGPT showcasing this trend. AI s integration into healthcare has addressed critical societal needs, particularly in responding to challenges such as COVID-19 and breakthrough drug discovery. While this study examines the evolution of AI in healthcare over the past three decades, certain areas, such as calculating the proportion of AI-related papers and exploring the relationship between impact factor and research quality, remain for future research. These aspects require careful planning and are beyond the current study s scope, but they are crucial for further enriching our findings and understanding AI s broader impact on healthcare. Future studies will aim to incorporate economic and population data and explore these relationships in greater depth, providing valuable insights for policymakers, researchers, and clinicians. The raw data supporting the conclusions of this article will be made available by the authors, without undue reservation. YX: Conceptualization, Formal analysis, Writing   original draft. YZ: Methodology, Supervision, Writing   review and editing. GL: Conceptualization, Project administration, Supervision, Writing   review and editing. The author(s) declare that financial support was received for the research, authorship, and/or publication of this article. This work funded by the Guangzhou Science and Technology Plan Project (grant number 202201011468) and the National Natural Science Foundation of China (grant number 81700294). YX was employed by Yangjiang Bainian Yanshen Medical Technology Co., Ltd. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The authors declare that no Generative AI was used in the creation of this manuscript. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. 1. Yu KH, Beam AL, Kohane IS. Artificial intelligence in healthcare. Nat Biomed Eng. (2018) 2:719 31. doi: 10.1038/s41551-018-0305-z PubMed Abstract | Crossref Full Text | Google Scholar 2. Waisberg E, Ong J, Masalkhi M. GPT-4: A new era of artificial intelligence in medicine. Ir J Med Sci. (2023) 192:3197 200. doi: 10.1007/s11845-023-03377-8 PubMed Abstract | Crossref Full Text | Google Scholar 3. Gupta NS, Kumar P. Perspective of artificial intelligence in healthcare data management: A journey towards precision medicine. Comput Biol Med. (2023) 162:107051. doi: 10.1016/j.compbiomed.2023.107051 PubMed Abstract | Crossref Full Text | Google Scholar 4. Van Den Eynde J, Lachmann M, Laugwitz KL, Manlhiot C, Kutty S. Successfully implemented artificial intelligence and machine learning applications in cardiology: State-of-the-art review. Trends Cardiovasc Med. (2023) 33:265 71. doi: 10.1016/j.tcm.2022.01.010 PubMed Abstract | Crossref Full Text | Google Scholar 5. Liu Q, Joshi A, Standing JF, Van Der Graaf PH. Artificial intelligence/machine learning: The new frontier of clinical pharmacology and precision medicine. Clin Pharma Ther. (2024) 115:637 42. doi: 10.1002/cpt.3198 PubMed Abstract | Crossref Full Text | Google Scholar 6. Van Leeuwen KG, De Rooij M, Schalekamp S, Van Ginneken B, Rutten MJCM. How does artificial intelligence in radiology improve efficiency and health outcomes? Pediatr Radiol. (2022) 52:2087 93. doi: 10.1007/s00247-021-05114-8 PubMed Abstract | Crossref Full Text | Google Scholar 7. Topol EJ. High-performance medicine: The convergence of human and artificial intelligence. Nat Med. (2019) 25:44 56. doi: 10.1038/s41591-018-0300-7 PubMed Abstract | Crossref Full Text | Google Scholar 8. Alowais SA, Alghamdi SS, Alsuhebany N. Revolutionizing healthcare: The role of artificial intelligence in clinical practice. BMC Med Educ. (2023) 23:689. doi: 10.1186/s12909-023-04698-z PubMed Abstract | Crossref Full Text | Google Scholar 9. Yasmin F, Shah SMI, Naeem A. Artificial intelligence in the diagnosis and detection of heart failure: The past, present, and future. Rev Cardiovasc Med. (2021) 22:1095. doi: 10.31083/j.rcm2204121 PubMed Abstract | Crossref Full Text | Google Scholar 10. Huang X, Wang H, She C. Artificial intelligence promotes the diagnosis and screening of diabetic retinopathy. Front Endocrinol. (2022) 13:946915. doi: 10.3389/fendo.2022.946915 PubMed Abstract | Crossref Full Text | Google Scholar 11. Nashef SAM, Ali J. Artificial intelligence and cardiac surgery risk assessment. Eur J Cardiothorac Surg. (2023) 63:ezad226. doi: 10.1093/ejcts/ezad226 PubMed Abstract | Crossref Full Text | Google Scholar 12. Yin J, Ngiam KY, Teo HH. Role of artificial intelligence applications in real-life clinical practice: Systematic review. J Med Internet Res. (2021) 23:e25759. doi: 10.2196/25759 PubMed Abstract | Crossref Full Text | Google Scholar 13. Jiang Y, Wang C, Zhou S. Artificial intelligence-based risk stratification, accurate diagnosis and treatment prediction in gynecologic oncology. Semin Cancer Biol. (2023) 96:82 99. doi: 10.1016/j.semcancer.2023.09.005 PubMed Abstract | Crossref Full Text | Google Scholar 14. Turcian D, Stoicu-Tivadar V. Artificial intelligence in primary care: An overview. Stud Health Technol Inform. (2022) 289:208 11. doi: 10.3233/SHTI210896 PubMed Abstract | Crossref Full Text | Google Scholar 15. Xie Y, Lu L, Gao F, He SJ, Zhao HJ, Fang Y, et al. Integration of artificial intelligence, blockchain, and wearable technology for chronic disease management: A new paradigm in smart healthcare. Curr Med Sci. (2021) 41:1123 33. doi: 10.1007/s11596-021-2485-0 PubMed Abstract | Crossref Full Text | Google Scholar 16. Dave T, Athaluri SA, Singh S. ChatGPT in medicine: An overview of its applications, advantages, limitations, future prospects, and ethical considerations. Front Artif Intell. (2023) 6:1169595. doi: 10.3389/frai.2023.1169595 PubMed Abstract | Crossref Full Text | Google Scholar 17. Liu J, Wang C, Liu S. Utility of ChatGPT in clinical practice. J Med Internet Res. (2023) 25:e48568. doi: 10.2196/48568 PubMed Abstract | Crossref Full Text | Google Scholar 18. Biswas SS. Role of Chat GPT in public health. Ann Biomed Eng. (2023) 51:868 9. doi: 10.1007/s10439-023-03172-7 PubMed Abstract | Crossref Full Text | Google Scholar 19. Datt M, Sharma H, Aggarwal N, Sharma S. Role of ChatGPT-4 for medical researchers. Ann Biomed Eng. (2024) 52:1534 6. doi: 10.1007/s10439-023-03336-5 PubMed Abstract | Crossref Full Text | Google Scholar 20. Mese I, Taslicay CA, Sivrioglu AK. Improving radiology workflow using ChatGPT and artificial intelligence. Clin Imaging. (2023) 103:109993. doi: 10.1016/j.clinimag.2023.109993 PubMed Abstract | Crossref Full Text | Google Scholar 21. Sallam M. ChatGPT utility in healthcare education, research, and practice: Systematic review on the promising perspectives and valid concerns. Healthcare. (2023) 11:887. doi: 10.3390/healthcare11060887 PubMed Abstract | Crossref Full Text | Google Scholar 22. Ayers JW, Poliak A, Dredze M. Comparing physician and artificial intelligence Chatbot responses to patient questions posted to a public social media forum. JAMA Intern Med. (2023) 183:589. doi: 10.1001/jamainternmed.2023.1838 PubMed Abstract | Crossref Full Text | Google Scholar 23. Alkaissi H, McFarlane SI. Artificial hallucinations in ChatGPT: Implications in scientific writing. Cureus. (2023) 15:e35179. doi: 10.7759/cureus.35179 PubMed Abstract | Crossref Full Text | Google Scholar 24. Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the feasibility of ChatGPT in healthcare: An analysis of multiple clinical and research scenarios. J Med Syst. (2023) 47:33. doi: 10.1007/s10916-023-01925-4 PubMed Abstract | Crossref Full Text | Google Scholar 25. Jordan MI, Mitchell TM. Machine learning: Trends, perspectives, and prospects. Science. (2015) 349:255 60. doi: 10.1126/science.aaa8415 PubMed Abstract | Crossref Full Text | Google Scholar 26. Qian Y, Yan R, Gao RX. A multi-time scale approach to remaining useful life prediction in rolling bearing. Mech Syst Signal Process. (2017) 83:549 67. doi: 10.1016/j.ymssp.2016.06.031 Crossref Full Text | Google Scholar 27. Bate A, Lindquist M, Edwards IR. A Bayesian neural network method for adverse drug reaction signal generation. Eur J Clin Pharmacol. (1998) 54:315 21. doi: 10.1007/s002280050466 PubMed Abstract | Crossref Full Text | Google Scholar 28. Wang L, Lin ZQ, Wong A. COVID-Net: A tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images. Sci Rep. (2020) 10:19549. doi: 10.1038/s41598-020-76550-z PubMed Abstract | Crossref Full Text | Google Scholar 29. AlShebli B, Memon SA, Evans JA, Rahwan T. China and the U.S. produce more impactful AI research when collaborating together. Sci Rep. (2024) 14:28576. doi: 10.1038/s41598-024-79863-5 PubMed Abstract | Crossref Full Text | Google Scholar 30. Charles Leek E, Leonardis A, Heinke D. Deep neural networks and image classification in biological vision. Vis Res. (2022) 197:108058. doi: 10.1016/j.visres.2022.108058 PubMed Abstract | Crossref Full Text | Google Scholar 31. Van Assen M, Lee SJ, De Cecco CN. Artificial intelligence from A to Z: From neural network to legal framework. Eur J Radiol. (2020) 129:109083. doi: 10.1016/j.ejrad.2020.109083 PubMed Abstract | Crossref Full Text | Google Scholar 32. Zhang H, Botler M, Kooman JP. Deep learning for image analysis in kidney care. Adv Kidney Dis Health. (2023) 30:25 32. doi: 10.1053/j.akdh.2022.11.003 PubMed Abstract | Crossref Full Text | Google Scholar 33. Li M, Jiang Y, Zhang Y, Zhu H. Medical image analysis using deep learning algorithms. Front Public Health. (2023) 11:1273253. doi: 10.3389/fpubh.2023.1273253 PubMed Abstract | Crossref Full Text | Google Scholar 34. S nchez De La Nava AM, Atienza F, Bermejo J, Fern ndez-Avil s F. Artificial intelligence for a personalized diagnosis and treatment of atrial fibrillation. Am J Physiol Heart Circ Physiol. (2021) 320:H1337 47. doi: 10.1152/ajpheart.00764.2020 PubMed Abstract | Crossref Full Text | Google Scholar 35. Azeem M, Javaid S, Khalil R. Neural networks for the detection of COVID-19 and other diseases: Prospects and challenges. Bioengineering. (2023) 10:850. doi: 10.3390/bioengineering10070850 PubMed Abstract | Crossref Full Text | Google Scholar 36. Comito C, Pizzuti C. Artificial intelligence for forecasting and diagnosing COVID-19 pandemic: A focused review. Artif Intellig Med. (2022) 128:102286. doi: 10.1016/j.artmed.2022.102286 PubMed Abstract | Crossref Full Text | Google Scholar 37. Yu CS, Chang SS, Chang TH. A COVID-19 pandemic artificial intelligence based system with deep learning forecasting and automatic statistical data acquisition: Development and implementation study. J Med Internet Res. (2021) 23:e27806. doi: 10.2196/27806 PubMed Abstract | Crossref Full Text | Google Scholar 38. Ayoobi N, Sharifrazi D, Alizadehsani R. Time series forecasting of new cases and new deaths rate for COVID-19 using deep learning methods. Results Phys. (2021) 27:104495. doi: 10.1016/j.rinp.2021.104495 PubMed Abstract | Crossref Full Text | Google Scholar 39. Chang Z, Zhan Z, Zhao Z. Application of artificial intelligence in COVID-19 medical area: A systematic review. J Thorac Dis. (2021) 13:7034 53. doi: 10.21037/jtd-21-747 PubMed Abstract | Crossref Full Text | Google Scholar 40. Tiwari PC, Pal R, Chaudhary MJ, Nath R. Artificial intelligence revolutionizing drug development: Exploring opportunities and challenges. Drug Dev Res. (2023) 84:1652 63. doi: 10.1002/ddr.22115 PubMed Abstract | Crossref Full Text | Google Scholar 41. Gupta R, Srivastava D, Sahu M, Tiwari S, Ambasta RK, Kumar P. Artificial intelligence to deep learning: Machine intelligence approach for drug discovery. Mol Divers. (2021) 25:1315 60. doi: 10.1007/s11030-021-10217-3 PubMed Abstract | Crossref Full Text | Google Scholar 42. Tripathi A, Misra K, Dhanuka R, Singh JP. Artificial intelligence in accelerating drug discovery and development. Recent Pat Biotechnol. (2023) 17:9 23. doi: 10.2174/1872208316666220802151129 PubMed Abstract | Crossref Full Text | Google Scholar 43. Chopra H, Baig AA, Gautam RK, Kamal MA. Application of artificial intelligence in drug discovery. Curr Pharm Des. (2022) 28:2690 703. doi: 10.2174/1381612828666220608141049 PubMed Abstract | Crossref Full Text | Google Scholar 44. De Novi G, Sofia N, Vasiliu-Feltes I, Yan Zang C, Ricotta F. Blockchain technology predictions 2024: Transformations in healthcare, patient identity and public health. Blockchain Healthc Today (2023) 6:2692 3. doi: 10.30953/bhty.v6.287 PubMed Abstract | Crossref Full Text | Google Scholar 45. Fatoum H, Hanna S, Halamka JD, Sicker DC, Spangenberg P, Hashmi SK. Blockchain integration with digital technology and the future of health care ecosystems: Systematic review. J Med Internet Res. (2021) 23:e19846. doi: 10.2196/19846 PubMed Abstract | Crossref Full Text | Google Scholar 46. Ahmed I, Chehri A, Jeon G. Artificial intelligence and blockchain enabled smart healthcare system for monitoring and detection of COVID-19 in biomedical images. IEEE/ACM Trans Comput Biol and Bioinf. (2023) 21:814 22. doi: 10.1109/TCBB.2023.3294333 PubMed Abstract | Crossref Full Text | Google Scholar 47. Wu G, Wang S, Ning Z, Zhu B. Privacy-preserved electronic medical record exchanging and sharing: A blockchain-based smart healthcare system. IEEE J Biomed Health Inform. (2022) 26:1917 27. doi: 10.1109/JBHI.2021.3123643 PubMed Abstract | Crossref Full Text | Google Scholar 48. Haug CJ, Drazen JM. Artificial intelligence and machine learning in clinical medicine, 2023. N Engl J Med. (2023) 388:1201 8. doi: 10.1056/NEJMra2302038 PubMed Abstract | Crossref Full Text | Google Scholar 49. Sarkar U, Bates DW. Using artificial intelligence to improve primary care for patients and clinicians. JAMA Intern Med. (2024) 184: 343 4. Google Scholar Keywords: artificial intelligence, health care, medicine, ChatGPT, bibliometric study Citation: Xie Y, Zhai Y and Lu G (2025) Evolution of artificial intelligence in healthcare: a 30-year bibliometric study. Front. Med. 11:1505692. doi: 10.3389/fmed.2024.1505692 Received: 03 October 2024; Accepted: 31 December 2024;Published: 15 January 2025. Edited by: Reviewed by: Copyright   2025 Xie, Zhai and Lu. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Yuansheng Zhai, zhaiysh2@mail.sysu.edu.cn; Guihua Lu, luguihua3@mail.sysu.edu.cn Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on",2
Artificial intelligence tools in supporting healthcare professionals for tailored patient care - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTE9BTUtSRXhjVEhtRzM3UGNsb19xdXZXTFptc2pHSENmZmRiV0FReWh1Z0tuWFNtNkFCLXRSZFFiY2FXblVhVzhfN21qWmhLUGp6eURoZTJ3R3F5eVJycTJ3?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement npj Digital Medicine volume 8, Article number: 210 (2025) Cite this article 10k Accesses 1 Citations 42 Altmetric Metrics details Artificial intelligence (AI) tools to support clinicians in providing patient-centered care can contribute to patient empowerment and care efficiency. We aimed to draft potential AI tools for tailored patient support corresponding to patients  needs and assess clinicians  perceptions about the usefulness of those AI tools. To define patients  issues, we analyzed 528,199 patient messages of 11,123 patients with diabetes by harnessing natural language processing and AI. Applying multiple prompt-engineering techniques, we drafted a series of AI tools, and five endocrinologists evaluated them for perceived usefulness and risk. Patient education and administrative support for timely and streamlined interaction were perceived as highly useful, yet deeper integration of AI tools into patient data was perceived as risky. This study proposes assorted AI applications as clinical assistance tailored to patients  needs substantiated by clinicians  evaluations. Findings could offer essential ramifications for developing potential AI tools for precision patient care for diabetes and beyond. More than 1.3 billion individuals are estimated to live with diabetes mellitus by 2050 globally1,2. Diabetes is a risk factor for numerous other diseases and increased morbidity and mortality3,4,5. Diabetes has been linked to socioeconomic status (SES), dietary risk, and physical activity, and it is crucial to provide tailored support for each patient subgroup to help them manage this lifelong disease condition6,7. Patient-centered care (PCC), which emphasizes care addressing each patient s particular needs and preferences8,9, has become an essential approach for the self-management of diabetes10. PCC was found to improve patient engagement in care and enhance self-care skills and confidence while reducing disease-related distress11,12. Moreover, PCC was associated with better quality of life and health outcomes10,13. Secure messaging through the patient portal has been an essential tool for patients to share concerns with clinicians and ask questions to care for their medical conditions14. Driven by the pandemic, the volume of patient messages increased by 50% in the past few years, reflecting patients  high demand for this secure communications channel15,16. Reportedly, diabetes was a strong predictor of increased severity and mortality of COVID-1917. Hence, patients with diabetes may have had substantial clinical issues or specific concerns during the pandemic. A systematic review found that those with diabetes were one of the most active patient groups in the patient portal18. Therefore, secure patient messages may contain a key component to enhance PCC, which may differ from the clinician s perspectives from clinical notes19. Although, patient messages reflect their concerns of interest, limited prior study exists on the range of content and inquiries of patients beyond broad process categorizations20. The restricted feasibility of analyzing this massive text data could potentially be one reason until the recent emergence of innovative artificial intelligence (AI). Natural language processing (NLP)-based topic modeling can extract key information from large volumes of text data. Recently introduced AI-enabled NLP models (e.g., bidirectional encoder representations from transformer (BERT)) have been widely used for the analysis of patient-generated health data, including social media or online forum text21,22. This AI/NLP-based approach of analyzing the secure patient messages allows us to gain insight into what clinical issues were brought up to the clinicians by patients with diabetes. Furthermore, generative AI, the most popular form known as ChatGPT-4, has obtained rapid popularity and has been actively tested in medicine for its potential applications23. While its expert level of medical knowledge and advanced clinical reasoning has been reported24,25,26, potential use as a clinical assistant in tailored diabetes care has been rarely explored despite its promising capacity. Thus, we aimed to draft potential AI tools that can assist clinicians in providing tailored diabetes care uniquely corresponding to patients  spoken needs and assess the usefulness of those AI tools. To illuminate patients  needs, we analyzed electronic patient messages from those with diabetes using NLP. We tackled prompt-engineered LLM to comprehensively draft AI tools to reflect patients  addressed issues. Finally, we performed a validation study with five endocrinologists to quantify clinicians  perceived usefulness of the suggested AI tools. The findings provide novel insights from both beneficiaries  perspectives into formulating AI tools as clinical assistants for tailored patient support, which could contribute to empowering patients and PCC. We identified a total of 11,151,561 unique message threads in 2013 2024, and further limited messages to clinical questions (Patient medical advice request (PMAR)), 66.9% n = 7,456,800/11,151,561), excluding patient scheduling, patient medication renewal request, or general questionnaire submissions. Of those, this study included unique message threads that were routed to Endocrinology divisions (7.1%, n = 528,199/7,456,800) (Fig. 1). a Individuals with diabetes mellitus: Determined by ICD-10 codes: E08 (underlying DM), E09 (induced DM), E10 (Type 1 DM), E11 (Type 2 DM), E13 (Secondary DM); b Number of unique message threads. LLM-interpreted primary issues of patients with diabetes are in Table 1: Topic 1) Dietary concerns and weight control; Topic 2) Interpreting lab results (e.g., blood, urine, and A1C); Topic 3) Thyroid management (e.g., medication, thyroid-stimulating hormone (TSH) level and test); Topic 4) Administrative challenges (e.g., paperwork and authorizations); Topic 5) Bone health (e.g., imaging and surgery for bone disease related to diabetes); Topic 6) Navigating lab orders and results; Topic 7) Appointment scheduling; Topic 8) Medication dosage management; Topic 9) Prescription and supplies refills and insurance; Topic 10) Data education needs for Dexcom and pump; Topic 11) Patient-reported hypoglycemia concerns; Topic 12) Blood glucose management in diabetes. In terms of the top 12 topics, the two time periods shared the same primary concerns. All word clouds and LLM-interpreted topics and titles are available in Supplementary Table 1. Table 2 presents LLM-drafted AI tools for supporting clinicians in patient care. The overall mean of clinicians  perceived usefulness for AI assistance was 4.30/5.00 ((SD) = 0.38; where 5 = Useful). Clinicians perceived some AI tools as highly useful, including 1) Provide evidence-based answers to frequently asked questions, reducing response time (m = 4.8, SD = 0.45); 2) Summarize policy changes and provide real-time updates on covered medications under popular insurance (m = 4.8, SD = 0.45); 3) Automate patient education on hypoglycemia management and prevention strategies (m = 4.6, SD = 0.89); 4) Offer templated but customizable responses for common lab-related inquiries (m = 4.8, SD = 0.45); and 5) Create templates for authorization letters to expedite processing (m = 5.0, SD = 0.00). While the overall perceived risk was moderate and low, m = 3.68/5.00 (SD = 0.42; where 5 = Strongly disagree with risk), a few AI assistance were perceived as relatively risky, including 1) Synthesizing patient data to help prioritize urgent request (m = 2.80, SD = 0.84); 2) AI-driven message triage system by urgency and topic (m = 2.80, SD = 0.84); and 3) Real-time glucose data interpretation and adjustment suggestions (m = 2.80, SD = 1.10). Commonly suggested mechanisms were automating feedback, creating educational content, and crafting templated responses for nutrition, TSH, and other lab tests and bone issues to personally respond to each patient to reduce the workload of clinicians. See Supplementary Tables 2 4 for full information. A total of 11,123 patients with diabetes were included in this study: 4530 individuals in the pre-COVID-19 group and 6593 individuals in the COVID-19 group. Our study population was mostly non-Hispanic ethnicity (82 83%) and married (65%), and nearly half were females (53 54%) and White race (43 50%) in both pre-COVID-19 and COVID-19 (Supplementary Table 5). A total of 324,109 messages from patients with diabetes were included, and pre-defined topics captured 80.9% (n = 262,249) of messages (Table 3). During COVID-19 (n = 200,657), approximately twice as many messages were included as compared to the pre-COVID group (n = 123,452). Messages from patients with type 2 diabetes were dominant (84.3%, n = 273,241). The most common primary topics were scheduling (20.7%, n = 54,401), meal/diet (19.2%, n = 50,471), pharmacy/refill (14.0%, n = 36,758), lab order/result (11.2%, n = 29,428), medication (9.4%, n = 24,605), and thyroid/hormone (8.6%, n = 22,560). After excluding scheduling and pharmacy refills, two already existing separate channels, the top 4 topics accounted for half of the total messages (Table 3). During COVID-19, meal/diet-related messages increased (from 17.5% to 20.3%, p < 0.0001) while lab order/result and thyroid/hormone-related messages decreased (from 13.5% to 9.9% and from 10.4% to 7.5%, respectively, both p < 0.0001) compared to pre-COVID-19 (Supplementary Table 6). Figure 2 presents topic clusters that show the size and distributions of neighboring topics. In this topic visualization, the thyroid/hormone topic was distinct from other topics and did not overlap with other topics. 12,000 messages were randomly selected for each diagnosis (type II and type I DM) for visualization. Message topics: Scheduling/canceling, Pharmacy/refill, Medication, Symptom (Pain/fell), Thyroid/hormone, Lab/test order, Treatment/surgery, Mental Health/sleep, Insurance/coverage, Device/Sensor/supplies, Meal/diet. The number of messages discussed on specific topics differed by patient characteristics among those with diabetes (Fig. 3). White patients had more messages on scheduling, lab order/result, pharmacy refill, thyroid/hormone, medication, device/sensor, insurance/coverage, symptom, imaging/surgery, and mental health/sleep (all, p < 0.0001) than all other racial groups (Fig. 3a). Compared to Hispanic patients, non-Hispanic ethnic groups had more messages on meal/diet (p = 0.035), lab order/result (p = 0.004), insurance/coverage (p = 0.007), symptom (p = 0.001), and imaging/surgery (p < 0.0001) (Fig. 3b). Female patients had more messages on scheduling, lab order/result, thyroid/hormone, medication, symptom, imaging/surgery, and mental health/sleep compared to male patients (all, p < 0.0001). Males had more messages on meal/diet and device/sensor than females (both, p < 0.0001) (Fig. 3c). Unmarried patients sought more advice on insurance/coverage (p = 0.001) and device/sensor (p < 0.0001) topics than their married counterparts (Fig. 3d). a RateA = The number of patients in subgroup A who had messages on the topic X /Total number of patients in subgroup A; a Race: non-White includes Asians, Blacks, Native Americans, Other, and Pacific Islanders; b Ethnicity (Hispanic vs. non-Hispanic); c Sex (Female vs. Male); d Marital status: Unmarried includes divorced, life partner, other, separated, single, and widowed; * When the rate difference is significant (p < 0.05). We showcased and assessed clinicians  perceptions of potential AI tools that can support clinicians in providing tailored patient care, reflecting patients  pressing issues defined from 528,199 patient portal messages from those with diabetes. Perceived highly useful AI tools were, including assisting expedited administrative processes (e.g., drafting templates of authorization letters) and patient education (e.g., creating educational materials for common lab inquiries and customizable responses for glucose monitoring and pump usage). These AI tools may help clinicians with timely interaction and streamlined support, yet in an efficient manner. Meanwhile, AI tools that directly handle patient data were perceived as risky (e.g., synthesizing patient data for message triage by urgency and topic). This study proposes assorted AI applications as clinical assistance tailored to patients  needs which were substantiated by clinicians  evaluations. Our work contributes to offering critical ramifications for the development and advancement of potential AI tools for precision diabetes care. Meal and dietary concerns related to blood glucose and insulin were the most actively discussed clinical topics, indicating an ongoing desire for patients with diabetes for support. As CGM devices have been widely adopted in the past few years, personalized nutrient suggestions based on real-time blood glucose levels may improve PCC27. Moreover, digital platform-based dietary suggestions were found to be acceptable and effective for glycemic control and weight loss among patients with diabetes28. Aligned with existing knowledge, in our study, clinicians perceived it as highly useful for AI-assisted carb-counting tools and personalized nutrition advice tailored to CGM and patient-reported data. A holistic approach that can account for all the essential data (e.g., CGM, dietary habits and lifestyles, medications, and insulin use) could be ultimately optimal. It can be started with AI tools that prepare evidence-based yet easy-to-understand educational materials on the impact of meal timing and nutrient composition with real-life examples to empower patients. This may help them understand those important relationships and have more autonomy in dietary management. Additionally, future studies may want to further develop AI tools that can craft meal plans for individuals with specific dietary restrictions to enhance tailored patient care and embrace diversity. Not surprisingly, AI s administrative and operational assistance was perceived as useful. Despite the existence of a separate scheduling channel, appointment-related issues were still the most common in the medical advice request channel. This highlights the need to reinforce the existing scheduling system. Perhaps a real-time interactive assistant could soon triage scheduling queries and efficiently schedule patient visits in the patient portal. Moreover, this AI-enabled conversational agent may especially help those with limited proficiency in direct scheduling, including older adults and non-English speakers, which could narrow the digital divide and possibly reduce related health access differences29. In addition, referral requests to other specialists, including ophthalmologists, podiatrists, or rheumatologists, for complications associated with diabetes may be streamlined. Lastly, real-time resource updates, including policy changes and medication supplies, could also be worth pursuing further. During the widespread shortage of glucagon-like peptide receptor agonizts (GLP-1RA) medications in the past few years, patients were exposed to the risks of falsified medications and unreasonable consequences30. AI-enhanced prediction models based on past medication use and supply could help avoid future imbalances of supply and demand, creating an environment for equitable access to demanding medications like GLP-1RA products. Notably, thyroid hormone messages were common and distinct from other topics in visualization, which demonstrates thyroid-related messages were likely silo and independent. This could highlight that patients with diabetes may have benefited from a separate communication channel specialized for thyroid hormone in the patient portal. Caring for thyroid dysfunction is important to prevent further complications associated with diabetes. Uncontrolled TSH levels were related to the increased risk of developing other complications in the eyes and kidneys31. We hypothesize that AI assistance could help triage messages, interpret TSH test results, and educate on normal ranges and levothyroxine dosages. Given that patients with diabetes have serious concerns about managing their hormone levels, particularly among female and white individuals in this study, further efforts to empower their self-care with high-quality information and guidance would be worth pursuing. Furthermore, evaluating patients  feedback and perspectives on the specific AI tool will also be essential. Furthermore, patients  primary concerns differed by race, ethnicity, sex, and marital status. White or female patients raised more concerns about bone-related issues than all other racial groups or males. Our findings align with previously reported risks of bone fracture among patients with diabetes, with higher risk in whites32 and women33. Through topic visualization, we observed that bone health issues were neighbored by pain, medication, and scheduling topics. Given that the combination of good glycemic control, medications for diabetes and osteoporosis, and a dietary and lifestyle-based approach is optimal for bone disease care in patients with diabetes, an interdisciplinary educational approach is important34. AI-driven automated Q&A for common queries for bone imaging and surgery and a decision-support tool for clinicians to optimize MRI referrals could be further refined for development. White and female patients shared another serious concern, mental distress (e.g., depression, anxiety, and sleep problems), compared to other racial groups or males. Given that mental health inequities by race and gender became wider during COVID-1935,36, proactive mental health screening or telepsychiatry-based support could be helpful, where AI assistance may step in as a frontline symptom screener along with contemporary efforts37. Future efforts could focus on targeted mental health support by harnessing AI tools for women as a start because women were found to use telehealth for mental health care more than men38. This study has several limitations. First, patient message data was from one academic institution. This limited variance of the source of the study population may restrain the generalizability of findings. However, we included messages from 22 affiliated health centers in northern California, and the race and ethnicity of included patients were diverse, with more than 50% non-White races and more than 11% Hispanic ethnicity. Second, we were unable to account for further SES for message analysis, including household income, education, and insurance which are important elements of social determinants of health (SDOH). However, this study provides the foundation for others to learn from and apply our results even with the minimal SDOH information available at the time of study. One of the study s future directions includes assessing patient messages from diverse data sources (e.g., other healthcare systems or geolocations, community clinics, or online forums) or longitudinally with comprehensive sociodemographic characteristics to enhance our understanding of patients  needs and issues that might differ by these elements. Third, we only captured the needs of patients communicating through secure messaging, while there might be other needs from patients not using the patient portal, which can add to the digital divide. Yet this study may have covered the majority of issues because patients with diabetes are highly active patient portal users. For instance, more than 70% already used secure messaging a decade ago20. In the future, assessing the needs of patients who are non-users of secure messaging should be considered to fill the knowledge gap. In conclusion, AI-powered analyses were able to comprehensively voice patients  needs and concerns, and we suggested various potential AI tools to assist clinicians in uniquely corresponding to such needs. Tailored patient support based on an enhanced understanding of their concerns may facilitate patient engagement and care, which is essential to achieving improved outcomes and lifelong management of diseases like diabetes. Demonstrated AI tools could expand the scope of AI s use in tailored patient care that is not limited to diabetes. We obtained patient portal messages of individuals with diabetes from a large academic hospital (Stanford Health Care) and 22 multiple affiliated centers in the Bay Area, July 2013 to April 2024. We defined patients with diabetes, including type I and type II diabetes, using the ICD-10 codes. We excluded patients who had more than one ICD-10 code to focus on specifying patient groups with diabetes as a primary condition. Through the portal, patients can specify the purpose of sending messages by scheduling, insurance, general questions, or medical advice. PMAR allows patients to bring up health issues to discuss with and obtain optimal medical advice from clinicians. This was secondary analysis, no human participants were involved, hence, no informed consent was needed. The Institutional Review Board at Stanford University approved this study. We performed topic modeling to identify the primary concerns and issues addressed through the patient portal messages. By applying two widely used NLP approaches, we intended to extrinsically validate the primary model s performance and generate different types of knowledge from the two models. Additionally, we stratified the messages by time, 2013 2020 and 2020 2024, to explore if there were unique topics during the pandemic. First, to overview the patient message topics, we conducted unsupervised topic modeling. For text analysis, we first removed unwanted patterns and excessive whitespace, cleaned up irrelevant characters and formatting codes from raw message data, then tokenized the cleaned text and converted the words to their base forms. We then transformed into sentences by a pre-calculated embedding model (all-miniLM-L6-v2)39. This model was pre-trained with over 600 million social media posts and 12 million medical journals. We used uniform mapping and approximation and projection (UMAP) to simplify embeddings and removed stop words and infrequent words through the ConvectVectorizer model. We applied cTF-IDF to weigh the words by frequency and significance, and the K-means model to generate the optimal number of clusters based on the silhouette coefficient and Davies Bouldin index, computational measurements that indicate the quality of clustering determined by similarity and overlapping40,41. To visualize these topics, we created word clouds, which consist of the top ten keywords for each cluster. The larger keywords represent greater frequency. Second, we performed semi-supervised topic modeling using the BERTopic package, which leverages BERT techniques42. In this model, another pre-calculated embedding model was used for sentence transformation (thenlper/gte-small), which was pre-trained for general purposes of text analysis43. This model encodes each message as a 384-vector number. We also applied UMAP and CounterVectorizor models to lower the dimensionality of embeddings and remove stopwords. For clustering, we took a zero-shot approach by providing pre-defined keywords for the topic model to cluster similar topics to the given keywords. We set the semantic similarity threshold as 0.82, and messages meeting this threshold, computed by cosine similarity, were categorized to the corresponding topic. The benefit of this approach includes the ability to capture messages of our interest even though those may not contain the exact keywords that we provided. To visualize the distribution of representative messages, we plotted low-dimensional UMAP by topics, in which we randomly sampled 12,000 messages from patients with diabetes. Additionally, the remaining messages that were not categorized into the pre-determined topics created their own clusters of new topics based on similarity. This enabled us to comprehend novel topics that we may need to pay attention to. Pre-defined topics and keywords were obtained from our first unsupervised topic modeling (e.g., Scheduling [ appointment,   schedule,   canceling,   available, ] Device/sensor [ freestyle,   sensor,   dexcom,   data,   CGM,   pump.   meter. ]) When the keyword repeatedly appeared in more than one cluster, we assigned the keyword to the group with significance determined by the size of the word in a word cloud. A full list of pre-defined topics and keywords is available in Supplementary Note 1. To enhance the interpretability of extracted topics, we applied a widely used large language model/generative AI (ChatGPT-4, OpenAI Inc.). We input the word clouds from our first NLP model into generative AI and directed it to perform two tasks: First, summarize the main issues and provide a title for each word cloud. Second, offer suggestions on how AI can assist healthcare professionals in providing personalized patient support. To improve the information quality, we used a prompt engineering strategy, which consists of multiple techniques44,45: 1) role prompting (e.g., as Dr. GPT, a professional endocrinologist), 2) directive commanding (e.g., First, summarize the primary issues; Second, suggest the title; Third, provide two or three suggestions), 3) expertize simulation (e.g., I myself am an endocrinologist in the hospital), 4) zero-shot chain of thought (e.g., take time to think deeply and step-by-step to be sure). The full-engineered prompts are available in Supplementary Note 2. Five experienced endocrinologists with various levels of experience and sub-specialty areas (C.D., S.H.K., R.A.L., S.M.S, and T.A.) independently evaluated the information that LLM provided: 1) their agreement with AI s interpretation about patients  primary concerns, and 2) perceived usefulness and 3) perceived risk of suggested AI s roles in assisting healthcare professionals in providing tailored patient support. We required the assessors to read the given instructions and assessment protocol thoroughly and follow it strictly. For perceived risk assessment, we adapted the AI Risk Management Framework from the National Institute of Standards and Technology46. Endocrinologists used the 5-point Likert scale for agreement (1-disagree; 5-agree), perceived usefulness (1-not useful; 5-useful), and perceived risk (1-Strongly agree with the risk of harm; 5-Strongly disagree with the risk of harm). To compute the mean and standard deviation (SD) for each topic, we averaged out five endocrinologists  scores, following an ensemble approach used when there is no ground truth to compare47. In this approach, the mean represents the degree of agreement among the assessors, and SD shows the uncertainty48. The guided protocols for clinicians  assessment and scores are available in Supplementary Tables 1 4, respectively. To investigate if the primary concerns and issues differ by patient demographics, we stratified patient characteristics, including sex (female vs male), race (White vs non-White including Asian, Black, Native American/Pacific Islander, and Other), ethnicity (Hispanic vs non-Hispanic), marital status (married vs unmarried, including single, divorced, separated, widowed, life partner, and other) for each message topic. Then, we calculated the rates of sending messages on specific topics by patient characteristics and compared them to identify demographic subgroups that addressed specific concerns more (e.g., RateA = The number of patients in subgroup A who had messages on the topic X /Total number of patients in subgroup A). Moreover, to investigate if the addressed concerns differed during the COVID-19 pandemic, we also stratified the analysis by time, before (2013 2020) and during COVID-19 (2020 2024). We applied two proportions z-test for quantitative comparisons of rates by demographics and obtained 95% confidence intervals (95% CIs) and p-values. Statistical significance was determined at a p < 0.05 level using Python 3.10 in Google Colab (Mountain View, CA, USA). The extended data, which were the further results of our analyses from the current study, are available from the corresponding author upon reasonable request. However, the original patient message dataset will not be shared to protect privacy. The underlying code for this study will be made available through a public website (GitHub) upon publication. Lancet, T. Diabetes: a defining disease of the 21st century. Lancet 401, 2087 (2023). Article Google Scholar Harding, J. L., Pavkov, M. E., Magliano, D. J., Shaw, J. E. & Gregg, E. W. Global trends in diabetes complications: a review of current evidence. Diabetologia 62, 3 16 (2019). Article PubMed Google Scholar Rustad, J. K., Musselman, D. L. & Nemeroff, C. B. The relationship of depression and diabetes: pathophysiological and treatment implications. Psychoneuroendocrinology 36, 1276 1286 (2011). Article PubMed Google Scholar Khaledi, M., Haghighatdoost, F., Feizi, A. & Aminorroaya, A. The prevalence of comorbid depression in patients with type 2 diabetes: an updated systematic review and meta-analysis on huge number of observational studies. Acta Diabetol. 56, 631 650 (2019). Article PubMed Google Scholar Dal Canto, E. et al. Diabetes as a cardiovascular risk factor: an overview of global trends of macro and micro vascular complications. Eur. J. Prev. Cardiol. 26, 25 32 (2019). Article PubMed Google Scholar Hills, A. P. et al. Epidemiology and determinants of type 2 diabetes in South Asia. Lancet Diab. Endocrinol. 6, 966 978 (2018). Article Google Scholar Powers, M. A. et al. Diabetes self-management education and support in adults with type 2 diabetes: a consensus report of the american diabetes association, the association of diabetes care & education specialists, the academy of nutrition and dietetics, the American Academy of Family Physicians, the American Academy of PAs, the American Association of Nurse Practitioners, and the American Pharmacists Association. Diabetes Care 43, 1636 1649 (2020). Article PubMed Google Scholar Kupfer, J. M. & Bond, E. U. Patient satisfaction and patient-centered care: necessary but not equal. JAMA 308, 139 140 (2012). Article CAS PubMed Google Scholar Bierman, A. S. & Tinetti, M. E. Precision medicine to precision care: managing multimorbidity. Lancet 388, 2721 2723 (2016). Article PubMed Google Scholar Williams, J. S., Walker, R. J., Smalls, B. L., Hill, R. & Egede, L. E. Patient-centered care, glycemic control, diabetes self-care, and quality of life in adults with type 2 diabetes. Diabetes Technol. Ther. 18, 644 649 (2016). Article PubMed PubMed Central Google Scholar Davies, M. J. et al. Management of hyperglycemia in type 2 diabetes, 2018. a consensus report by the American Diabetes Association and the European Association for the study of diabetes. Diabetes Care 41, 2669 2701 (2018). Article PubMed PubMed Central Google Scholar Rutten, G. E. H. M., Van Vugt, H. & de Koning, E. Person-centered diabetes care and patient activation in people with type 2 diabetes. BMJ Open Diabetes Res. Care 8, e001926 (2020). Article PubMed PubMed Central Google Scholar Asmat, K., Dhamani, K., Gul, R. & Froelicher, E. S. The effectiveness of patient-centered care vs. usual care in type 2 diabetes self-management: a systematic review and meta-analysis. Front. Public Health 10, 994766 (2022). Article PubMed PubMed Central Google Scholar Wade-Vuturo, A. E., Mayberry, L. S. & Osborn, C. Y. Secure messaging and diabetes management: experiences and perspectives of patient portal users. J. Am. Med. Inform. Assoc. 20, 519 525 (2013). Article PubMed Google Scholar Holmgren, A. J. et al. Assessing the impact of the COVID-19 pandemic on clinician ambulatory electronic health record use. J. Am. Med. Inform. Assoc. 29, 453 460 (2022). Article PubMed Google Scholar Sun, R., Blayney, D. W. & Hernandez-Boussard, T. Health management via telemedicine: learning from the COVID-19 experience. J. Am. Med. Inform. Assoc. JAMIA 28, 2536 2540 (2021). Article PubMed Google Scholar Kumar, A. et al. Is diabetes mellitus associated with mortality and severity of COVID-19? A meta-analysis. Diabetes Metab. Syndr. 14, 535 545 (2020). Article PubMed PubMed Central Google Scholar Brands, M. R. et al. Patient-centered digital health records and their effects on health outcomes: systematic review. J. Med. Internet Res. 24, e43086 (2022). Article PubMed PubMed Central Google Scholar Sarraju, A. et al. Identifying reasons for statin nonuse in patients with diabetes using deep learning of electronic health records. J. Am. Heart Assoc. 12, e028120 (2023). Article PubMed PubMed Central Google Scholar Chung, S., Panattoni, L., Chi, J. & Palaniappan, L. Can secure patient-provider messaging improve diabetes care? Diabetes Care 40, 1342 1348 (2017). Article PubMed Google Scholar Somani, S., van Buchem, M. M., Sarraju, A., Hernandez-Boussard, T. & Rodriguez, F. Artificial intelligence enabled analysis of statin-related topics and sentiments on social media. JAMA Netw. Open 6, e239747 (2023). Article PubMed PubMed Central Google Scholar Uncovska, M., Freitag, B., Meister, S. & Fehring, L. Rating analysis and BERTopic modeling of consumer versus regulated mHealth app reviews in Germany. NPJ Digit. Med. 6, 115 (2023). Article PubMed PubMed Central Google Scholar Rajpurkar, P., Chen, E., Banerjee, O. & Topol, E. J. AI in health and medicine. Nat. Med. 28, 31 38 (2022). Article CAS PubMed Google Scholar Beam, K. et al. Performance of a large language model on practice questions for the neonatal board examination. JAMA Pediatr. 177, 977 979 (2023). Article PubMed PubMed Central Google Scholar Strong, E. et al. Chatbot vs medical student performance on free-response clinical reasoning examinations. JAMA Intern. Med. 183, 1028 1030 (2023). Article PubMed PubMed Central Google Scholar Kim, J., Cai, Z. R., Chen, M. L., Simard, J. F. & Linos, E. Assessing biases in medical decisions via clinician and AI chatbot responses to patient vignettes. JAMA Netw. Open 6, e2338050 (2023). Article PubMed PubMed Central Google Scholar Martens, T. et al. Effect of continuous glucose monitoring on glycemic control in patients with type 2 diabetes treated with basal insulin: a randomized clinical trial. JAMA 325, 2262 2272 (2021). Article CAS PubMed Google Scholar Lee, Y.-B. et al. An integrated digital health care platform for diabetes management with ai-based dietary management: 48-week results from a randomized controlled trial. Diabetes Care 46, 959 966 (2023). Article PubMed Google Scholar Ganguli, I., Orav, E. J., Lupo, C., Metlay, J. P. & Sequist, T. D. Patient and visit characteristics associated with use of direct scheduling in primary care practices. JAMA Netw. Open 3, e209637 (2020). Article PubMed PubMed Central Google Scholar WHO issues warning on falsified medicines used for diabetes treatment and weight loss. https://www.who.int/news/item/20-06-2024-who-issues-warning-on-falsified-medicines-used-for-diabetes-treatment-and-weight-loss (WHO, 2024). Manshahia, P. K. et al. Systematic review to gauge the effect of levothyroxine substitution on progression of diabetic nephropathy in patients with hypothyroidism and type 2 diabetes mellitus. Cureus 15, e44729 (2023). PubMed PubMed Central Google Scholar Sellmeyer, D. E. et al. Skeletal metabolism, fracture risk, and fracture outcomes in type 1 and type 2 diabetes. Diabetes 65, 1757 1766 (2016). Article CAS PubMed PubMed Central Google Scholar Ferrari, S. L. et al. Diagnosis and management of bone fragility in diabetes: an emerging challenge. Osteoporos. Int. 29, 2585 2596 (2018). Article CAS PubMed PubMed Central Google Scholar Wu, B. et al. A narrative review of diabetic bone disease: characteristics, pathogenesis, and treatment. Front. Endocrinol. 13, 1052592 (2022). Article Google Scholar Patel, V., Fancourt, D., Furukawa, T. A. & Kola, L. Reimagining the journey to recovery: the COVID-19 pandemic and global mental health. PLOS Med 20, e1004224 (2023). Article PubMed PubMed Central Google Scholar Kim, J. et al. Prevalence and associations of poor mental health in the third year of COVID-19: U.S. population-based analysis from 2020 to 2022. Psychiatry Res. 330, 115622 (2023). Article PubMed Google Scholar Swaminathan, A. et al. Natural language processing system for rapid detection and intervention of mental health crisis chat messages. npj Digit. Med. 6, 1 9 (2023). Article Google Scholar Kim, J. et al. Telehealth utilization and associations in the united states during the third year of the covid-19 pandemic: population-based survey study in 2022. JMIR Public Health Surveill. 10, e51279 (2024). Article PubMed PubMed Central Google Scholar sentence-transformers/all-MiniLM-L6-v2   Hugging Face. https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2 (2024). Rousseeuw, P. J. Silhouettes: a graphical aid to the interpretation and validation of cluster analysis. J. Comput. Appl. Math. 20, 53 65 (1987). Article Google Scholar Davies, D. L. & Bouldin, D. W. A cluster separation measure. IEEE Trans. Pattern Anal. Mach. Intell. PAMI-1, 224 227 (1979). Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training of deep bidirectional transformers for language understanding. in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (eds. Burstein, J., Doran, C. & Solorio, T.) 4171 4186 (Association for Computational Linguistics, Minneapolis, Minnesota, 2019). https://doi.org/10.18653/v1/N19-1423. thenlper/gte-small   Hugging Face. https://huggingface.co/thenlper/gte-small. Leypold, T., Sch fer, B., Boos, A. & Beier, J. P. Can AI think like a plastic surgeon? Evaluating GPT-4 s clinical judgment in reconstructive procedures of the upper extremity. Plast. Reconstr. Surg. Glob. Open 11, e5471 (2023). Article PubMed PubMed Central Google Scholar Kojima, T. et al. Large language models are zero-shot reasoners. Adv. Neural Inf. Process. Syst. 35, 22199 22213 (2022). Google Scholar Artificial intelligence risk management framework (AI RMF 1.0) (NIST, 2023). Chang, N., Lee-Goldman, R. & Tseng, M. Linguistic wisdom from the crowd. Proc. AAAI Conf. Hum. Comput. Crowdsour. 3, 1 8 (2015). CAS Google Scholar Subjectivity, ambiguity and disagreement in crowdsourcing. https://www.aconf.org/conf_160152.html (SAD, 2018). Download references J.K. is supported by the NIH (K01MH137386). E.L. is supported by the NIH (grants R01AR082109 and K24AR075060). The content is solely the responsibility of the authors and does not necessarily represent the official views of the NIH. The funding organizations had no role in the design and conduct of the study; collection, management, analysis, and interpretation of the data; preparation, review, or approval of the manuscript; and decision to submit the manuscript for publication. Stanford Center for Digital Health, Department of Medicine, Stanford University, Palo Alto, CA, USA Jiyeong Kim, Michael L. Chen, Shawheen J. Rezaei & Eleni Linos Department of Biomedical Data Science, School of Medicine, Stanford University, Stanford, CA, USA Tina Hernandez-Boussard Stanford Center for Biomedical Informatics Research, School of Medicine, Stanford University, Palo Alto, CA, USA Tina Hernandez-Boussard, Jonathan H. Chen & Summer S. Han Division of Cardiovascular Medicine and Cardiovascular Institute, Department of Medicine, Stanford University, Stanford, CA, USA Fatima Rodriguez Division of Endocrinology, Gerontology, and Metabolism, Department of Medicine, Stanford University, Stanford, CA, USA Rayhan A. Lal, Sun H. Kim, Chrysoula Dosiou, Susan M. Seav & Tugce Akcan Department of Psychiatry and Behavioral Sciences, School of Medicine, Stanford University, Palo Alto, CA, USA Carolyn I. Rodriguez Veterans Affairs Palo Alto Health Care System, Palo Alto, CA, USA Carolyn I. Rodriguez Division of Primary Care and Population Health, Department of Medicine, Stanford University, Stanford, CA, USA Steven M. Asch Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar J.K. and E.L. had full access to all the data in this study and take responsibility for the integrity of the data and the accuracy of the data analysis. J.K. conceived and designed the study. J.K. and E.L. obtained the data, and J.K. and M.L.C. developed the analytic codes for analysis. J.K., M.L.C., S.J.R., and S.S.H. analyzed and interpreted the results. R.A.L., S.H.K., C.D., S.M.S., and T.A. provided material supports. J.H.C., T.H., F.R., C.I.R., and S.M.A. provided clinical insights into the study, critically reviewed, and conducted several rounds of revisions. E.L. supervised the study. All authors read and approved the final version of the manuscript and agreed to submit it for publication. Correspondence to Jiyeong Kim. In the last 3 years, CIR has served as a consultant for Biohaven Pharmaceuticals, Osmind, and Biogen; and receives research grant support from Biohaven Pharmaceuticals, a stipend from American Psychiatric Association Publishing for her role as Deputy Editor at The American Journal of Psychiatry, and book royalties from American Psychiatric Association Publishing. The other authors declare no competing interests. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions Kim, J., Chen, M.L., Rezaei, S.J. et al. Artificial intelligence tools in supporting healthcare professionals for tailored patient care. npj Digit. Med. 8, 210 (2025). https://doi.org/10.1038/s41746-025-01604-3 Download citation Received: 30 September 2024 Accepted: 01 April 2025 Published: 16 April 2025 DOI: https://doi.org/10.1038/s41746-025-01604-3 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Sign up for the Nature Briefing: Translational Research newsletter   top stories in biotechnology, drug discovery and pharma.",2
Mapping the use of artificial intelligence in medical education: a scoping review - BMC Medical Education,https://news.google.com/rss/articles/CBMifEFVX3lxTFA2V0o0NzlmM001WXp0ekpkRXQ3M0RNOWZsdGVlVy1zSUJLN0tpSzVra044RzRONTFzcUV6SnprTUhMSmFFejVNRjB1UE9FeTlqZjNuZ0h3TnVBWGdiX1dvSVhFSG0zLTFrbFlOcU5iSHRyM0QxaEVxbkNqNEc?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Advertisement BMC Medical Education volume 25, Article number: 526 (2025) Cite this article 5584 Accesses 2 Citations 7 Altmetric Metrics details The integration of artificial intelligence (AI) in healthcare has transformed clinical practices and medical education, with technologies like diagnostic algorithms and clinical decision support increasingly incorporated into curricula. However, there is still a gap in preparing future physicians to use these technologies effectively and ethically. This scoping review maps the integration of artificial intelligence (AI) in undergraduate medical education (UME), focusing on curriculum development, student competency enhancement, and institutional barriers to AI adoption. A comprehensive search in PubMed, Scopus, and BIREME included articles from 2019 onwards, limited to English and Spanish publications on AI in UME. Exclusions applied to studies focused on postgraduate education or non-medical fields. Data were analyzed using thematic analysis to identify patterns in AI curriculum development and implementation. A total of 34 studies were reviewed, representing diverse regions and methodologies, including cross-sectional studies, narrative reviews, and intervention studies. Findings revealed a lack of standardized AI curriculum frameworks and notable global discrepancies. Key elements such as ethical training, collaborative learning, and digital competence were identified as essential, with an emphasis on transversal skills that support AI as a tool rather than a standalone subject. This review underscores the need for a standardized, adaptable AI curriculum in UME that prioritizes transversal skills, including digital competence and ethical awareness, to support AI's gradual integration. Embedding AI as a practical tool within interdisciplinary, patient-centered frameworks fosters a balanced approach to technology in healthcare. Further regional research is recommended to develop frameworks that align with cultural and educational needs, ensuring AI integration in UME promotes both technical and ethical competencies. Peer Review reports In recent decades, the integration of digital technologies in healthcare, commonly referred to as E-Health, has revolutionized the delivery of medical services [1]. E-Health encompasses a wide range of tools, including electronic health records, mobile health applications, telehealth, and remote monitoring platforms, all of which have significantly improved the efficiency of healthcare systems and optimized patient care globally [2]. These advancements have enhanced accessibility and personalization of healthcare, marking a significant shift in how patients and healthcare professionals interact with information and services [3, 4]. The COVID- 19 pandemic further accelerated the adoption of telehealth, including teleconsultations and remote monitoring, which emerged as essential components of healthcare delivery [5]. Besides expanding access to care, telehealth has provided new opportunities for medical education, allowing students to gain clinical experience through remote simulations and consultations, regardless of geographical constraints [6]. Amid this ongoing digital transformation, artificial intelligence (AI) has become a transformative tool in healthcare, with applications ranging from clinical decision support systems to diagnostic algorithms and personalized treatment solutions [7,8,9]. AI refers to the use of computational algorithms and machine learning techniques that enable systems to analyze data, recognize patterns, and perform decision-making tasks traditionally carried out by humans. In medical education, AI encompasses technologies such as clinical decision support systems, intelligent tutoring systems, and natural language processing tools that enhance learning by providing personalized feedback and improving diagnostic reasoning [9]. AI has demonstrated the capacity to analyze vast amounts of health data and generate precise predictions, improving both efficiency and accuracy in clinical care [10]. In specialties like radiology, AI has shown potential to perform comparably to experienced clinicians, suggesting that it could complement healthcare delivery and decision-making processes [11]. However, the rapid integration of AI also necessitates an evolution in the competencies required of healthcare professionals. As AI becomes an indispensable part of medical practice, it is crucial for future physicians to develop the skills necessary to use these technologies effectively and ethically [12]. AI is increasingly used in both clinical decision-making and medical education, but these applications serve distinct purposes. In clinical settings, AI supports tasks such as diagnostic imaging, predictive analytics, and patient management by assisting healthcare professionals in decision-making. In contrast, in medical education, AI is used primarily to enhance learning through technologies such as intelligent tutoring systems (ITS), virtual simulations, adaptive learning platforms, and automated assessment tools. While some AI-driven tools, like clinical decision support systems (CDSS), may overlap, this review specifically focuses on AI s role in enhancing medical education rather than its direct application in patient care [7,8,9]. The World Health Organization (WHO) has established ethical principles to guide AI applications in healthcare, emphasizing the importance of human autonomy, transparency, and accountability to ensure AI benefits patients and communities while minimizing risks associated with surveillance, discrimination, and inequitable access [13]. In the context of medical education, AI is already enhancing learning experiences and preparing students for an AI-driven healthcare environment. Specific applications include Intelligent Tutoring Systems that provide personalized learning for decision-making, AI-assisted learner assessments using Latent Semantic Analysis and Natural Language Processing to grade and provide feedback on clinical cases, and AI-powered Chatbots that help students review medical literature, prepare for exams like the United States Medical Licensing Examination (USMLE), and develop critical thinking and communication skills [14]. The incorporation of these digital learning tools such as clinical simulations, online classes, and collaborative platforms has already enhanced the accessibility of medical education globally [15]. Moreover, AI-driven personalized learning platforms create adaptive learning paths for students, while Virtual Reality simulations powered by AI offer surgical training experiences that were previously limited to in-person settings, contributing to skill development without geographical constraints [14]. Despite these advancements, the integration of AI into medical curricula remains insufficient, with most programs failing to address the specific competencies required for students to navigate AI-driven clinical environments effectively. This has created a critical gap in medical education, leaving future healthcare professionals underprepared to utilize AI technologies in practice and to address their associated ethical and operational challenges [16]. In several academic circles, it is suggested that rather than training specific AI competencies, the focus should be on strengthening transversal skills, including autonomous learning, to facilitate the gradual incorporation of emerging technologies and other evolving components [17, 18]. Despite the increasing use of AI in clinical practice, medical education has often struggled to keep pace with these advances. To address this gap, this scoping review maps the integration of AI in undergraduate medical education (UME), focusing on three key areas: (1) the development and implementation of AI-related curricula, (2) the impact of AI-driven education on student competency development, and (3) institutional and systemic barriers that hinder AI adoption. By identifying challenges and emerging trends, this review aims to provide insights into the current state of AI education in UME and propose evidence-based recommendations for its effective integration. A scoping review was conducted in October 2024, following the methods of Arksey et al. [19] and Levac et al. [20], and reported according to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) Extension for Scoping Reviews [21]. The research question guiding this scoping review is: How is artificial intelligence (AI) currently integrated into undergraduate medical education (UME), and what are the key challenges and trends in AI-related curriculum development, student competency enhancement, and institutional adoption? The Arksey and O Malley framework involves five key steps [19]: identifying the research question, identifying relevant studies, selecting studies, charting the data, collating, summarizing, and reporting results. This review adhered to the five steps, with a structured approach to ensure methodological rigor. The research question was defined to explore the role of artificial intelligence in undergraduate medical education, guiding the search strategy and inclusion criteria. Relevant studies were identified through systematic searches in selected databases, and the inclusion process was detailed in Sect. 2.3. The data charting stage involved extracting key information on study objectives, methods, findings, and implications, ensuring consistent and comprehensive documentation. The results were then analyzed and synthesized as described in Sect. 2.5. Studies were identified by searching PubMed, Scopus, and BIREME for articles pertinent to our research question. The search covered studies from 2019 onwards, with a combination of Medical Subject Headings (MeSH) and Health Science Descriptors (DeCS), including terms such as:  Artificial Intelligence""[MeSH] AND""Education, Medical""[MeSH]. The search was restricted to publications in English and Spanish from January 1, 2019, onwards to ensure both the relevance of the data and a wider geographical inclusion. The choice of these three databases was based on their relevance to the study's scope and their comprehensive coverage of medical and health sciences literature. PubMed was selected for its focus on biomedical research, Scopus for its interdisciplinary content and broader citation data, and BIREME for its inclusion of Latin American and Caribbean studies, ensuring geographical diversity. Additional relevant studies were located by hand-searching the reference lists of the included articles, and the search was updated regularly to include any new research until the completion of the review (Fig. 1). Methods flow chart All retrieved citations were imported into Rayyan [22], an online tool designed for managing systematic reviews, for efficient study selection and data management. Study selection was performed in three steps (Fig. 1): Initial Screening: Titles and abstracts were independently screened by three reviewers, DJ, JP, and LC, using Rayyan to determine eligibility based on predefined inclusion and exclusion criteria. Any disagreements were resolved through discussion. Criteria Refinement: After the initial screening, the inclusion and exclusion criteria were refined post hoc to ensure the selection process accurately aligned with the research objectives. Full-Text Screening: Eligible articles were then assessed at the full-text level by two reviewers (DJ, LC). Articles were included if they broadly discussed AI in undergraduate medical education (UME). Studies were included if they: Discussed the integration of artificial intelligence in undergraduate medical education. Included primary research, perspective, or commentary articles with full-text availability. Addressed AI within the UME context, specifically targeting machine learning, deep learning, natural language processing, clinical decision support systems, intelligent tutoring systems, and AI-driven simulation tools. Studies were excluded if they: Focused solely on postgraduate or continuing medical education. However, studies discussing AI applications that are relevant to both UME and postgraduate training such as clinical decision-support systems were included if they explicitly addressed their role in undergraduate education. Studies primarily focused on AI for clinical decision-making (e.g., AI-assisted diagnostics, treatment recommendations, or patient monitoring) were excluded unless they explicitly addressed their role in medical education. Discussed the application of AI for allied health professionals or osteopathic medicine. Excluded studies that addressed AI in the context of patient education. Were conference abstracts or lacked full-text availability. Data extraction was performed using a structured data charting form (Fig. 2). The form, developed iteratively and tested by the review team, aimed to capture essential elements. The charting form was refined throughout the process to ensure consistency and relevancy of data extracted. The data extraction process was conducted by three reviewers (DJ, LC, and JP), who extracted data from all full-text articles. Any disagreements during data extraction were resolved through consensus discussions. The data charting was conducted iteratively, allowing continuous refinement of the form to better capture emerging themes in the literature. Key themes and implications of artificial intelligence in undergraduate medical education The study demographics were summarized using descriptive statistics to provide an overview of the types, locations, and focus areas of the included studies, enabling the identification of trends and gaps. Thematic analysis was systematically applied to qualitative data to identify key themes. The process began with a thorough review of the data, assigning descriptive codes to capture recurring topics aligned with the study objectives. Two independent reviewers initially coded the data to ensure reliability and minimize bias. Descriptive codes were then clustered into broader pattern codes, consolidating findings into overarching themes. Any discrepancies in coding were resolved through discussion, with a third reviewer involved in cases where consensus was not reached. This refinement process was iterative, ensuring themes accurately represented the findings while aligning with the research focus on AI in undergraduate medical education (UME). To enhance reliability and comparability, themes were visualized using thematic matrices, allowing for a systematic identification of similarities, differences, and overarching patterns across studies. The thematic structure was reviewed by all authors to ensure consistency and coherence in relation to the study s objectives. Additionally, to assess the inter-reviewer agreement, Cohen s kappa coefficient was calculated at key stages of the review process: Cohen s kappa was applied to measure agreement between reviewers during the title/abstract and full-text screening stages, achieving a final inter-rater agreement of 0.85, indicating near perfect agreement. A subset of studies was independently analyzed by two reviewers to assess coding reliability. The agreement on initial codes was quantified (kappa = 0.82), and discrepancies were resolved through discussions. All themes were cross-checked by multiple reviewers to confirm coherence and relevance, ensuring a robust interpretation of findings. Our search identified 7,870 records, of which 34 full-text articles were included in our final analysis. The most prevalent study type was the cross-sectional study (n = 6, 17.6%), followed by narrative reviews (n = 5, 14.7%). Interdisciplinary teaching and research projects, as well as educational intervention studies, each accounted for 4 articles (11.8% each). Opinion articles made up 8.8% of the studies (n = 3). Other study types, including conceptual analysis, pedagogical discussions, exploratory case studies, commentary, and quasi-experimental studies, each represented 5.9% of the total with 2 studies per category. Additionally, e-Delphi studies, prospective analyses, and qualitative exploratory studies were less common, each comprising 2.9% of the total with 1 study in each category. In terms of geographical distribution, as shown in Fig. 3, the United States contributed the highest number of studies (n = 6, 15%), followed by Canada (n = 4, 10%) and Australia (n = 3, 7.5%). There was also representation from countries such as Germany, the Netherlands, Turkey, China, and the UK, each with 2 studies (5%). Single studies came from various locations, including Oman, Bahrain, Malaysia, Brunei, Mexico, Nigeria, India, Chile, Poland, Egypt, and Taiwan (2.5% each). Distribution of studies by location The publication years varied, as illustrated in Fig. 4, with the majority of studies from 2024 (n = 11, 27.5%) and 2023 (n = 10, 25%). Other years included 2022 (n = 7, 17.5%), 2020 (n = 4, 10%), and 2019 (n = 2, 5%). This diverse range of studies highlights the varied methodological approaches and global representation in the examination of artificial intelligence in medical education. Distribution of studies by year of publication Five studies [23,24,25,26,27] explored the impact of AI on medical education, emphasizing the need for interdisciplinary learning by integrating AI concepts into medical curricula. These studies highlighted the importance of collaboration across fields such as medicine, natural sciences, and technology [27]. A recurring concern was the need to balance technological and human aspects of medicine, ensuring that AI supports patient-centered care rather than replacing it with algorithmic decision-making [26]. Some studies also noted that AI could enhance training efficiency and medical knowledge acquisition, making education more accessible [24]. Furthermore, the integration of AI in curricula was seen as vital to equip students with digital literacy and skills necessary for effective patient collaboration [23]. Six studies [28,29,30,31,32] discussed the integration of AI competencies into medical curricula, suggesting they become a mandatory component of medical education to prepare physicians for AI-based clinical decision-making [28]. A staged model was recommended, where students learn foundational AI concepts in pre-clinical years and apply them in clinical contexts later [29]. The studies also emphasized ethical education, highlighting the need to teach students about patient confidentiality, algorithmic biases, and informed consent [30]. Training programs must ensure that medical students use AI technologies responsibly, aligning with legal and ethical standards [31]. Preparing students to navigate AI s role while maintaining ethical and humanistic care was deemed essential [32]. Four studies [33,34,35,36,37] examined medical students  attitudes towards AI across different regions. In Nigeria, students expressed both excitement and concern, fearing that AI could undermine physicians'skills [35]. In Canada, students saw AI as crucial for their careers but felt unprepared, advocating for more formal training [37]. In India, students showed interest in AI but worried about losing empathy due to overreliance on technology [34]. Similarly, UK students recognized AI s potential in radiology but feared it might limit career opportunities [36]. Arab students, despite limited exposure to AI, supported its inclusion in curricula, especially in radiology [33]. These varied perceptions highlight global recognition of AI s impact and the need for tailored educational approaches. Twelve of the 34 articles reviewed highlight the usefulness of AI in various areas of medical education, such as plagiarism detection, clinical simulations and homework support [38,39,40]. In addition, different authors mention that AI improves self-learning [41, 42] and interdisciplinary teaching, although it faces challenges in exam standardization [43] and requires human supervision to ensure accuracy [44, 45]. Numerous studies have analyzed the advantages of using chatbots such as ChatGPT in medical education [42, 43]. An innovative example of their application is described in the Canadian Medical Education Journal, where a medical data labeling program was implemented on lung ultrasound images. In this context, students were actively involved, which opened new possibilities for integrating AI into medical education [46]. Most studies highlight the need to train students in the effective and consistent use of AI. A key advantage of AI in medical education, particularly in the study of anatomy, is its ability to overcome limitations of the traditional approach, such as the scarcity of cadavers for dissection and the risks associated with handling toxic chemicals during embalming and dissection procedures [47]. Six reviewed articles address the ethical considerations surrounding the implementation of AI in medical education [29, 39, 44, 48, 49]. These studies agree on the importance of generating awareness in medical students about the lack of human essence in AI, highlighting the need to reinforce empathy and warmth in dealing with patients. In addition, two authors point out that AI often introduces information bias, which makes it necessary to counteract this problem by thoroughly reviewing the data that students acquire from these technologies. They also suggest that traditional teaching should not be discontinued until this shortcoming is significantly mitigated [29, 48]. From the available evidence, it was observed that 5 of the papers found sought to identify the role of AI in medical education assessment and assignment [33, 47, 50,51,52]. Among the advantages mentioned in basic medical science education, AI can be applied by giving feedback on anatomy practice, with simulated clinical anatomy sessions, creating videos for dissections and practices, and allowing online work [47]. AI via Machine Learning (ML) and Deep Learning (DL) can address the inherent limitations of traditional education methods to build a personalized, effective and fair educational landscape, functioning as a trainer or coach with predictive analytics [52]. On the other hand, the construction of a self-learning system mediated by artificial intelligence, allowed undergraduate students to access a friendly knowledge system, with a high degree of satisfaction and good performance in tests of quality and retention of information [51]. In addition, one of the most outstanding aspects lies in the capacity offered by AI to effectively address plagiarism based on a personalized and relevant educational model. Other challenges to be considered when discussing this aspect are privacy, transparency and the development of propagation bias [33, 47, 52]. We pooled the information available from 3 articles that provide a glimpse into the future of artificial intelligence in medical education [25, 34, 53]. It is interesting to note that there is so much evidence regarding the intervention of these digital systems in medical education that it is practically impossible for the human mind to summarize [25]. It is stated that to ensure the success of AI in medical education, it is crucial to foster interdisciplinary collaboration and increase investment in education and training [34]. In this respect, the integration of AI in education has not yet been described in a way that allows us to clearly glimpse its impact, although it has been shown that when the learner has to talk to an Intelligent Tutoring System (ITS), his understanding and handling of the subject increases, which allows us to imagine where the educational models will migrate to [53]. In addition, it outlines several challenges that must be overcome to achieve the implementation of artificial intelligence in medical education such as the complete overhaul of medical school curricula to provide a focus on the effective use of AI in medicine and the construction of a robust ethical framework to ensure patient safety, privacy and autonomy, promoting equity and inclusivity [25, 34]. This scoping review provides an comprehensive overview of the integration of artificial intelligence in UME, highlighting the diversity of approaches, ethical considerations, and global discrepancies. The review also identifies critical research gaps and limitations while proposing recommendations for enhancing AI curricula to meet the evolving needs of healthcare. The studies reviewed demonstrated considerable variability in AI curriculum development and delivery. Some focused on foundational AI concepts, such as machine learning and deep learning, introduced during pre-clinical years, while others employed a staged approach, integrating these concepts in clinical contexts [41, 45,46,47, 54]. This heterogeneity underscores the lack of a standardized framework, complicating the creation of cohesive and comprehensive AI education models that align with modern healthcare demands. As highlighted in the results described in 3.2, collaborative learning, teamwork skills, interdisciplinary abilities, and basic digital skills are emerging as transversal competencies. These competencies provide curricular support for integrating AI as a tool rather than as a subject of study itself, facilitating its adoption across diverse educational settings without necessarily establishing it as standalone content. Establishing consensus on essential AI skills and competencies is necessary for ensuring consistency and effectiveness in AI integration across UME programs. Ethical training and digital literacy emerged as fundamental components in AI curricula. Many studies stressed the importance of preparing students to navigate ethical challenges related to AI, such as maintaining patient confidentiality, recognizing algorithmic biases, and ensuring informed consent [28,29,30,31,32]. However, despite this consensus, the literature lacks comprehensive strategies and standardized curricula to systematically teach these competencies. This gap indicates a need for further curriculum development to incorporate structured and evidence-based approaches to ethical education. Existing studies have demonstrated that AI applications, such as Intelligent Tutoring Systems and virtual simulations, significantly enhance learning outcomes by providing adaptive, personalized training and reducing geographical barriers to education [23, 24, 53]. These tools have been shown to improve self-directed learning and critical thinking skills, essential for navigating AI-integrated clinical environments [24, 25]. Early research on learning outcomes also highlights that a staged integration of foundational AI concepts, followed by their application in clinical settings, fosters better knowledge retention and practical skill development [29, 32, 51]. Faculty readiness is a critical factor in the successful integration of AI into medical education. Many medical educators lack formal training in AI, which can hinder effective curriculum implementation. Providing faculty development programs, workshops, and interdisciplinary collaborations with AI specialists can bridge this gap. Furthermore, AI literacy training should focus not only on technical aspects but also on ethical considerations, clinical applications, and teaching strategies. Without adequate faculty support, AI education risks being inconsistently implemented, leaving students without the necessary guidance to critically engage with AI-driven tools in their future clinical practice. By addressing these gaps through standardized curricula, medical education can better prepare future physicians to responsibly and effectively use AI technologies while upholding the ethical principles critical to patient care. A variety of pedagogical methods were proposed across the studies, including lectures, e-learning modules, collaborative learning platforms, and hands-on experiences with AI tools [46, 47, 50, 54,55,56]. Experiential learning was consistently recommended as a critical component for effective AI training; however, the lack of robust evaluations and evidence-based frameworks limits understanding of the most effective approaches. AI, as a tool for teaching and learning, can be used interactively, allowing students to learn from it without necessarily being the object of study itself. Further research is needed to identify optimal pedagogical strategies and establish structured evaluations to assess student outcomes comprehensively. The lack of standardization in AI curricula across medical schools presents a major challenge to ensuring consistency in AI education. While some institutions have introduced AI as a standalone subject, others integrate it into existing courses with varying levels of depth. A possible solution is to develop a core competency framework outlining the essential AI-related skills that all medical students should acquire. Collaborative efforts between medical education governing bodies, AI experts, and curriculum developers could help establish standardized AI learning objectives. Additionally, accreditation bodies could play a role in encouraging AI integration by setting minimum competency requirements for medical graduates, ensuring that all students, regardless of institution, receive foundational training in AI applications relevant to clinical practice. The geographical diversity of the studies highlights significant global discrepancies in AI integration and student perceptions. While North American and European studies emphasized the need for formal, structured AI training to enhance clinical decision-making, studies from regions like Nigeria and India expressed concerns about the dehumanization of healthcare and the risk of technology overreliance [33,34,35,36,37]. Additionally, North American and European models often integrated interdisciplinary approaches, fostering collaboration between medical and technological disciplines to enhance critical thinking and teamwork skills [23,24,25,26,27]. In contrast, studies in regions like Nigeria highlighted a limited exposure to interdisciplinary applications, focusing more on standalone AI tools as solutions to immediate challenges [33,34,35,36,37]. These regional differences suggest that, although there is a shared acknowledgment of AI s importance, curricula must be tailored to cultural and institutional contexts to ensure their relevance and effectiveness globally. Through the strategic use of AI, countries in development can overcome some challenges and improve the quality and reach of medical training. Among the key approaches, AI enables access to personalized learning materials, using low-cost applications to tailor educational resources to individual levels of knowledge. AI also enables the adaptation of content to local languages and cultural contexts, helping learners better understand information. In addition, chatbots and virtual tutors can offer constant support, resolving doubts and providing feedback in real time, also facilitating virtual clinical simulations and practices, training students in complex situations without the need for physical facilities and at lower cost, while image recognition algorithms support learning in assisted diagnosis, especially useful in areas with few specialists [23, 32, 33]. Despite its advantages, the use of AI in low-resource settings faces challenges, such as limited technological infrastructure, lack of connectivity in rural areas, and the need for technology training for teachers and students. However, if implemented in an adaptable, accessible and sustainable manner, AI could have a positive and lasting impact on medical education in these communities. These findings underscore the necessity of aligning AI education frameworks with regional healthcare priorities and cultural values. Tailoring interdisciplinary models to fit local contexts ensures that AI integration addresses the diverse needs of medical education systems globally, enhancing both its applicability and impact. The ethical implications of AI integration in medical education are critical for ensuring that the use of AI enhances learning without compromising the human values inherent to healthcare practice. AI lacks the capacity for empathy and moral judgment, which are essential characteristics of health professionals [29]. This raises concerns about the potential impact of increased AI reliance on the development of interpersonal skills and the ability to respond compassionately to patients  emotional and psychological needs [55]. As AI tools become more prominent, bioethics education must adapt, ensuring that AI is viewed as a complementary tool while emphasizing the healthcare professional s responsibility in decision-making [48]. Additionally, equity and access are ethical considerations that must be addressed. While AI has the potential to improve medical education globally, unequal access could amplify existing disparities, disadvantaging students and professionals in resource-limited settings [44]. Concerns about privacy and data security are also significant, particularly when using AI tools like chatbots that handle sensitive patient information [40]. Developing strict regulations and ethical frameworks is essential to protect confidentiality and prevent legal issues. AI is revolutionizing medical education by providing innovative tools that enhance both teaching and learning [38, 55, 56]. On the instructional side, AI applications are used for plagiarism detection, curriculum development, automated feedback, and clinical simulations [47]. For students, AI serves as a resource for addressing questions, practicing clinical cases in safe environments, and receiving real-time feedback. Perhaps the most promising focus in medical education is precision medical education, similar to precision medicine, which allows for personalized competency development in students. AI, with its foundational capabilities in analytics, supports this approach, enabling tailored learning experiences [46]. In this regard, and contrasting with geographical variability, there is a highlighted need for standardization and adaptation of foundational skills necessary for AI adoption, continual development, and innovation within medical education. These tools not only promote interactive and efficient education but also prepare future physicians to navigate technological changes and the globalized healthcare landscape. Despite AI s transformative potential in medical education, its integration remains uneven across regions, particularly in low-resource environments. Systemic barriers ranging from infrastructure limitations to faculty readiness and regulatory gaps vary significantly depending on local economic and technological contexts. Medical schools in low- and middle-income countries (LMICs) often lack access to AI-driven educational tools, including high-speed internet, cloud computing, and AI-based simulation platforms. Institutions in high-income countries, by contrast, benefit from substantial investments in AI-enhanced learning environments. AI implementation in LMICs is hindered by limited digital infrastructure and the high costs of AI integration, restricting access to advanced AI-powered educational tools and training programs   [57]. Cloud computing has been proposed as a potential solution, allowing resource-limited institutions to access AI tools without the need for significant in-house infrastructure investments  [57]. Faculty AI literacy varies across regions, with educators in resource-limited settings often having limited access to AI training programs. Without structured faculty development, AI integration remains inconsistent. One of the major barriers identified is the scarcity of AI-related instructional resources tailored to LMICs, which restricts faculty engagement with AI-based medical education [58]. Expanding access to faculty training programs and leveraging online AI education platforms could help mitigate these barriers. Many institutions face unclear policies regarding AI in medical education, particularly concerning data privacy, AI-assisted assessments, and ethical guidelines. In LMICs, AI adoption in healthcare is further challenged by a lack of regulatory frameworks ensuring AI tools align with local health systems and ethical standards, leading to inconsistencies in implementation  [57]. Additionally, the limited availability of high-quality training datasets specific to LMICs creates further challenges for AI development and deployment, as most AI models are trained on data from high-income countries, potentially introducing bias and reducing local applicability  [58]. Many studies in this review exhibit limitations in methodological rigor, including the absence of standardized evaluation metrics and reliance on self-reported data from small, non-representative samples, which affects the strength and generalizability of findings. Additionally, the scarcity of longitudinal studies impedes a comprehensive understanding of AI training s long-term impact on clinical decision-making and skill transfer. Addressing these methodological limitations through multi-institutional studies with robust designs is essential for creating reliable, scalable models for AI education. A critical gap identified is the limited exploration of AI integration in medical education within low-resource settings. Investigating how AI curricula can be adapted to regions with limited technological infrastructure, while still fostering essential competencies, presents a valuable research opportunity. Future studies should prioritize this area to develop adaptable and accessible AI education models that promote equitable training standards globally. The lack of standardized pedagogical approaches in AI curricula hinders the development of cohesive training frameworks. Further research is required to examine how standardized curricula influence clinical skills and decision-making. Multi-institutional, diverse studies can address this gap, providing insights into the effectiveness of different educational models and informing evidence-based best practices for AI education in medical contexts. This scoping review acknowledges potential biases in the study selection and analysis processes. Limiting the inclusion criteria to studies published in English and Spanish may have excluded relevant research in other languages, reducing the diversity of perspectives. Similarly, the reliance on specific databases like PubMed, Scopus, and BIREME might have introduced selection bias by underrepresenting certain regions or less-accessible research. This review excluded conference abstracts and grey literature, which may contain early-stage findings, pilot programs, and evolving technological advancements in AI applications for medical education. While these sources can provide valuable insights, they often lack peer review, standardized methodologies, and comprehensive data, making it challenging to assess their rigor and reproducibility. Additionally, the thematic analysis method, while systematic, involves subjective interpretation, which may have unintentionally emphasized certain findings over others. The iterative refinement of inclusion criteria during the study selection process may also have influenced the final set of included studies, potentially impacting the comprehensiveness of the results. These limitations highlight the need for caution when generalizing findings, particularly for underrepresented regions and contexts. Future research should consider expanding the linguistic and geographical scope, incorporating select grey literature, and employing independent validation methods to enhance the reliability and applicability of results. Developing a standardized AI curriculum requires actionable steps to ensure its effective implementation. Curriculum developers should organize interdisciplinary workshops involving educators, AI experts, and healthcare professionals to identify core competencies. To achieve this, working groups should be established, bringing together medical educators, AI specialists, and accreditation bodies to define the core AI competencies required for medical students. Using the Delphi method can help reach consensus on minimum AI-related learning objectives, ensuring alignment with accreditation standards. Additionally, pilot programs should be introduced across multiple institutions, with data collection on student performance to evaluate effectiveness before large-scale implementation. Embedding ethical training into AI-related courses can be achieved by designing case-based learning modules that address real-world challenges such as algorithmic bias, patient privacy, and decision accountability. Interdisciplinary collaborations between medical faculties and bioethics departments should be established to develop structured ethical AI coursework. Furthermore, mandatory assessments should be incorporated to evaluate students  ability to navigate ethical AI applications in clinical practice. These measures will help ensure that students not only understand AI s technical capabilities but also develop the ethical reasoning necessary to apply AI responsibly in healthcare. Experiential learning should focus on simulation-based activities where students engage with AI tools like diagnostic algorithms and clinical decision support systems. Introducing AI-powered clinical simulations will allow students to interact with AI-driven decision-support tools in a controlled environment, helping them understand their potential and limitations. Additionally, AI-focused problem-based learning (PBL) sessions should be incorporated, where students analyze real medical cases using AI-generated insights. Partnering with health technology companies will provide students with hands-on exposure to AI applications in medical practice, further reinforcing their practical skills. Validating the impact of AI curricula involves implementing pilot programs to test their effectiveness. Institutions should collect baseline data and track outcomes such as clinical competency, decision-making skills, and patient care quality. Longitudinal studies should be conducted to assess the long-term impact of AI education on medical students'diagnostic accuracy and clinical reasoning. Standardized evaluation metrics should be developed to compare AI curricula across different institutions, ensuring a comprehensive understanding of best practices. Findings from these studies should be disseminated through international AI in medical education conferences to promote continuous improvements and knowledge sharing. Adapting AI curricula to regional contexts requires conducting needs assessments to identify barriers, such as limited infrastructure or access to AI tools. Flexible curriculum models should be developed, offering both high-tech AI learning tools and low-resource alternatives, such as AI case studies and offline learning modules for regions with limited digital access. Additionally, partnerships with governmental and non-governmental organizations should be encouraged to provide funding and support for AI curriculum implementation, ensuring inclusivity and relevance across diverse educational settings. This scoping review underscores the critical need for standardized curricula to integrate artificial intelligence effectively into undergraduate medical education. While diverse approaches exist across regions, a cohesive framework is essential to align AI competencies with the demands of modern healthcare. Standardizing AI education will ensure that future physicians are equipped with both technical skills and ethical insight, emphasizing the importance of interdisciplinary training that merges digital competence with patient-centered, ethical practices. As AI reshapes medical practice, medical education must adapt to foster a balanced approach that includes ethical considerations alongside technological proficiency. Establishing a structured, evidence-based AI curriculum will prepare students not only for clinical applications but also for navigating the broader ethical landscape, ensuring that they can harness AI s potential responsibly to improve healthcare quality while maintaining the human aspects of the patient-physician relationship. The authors used the o1 model preview developed by OpenAI during October 2024 and Microsoft Copilot software to improve the readability and language of the manuscript. This tool was utilized specifically for language editing, including grammar correction and sentence restructuring. The AI model was applied with their oversight and control, and they carefully reviewed and edited the results. The authors take full responsibility for the integrity and scientific accuracy of the content. AI Implications in Medical Education Articles Subthemes Impact of AI on Medical Education [23,24,25,26,27] Interdisciplinary AI Education in Medical Curricula   Promote interdisciplinary learning by integrating AI concepts into medical education, fostering critical thinking and collaboration across diverse fields of study, including medicine and natural sciences [27] Balancing Technological and Human Aspects of Medicine   AI must be balanced with human-centered, hands-on medical practice to avoid reducing patient care to algorithmic decision-making [26] Redesigning Medical Curricula for AI Integration   Medical education must shift from information acquisition to knowledge management, with a focus on using AI for precision medicine and enhancing physician-patient communication [3] AI-Enhanced Learning and Training Efficiency   AI-integrated models could accelerate medical knowledge acquisition and streamline training, making education more efficient and accessible [24] AI's Role in Evolving Medical Education Frameworks   AI-driven changes require a shift in medical education, focusing on digital literacy and collaboration with patients to ensure ethical and effective use of AI [23] AI in Curriculum Development [28,29,30,31,32] Incorporating AI Competencies into Medical Curricula   AI competencies should become an integral part of the mandatory medical school curriculum to equip future physicians with the necessary skills to interact with AI systems and make informed decisions [28] Staged Model for AI Integration in Medical Education   A staged approach to AI education is recommended, with students learning AI fundamentals in pre-clinical years and applying AI-based clinical applications in later clinical years [29] Ethical Foundations for AI Integration in Medical Education   AI education in medical curricula must prioritize ethical considerations, ensuring that future physicians understand the implications of AI, including patient confidentiality, algorithmic biases, and informed consent [30] Ethical and Purposeful Use of AI in Medical Education   Medical students must be trained to use health data and AI technologies in alignment with ethical and legal standards, ensuring the appropriate application of AI in clinical practice and safeguarding patient records for AI processing [31] Preparing Medical Students for AI Integration in Medicine   Medical students need structured education on AI to navigate the evolving healthcare landscape, ensuring they understand AI's role while maintaining ethical standards and the humanistic aspects of patient care [32] Perceptions and Attitudes towards AI [33,34,35,36,37] Exploring AI in Medical Education in Nigeria [45]   Medical students in Nigeria show both excitement and apprehension towards AI, with a significant number fearing that AI could diminish physicians  skills and dehumanize healthcare [35] AI in Canadian Medical Education   Canadian medical students believe AI will be integral to their careers, yet a majority feel that AI education is insufficient, highlighting the need for formal AI training in the curriculum [37] AI Acceptance in India s Medical Education   While Indian medical students demonstrate strong interest in AI integration, they express concerns about overreliance on AI and the risk of losing empathy in patient care [34] Perceptions of AI in UK Medical Education   UK medical students recognize the importance of AI in healthcare but feel inadequately prepared, especially with respect to radiology, where they fear AI may reduce career opportunities [36] AI in Medical Education across Arab Countries   Arab medical students, despite their limited AI training, acknowledge the potential of AI to revolutionize radiology and advocate for its inclusion in medical curricula [33] AI Applications in Teaching and Learning [46, 47, 50, 54,55,56] Teaching, Learning and Assessing Anatomy with Artificial Intelligence: The Road to a Better Future   AI can be used for plagiarism detection, curriculum design, feedback on practice, simulated clinical sessions, creating dissection videos, and supporting online assignments [47] Artificial intelligence and medical education: application in classroom instruction and student assessment using a pharmacology & therapeutics case study   AI significantly enhances medical education through improved self-study support, integrated interdisciplinary learning, and relevant aspect identification, despite challenges in multiple-choice question construction and standardization [54] Difficulties of artificial intelligence in medical education   The formality and rigidity of AI models can make it difficult to teach anatomical  uncertainty,  so it is recommended that the anatomy education community collaborate with developers to integrate this variability into AI tools [56]   AI generates adequate test items but requires human review to ensure accuracy and avoid errors. Limitations include the possibility of memorizing answers and extracting data from unauthenticated sources. It is suggested that teaching staff be trained in the use of AI and that the security of these tools be further improved [55]   Using ChatGPT to create simulated clinical cases has been successful in diversifying topics but has been presented with errors in accuracy and cultural and medical detail [50] Using artificial intelligence as an innovative learning strategy   An innovative program allowed medical students to participate in labeling lung ultrasound image data, improving understanding of AI and medicine [50]   The use of AI could improve self-assessment, data analysis, and hands-on training with clinical simulations to automate processes, reduce risks and costs, and adapt to post-pandemic educational changes, incorporating advanced technology into medical education [55]   The mobile app offers personalized recommendations and functionalities such as tournaments, rankings and challenges to foster student collaboration and leadership [46] Use of Chatbots and ChatGPT in Medical Education [38,39,40,41,42,43,44,45] Potential of AI language models in clinical decision making   ChatGPT can help formulate article outlines, paraphrase texts, and speed up literature review. However, there are challenges with verifying AI-generated sources and citations [38] Weaknesses arising from the use of chatbots in medical education   Misuse of ChatGPT can compromise academic integrity and lead to over-reliance, raising the need for policies and strategies to mitigate these risks [44]   Chatbots are underused in UK medical education due to a lack of development appropriate to student needs, lack of promotion by institutions, and students'lack of awareness of the potential benefits [45] Opportunities for integrating chatbots in clinical care, teaching and research   Chatbots can be useful in the early years of medical training, providing a safe environment for learning before direct clinical contact [41]   By providing accurate medical guidance, they reduce costs and improve patient outcomes. ChatGPT- 4 could be integrated into patient simulations and personalized learning, as well as assisting in research production [41]   The integration of tools such as ChatGPT can reduce knowledge gaps, expand differential diagnoses, question medical axioms and support decision making in acute care and complex cases [40]   ChatGPT facilitates the dissemination of popular information, can marginalize local cultures and languages, affecting cultural diversity. Furthermore, its use could open new opportunities in areas such as medical AI and information management [39]   Chatbots could be used as patient simulators, review tools or to standardize exams, thus improving self-learning and reducing tutor intervention [38] Ethical Considerations and Challenges [29, 38, 44, 48, 49] Ethical perceptions surrounding the use of artificial intelligence in medical education   The ethical management of AI is crucial to ensure its responsible use and equitable distribution of benefits, as access to the advantages of AI is not uniformly available [49]   There are many ethical and legal obstacles surrounding the implementation of AI in healthcare, an example of this is the negative perception of AI due to its lack of human essence [38]   It is necessary to critically and ethically evaluate the use of artificial intelligence, despite the convenience of obtaining easily generated information [44]   Awareness of the misinformation generated by AI about biomedical knowledge and the pathophysiology of diseases must be generated to avoid its use [29, 48] AI in Assessment and Feedback [47, 50,51,52] Anatomy Revolution with AI   AI revolutionizes anatomy education through 3D virtual reality, aiding in curriculum design, feedback, and online learning, though logistical considerations are essential [47] AI in Pharmacology Education   AI supports medical education by enhancing self-study, offering an integrated interdisciplinary approach, and facilitating key learning aspects. However, it faces challenges with multiple-choice question construction and standardization [50] Personalized Education with AI   Precision education, driven by AI, offers personalized and effective educational experiences while addressing traditional limitations, though it must navigate challenges in transparency, privacy, and bias propagation [52] MEKAS System in Otorhinolaryngology   The MEKAS system significantly enhances self-learning in otorhinolaryngology, validated by substantial improvement in results and high satisfaction [51] The Future of AI in Medical Education [25, 48, 53] Curriculum Redesign in the AI Era   A complete overhaul of medical school curricula is essential to focus on the effective use of AI in medicine [25] Ethical Framework for AI Education   Establishing a robust ethical framework is vital for ensuring safety, privacy, and equity in the integration of AI in healthcare [48] Professional Development with AI.   Evaluating and promoting AI in healthcare education can significantly enhance learners'comprehension and skills [53] All data generated or analysed during this study are included in this published article. World Health Organization (WHO). WHO guideline: recommendations on digital interventions for health system strengthening: evidence and recommendations. [Internet]. Geneva: World Health Organization; 2019. Available from: https://www.who.int/publications/i/item/9789241550505. da Fonseca MH, Kovaleski F, Picinin CT, Pedroso B, Rubbo P. E-health practices and technologies: a systematic review from 2014 to 2019. Healthcare (Basel). 2021;9(1192):1 32. Whitelaw S, Mamas MA, Topol E, Van Spall HGC. Applications of digital technology in COVID-19 pandemic planning and response. Lancet Digital Health. 2020;2:e435 40. Article Google Scholar Blumenthal D, Tavenner M. The,  Meaningful Use  regulation for electronic health records. N Eng J Med. 2010;363(6):501 4. Article Google Scholar Dhaliwal JK, Hall TD, Larue JL, Maynard SE, Pierre PE, Bransby KA. Expansion of telehealth in primary care during the COVID-19 pandemic: Benefits and barriers. J Am Assoc Nurse Pract. 2022;34(2):224 9. Dorsey ER, Topol EJ. Telemedicine 2020 and the next decade. Lancet. 2020;395(10227):859. Article Google Scholar Dlugatch R, Georgieva A, Kerasidou A. AI-driven decision support systems and epistemic reliance: a qualitative study on obstetricians  and midwives  perspectives on integrating AI-driven CTG into clinical decision making. BMC Med Ethics. 2024;25(1):6. Article Google Scholar Huang Z, George MM, Tan YR, Natarajan K, Devasagayam E, Tay E, et al. Are physicians ready for precision antibiotic prescribing? A qualitative analysis of the acceptance of artificial intelligence-enabled clinical decision support systems in India and Singapore. J Glob Antimicrob Resist. 2023;35:76 85. Bekbolatova M, Mayer J, Ong CW, Toma M. Transformative potential of AI in healthcare: definitions, applications, and navigating the ethical landscape and public perspectives. Healthcare (Switzerland). 2024;12:125. Google Scholar Khalifa M, Albadawy M. Artificial Intelligence for Clinical Prediction: Exploring Key Domains and Essential Functions. Comput Methods Programs Biomed Update. 2024;5(100148):1 11. Davenport T, Kalakota R. The potential for artificial intelligence in healthcare. Future Healthc J. 2019;6(2):94 8. Article Google Scholar Alowais SA, Alghamdi SS, Alsuhebany N, Alqahtani T, Alshaya AI, Almohareb SN, et al. Revolutionizing healthcare: the role of artificial intelligence in clinical practice. BMC Med Educ. 2023;23:689. Article Google Scholar Organizaci n Mundial de la Salud.  tica Y Gobernanza De La Inteligencia Artificial En El  mbito De La Salud. World Health Organization. 2021. Gordon M, Daniel M, Ajiboye A, Uraiby H, Xu NY, Bartlett R, et al. A scoping review of artificial intelligence in medical education: BEME guide no. 84. Med Teach. 2024;46:446 70. Article Google Scholar Narayanan S, Ramakrishnan R, Durairaj E, Das A. Artificial intelligence revolutionizing the field of medical education. Cureus. 2023;15(11):1 9. Lee J, Wu AS, Li D, Kulasegaram KM. Artificial Intelligence in Undergraduate Medical Education: A Scoping Review. Acad Med. 2021;96(11S):S62 S70. Walter Y. Embracing the future of Artificial Intelligence in the classroom: the relevance of AI literacy, prompt engineering, and critical thinking in modern education. Int J Educ Technol Higher Educ. 2024;21(15):1 29. Nordin MN Bin, Maidin SS, Rajoo M, Mahmod M, Jani WNFA, Yusoh MP, et al. International Frameworks For 21st century competences: comparative education. Res Militaris. 2022;12(2):7332 44. Arksey H, O Malley L. Scoping studies: Towards a methodological framework. Int J Soc Res Methodol. 2005;8(1):19 32. Levac D, Colquhoun H, O Brien KK. Scoping studies: advancing the methodology. Implement Sci. 2010;5(1):69. Article Google Scholar Tricco AC, Lillie E, Zarin W, O Brien KK, Colquhoun H, Levac D, et al. PRISMA extension for scoping reviews (PRISMA-ScR): checklist and explanation. Ann Intern Med. 2018;169:467 73. Article Google Scholar Ouzzani M, Hammady H, Fedorowicz Z, Elmagarmid A. Rayyan-a web and mobile app for systematic reviews. Syst Rev. 2016;5(1):210. Article Google Scholar Rampton V, Mittelman M, Goldhahn J. Implications of artificial intelligence for medical education. Lancet Digit Health. 2020;2:e111 2. Article Google Scholar Xu Y, Jiang Z, Ting DSW, Kow AWC, Bello F, Car J, et al. Medical education and physician training in the era of artificial intelligence. Singapore Med J. 2024;65:159 66. Article Google Scholar Wartman SA, Combs CD. Reimagining medical education in the age of AI. AMA J Ethics. 2019;21(2):E146-152. Article Google Scholar van der Niet AG, Bleakley A. Where medical education meets artificial intelligence:  Does technology care?  Med Educ. 2021;55(1):30 6. Article Google Scholar Lang J, Repp H. Artificial intelligence in medical education and the meaning of interaction with natural intelligence - an interdisciplinary approach. GMS J Med Educ. 2020;37(6):Doc59. Google Scholar Mosch L, Agha-Mir-Salim L, Sarica MM, Balzer F, Poncette AS. Artificial Intelligence in Undergraduate Medical Education. Stud Health Technol Inform. 2022;294:821 2. Ngo B, Nguyen D, Vansonnenberg E. The cases for and against artificial intelligence in the medical school curriculum. Radiol Artif Intell. 2022;4(5):e220074. Article Google Scholar Pregowska A, Perkins M. Artificial intelligence in medical education: typologies and ethical approaches. Ethics Bioethics Sciendo. 2024;14(1 2):96 113. Article Google Scholar  al kan SA, Demir K, Karaca O. Artificial intelligence in medical education curriculum: an e-Delphi study for competencies. PLoS One. 2022;17(7):e0271872. Article Google Scholar Civaner MM, Uncu Y, Bulut F, Chalil EG, Tatli A. Artificial intelligence in medical education: a cross-sectional needs assessment. BMC Med Educ. 2022;22(1):772. Article Google Scholar Allam AH, Eltewacy NK, Alabdallat YJ, Owais TA, Salman S, Ebada MA, et al. Knowledge, attitude, and perception of Arab medical students towards artificial intelligence in medicine and radiology: a multi-national cross-sectional study. Eur Radiol. 2024;34(7):1 14. Article Google Scholar Sharma V, Saini U, Pareek V, Sharma L, Kumar S. Artificial Intelligence (AI) integration in medical education: a pan-India cross-sectional observation of acceptance and understanding among students. Scripta Medica (Banja Luka). 2023;54(4):343 52. Oluwadiya KS, Adeoti AO, Agodirin SO, Nottidge TE, Usman MI, Gali MB, et al. Exploring artificial intelligence in the Nigerian medical educational space: An online cross-sectional study of perceptions, risks and benefits among students and lecturers from ten universities. Niger Postgrad Med J. 2023;30(4):285 92. Article Google Scholar Sit C, Srinivasan R, Amlani A, Muthuswamy K, Azam A, Monzon L, et al. Attitudes and perceptions of UK medical students towards artificial intelligence and radiology: a multicentre survey. Insights Imaging. 2020;11(1):14. Article Google Scholar Pucchio A, Rathagirishnan R, Caton N, Gariscsak PJ, Del Papa J, Nabhen JJ, et al. Exploration of exposure to artificial intelligence in undergraduate medical education: a Canadian cross methods study. BMC Med Educ. 2022;22(1):815. Article Google Scholar Boscardin CK, Gin B, Golde PB, Hauer KE. ChatGPT and Generative Artificial Intelligence for Medical Education: Potential Impact and Opportunity. Acad Med. 2024;99(1):22 7. Article Google Scholar Xie Y, Seth I, Hunter-Smith DJ, Rozen WM, Seifman MA. Investigating the impact of innovative AI chatbot on post-pandemic medical education and clinical assistance: a comprehensive analysis. ANZ J Surg. 2024;94(1 2):68 77. Article Google Scholar Vignesh R, Pradeep P, Balakrishnan P. A T te- -t te with ChatGPT on the impact of artificial intelligence in medical education. Med J Malaysia. 2023;78(4):547 9. Google Scholar Lower K, Seth I, Lim B, Seth N. ChatGPT-4: Transforming Medical Education and Addressing Clinical Exposure Challenges in the Post-pandemic Era. Indian J Orthop. 2023;57(9):1527 44. Article Google Scholar Skryd A, Lawrence K. ChatGPT as a Tool for Medical Education and Clinical Decision-Making on the Wards: Case Study. JMIR Form Res. 2024;8:e51346. Xu T, Weng H, Liu F, et al. Current Status of ChatGPT Use in Medical Education: Potentials, Challenges, and Strategies. J Med Internet Res. 2024;26:e57896. Guti rrez-Cirlos C, Carrillo-P rez DL, Berm dez-Gonz lez JL, Hidrogo-Montemayor I, Carrillo-Esper R, S nchez-Mendiola M. ChatGPT: opportunities and risks in the fields of medical care, teaching, and research. Gac Med Mex Academia Nacional de Medicina. 2023;159:382 9. Google Scholar Kaur A, Singh S, Chandan JS, Robbins T, Patel V. Qualitative exploration of digital chatbot use in medical education: A pilot study. Digit Health. 2021;7:20552076211038151. Tschirhart J, Woolsey A, Skinner J, Ahmed K, Fleming C, Kim J, et al. Introducing medical students to deep learning through image labelling: a new approach to meet calls for greater artificial intelligence fluency among medical trainees. Can Med Educ J. 2022. Abdellatif H, Al Mushaiqri M, Albalushi H, Al-Zaabi AA, Roychoudhury S, Das S. Teaching, Learning and Assessing Anatomy with Artificial Intelligence: The Road to a Better Future. Int J Environ Res Public Health. 2022;19(21):14209. Knopp MI, Warm EJ, Weber D, Kelleher M, Kinnear B, Schumacher DJ, et al. AI-Enabled Medical Education: Threads of Change, Promising Futures, and Risky Realities Across Four Potential Future Worlds. JMIR Med Educ. 2023;25(9): e50373. Article Google Scholar Alkhodari M, Xiong Z, Khandoker AH, Hadjileontiadis LJ, Leeson P, Lapidaire W. The role of artificial intelligence in hypertensive disorders of pregnancy: towards personalized healthcare. Expert Review of Cardiovascular Therapy. 2023;21(7):531 43. Sridharan K, Sequeira RP. Artificial intelligence and medical education: application in classroom instruction and student assessment using a pharmacology & therapeutics case study. BMC Med Educ. 2024 Dec 1;24(1). Su JM, Hsu SY, Fang TY, Wang PC. Developing and validating a knowledge-based AI assessment system for learning clinical core medical knowledge in otolaryngology. Comput Biol Med. 2024;1:178. Google Scholar Turner L, Hashimoto DA, Vasisht S, Schaye V. Demystifying AI: Current State and Future Role in Medical Education Assessment. Acad Med. 2024;99(4):S42 7. Article Google Scholar Randhawa GK, Jackson M. The role of artificial intelligence in learning and professional development for healthcare professionals. Healthc Manage Forum. 2020;33(1):19 24. Article Google Scholar Bakkum MJ, Hartjes MG, Pi t JD, Donker EM, Likic R, Sanz E, et al. Using artificial intelligence to create diverse and inclusive medical case vignettes for education. Br J Clin Pharmacol. 2024;90(3):640 8. Article Google Scholar Lazarus MD, Truong M, Douglas P, Selwyn N. Artificial intelligence and clinical anatomical education: promises and perils. Anat Sci Educ. 2024;17(2):249 62. Article Google Scholar Castellano MS, Contreras-McKay I, Neyem A, Farf n E, Inzunza O, Ottone NE, et al. Empowering human anatomy education through gamification and artificial intelligence: an innovative approach to knowledge appropriation. Clin Anat. 2024;37(1):12 24. Article Google Scholar Wahl B, Cossy-Gantner A, Germann S, Schwalbe NR. Artificial intelligence (AI) and global health: how can AI contribute to health in resource-poor settings?. BMJ Glob Health. 2018;3(4):e000798. Ciecierski-Holmes T, Singh R, Axt M, Brenner S, Barteit S. Artificial intelligence for strengthening healthcare systems in low- and middle-income countries: a systematic scoping review. NPJ Digit Med. 2022;5(1):162. Download references Not applicable. This work was supported by Universidad de La Sabana derived from project MED- 342 - 2023 at Universidad de La Sabana, Colombia. Department of Family Medicine and Public Health, Facultad de Medicina, Universidad de La Sabana, Campus del Puente del Com n, Km. 7, Autopista Norte de Bogot , Ch a, Cundinamarca, Colombia Erwin Hernando Hern ndez Rinc n & Claudia Liliana Jaimes Pe uela Facultad de Medicina, Universidad de La Sabana, Campus del Puente del Com n, Km. 7, Autopista Norte de Bogot , Ch a, Cundinamarca, Colombia Daniel Jimenez, Lizeth Alexandra Chavarro Aguilar & Juan Miguel P rez Fl rez Department of Psychiatry and Mental Health, Facultad de Medicina, Universidad de La Sabana, Campus del Puente del Com n, Km. 7, Autopista Norte de Bogot , Ch a, Cundinamarca, Colombia  lvaro Enrique Romero Tapia Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar All authors contributed to the study as follows: DJ and EHR were responsible for conceptualization. DJ, JP, and LC developed the methodology and managed the software. Validation was carried out by EHR, ART, and CJ. Formal analysis was performed by DJ, while investigation involved DJ, EHR, ART, CJ, JP, and LC. Resources were provided by EHR, ART, and CJ, and data curation was handled by DJ and EHR. DJ, EHR, JP, and LC contributed to the original draft, and DJ, EHR, CJ, and ART participated in the review and editing process. Visualization of the findings was managed by DJ, EHR, CJ, and ART. Supervision was provided by EHR, with project administration led by DJ and EHR. Funding acquisition was obtained by EHR, ART, and CJ. Correspondence to Erwin Hernando Hern ndez Rinc n. Not applicable. Not applicable. The authors declare no competing interests. Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. Reprints and permissions Rinc n, E.H.H., Jimenez, D., Aguilar, L.A.C. et al. Mapping the use of artificial intelligence in medical education: a scoping review. BMC Med Educ 25, 526 (2025). https://doi.org/10.1186/s12909-025-07089-8 Download citation Received: 31 October 2024 Accepted: 01 April 2025 Published: 12 April 2025 DOI: https://doi.org/10.1186/s12909-025-07089-8 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative Advertisement ISSN: 1472-6920 By using this website, you agree to our Terms and Conditions, Your US state privacy rights, Privacy statement and Cookies policy. Your privacy choices/Manage cookies we use in the preference centre.   2025 BioMed Central Ltd unless otherwise stated. Part of Springer Nature.",2
Artificial Intelligence in Sports Medicine - Nature,https://news.google.com/rss/articles/CBMiWEFVX3lxTFBBNkM4bWVuSXRiVXJLd2RxTEdCaEJyNXk4dUxnSmw0VUZ4ek81X0NkbndSMHZSaWcxcTY2ZWt2a3QtUk5sNWM1aTFKODViQkhVSDhDemNJQWc?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement Collection Artificial intelligence (AI) is transforming many industries worldwide from healthcare to education. Despite notable advancements, AI in sports medicine remains in its infancy. While some cutting-edge applications are making an impact, significant potential remains untapped, offering new opportunities to enhance sports medicine. Sports medicine is an interdisciplinary field encompassing several healthcare and science disciplines focused on athlete health, wellbeing and performance. This includes sport and exercise medicine as a core or subspeciality for physicians, sports surgery, sports science, sports rehabilitation, sports physiotherapy / physical therapy, sports dentistry, sports cardiology, sports podiatry, sports psychology, sports nutrition, etc. AI has the potential to transform these sports medicine environments providing innovative solutions that could bolster athlete health, wellbeing and performance. This collection invites original research, reviews, commentaries or case studies that explore the ethical and equitable development and use of AI systems (machine learning, deep learning, generative AI) in sports medicine. We welcome research that explores how AI systems could address the specific individual needs across the spectrum from elite Olympic or Para athletes to recreational athletes or people who exercise for health (e.g. living with chronic disease). Topics of interest include, but are not limited to: Image credit: [M] Aflo Images npj Digital Medicine (npj Digit. Med.) ISSN 2398-6352 (online)   2025 Springer Nature Limited Aspetar Orthopaedic and Sports Medicine Hospital, Qatar; Nuffield Department of Orthopaedics, Rheumatology and Musculoskeletal Sciences, University of Oxford, UK University of Stirling, Stirling, UK",2
Towards conversational diagnostic artificial intelligence - Nature,https://news.google.com/rss/articles/CBMiX0FVX3lxTFBUaFd6YjMtQVFrTkRXVzZJb2tuU1dKaXdPUGdLU01sVFBDd1NWTWxTRmZwanFmQTRRMzJQSy01TEUyd2JQdTZwMk80VXMwSnluUi1yYmVHaUtUbmMtaFJj?oc=5&hl=en-US&gl=US&ceid=US:en,"We use essential cookies to make sure the site can function. We also use optional cookies for advertising, personalisation of content, usage analysis, and social media. By accepting optional cookies, you consent to the processing of your personal data - including transfers to third parties. Some third parties are outside of the European Economic Area, with varying standards of data protection. See our privacy policy for more information on the use of your personal data. Manage preferences for further information and to change your choices. Thank you for visiting nature.com. You are using a browser version with limited support for CSS. To obtain the best experience, we recommend you use a more up to date browser (or turn off compatibility mode in Internet Explorer). In the meantime, to ensure continued support, we are displaying the site without styles and JavaScript. Advertisement Nature volume 642, pages 442 450 (2025)Cite this article 92k Accesses 32 Citations 303 Altmetric Metrics details At the heart of medicine lies physician patient dialogue, where skillful history-taking enables effective diagnosis, management and enduring trust1,2. Artificial intelligence (AI) systems capable of diagnostic dialogue could increase accessibility and quality of care. However, approximating clinicians  expertise is an outstanding challenge. Here we introduce AMIE (Articulate Medical Intelligence Explorer), a large language model (LLM)-based AI system optimized for diagnostic dialogue. AMIE uses a self-play-based3 simulated environment with automated feedback for scaling learning across disease conditions, specialties and contexts. We designed a framework for evaluating clinically meaningful axes of performance, including history-taking, diagnostic accuracy, management, communication skills and empathy. We compared AMIE s performance to that of primary care physicians in a randomized, double-blind crossover study of text-based consultations with validated patient-actors similar to objective structured clinical examination4,5. The study included 159 case scenarios from providers in Canada, the United Kingdom and India, 20 primary care physicians compared to AMIE, and evaluations by specialist physicians and patient-actors. AMIE demonstrated greater diagnostic accuracy and superior performance on 30 out of 32 axes according to the specialist physicians and 25 out of 26 axes according to the patient-actors. Our research has several limitations and should be interpreted with caution. Clinicians used synchronous text chat, which permits large-scale LLM patient interactions, but this is unfamiliar in clinical practice. While further research is required before AMIE could be translated to real-world settings, the results represent a milestone towards conversational diagnostic AI. The dialogue between the physician and the patient is fundamental to effective and compassionate care. The medical interview has been termed  the most powerful, sensitive, and most versatile instrument available to the physician 2. In some settings, it is believed that 60 80% of diagnoses are made through clinical history-taking alone6. The physician patient dialogue extends beyond history-taking and diagnosis it is a complex interaction that establishes rapport and trust, serves as a tool for addressing health needs and can empower patients to make informed decisions that account for their preferences, expectations and concerns7. While there is wide variation in communication skills among clinicians, well-trained professionals can wield considerable skills in clinical history-taking and the wider  diagnostic dialogue . However, access to this expertise remains episodic and globally scarce8. Recent progress in general-purpose large language models (LLMs)9,10,11 has shown that artificial intelligence (AI) systems have the capability to plan, reason and incorporate relevant context enough to hold naturalistic conversations. This progress affords an opportunity to rethink the possibilities of AI in medicine towards the development of fully interactive conversational AI. Such medical AI systems would understand clinical language, intelligently acquire information under uncertainty and engage in natural, diagnostically useful medical conversations with patients and those who care for them. The potential real-world utility of AI systems capable of clinical and diagnostic dialogue is broad, with the development of such capabilities possibly improving access to diagnostic and prognostic expertise, thus improving the quality, consistency, availability and affordability of care. A health equity-centric approach to integrating such technology into existing workflows, which implies work in the development, implementation and policy stages, may have the potential to help realize better health outcomes (particularly for populations facing healthcare disparities). However, while LLMs have been shown to encode clinical knowledge and have proven capable of highly accurate single-turn medical question-answering12,13,14, their conversational capabilities have been tailored to domains outside clinical medicine15,16. Earlier work in LLMs for health12,13,14,17,18 has not yet rigorously examined the clinical history-taking and diagnostic dialogue capabilities of AI systems or contextualized this by comparison to the extensive capabilities of practicing generalist physicians. Clinical history-taking and diagnostic dialogue, through which clinicians derive diagnosis and management plans, represent a complex skill1 whose optimal conduct is highly dependent on context. Thus, multiple evaluation axes are needed to assess the quality of a diagnostic dialogue, including the structure and completeness of the elicited history, diagnostic accuracy, the appropriateness of management plans and their rationale, and patient-centred considerations, such as relationship-building, respect for the individual and communication efficacy19. If the conversational potential of LLMs is to be realized in medicine, there is an important unmet need to better optimize the development and evaluation of medical AI systems for characteristics such as these, which are unique to history-taking and diagnostic dialogue between clinicians and patients. Here we detail our progress towards a conversational medical AI system for clinical history-taking, diagnostic reasoning and communication efficacy. We also outline some key limitations and directions for future research. Our key contributions (Fig. 1) are summarized here. We first introduced AMIE (Articulate Medical Intelligence Explorer), an LLM-based AI system optimized for clinical history-taking and diagnostic dialogue. To scale AMIE across a multitude of specialties and scenarios, we developed a self-play-based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE s diagnostic accuracy and conversation quality. Then we developed a pilot evaluation rubric to assess the history-taking, diagnostic reasoning, communication skills and empathy of diagnostic conversational medical AI, encompassing both clinician-centred and patient-centred metrics. Next we designed and conducted a blinded, remote objective structured clinical examination (OSCE) study (Fig. 2) using 159 case scenarios from clinical providers in Canada, the United Kingdom and India, enabling the randomized and counterbalanced comparison of AMIE to primary care physicians (PCPs) when performing consultations with validated patient-actors. AMIE exhibited superior diagnostic accuracy compared to the PCPs, as assessed by various measures (for example, top-1 and top-3 accuracy of the differential diagnosis (DDx) list). Across 30 out of 32 evaluation axes from the specialist physician perspective and 25 out of 26 evaluation axes from the patient-actor perspective, AMIE was rated superior to PCPs while being non-inferior on the rest. Finally we performed a range of ablations to further understand and characterize the capabilities of AMIE, highlighting important limitations, and have proposed key next steps for the real-world clinical translation of AMIE. AMIE is a conversational medical AI optimized for diagnostic dialogue. It is instruction fine-tuned with a combination of real-world and simulated medical dialogues, alongside a diverse set of medical reasoning, question-answering (QA) and summarization datasets. Notably, we designed a self-play-based simulated dialogue environment with automated feedback mechanisms to scale AMIE s capabilities across various medical contexts and specialties. Specifically, this iterative self-improvement process consisted of two self-play loops: (1) an  inner  self-play loop, where AMIE leveraged in-context critic feedback to refine its behaviour on simulated conversations with an AI patient agent; and (2) an  outer  self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. During online inference, AMIE used a chain-of-reasoning strategy to progressively refine its response, conditioned on the current conversation, to arrive at an accurate and grounded reply to the patient in each dialogue turn. We designed and conducted a blinded remote OSCE with validated patient-actors interacting with AMIE or PCPs by means of a text chat interface. Across multiple axes, corresponding to both specialist physician (30 out of 32) and patient-actor (25 out of 26) perspectives, AMIE was rated as superior to PCPs while being non-inferior on the rest. A PCP and AMIE perform (in a randomized order) a virtual remote OSCE with simulated patients by means of an online multi-turn synchronous text chat and produce answers to a post-questionnaire. Both the PCP and AMIE are then evaluated by both the patient-actors and specialist physicians. Our research has important limitations, most notably that we utilized a text-chat interface, which, although enabling potentially large-scale interaction between patients and LLMs specialized for diagnostic dialogue, was unfamiliar to the PCPs for remote consultation. Thus, our study should not be regarded as representative of usual practice in (tele)medicine. AMIE s diagnostic accuracy was assessed as higher than that of the PCPs. Figure 3 shows the top-k accuracy for AMIE and the PCPs, considering matches with the ground-truth diagnosis (Fig. 3a) and matches with any item on the accepted differential (Fig. 3b). AMIE showed significantly higher top-k accuracy than that of the PCPs across all values of k (P < 0.05). Note that, unlike AMIE, the PCPs did not always provide ten diagnoses in their DDxs (min = 3, mean = 5.36). Additionally, we performed a comparison of DDx accuracy between AMIE and the PCPs by varying the criteria for determining a match (that is, requiring an exact match versus just a highly relevant diagnosis). The results depicted in Supplementary Fig. 2 further substantiate AMIE s superior DDx performance across various matching criteria. a,b, The AMIE and PCP top-k DDx accuracies, determined by the majority vote of three specialists, are compared across 159 scenarios with respect to the ground-truth diagnosis (a) and all diagnoses in the accepted differential (b). Centrelines correspond to the average top-k accuracies, with the shaded areas indicating 95% confidence intervals computed from two-sided bootstrap testing (n = 10,000). All top-k differences between AMIE and PCP DDx accuracy are significant, with P < 0.05 after FDR correction. The FDR-adjusted P values for ground-truth comparison are: 0.0017 (k = 1), 0.0002 (k = 2), 0.0002 (k = 3), 0.0002 (k = 4), 0.0002 (k = 5), 0.0003 (k = 6), 0.0003 (k = 7), 0.0003 (k = 8), 0.0002 (k = 9) and 0.0002 (k = 10) (a). The FDR-adjusted P values for accepted differential comparison are: 0.0001 (k = 1), 0.0001 (k = 2), 0.0002 (k = 3), 0.0002 (k = 4), 0.0001 (k = 5), 0.0001 (k = 6), 0.0001 (k = 7), 0.0001 (k = 8), 0.0001 (k = 9) and 0.0001 (k = 10) (b). Ten of the scenarios performed by AMIE and the PCPs were designed to primarily describe patients with no new concerning diagnosis (for example, a ground-truth diagnosis of resolved constipation, or the recurrence of a prior-known disease state of gastroesophageal-reflux-disease-induced chest pain). These were two scenarios each from the cardiovascular, gastroenterology, internal medicine, neurology and respiratory specialties. Here we plotted the top-k DDx accuracy, as rated by the majority vote of three specialists for these non-disease-state cases. Although our results are not statistically significant, as they only consist of ten scenarios, AMIE appears to maintain the same trend of better performance on these mostly negative scenarios (Extended Data Fig. 2). AMIE has superior DDx accuracy on the set of 149 primarily positive disease state scenarios (in which only three scenarios had a ground-truth of a non-disease state). Extended Data Fig. 3 illustrates the DDx accuracy achieved by AMIE and the PCPs across the six medical specialties covered by the scenarios in our study. We observed that AMIE s performance matched or surpassed PCP performance for all specialties except for obstetrics and gynaecology/urology, with the most pronounced improvements being in the respiratory and internal medicine specialties. We observed that both AMIE and the PCPs had higher diagnostic accuracy in consultations performed in the Canada OSCE lab compared to those enacted in the India OSCE lab. However, the differences were not statistically significant and, in a subset of 40 scenarios enacted in both the Canada and India OSCE labs, the performances of both AMIE and the PCPs were equivalent (Extended Data Fig. 4). We reproduced the DDx accuracy analysis with our model-based DDx auto-evaluator using the same procedure as in Fig. 3. The overall performance trends obtained through the auto-evaluator align well with specialist assessments despite marginal differences in the computed accuracy values, as shown in Extended Data Fig. 5a,b. Additionally, we present a fully-simulated ablation testing different patient behaviours (Supplementary Fig. 3), which showed that AMIE was robust to many different patient personalities, although it had reduced DDx performance when interviewing patients with low English literacy. To investigate whether AMIE s superior DDx performance observed in Fig. 3 stemmed from improved information acquisition or from better diagnostic reasoning capability, we compared AMIE s diagnoses based on its own consultations with AMIE s diagnoses generated from the corresponding PCP consultations, using the DDx auto-evaluator. The results depicted in Extended Data Fig. 5c,d revealed markedly similar DDx performance, indicating that the diagnostic performance remained consistent regardless of whether AMIE processed information from its own dialogue or from the PCP s conversation. Both methods significantly outperformed the DDxs produced by the PCPs. These results suggest that AMIE was approximately equivalent to the PCPs at information acquisition, but better than the PCPs at interpreting that information to produce an accurate or complete DDx. Although AMIE displayed greater verbosity compared to the PCPs, in terms of total number of words generated in their responses during the consultation, the number of conversational turns and the number of words elicited from the patient-actors were similar across both OSCE agents, as illustrated in Extended Data Fig. 6a c. This suggests that both AMIE and the PCPs acquired a similar amount of information from the patients during the encounter. To investigate how efficient AMIE or the PCPs were at gathering sufficient information to formulate a correct diagnosis, we truncated the conversations at various turn counts and used AMIE to generate DDxs based on these partial conversations. The results in Extended Data Fig. 6d,e illustrate that both AMIE and the PCPs were able to acquire the information necessary for formulating an accurate differential in the early stages (first ten turns) of the conversation. With comparable performance at all conversation lengths, neither AMIE nor the PCPs seemed to have a significant advantage in the speed, efficiency or diagnostic utility of information acquisition. Conversation quality was assessed using patient-actor ratings, specialist ratings and outputs from auto-evaluation. Supplementary Table 5 shows two example consultations with the same simulated patient from AMIE and a PCP. Figure 4 presents the various conversation qualities the patient-actors assessed following their consultations with the OSCE agents. Overall, AMIE s consultations were rated significantly better (P < 0.05) by the patient-actors than those with the PCPs across 25 of 26 axes. No significant differences in ratings were detected for one of the patient-centred communication best practice (PCCBP) axes19,  Acknowledging mistakes  (N = 46). For this criterion, the number of exclusions was substantially higher because the question applied only when mistakes were made by the OSCE agent and were pointed out in the conversation. Conversation qualities, as assessed by the patient-actors upon conclusion of the consultation. For illustration purposes, all responses from the five-point rating scales were mapped to a generic five-point scale ranging from  Very favourable  to  Very unfavourable . For Yes/No (Y/N) questions, a (positive)  Yes  response was mapped to the same colour as  Favourable  and a (negative)  No  response to the same colour as  Unfavourable . The rating scales were adapted from the GMCPQ, PACES and a narrative review about PCCBP. Details on question-wording and response options are provided in Extended Data Tables 1 and 2. The evaluation involved 159 simulated patients. The P values were determined using two-sided Wilcoxon signed-rank tests with FDR correction. Cases where either AMIE or the PCP received  Cannot rate/Does not apply  were excluded from the test. Specialist physicians evaluated both the conversational quality as well as the responses to the post-questionnaire for scenarios within their domain expertise (Fig. 5). Again, AMIE s responses were rated significantly better by the specialists than those from the PCPs on 30 out of 32 evaluation axes, with the specialists preferring AMIE s consultations, diagnoses and management plans over those from the PCPs. For this set of evaluations, the differences in specialist ratings between AMIE and the PCPs were statistically significant (P < 0.05). See Supplementary Information section 7 for the inter-rater reliability between the three specialist raters per scenario. No significant differences in ratings were detected for two of the axes in the Diagnosis and management rubric namely,  Escalation recommendation appropriate  and  Confabulation absent despite no exclusions (N = 159). Conversation and reasoning qualities, as assessed by specialist physicians. For illustration purposes, all responses from the five-point rating scales were mapped to a generic five-point scale ranging from  Very favourable  to  Very unfavourable . The only four-point scale (DDx comprehensiveness) was mapped to the same scale, ignoring the  Neither favourable nor unfavourable  option. For Yes/No questions, a (positive)  Yes  response was mapped to the same colour as  Favourable  and a (negative)  No  response to the same colour as  Unfavourable . The rating scales were adapted from PACES, a narrative review about PCCBP and other sources. Details on question-wording and response options are provided in Extended Data Tables 1 3. The evaluation involved 159 simulated patients, with the ratings from three distinct specialist physician raters for each case being aggregated using the median. The P values were determined using two-sided Wilcoxon signed-rank tests with FDR correction. Cases where either AMIE or the PCP received  Cannot rate/Does not apply  were excluded from the test. We leveraged a model-based self-chain-of-thought auto-evaluation strategy (Supplementary Table 2) to rate conversations on four evaluation axes from the Practical Assessment of Clinical Examination Skills (PACES) rubric20, and validated that these auto-evaluation ratings were accurate and well aligned with the specialist ratings (Supplementary Fig. 1b). Comparing the simulated dialogues generated before and after the self-play procedure, we found that the inner self-play loop improved simulated dialogue quality on these axes, as indicated in Supplementary Fig. 1c. In this study, we introduced AMIE, an LLM-based AI system optimized for clinical dialogue with diagnostic reasoning capabilities. We compared AMIE consultations to those performed by PCPs using a randomized, double-blind crossover study with human simulated patients in the style of an OSCE. Notably, our study was not designed to be representative of clinical conventions either for traditional OSCE evaluations, for remote- or telemedical consultation practices or for the ways clinicians usually use text and chat messaging to communicate with patients. Our evaluation instead mirrored the most common way by which people interact with LLMs today, leveraging a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. In this setting, we observed that AMIE, an AI system optimized specifically for the task, outperformed the PCPs on simulated diagnostic conversations when evaluated along multiple clinically meaningful axes of consultation quality. The DDxs provided by AMIE were more accurate and complete than those provided by the board-certified PCPs when both were evaluated by specialist physicians. Previous research has shown that AI systems may match or exceed human diagnostic performance in specific, narrow tasks21,22 in retrospective evaluation. However, these situations typically involved both the AI and physicians interpreting the same fixed input (for example, identifying the presence of a specific finding in a medical image). Our study was significantly more challenging because it required the AI system to actively acquire relevant information through conversation, rather than relying on clinical information collated by human efforts23. Therefore the system s downstream DDxs depended on not only its diagnostic inference capability, but also the quality of information gathered under uncertainty through natural conversation and building rapport. Our results suggested that AMIE was as adept as the PCPs in eliciting pertinent information during the simulated consultations, and was more accurate than the PCPs in formulating a complete DDx if given the same amount of acquired information. This finding corroborates other work that LLMs may be able to produce more complete DDxs given the same clinical information as physicians in challenging cases22. Although not explored in this study, the assistive performance of AMIE therefore represents an interesting and important avenue for future research, particularly given the real-world importance of expert oversight for AI systems in safety-critical settings, such as medicine. Our study utilized a wide variety of simulated patients, comprising actors trained in both Canada and India, and scenarios across a range of specialties. This allowed us to explore how performance varied along multiple axes by specialty, and by the locations in which the scenario was derived and enacted. While we observed that both the PCPs and AMIE performed worse in gastroenterology and internal medicine scenarios than with other specialties (Extended Data Fig. 3), the study was not powered or designed to compare performance between different specialty topics and locations, and we cannot exclude that the scenarios in some specialties might have been harder than others. The patient-actors and specialist raters both evaluated AMIE s performance to be higher than that of the PCPs on metrics related to empathy and communication skills. These axes comprised a majority of the dimensions that were evaluated. This general finding is consistent with a prior study, where LLM responses were found to be more empathetic than the responses from clinicians to health questions posted on Reddit24. However, the findings in that study cannot be generalized directly to our setting due to the differences in study design. Specifically, prior work has not involved a direct, randomized comparison of physicians and AI systems in a prospective simulation of multi-turn dialogue with the same patient. In both settings, the lack of voice-based and non-verbal visual communication may have been an unfair disadvantage to the clinicians. The text-based chat interface used in this study introduced both advantages and disadvantages. People today most commonly engage with LLMs through synchronous text-chat interfaces25, and patients often use patient portals to send messages to their providers. We therefore chose this mode of interaction as a representative interface for LLMs to perform multi-turn conversation, adapting the virtual OSCE framework accordingly. While this allowed a fair comparison of diagnostic dialogue between the LLMs and the clinicians when both were restricted to a synchronous text chat, it is important to acknowledge that our experiments did not emulate the expected quality of diagnostic dialogue in real clinical practice (including telemedicine). Physicians may be more used to history-taking and diagnostic dialogue by telephone or video consultation than synchronous text-chat communication26. Instead, text is more commonly used by clinicians to communicate with patients for episodic or asynchronous needs, such as prescription refills or communication about specific test results27. Physicians may thus be more familiar with text/SMS or email rather than the synchronous text-chat medium we employed in this study. In both text/SMS and email, the conventions and expectations for communicating naturally and with empathic style might be different28. It is possible that the PCPs in our study had not yet become accustomed to the setting, and may have performed differently if subjected to a specific training programme (similar in spirit to the training process for AMIE). Clinicians participating in the study undertook two preparatory pilot sessions of consultations with our synchronous text interface before the evaluation began, but this was not a formal training programme, nor was it designed to optimize the clinicians  performance. Future research could explore this question more thoroughly, including monitoring for the impact of a learning curve or exploring whether performance varies according to the extent to which participating clinicians or simulated patients are familiar with telemedicine. Note that the conversations in our study were time-limited to follow typical OSCE conventions. While real-world patient physician consultations often also take place under time constraints, the specific time limit imposed in our study may not be reflective of real-world scenarios. Additionally, our findings regarding empathic communication could also be partially attributed to the fact that the AMIE responses were significantly longer than the clinician responses (Extended Data Fig. 6), and presented with greater structure. This could potentially suggest to an observer that more time was spent preparing the response, analogous to known findings that patient satisfaction increases with time spent with their physicians29. Collectively, our findings suggest many avenues for further research that might leverage human AI complementarity30, combining clinicians  skills in the analysis of verbal and non-verbal cues with the potential strengths of LLMs to suggest more enriched conversational responses, including empathic statements, structure, eloquence or more complete DDxs. The use of simulated data allowed us to quickly scale the training to a broad set of conditions and patient contexts, while the injection of knowledge from search encouraged these dialogues to remain grounded and realistic. Although the simulated patients encompassed a wide range of conditions, they failed to capture the full range of potential patient backgrounds, personalities and motivations. Indeed, the simulated experiments shown in Supplementary Fig. 3 suggested that, while AMIE appears robust to certain variations in patient characteristics and behaviour, it has significant difficulty with some types of patients, such as those with low English literacy. Through the inner self-play procedure, we were able to iteratively improve the simulated dialogue we generated and used in fine-tuning. However, these improvements were limited by our ability to articulate what made good dialogue in the critic instructions, the critic s ability to produce effective feedback and AMIE s ability to adapt to such feedback. For example, in the simulated environment we imposed that AMIE reaches a proposed differential and testing/treatment plan for the patient, but such an endpoint may be unrealistic for some conditions, especially in the virtual chat-based setting. This limitation also applies in the real-world setting. Additionally, the task of producing reward signals for the quality of medical diagnostic conversations is more challenging than evaluating outcomes in rule-based constrained environments where success is well-defined (for example, winning or losing a game of Go31). Our process for generating synthetic vignettes was designed with this consideration in mind. Because we knew the ground-truth condition for each vignette and the corresponding simulated dialogue(s) rollout, we were able to automatically assess the correctness of AMIE s DDx predictions as a proxy reward signal. This reward signal was used to filter out  unsuccessful  simulated dialogues, such as those for which AMIE failed to produce an accurate DDx prediction during this self-play process. Beyond DDx accuracy, the self-play critic agent also assessed other qualities, including the level of empathy, professionalism and coherence conveyed by the doctor agent for each simulated dialogue. While these latter constructs are more subjective compared to diagnostic accuracy, they served as domain-specific heuristics imposed by clinical experts from our research team to help steer AMIE s development towards alignment with established clinical values. We also note that, in our preliminary analysis described in this work, our auto-evaluation framework for assessing the conversations along such rubrics was found to be in good alignment with human ratings and comparable to the inter-specialist agreement on these criteria. Note that the majority of scenarios in our evaluation set assumed an underlying disease state, while only a small subset assumed the absence of disease. This is an important limitation of this work because it does not reflect the population-level epidemiological realities of primary care, where the majority of work in assessing patients involves ruling out disease, rather than ruling it in. We encourage future work to explore evaluation with various distributions of disease versus non-disease states. Therefore, even within the distribution of diseases and specialties we addressed, our findings should be interpreted with humility and caution. There is a need for further research to examine varied presentations of the same diseases, alongside an exploration of alternative approaches to evaluating history-taking and clinical dialogue in situations of different patient needs, preferences, behaviours and circumstances. The evaluation protocol presented in this paper was limited in terms of its ability to capture potential issues related to fairness and bias, which remains an important open question that we will aim to address in subsequent system evaluations. Recent advances in the development of comprehensive frameworks for bias detection in LLMs32 present a promising starting point for establishing such an approach. It should be noted that medical diagnostic dialogue is a particularly challenging use case, due to the complexity of the medical domain, the interactive information-gathering nature of the dialogue and the outcome-driven setting, with the potential of associated harms in cases of incorrect diagnosis or incorrect medical advice. Nevertheless, disentangling these issues is an important further research area if LLMs in the domain are to overcome, rather than propagate, inequities in healthcare. For example, previous studies have found that physicians approach communication with their patients differently, on average, depending on the patients  race, resulting in Black patients receiving communication that was less patient-centred and had a lower positive affect33. Other studies have found differences in physicians  communication styles and conversation length based on gender34 and on patients  level of health literacy35. Effective intercultural communication skills are essential36. There is therefore a non-negligible risk that such historical conversational biases may be replicated or amplified in an AI dialogue system, but at the same time, there is also an opportunity to work towards designing conversational systems that can be more inclusive, and more personalized to the individual patient s needs. To help inform the development of the necessary fairness, bias and equity frameworks, it was important to employ a participatory approach to solicit representative views across a wide range of patient demographics, as well as clinical and health equity domain experts. Such evaluation frameworks should be complemented by extensive model red-teaming and an adversarial approach to identifying any remaining gaps and failure modes. Recent advances in red-teaming LLMs could be useful in this scenario37, where human raters or other AI systems (that is, the red team) simulate the role of an adversary to identify vulnerabilities and security gaps in these LLMs. These practices should not only inform the evaluation of the final model, but also its development and iterative refinement. Model development should follow the established data and model reporting practices and provide transparency into the training data and the associated decision processes38,39,40. The dialogue research dataset contributing to the AMIE training data in our study was de-identified, reducing the availability of socioeconomic factors, patient demographics and information about clinical settings and locations. To mitigate the risk that our synthetic vignettes would skew towards certain demographic groups, we leveraged web search to retrieve a range of demographics and associated symptoms relevant to each condition. We used these as input to the prompt template for vignette generation, instructing the model to produce multiple different vignettes given this range of inputs. While this mechanism was designed with the intent of mitigating risks of bias amplification, a comprehensive evaluation of conversational diagnostic models, such as AMIE, for equity, fairness and bias is an important scope for future work. Further work is also needed to ensure the robustness of medical LLMs in multilingual settings41, and particularly their performance in minority languages42. The great variety of cultures43, languages, localities, identities and localized medical needs makes the task of generating a priori static yet comprehensive fairness benchmarks practically infeasible. The measurement and mitigation of bias must move beyond the traditional narrow focus on specific axes that fails to scale globally44. With LLM-based evaluators, a potential solution is presented for preliminary assessments in languages where there are no systematic benchmarks, although prior studies have found these auto-evaluation frameworks to be biased, underscoring the need for calibrating them on native speaker evaluations, and using them with caution45. This study demonstrates the potential of LLMs for future use in healthcare in the context of diagnostic dialogue. Transitioning from an LLM research prototype that has been evaluated in this study to a safe and robust tool that can be used by healthcare providers, administrators and people will require significant additional research to ensure the safety, reliability, efficacy and privacy of the technology. Careful consideration will need to be given to the ethical deployment of this technology, including rigorous quality assessment across different clinical settings and research into reliable uncertainty estimation methods46 that would allow for deferral to human clinical experts when needed. These and other guardrails are needed to mitigate the potential overreliance on LLM technologies, with other specific measures for attention to ethical and regulatory requirements particular to future use cases and the presence of qualified physicians in the loop to safeguard any model outputs. Additional research will also be needed to assess the extent to which biases and security vulnerabilities might arise, either from base models or the circumstances of use in deployment, as we have highlighted in our prior work12. Given the continuous evolution of clinical knowledge, it will also be important to develop ways for LLMs to utilize up-to-date clinical information47. The utility of medical AI systems could be greatly improved if they are better able to interact conversationally, anchoring on large-scale medical knowledge, while communicating with appropriate levels of empathy and trust. This work demonstrates the great potential capabilities of LLM-based AI systems for settings involving clinical history-taking and diagnostic dialogue. The performance of AMIE in simulated consultations represents a milestone for the field, given it was assessed along an evaluation framework that considered multiple clinically relevant axes for conversational diagnostic medical AI. However, the results should be interpreted with appropriate caution. Translating from this limited scope of experimental simulated history-taking and diagnostic dialogue towards real-world tools for people and those who provide care for them requires a substantial amount of additional research and development to ensure the safety, reliability, fairness, efficacy and privacy of the technology. If successful, we believe AI systems, such as AMIE, can be at the core of next-generation-learning health systems that help scale world-class healthcare to everyone. AMIE was developed using a diverse suite of real-world datasets, including multiple-choice medical question-answering, expert-curated long-form medical reasoning, electronic health record (EHR) note summaries and large-scale transcribed medical conversation interactions. As described in detail below, in addition to dialogue generation tasks, the training task mixture for AMIE consisted of medical question-answering, reasoning and summarization tasks. We used the MedQA (multiple-choice) dataset, consisting of US Medical Licensing Examination multiple-choice-style open-domain questions with four or five possible answers48. The training set consisted of 11,450 questions and the test set had 1,273 questions. We also curated 191 MedQA questions from the training set where clinical experts had crafted step-by-step reasoning leading to the correct answer13. The dataset used here consisted of expert-crafted long-form responses to 64 questions from HealthSearchQA, LiveQA and Medication QA in MultiMedQA12. A dataset consisting of 65 clinician-written summaries of medical notes from MIMIC-III, a large, publicly available database containing the medical records of intensive care unit patients49, was used as additional training data for AMIE. MIMIC-III contains approximately two million notes spanning 13 types, including cardiology, respiratory, radiology, physician, general, discharge, case management, consult, nursing, pharmacy, nutrition, rehabilitation and social work. Five notes from each category were selected, with a minimum total length of 400 tokens and at least one nursing note per patient. Clinicians were instructed to write abstractive summaries of individual medical notes, capturing key information while also permitting the inclusion of new informative and clarifying phrases and sentences not present in the original note. Here we used a de-identified dataset licensed from a dialogue research organization, comprising 98,919 audio transcripts of medical conversations during in-person clinical visits from over 1,000 clinicians over a ten-year period in the United States50. It covered 51 medical specialties (primary care, rheumatology, haematology, oncology, internal medicine and psychiatry, among others) and 168 medical conditions and visit reasons (type 2 diabetes, rheumatoid arthritis, asthma and depression being among the common conditions). Audio transcripts contained utterances from different speaker roles, such as doctors, patients and nurses. On average, a conversation had 149.8 turns (P0.25 = 75.0, P0.75 = 196.0). For each conversation, the metadata contained information about patient demographics, reason for the visit (follow-up for pre-existing condition, acute needs, annual exam and more), and diagnosis type (new, existing or other unrelated). Refer to ref. 50 for more details. For this study, we selected dialogues involving only doctors and patients, but not other roles, such as nurses. During preprocessing, we removed paraverbal annotations, such as  [LAUGHING]  and  [INAUDIBLE] , from the transcripts. We then divided the dataset into training (90%) and validation (10%) sets using stratified sampling based on condition categories and reasons for visits, resulting in 89,027 conversations for training and 9,892 for validation. While passively collecting and transcribing real-world dialogues from in-person clinical visits is feasible, two substantial challenges limit its effectiveness in training LLMs for medical conversations: (1) existing real-world data often fail to capture the vast range of medical conditions and scenarios, hindering its scalability and comprehensiveness; and (2) the data derived from real-world dialogue transcripts tend to be noisy, containing ambiguous language (including slang, jargon and sarcasm), interruptions, ungrammatical utterances and implicit references. This, in turn, may have limited AMIE s knowledge, capabilities and applicability. To address these limitations, we designed a self-play-based simulated learning environment for diagnostic medical dialogues in a virtual care setting, enabling us to scale AMIE s knowledge and capabilities across a multitude of medical conditions and contexts. We used this environment to iteratively fine-tune AMIE with an evolving set of simulated dialogues in addition to the static corpus of medical question-answering, reasoning, summarization and real-world dialogue data described above. This process consisted of two self-play loops: An inner self-play loop where AMIE leveraged in-context critic feedback to refine its behaviour on simulated conversations with an AI patient agent. An outer self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a continuous learning cycle. At each iteration of fine-tuning, we produced 11,686 dialogues, stemming from 5,230 different medical conditions. The conditions were selected from three datasets: The Health QA dataset12, which contained 613 common medical conditions. The MalaCards Human Disease Database (https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/malacards-diseases.json), which contained 18,455 less-common disease conditions. The MedicineNet Diseases & Conditions Index (https://github.com/Shivanshu-Gupta/web-scrapers/blob/master/medical_ner/medicinenet-diseases.json), which contained 4,617 less-common conditions. At each self-play iteration, four conversations were generated from each of the 613 common conditions, while two conversations were generated from each of the 4,617 less-common conditions randomly chosen from MedicineNet and MalaCards. The average simulated dialogue conversation length was 21.28 turns (P0.25 = 19.0, P0.75 = 25.0). To produce high-quality simulated dialogues at scale, we developed a new multi-agent framework that comprised three key components: A vignette generator: AMIE leverages web searches to craft unique patient vignettes given a specific medical condition. A simulated dialogue generator: three LLM agents play the roles of patient agent, doctor agent and moderator, engaging in a turn-by-turn dialogue simulating realistic diagnostic interactions. A self-play critic: a fourth LLM agent acts as a critic to give feedback to the doctor agent for self-improvement. Notably, AMIE acted as all agents in this framework. The prompts for each of these steps are listed in Supplementary Table 3. The vignette generator aimed to create varied and realistic patient scenarios at scale, which could be subsequently used as context for generating simulated doctor patient dialogues, thereby allowing AMIE to undergo a training process emulating exposure to a greater number of conditions and patient backgrounds. The patient vignette (scenario) included essential background information, such as patient demographics, symptoms, past medical history, past surgical history, past social history and patient questions, as well as an associated diagnosis and management plan. For a given condition, patient vignettes were constructed using the following process. First, we retrieved 60 passages (20 each) on the range of demographics, symptoms and management plans associated with the condition from using an internet search engine. To ensure these passages were relevant to the given condition, we used the general-purpose LLM, PaLM 2 (ref. 10), to filter these retrieved passages, removing any passages deemed unrelated to the given condition. We then prompted AMIE to generate plausible patient vignettes aligned with the demographics, symptoms and management plans retrieved from the filtered passages, by providing a one-shot exemplar to enforce a particular vignette format. Given a patient vignette detailing a specific medical condition, the simulated dialogue generator was designed to simulate a realistic dialogue between a patient and a doctor in an online chat setting where in-person physical examination may not be feasible. Three specific LLM agents (patient agent, doctor agent and moderator), each played by AMIE, were tasked with communicating among each other to generate the simulated dialogues. Each agent had distinct instructions. The patient agent embodied the individual experiencing the medical condition outlined in the vignette. Their role involved truthfully responding to the doctor agent s inquiries, as well as raising any additional questions or concerns they may have had. The doctor agent played the role of an empathetic clinician seeking to comprehend the patient s medical history within the online chat environment51. Their objective was to formulate questions that could effectively reveal the patient s symptoms and background, leading to an accurate diagnosis and an effective treatment plan. The moderator continually assessed the ongoing dialogue between the patient agent and doctor agent, determining when the conversation had reached a natural conclusion. The turn-by-turn dialogue simulation started with the doctor agent initiating the conversation:  Doctor: So, how can I help you today? . Following this, the patient agent responded, and their answer was incorporated into the ongoing dialogue history. Subsequently, the doctor agent formulated a response based on the updated dialogue history. This response was then appended to the conversation history. The conversation progressed until the moderator detected the dialogue had reached a natural conclusion, when the doctor agent had provided a DDx, treatment plan, and adequately addressed any remaining patient agent questions, or if either agent initiated a farewell. To ensure high-quality dialogues, we implemented a tailored self-play3,52 framework specifically for the self-improvement of diagnostic conversations. This framework introduced a fourth LLM agent to act as a  critic , which was also played by AMIE, and that was aware of the ground-truth diagnosis to provide in-context feedback to the doctor agent and enhance its performance in subsequent conversations. Following the critic s feedback, the doctor agent incorporated the suggestions to improve its responses in subsequent rounds of dialogue with the same patient agent from scratch. Notably, the doctor agent retained access to its previous dialogue history in each new round. This self-improvement process was repeated twice to generate the dialogues used for each iteration of fine-tuning. See Supplementary Table 4 as an example of this self-critique process. We noted that the simulated dialogues from self-play had significantly fewer conversational turns than those from the real-world data described in the previous section. This difference was expected, given that our self-play mechanism was designed through instructions to the doctor and moderator agents to simulate text-based conversations. By contrast, real-world dialogue data was transcribed from in-person encounters. There are fundamental differences in communication styles between text-based and face-to-face conversations. For example, in-person encounters may afford a higher communication bandwidth, including a higher total word count and more  back and forth  (that is, a greater number of conversational turns) between the physician and the patient. AMIE, by contrast, was designed for focused information gathering by means of a text-chat interface. AMIE, built upon the base LLM PaLM 2 (ref. 10), was instruction fine-tuned to enhance its capabilities for medical dialogue and reasoning. We refer the reader to the PaLM 2 technical report for more details on the base LLM architecture. Fine-tuning examples were crafted from the evolving simulated dialogue dataset generated by our four-agent procedure, as well as the static datasets. For each task, we designed task-specific instructions to instruct AMIE on what task it would be performing. For dialogue, this was assuming either the patient or doctor role in the conversation, while for the question-answering and summarization datasets, AMIE was instead instructed to answer medical questions or summarize EHR notes. The first round of fine-tuning from the base LLM only used the static datasets, while subsequent rounds of fine-tuning leveraged the simulated dialogues generated through the self-play inner loop. For dialogue generation tasks, AMIE was instructed to assume either the doctor or patient role and, given the dialogue up to a certain turn, to predict the next conversational turn. When playing the patient agent, AMIE s instruction was to reply to the doctor agent s questions about their symptoms, drawing upon information provided in patient scenarios. These scenarios included patient vignettes for simulated dialogues or metadata, such as demographics, visit reason and diagnosis type, for the real-world dialogue dataset. For each fine-tuning example in the patient role, the corresponding patient scenario was added to AMIE s context. In the doctor agent role, AMIE was instructed to act as an empathetic clinician, interviewing patients about their medical history and symptoms to ultimately arrive at an accurate diagnosis. From each dialogue, we sampled, on average, three turns for each doctor and patient role as the target turns to predict based on the conversation leading up to that target turn. Target turns were randomly sampled from all turns in the dialogue that had a minimum length of 30 characters. Similarly, for the EHR note summarization task, AMIE was provided with a clinical note and prompted to generate a summary of the note. Medical reasoning/QA and long-form response generation tasks followed the same set-up as in ref. 13. Notably, all tasks except dialogue generation and long-form response generation incorporated few-shot (1 5) exemplars in addition to task-specific instructions for additional context. To address the core challenge in diagnostic dialogue effectively, acquiring information under uncertainty to enhance diagnostic accuracy and confidence, while maintaining positive rapport with the patient AMIE employed a chain-of-reasoning strategy before generating a response in each dialogue turn. Here  chain-of-reasoning  refers to a series of sequential model calls, each dependent on the outputs of prior steps. Specifically, we used a three-step reasoning process, described as follows: Analysing patient information. Given the current conversation history, AMIE was instructed to: (1) summarize the positive and negative symptoms of the patient as well as any relevant medical/family/social history and demographic information; (2) produce a current DDx; (3) note missing information needed for a more accurate diagnosis; and (4) assess confidence in the current differential and highlight its urgency. Formulating response and action. Building upon the conversation history and the output of step 1, AMIE: (1) generated a response to the patient s last message and formulated further questions to acquire missing information and refine the DDx; and (2) if necessary, recommended immediate action, such as an emergency room visit. If confident in the diagnosis, based on the available information, AMIE presented the differential. Refining the response. AMIE revised its previous output to meet specific criteria based on the conversation history and outputs from earlier steps. The criteria were primarily related to factuality and formatting of the response (for example, avoid factual inaccuracies on patient facts and unnecessary repetition, show empathy, and display in a clear format). This chain-of-reasoning strategy enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply. Prior works developing models for clinical dialogue have focused on metrics, such as the accuracy of note-to-dialogue or dialogue-to-note generations53,54, or natural language generation metrics, such as BLEU or ROUGE scores that fail to capture the clinical quality of a consultation55,56. In contrast to these prior works, we sought to anchor our human evaluation in criteria more commonly used for evaluating the quality of physicians  expertise in history-taking, including their communication skills in consultation. Additionally, we aimed to evaluate conversation quality from the perspective of both the lay participant (the participating patient-actor) and a non-participating professional observer (a physician who was not directly involved in the consultation). We surveyed the literature and interviewed clinicians working as OSCE examiners in Canada and India to identify a minimum set of peer-reviewed published criteria that they considered comprehensively reflected the criteria that are commonly used in evaluating both patient-centred and professional-centred aspects of clinical diagnostic dialogue that is, identifying the consensus for PCCBP in medical interviews19, the criteria examined for history-taking skills by the Royal College of Physicians in the United Kingdom as part of their PACES (https://www.mrcpuk.org/mrcpuk-examinations/paces/marksheets)20 and the criteria proposed by the UK GMCPQ (https://edwebcontent.ed.ac.uk/sites/default/files/imports/fileManager/patient_questionnaire%20pdf_48210488.pdf) for doctors seeking patient feedback as part of professional revalidation (https://www.gmc-uk.org/registration-and-licensing/managing-your-registration/revalidation/revalidation-resources). The resulting evaluation framework enabled assessment from two perspectives the clinician, and lay participants in the dialogues (that is, the patient-actors). The framework included the consideration of consultation quality, structure and completeness, and the roles, responsibilities and skills of the interviewer (Extended Data Tables 1 3). To compare AMIE s performance to that of real clinicians, we conducted a randomized crossover study of blinded consultations in the style of a remote OSCE. Our OSCE study involved 20 board-certified PCPs and 20 validated patient-actors, ten each from India and Canada, respectively, to partake in online text-based consultations (Extended Data Fig. 1). The PCPs had between 3 and 25 years of post-residency experience (median 7 years). The patient-actors comprised of a mix of medical students, residents and nurse practitioners with experience in OSCE participation. We sourced 159 scenario packs from India (75), Canada (70) and the United Kingdom (14). The scenario packs and simulated patients in our study were prepared by two OSCE laboratories (one each in Canada and India), each affiliated with a medical school and with extensive experience in preparing scenario packs and simulated patients for OSCE examinations. The UK scenario packs were sourced from the samples provided on the Membership of the Royal Colleges of Physicians UK website. Each scenario pack was associated with a ground-truth diagnosis and a set of acceptable diagnoses. The scenario packs covered conditions from the cardiovascular (31), respiratory (32), gastroenterology (33), neurology (32), urology, obstetric and gynaecology (15) domains and internal medicine (16). The scenarios are listed in Supplementary Information section 8. The paediatric and psychiatry domains were excluded from this study, as were intensive care and inpatient case management scenarios. Indian patient-actors played the roles in all India scenario packs and 7 of the 14 UK scenario packs. Canadian patient-actors participated in scenario packs for both Canada and the other half of the UK-based scenario packs. This assignment process resulted in 159 distinct simulated patients (that is, scenarios). Below, we use the term  OSCE agent  to refer to the conversational counterpart interviewing the patient-actor that is, either the PCP or AMIE. Supplementary Table 1 summarizes the OSCE assignment information across the three geographical locations. Each of the 159 simulated patients completed the three-step study flow depicted in Fig. 2. The PCPs and patient-actors were primed with sample scenarios and instructions, and participated in pilot consultations before the study began to familiarize them with the interface and experiment requirements. For the experiment, each simulated patient completed two online text-based consultations by means of a synchronous text-chat interface (Extended Data Fig. 1), one with a PCP (control) and one with AMIE (intervention). The ordering of the PCP and AMIE was randomized and the patient-actors were not informed as to which they were talking to in each consultation (counterbalanced design to control for any potential order effects). The PCPs were located in the same country as the patient-actors, and were randomly drawn based on availability at the time slot specified for the consultation. The patient-actors role-played the scenario and were instructed to conclude the conversation after no more than 20 minutes. Both OSCE agents were asked (the PCPs through study-specific instructions and AMIE as part of the prompt template) to not reveal their identity, or whether they were human, under any circumstances. Upon conclusion of the consultation, the patient-actor and OSCE agent each filled in a post-questionnaire in light of the resulting consultation transcript (Extended Data Fig. 1). The post-questionnaire for patient-actors consisted of the complete GMCPQ, the PACES components for  Managing patient concerns  and  Maintaining patient welfare  (Extended Data Table 1) and a checklist representation of the PCCBP category for  Fostering the relationship  (Extended Data Table 2). The responses the patient-actors provided to the post-questionnaire are referred to as  patient-actor ratings . The post-questionnaire for the OSCE agent asked for a ranked DDx list with a minimum of three and no more than ten conditions, as well as recommendations for escalation to in-person or video-based consultation, investigations, treatments, a management plan and the need for a follow-up. Finally, a pool of 33 specialist physicians from India (18), North America (12) and the United Kingdom (3) evaluated the PCPs and AMIE with respect to the quality of their consultation and their responses to the post-questionnaire. During evaluation, the specialist physicians also had access to the full scenario pack, along with its associated ground-truth differential and additional accepted differentials. All of the data the specialist physicians had access to during evaluation are collectively referred to as  OSCE data . Specialist physicians were sourced to match the specialties and geographical regions corresponding to the scenario packs included in our study, and had between 1 and 32 years of post-residency experience (median 5 years). Each set of OSCE data was evaluated by three specialist physicians randomly assigned to match the specialty and geographical region of the underlying scenario (for example, Canadian pulmonologists evaluated OSCE data from the Canada-sourced respiratory medicine scenario). Each specialist evaluated the OSCE data from both the PCP and AMIE for each given scenario. Evaluations for the PCP and AMIE were conducted by the same set of specialists in a randomized and blinded sequence. Evaluation criteria included the accuracy, appropriateness and comprehensiveness of the provided DDx list, the appropriateness of recommendations regarding escalation, investigation, treatment, management plan and follow-up (Extended Data Table 3) and all PACES (Extended Data Table 1) and PCCBP (Extended Data Table 2) rating items. We also asked specialist physicians to highlight confabulations in the consultations and questionnaire responses that is, text passages that were non-factual or that referred to information not provided in the conversation. Each OSCE scenario pack additionally supplied the specialists with scenario-specific clinical information to assist with rating the clinical quality of the consultation, such as the ideal investigation or management plans, or important aspects of the clinical history that would ideally have been elucidated for the highest quality of consultation possible. This follows the common practice for instructions for OSCE examinations, in which specific clinical scenario-specific information is provided to ensure consistency among examiners, and follows the paradigm demonstrated by Membership of the Royal Colleges of Physicians sample packs. For example, this scenario (https://www.thefederation.uk/sites/default/files/Station%202%20Scenario%20Pack%20%2816%29.pdf) informs an examiner that, for a scenario in which the patient-actor has haemoptysis, the appropriate investigations would include a chest X-ray, a high-resolution computed tomography scan of the chest, a bronchoscopy and spirometry, whereas bronchiectasis treatment options a candidate should be aware of should include chest physiotherapy, mucolytics, bronchodilators and antibiotics. We evaluated the top-k accuracy of the DDx lists generated by AMIE and the PCPs across all 159 simulated patients. Top-k accuracy was defined as the percentage of cases where the correct ground-truth diagnosis appeared within the top-k positions of the DDx list. For example, top-3 accuracy is the percentage of cases for which the correct ground-truth diagnosis appeared in the top three diagnosis predictions from AMIE or the PCP. Specifically, a candidate diagnosis was considered a match if the specialist rater marked it as either an exact match with the ground-truth diagnosis, or very close to or closely related to the ground-truth diagnosis (or accepted differential). Each conversation and DDx was evaluated by three specialists, and their majority vote or median rating was used to determine the accuracy and quality ratings, respectively. The statistical significance of the DDx accuracy was determined using two-sided bootstrap tests57 with 10,000 samples and false discovery rate (FDR) correction58 across all k. The statistical significance of the patient-actor and specialist ratings was determined using two-sided Wilcoxon signed-rank tests59, also with FDR correction. Cases where either agent received  Cannot rate/Does not apply  were excluded from the test. All significance results are based on P values after FDR correction. Additionally, we reiterate that the OSCE scenarios themselves were sourced from three different countries, the patient-actors came from two separate institutions in Canada and India, and the specialist evaluations were triplicate rated in this study. History-taking and the clinical interview are widely taught in both medical schools and postgraduate curricula60,61,62,63,64,65. Consensus on physician patient communication has evolved to embrace patient-centred communication practices, with recommendations that communication in clinical encounters should address six core functions fostering the relationship, gathering information, providing information, making decisions, responding to emotions and enabling disease- and treatment-related behaviour19,66,67. The specific skills and behaviours for meeting these goals have also been described, taught and assessed19,68 using validated tools68. Medical conventions consistently cite that certain categories of information should be gathered during a clinical interview, comprising topics such as the presenting complaint, past medical history and medication history, social and family history, and systems review69,70. Clinicians  ability to meet these goals is commonly assessed using the framework of an OSCE4,5,71. Such assessments vary in their reproducibility or implementation, and have even been adapted for remote practice as virtual OSCEs with telemedical scenarios, an issue of particular relevance during the COVID-19 pandemic72. Conversational AI systems for goal-oriented dialogue and task completion have a rich history73,74,75. The emergence of transformers76 and large language models15 have led to renewed interest in this direction. The development of strategies for alignment77, self-improvement78,79,80,81 and scalable oversight mechanisms82 has enabled the large-scale deployment of such conversational systems in the real world16,83. However, the rigorous evaluation and exploration of conversational and task-completion capabilities of such AI systems remains limited for clinical applications, where studies have largely focused on single-turn interaction use cases, such as question-answering or summarization. The majority of explorations of AI as tools for conducting medical consultations have focused on  symptom-checker  applications rather than a full natural dialogue, or on topics such as the transcription of medical audio or the generation of plausible dialogue, given clinical notes or summaries84,85,86,87. Language models have been trained using clinical dialogue datasets, but these have not been comprehensively evaluated88,89. Studies have been grounded in messages between doctors and patients in commercial chat platforms (which may have altered doctor patient engagement compared to 1:1 medical consultations)55,90,91. Many have focused largely on predicting next turns in the recorded exchanges rather than clinically meaningful metrics. Also, to date, there have been no reported studies that have examined the quality of AI models for diagnostic dialogue using the same criteria used to examine and train human physicians in dialogue and communication skills, nor studies evaluating AI systems in common frameworks, such as the OSCE. Prior frameworks for the human evaluation of AI systems  performance in diagnostic dialogue have been limited in detail. They have not been anchored in established criteria for assessing communication skills and the quality of history-taking. For example, ref. 56 reported a five-point scale describing overall  human evaluation , ref . 90 reported  relevance, informativeness and human likeness , and ref . 91 reported  fluency, expertise and relevance , whereas other studies have reported  fluency and adequacy 92 and  fluency and specialty 93. These criteria are far less comprehensive and specific than those taught and practiced by medical professionals. A multi-agent framework for assessing the conversational capabilities of LLMs was introduced in ref. 88, the study, however, was performed in the restricted setting of dermatology, used AI models to emulate both the doctor and patient sides of simulated interactions, and it performed limited expert evaluation of the history-taking as being complete or not. Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Many of the real-world datasets used in the development of AMIE are open-source, including MedQA (https://github.com/jind11/MedQA), MultiMedQA (https://www.nature.com/articles/s41586-023-06291-2#data-availability) and MIMIC-III (https://physionet.org/content/mimiciii/1.4/). The scenario packs from the United Kingdom used in the OSCE study are also available for download from https://www.thefederation.uk/sites/default/files/documents/Station%202%20Scenario%20Pack%20%2816%29.pdf. Additional scenario packs used in the study will be made available upon request. AMIE is an LLM-based research AI system for diagnostic dialogue. Reviewers were provided access to the system through a testing program to interact with the system and evaluate the performance. We are not open-sourcing model code and weights due to the safety implications of the unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators and providers to validate and explore safe onward uses of AMIE. For reproducibility, we have documented technical deep-learning methods while keeping the paper accessible to a clinical and general scientific audience. Our work builds upon PaLM 2, for which technical details have been described extensively in the technical report10. All analyses were conducted using Python v.2.7.18 (https://www.python.org/). Levine, D. History taking is a complex skill. Br. Med. J. 358, j3513 (2017). Engel, G. L. & Morgan, W. L. Interviewing the Patient (W. B. Saunders, 1973). Fu, Y., Peng, H., Khot, T. & Lapata, M. Improving language model negotiation with self-play and in-context learning from AI feedback. Preprint at https://arxiv.org/abs/2305.10142 (2023). Sloan, D. A., Donnelly, M. B., Schwartz, R. W. & Strodel, W. E. The objective structured clinical examination. The new gold standard for evaluating postgraduate clinical performance. Ann. Surg. 222, 735 (1995). Article CAS PubMed PubMed Central Google Scholar Carraccio, C. & Englander, R. The objective structured clinical examination: a step in the direction of competency-based evaluation. Arch. Pediatr. Adolesc. Med. 154, 736 741 (2000). Article CAS PubMed Google Scholar Peterson, M. C., Holbrook, J. H., Von Hales, D., Smith, N. & Staker, L. Contributions of the history, physical examination, and laboratory investigation in making medical diagnoses. West. J. Med. 156, 163 (1992). CAS PubMed PubMed Central Google Scholar Silverman, J., Kurtz, S. & Draper, J. Skills for Communicating with Patients 3rd edn (CRC, 2016). Rennie, T., Marriott, J. & Brock, T. P. Global supply of health professionals. N. Engl. J. Med. 370, 2246 2247 (2014). Article CAS PubMed Google Scholar OpenAI et al. GPT-4 technical report. Preprint at https://arxiv.org/abs/2303.08774 (2023). Anil, R. et al. PaLM 2 technical report. Preprint at https://arxiv.org/abs/2305.10403 (2023). Gemini Team Google et al. Gemini: a family of highly capable multimodal models. Preprint at https://arxiv.org/abs/2312.11805 (2023). Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172 180 (2023). Article ADS CAS PubMed PubMed Central Google Scholar Singhal, K. et al. Toward expert-level medical question answering with large language models. Nat. Med. 31, 943 950 (2025). Nori, H. et al. Can generalist foundation models outcompete special-purpose tuning? Case study in medicine. Preprint at https://arxiv.org/abs/2311.16452 (2023). Thoppilan, R. et al. LaMDA: language models for dialog applications. Preprint at https://arxiv.org/abs/2201.08239 (2022). Introducing ChatGPT. OpenAI https://openai.com/blog/chatgpt (2022). Toma, A. et al. Clinical Camel: an open-source expert-level medical language model with dialogue-based knowledge encoding. Preprint at https://arxiv.org/abs/2305.12031 (2023). Chen, Z. et al. MEDITRON-70B: scaling medical pretraining for large language models. Preprint at https://arxiv.org/abs/2311.16079 (2023). King, A. & Hoppe, R. B.  Best practice  for patient-centered communication: a narrative review. J. Grad. Med. Educ. 5, 385 393 (2013). Article PubMed PubMed Central Google Scholar Dacre, J., Besser, M. & White, P. MRCP(UK) part 2 clinical examination (PACES): a review of the first four examination sessions (June 2001   July 2002). Clin. Med. 3, 452 459 (2003). Article Google Scholar Kelly, C. J., Karthikesalingam, A., Suleyman, M., Corrado, G. & King, D. Key challenges for delivering clinical impact with artificial intelligence. BMC Med. 17, 195 (2019). Article PubMed PubMed Central Google Scholar McDuff, D. et al. Towards accurate differential diagnosis with large language models. Nature https://doi.org/10.1038/s41586-025-08869-4 (2025). Semigran, H. L., Linder, J. A., Gidengil, C. & Mehrotra, A. Evaluation of symptom checkers for self diagnosis and triage: audit study. Br. Med. J. 351, h3480 (2015). Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Intern. Med. 183, 589 596 (2023). Chatgpt. OpenAI https://chat.openai.com/chat (2023). Carrillo de Albornoz, S., Sia, K.-L. & Harris, A. The effectiveness of teleconsultations in primary care: systematic review. Fam. Pract. 39, 168 182 (2022). Article PubMed Google Scholar Fuster-Casanovas, A. & Vidal-Alaball, J. Asynchronous remote communication as a tool for care management in primary care: a rapid review of the literature. Int. J. Integr. Care 22, 7 (2022). Hammersley, V. et al. Comparing the content and quality of video, telephone, and face-to-face consultations: a non-randomised, quasi-experimental, exploratory study in UK primary care. Br. J. Gen. Pract. 69, e595 e604 (2019). Article PubMed PubMed Central Google Scholar Gross, D. A., Zyzanski, S. J., Borawski, E. A., Cebul, R. D. & Stange, K. C. Patient satisfaction with time spent with their physician. J. Fam. Pract. 47, 133 138 (1998). CAS PubMed Google Scholar Dvijotham, K. et al. Enhancing the reliability and accuracy of AI-enabled diagnosis via complementarity-driven deferral to clinicians. Nat. Med. 29, 1814 1820 (2023). Article CAS PubMed Google Scholar Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484 489 (2016). Article ADS CAS PubMed Google Scholar Gallegos, I. O. et al. Bias and fairness in large language models: a survey. Comput. Linguist. 50, 1 79 (2024). Johnson, R. L., Roter, D., Powe, N. R. & Cooper, L. A. Patient race/ethnicity and quality of patient physician communication during medical visits. Am. J. Public Health 94, 2084 2090 (2004). Article PubMed PubMed Central Google Scholar Roter, D. L., Hall, J. A. & Aoki, Y. Physician gender effects in medical communication: a meta-analytic review. JAMA 288, 756 764 (2002). Article PubMed Google Scholar Schillinger, D. et al. Precision communication: physicians  linguistic adaptation to patients  health literacy. Sci. Adv. 7, eabj2836 (2021). Article ADS PubMed PubMed Central Google Scholar Rahman, U. & Cooling, N. Inter-cultural communication skills training in medical schools: a systematic review. Med. Res. Arch. 11, mra.v11i4.3757 (2023). Ganguli, D. et al. Red teaming language models to reduce harms: methods, scaling behaviors, and lessons learned. Preprint at https://arxiv.org/abs/2209.07858 (2022). Mitchell, M. et al. Model cards for model reporting. In Proc. Conference on Fairness, Accountability, and Transparency 220 229 (Association for Computing Machinery, 2019). Crisan, A., Drouhard, M., Vig, J. & Rajani, N. Interactive model cards: a human-centered approach to model documentation. In Proc. 2022 ACM Conference on Fairness, Accountability, and Transparency 427 439 (Association for Computing Machinery, 2022). Pushkarna, M., Zaldivar, A. & Kjartansson, O. Data cards: purposeful and transparent dataset documentation for responsible AI. In Proc. 2022 ACM Conference on Fairness, Accountability, and Transparency 1776 1826 (Association for Computing Machinery, 2022). Choudhury, M. & Deshpande, A. How linguistically fair are multilingual pre-trained language models? In Proc. AAAI Conference on Artificial Intelligence Vol. 35 12710 12718 (Association for the Advancement of Artificial Intelligence, 2021). Nguyen, X.-P., Aljunied, S. M., Joty, S. & Bing, L. Democratizing LLMs for low-resource languages by leveraging their English dominant abilities with linguistically-diverse prompts. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics Vol. 1 (eds Ku, L.-W. et al.) 3501 3516 (Association for Computational Linguistics, 2024). Naous, T., Ryan, M. J., Ritter, A. & Xu, W. Having beer after prayer? Measuring cultural bias in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics Vol. 1 (eds Ku, L.-W. et al.) 16366 16393 (Association for Computational Linguistics, 2024). Ramesh, K., Sitaram, S. & Choudhury, M. Fairness in language models beyond English: gaps and challenges. In Findings of the Association for Computational Linguistics: EACL 2023 (eds Vlachos, A. & Augenstein, I.) 2106 2119 (Association for Computational Linguistics, 2023). Hada, R. et al. Are large language model-based evaluators the solution to scaling up multilingual evaluation? In Findings of the Association for Computational Linguistics: EACL 2024 (eds Graham, Y. & Purver, M.) 1051 1070 (Association for Computational Linguistics, 2024). Quach, V. et al. Conformal language modeling. Preprint at https://arxiv.org/abs/2306.10193 (2023). Lazaridou, A. et al. Mind the gap: assessing temporal generalization in neural language models. Adv. Neural Inf. Process. Syst. 34, 29348 29363 (2021). Google Scholar Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021). Article CAS Google Scholar Johnson, A. E. et al. MIMIC-III, a freely accessible critical care database. Sci. Data 3, 160035 (2016). Article CAS PubMed PubMed Central Google Scholar Chiu, C.-C. et al. Speech recognition for medical conversations. In Proc. Interspeech (ed. Yegnanarayana, B.) 2972 2976 (International Speech Communication Association, 2018). Sharma, A., Miner, A., Atkins, D. & Althoff, T. A computational approach to understanding empathy expressed in text-based mental health support. In Proc. 2020 Conference on Empirical Methods in Natural Language Processing (eds Webber, B. et al.) 5263 5276 (Association for Computational Linguistics, 2020). Aksitov, R. et al. Rest meets ReAct: self-improvement for multi-step reasoning LLM agent. Preprint at https://doi.org/10.48550/arXiv.2312.10003 (2023). Abacha, A. B., Yim, W.-W., Adams, G., Snider, N. & Yetisgen-Yildiz, M. Overview of the MEDIQA-chat 2023 shared tasks on the summarization & generation of doctor-patient conversations. In Proc. 5th Clinical Natural Language Processing Workshop (eds Naumann, T. et al.) 503 513 (Association for Computational Linguistics, 2023). Ionescu, B. et al. in Experimental IR Meets Multilinguality, Multimodality, and Interaction. CLEF 2023 Lecture Notes in Computer Science Vol. 14163 (eds Arampatzis, A. et al.) 370 396 (Springer, 2023). He, Z. et al. DIALMED: a dataset for dialogue-based medication recommendation. In Proc. 29th International Conference on Computational Linguistics (eds Calzolari, N. et al.) 721 733 (International Committee on Computational Linguistics, 2022). Naseem, U., Bandi, A., Raza, S., Rashid, J. & Chakravarthi, B. R. Incorporating medical knowledge to transformer-based language models for medical dialogue generation. In Proc. 21st Workshop on Biomedical Language Processing (eds Demner-Fushman, D. et al.) 110 115 (Association for Computational Linguistics, 2022). Horowitz, J. L. in Handbook of Econometrics, Vol. 5 (eds Heckman, J. J. & Leamer, E.) 3159 3228 (Elsevier, 2001). Benjamini, Y. & Hochberg, Y. Controlling the false discovery rate: a practical and powerful approach to multiple testing. J. R. Stat. Soc. Ser. B Methodol. 57, 289 300 (1995). Article MathSciNet Google Scholar Woolson, R. F. in Wiley Encyclopedia of Clinical Trials (eds D Agostino, R. B. et al.) 1 3 (Wiley, 2007). Keifenheim, K. E. et al. Teaching history taking to medical students: a systematic review. BMC Med. Educ. 15, 159 (2015). Article PubMed PubMed Central Google Scholar Yedidia, M. J. et al. Effect of communications training on medical student performance. JAMA 290, 1157 1165 (2003). Article CAS PubMed Google Scholar Makoul, G. Communication skills education in medical school and beyond. JAMA 289, 93 93 (2003). Article PubMed Google Scholar Tan, X. H. et al. Teaching and assessing communication skills in the postgraduate medical setting: a systematic scoping review. BMC Med. Educ. 21, 483 (2021). Article PubMed PubMed Central Google Scholar Raper, S. E., Gupta, M., Okusanya, O. & Morris, J. B. Improving communication skills: a course for academic medical center surgery residents and faculty. J. Surg. Educ. 72, e202 e211 (2015). Article PubMed Google Scholar Von Fragstein, M. et al. UK consensus statement on the content of communication curricula in undergraduate medical education. Med. Educ. 42, 1100 1107 (2008). Article Google Scholar De Haes, H. & Bensing, J. Endpoints in medical communication research, proposing a framework of functions and outcomes. Patient Educ. Couns. 74, 287 294 (2009). Article PubMed Google Scholar Epstein, R. M. & Street Jr, R. L. Patient-Centered Communication in Cancer Care: Promoting Healing and Reducing Suffering (National Cancer Institute, 2007). Schirmer, J. M. et al. Assessing communication competence: a review of current tools. Fam. Med. 37, 184 92 (2005). PubMed Google Scholar Nichol, J. R., Sundjaja, J. H. & Nelson, G. Medical History (StatPearls, 2018). Denness, C. What are consultation models for? InnovAiT 6, 592 599 (2013). Article Google Scholar Epstein, R. M. & Hundert, E. M. Defining and assessing professional competence. JAMA 287, 226 235 (2002). Article PubMed Google Scholar Chan, S. C. C., Choa, G., Kelly, J., Maru, D. & Rashid, M. A. Implementation of virtual OSCE in health professions education: a systematic review. Med. Educ. 57, 833 843 (2023). Budzianowski, P. et al. MultiWOZ a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proc. 2018 Conference on Empirical Methods in Natural Language Processing (eds Riloff, E. et al.) 5016 5026 (Association for Computational Linguistics, 2018). Wei, W., Le, Q., Dai, A. & Li, J. AirDialogue: an environment for goal-oriented dialogue research. In Proc. 2018 Conference on Empirical Methods in Natural Language Processing (eds Riloff, E. et al.) 3844 3854 (Association for Computational Linguistics, 2018). Lin, J., Tomlin, N., Andreas, J. & Eisner, J. Decision-oriented dialogue for human-AI collaboration. Trans. Assoc. Comput. Linguist. 12, 892 911 (2023). Vaswani, A. et al. Attention is all you need. In Proc. 31st Conference on Neural Information Processing Systems (eds Guyon, I. et al.) 6000 6010 (Curran Associates, 2017). Ouyang, L. et al. Training language models to follow instructions with human feedback. Adv. Neural Inf. Process. Syst. 35, 27730 27744 (2022). Google Scholar Zhao, J., Khashabi, D., Khot, T., Sabharwal, A. & Chang, K.-W. Ethical-advice taker: do language models understand natural language interventions? In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021 (eds Zong, C. et al.) 4158 4164 (Association for Computational Linguistics, 2021). Saunders, W. et al. Self-critiquing models for assisting human evaluators. Preprint at https://arxiv.org/abs/2206.05802 (2022). Scheurer, J. et al. Training language models with language feedback at scale. Preprint at https://arxiv.org/abs/2303.16755 (2023). Glaese, A. et al. Improving alignment of dialogue agents via targeted human judgements. Preprint at https://arxiv.org/abs/2209.14375 (2022). Bai, Y. et al. Constitutional AI: harmlessness from AI feedback. Preprint at https://arxiv.org/abs/2212.08073 (2022). Askell, A. et al. A general language assistant as a laboratory for alignment. Preprint at https://arxiv.org/abs/2112.00861 (2021). Shor, J. et al. Clinical BERTScore: an improved measure of automatic speech recognition performance in clinical settings. In Proc. 5th Clinical Natural Language Processing Workshop (eds Naumann, T. et al.) 1 7 (Association for Computational Linguistics, 2023). Abacha, A. B., Agichtein, E., Pinter, Y. & Demner-Fushman, D. Overview of the medical question answering task at TREC 2017 LiveQA. In Proc. 26th Text Retrieval Conference, TREC 2017 (eds Voorhees, E. M. & Ellis, A.) 1 12 (National Institute of Standards and Technology and the Defense Advanced Research Projects Agency, 2017). Wallace, W. et al. The diagnostic and triage accuracy of digital and online symptom checker tools: a systematic review. NPJ Digit. Med. 5, 118 (2022). Article PubMed PubMed Central Google Scholar Zeltzer, D. et al. Diagnostic accuracy of artificial intelligence in virtual primary care. Mayo Clin. Proc. Digital Health 1, 480 489 (2023). Article PubMed Google Scholar Johri, S. et al. Testing the limits of language models: a conversational framework for medical AI assessment. Preprint at medRxiv https://doi.org/10.1101/2023.09.12.23295399 (2023). Wu, C.-K., Chen, W.-L. & Chen, H.-H. Large language models perform diagnostic reasoning. Preprint at https://arxiv.org/abs/2307.08922 (2023). Zeng, G. et al. MedDialog: large-scale medical dialogue datasets. In Proc. 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (eds Webber, B. et al.) 9241 9250 (Association for Computational Linguistics, 2020). Liu, W. et al. MedDG: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In Proc. 11th CCF International Conference on Natural Language Processing and Chinese Computing (eds Lu, W. et al.) 447 459 (Springer, 2022). Varshney, D., Zafar, A., Behera, N. & Ekbal, A. CDialog: a multi-turn COVID-19 conversation dataset for entity-aware dialog generation. In Proc. 2022 Conference on Empirical Methods in Natural Language Processing (eds Goldberg, Y. et al.) 11373 11385 (Association for Computational Linguistics, 2022). Yan, G. et al. ReMeDi: resources for multi-domain, multi-service, medical dialogues. In Proc. 45th International ACM SIGIR Conference on Research and Development in Information Retrieval 3013 3024 (Association for Computing Machinery, 2022). Download references This project represents an extensive collaboration between several teams at Google Research and Google DeepMind. We thank Y. Liu, D. McDuff, J. Sunshine, A. Connell, P. McGovern and Z. Ghahramani for their comprehensive reviews and detailed feedback on early versions of the manuscript. We also thank S. Lachgar, L. Winer, J. Guilyard and M. Shiels for contributions to the narratives and visuals. We are grateful to J. A. Seguin, S. Goldman, Y. Vasilevski, X. Song, A. Goel, C.-l. Ko, A. Das, H. Yu, C. Liu, Y. Liu, S. Man, B. Hatfield, S. Li, A. Joshi, G. Turner, A. Um rani, D. Pandya and P. Singh for their valuable insights, technical support and feedback during our research. We also thank GoodLabs Studio Inc., Intel Medical Inc. and C. Smith for their partnership in conducting the OSCE study in North America, and the JSS Academy of Higher Education and Research and V. Patil for their partnership in conducting the OSCE study in India. Finally, we are grateful to D. Webster, E. Dominowska, D. Fleet, P. Mansfield, S. Prakash, R. Wong, S. Thomas, M. Howell, K. DeSalvo, J. Dean, J. Manyika, Z. Ghahramani and D. Hassabis for their support during the course of this project. These authors contributed equally: Tao Tu, Mike Schaekermann, Anil Palepu These authors jointly supervised this work: Alan Karthikesalingam, Vivek Natarajan Google Research, Mountain View, CA, USA Tao Tu, Mike Schaekermann, Anil Palepu, Khaled Saab, Jan Freyberg, Amy Wang, Brenna Li, Mohamed Amin, Elahe Vedadi, Karan Singhal, Le Hou, Kavita Kulkarni, Christopher Semturs, Juraj Gottweis, Katherine Chou, Greg S. Corrado, Yossi Matias, Alan Karthikesalingam & Vivek Natarajan Google DeepMind, Mountain View, CA, USA Ryutaro Tanno, Yong Cheng, Nenad Tomasev, Shekoofeh Azizi, Albert Webson, S. Sara Mahdavi & Joelle Barral Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar Search author on:PubMed Google Scholar A.P., M.S., T.T., S.S.M., K. Singhal, S.A., A.K., R.T., J.F. and V.N. contributed to the conception and design of the work; A.P., M.S., T.T., S.S.M., K. Saab, A.K., A. Wang, K.K. and V.N. contributed to the data acquisition and curation; A.P., M.S., T.T., K. Saab, A.K., Y.C., R.T., J.F., N.T., E.V., B.L., M.A. and V.N. contributed to the technical implementation; A.K., V.N., M.S., T.T., A.P. and N.T. contributed to the evaluation framework used in the study; Y.C., L.H., A. Webson and J.G. provided technical and infrastructure guidance; A.K. provided clinical inputs to the study; C.S., J.G., J.B., K.C., G.S.C. and Y.M. contributed to the ideation and execution of the work. All authors contributed to the drafting and revising of the manuscript. Correspondence to Tao Tu, Mike Schaekermann, Alan Karthikesalingam or Vivek Natarajan. This study was funded by Alphabet Inc. and/or a subsidiary thereof ( Alphabet ). All authors are employees of Alphabet and may own stock as part of the standard compensation package. Nature thanks Dean Schillinger and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available. Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Online consultations between patient actors and either AMIE or the primary care physicians (PCPs) were conducted by means of a synchronous text-based chat interface. The evaluation process was facilitated through a rating interface in which specialist physicians were provided the scenario information including differential diagnosis answer key, as well as a consultation transcript along with post-questionnaire responses from AMIE or the PCPs. Rating prompts were provided alongside these pieces of information. a,b: Specialist rated DDx top-k accuracy for the 149  positive  scenarios with respect to (a) the ground-truth diagnosis and (b) the accepted differentials. c,d: Specialist rated DDx top-k accuracy for the 10  negative  scenarios with respect to (c) the ground-truth diagnosis and (d) the accepted differentials. Using two-sided bootstrap tests (n = 10,000) with FDR correction, differences in the  positive  scenarios were significant (P <0.05) for all k, but differences in  negative  scenarios were not significant due to the small sample size. Centrelines correspond to the average top-k accuracy, with 95% confidence intervals shaded. The FDR-adjusted P values for positive disease states, ground-truth comparison: 0.0041 (k = 1), 0.0002 (k = 2), 0.0001 (k = 3), 0.0002 (k = 4), 0.0001 (k = 5), 0.0002 (k = 6), 0.0002 (k = 7), 0.0003 (k = 8), 0.0001 (k = 9) and 0.0001 (k = 10) (a). The FDR-adjusted P values for positive disease states, accepted differential comparison: 0.0002 (k = 1), 0.0001 (k = 2), 0.0002 (k = 3), 0.0003 (k = 4), 0.0001 (k = 5), 0.0001 (k = 6), 0.0001 (k = 7), 0.0001 (k = 8), 0.0001 (k = 9) and 0.0001 (k = 10) (b). The FDR-adjusted P values for non-disease states, ground-truth comparison: 0.1907 (k = 1), 0.1035 (k = 2), 0.1035 (k = 3), 0.1035 (k = 4), 0.1035 (k = 5), 0.1035 (k = 6), 0.1035 (k = 7), 0.1035 (k = 8), 0.1035 (k = 9) and 0.1035 (k = 10) (c). The FDR-adjusted P values for non-disease states, accepted differential comparison: 0.1035 (k = 1), 0.1035 (k = 2), 0.1829 (k = 3), 0.1035 (k = 4), 0.1035 (k = 5), 0.1035 (k = 6), 0.1035 (k = 7), 0.1035 (k = 8), 0.1035 (k = 9) and 0.1035 (k = 10) (d). Top-k DDx accuracy for scenarios with respect to the ground-truth in (a) Cardiology (N = 31, not significant), (b) Gastroenterology (N = 33, not significant), (c) Internal Medicine (N = 16, significant for all k), (d) Neurology (N = 32, significant for k > 5), (e) Obstetrics and Gynaecology (OBGYN)/Urology (N = 15, not significant), (f) Respiratory (N = 32, significant for all k). Two-sided bootstrap tests (n = 10,000) with FDR correction were used to assess significance (P < 0.05) on these cases. Centrelines correspond to the average top-k accuracy, with 95% confidence intervals shaded. The FDR-adjusted P values for Cardiology: 0.0911 (k = 1), 0.0637 (k = 2), 0.0637 (k = 3), 0.0911 (k = 4), 0.0911 (k = 5), 0.0929 (k = 6), 0.0929 (k = 7), 0.0929 (k = 8), 0.0929 (k = 9) and 0.0929 (k = 10) (a). The FDR-adjusted P values for Gastroenterology: 0.4533 (k = 1), 0.1735 (k = 2), 0.1735 (k = 3), 0.1735 (k = 4), 0.1735 (k = 5), 0.1735 (k = 6), 0.1735 (k = 7), 0.1735 (k = 8), 0.1735 (k = 9) and 0.1735 (k = 10) (b). The FDR-adjusted P values for Internal Medicine: 0.0016 (k = 1), 0.0102 (k = 2), 0.0216 (k = 3), 0.0216 (k = 4), 0.0013 (k = 5), 0.0013 (k = 6), 0.0013 (k = 7), 0.0013 (k = 8), 0.0013 (k = 9) and 0.0013 (k = 10) (c). The FDR-adjusted P values for Neurology: 0.2822 (k = 1), 0.1655 (k = 2), 0.1655 (k = 3), 0.069 (k = 4), 0.069 (k = 5), 0.0492 (k = 6), 0.0492 (k = 7), 0.0492 (k = 8), 0.0492 (k = 9) and 0.0492 (k = 10) (d). The FDR-adjusted P values for OBGYN/Urology: 0.285 (k = 1), 0.1432 (k = 2), 0.1432 (k = 3), 0.1432 (k = 4), 0.1432 (k = 5), 0.1432 (k = 6), 0.1432 (k = 7), 0.1432 (k = 8), 0.1432 (k = 9) and 0.1432 (k = 10) (e). The FDR-adjusted P values for Respiratory: 0.0004 (k = 1), 0.0004 (k = 2), 0.0004 (k = 3), 0.0004 (k = 4), 0.0004 (k = 5), 0.0006 (k = 6), 0.0006 (k = 7), 0.0006 (k = 8), 0.0006 (k = 9) and 0.0006 (k = 10) (f). a, b: Specialist DDx rating of AMIE and the PCPs with respect to the ground-truth for the 77 cases conducted in Canada (a) and 82 cases in India (b). The differences between AMIE and the PCPs performance are significant for all values of k. c, d: Auto-evaluation rated DDx for 40 scenarios which were duplicated in both Canada and India for AMIE (c) and the PCPs (d). The differences between Canada and India performance are not significant on these shared scenarios, for both AMIE and the PCPs. Significance was determined using two-sided bootstrap tests (n = 10,000) with FDR correction. Centrelines correspond to the average top-k accuracy, with 95% confidence intervals shaded. The FDR-adjusted P values for Canada comparison: 0.0438 (k = 1), 0.0289 (k = 2), 0.0438 (k = 3), 0.0305 (k = 4), 0.0267 (k = 5), 0.0267 (k = 6), 0.0267 (k = 7), 0.0305 (k = 8), 0.0305 (k = 9) and 0.0276 (k = 10) (a). The FDR-adjusted P values for India comparison: 0.0037 (k = 1), 0.0005 (k = 2), 0.0005 (k = 3), 0.0013 (k = 4), 0.0013 (k = 5), 0.0009 (k = 6), 0.0009 (k = 7), 0.0005 (k = 8), 0.0005 (k = 9) and 0.0005 (k = 10) (b). The FDR-adjusted P values for shared AMIE scenarios: 0.3465 (k = 1), 0.3465 (k = 2), 0.4109 (k = 3), 0.4109 (k = 4), 0.3465 (k = 5), 0.3465 (k = 6), 0.3465 (k = 7), 0.3465 (k = 8), 0.3465 (k = 9) and 0.3465 (k = 10) (c). The FDR-adjusted P values for shared PCP scenarios: 0.3905 (k = 1), 0.4356 (k = 2), 0.3905 (k = 3), 0.3905 (k = 4), 0.3905 (k = 5), 0.3905 (k = 6), 0.3905 (k = 7), 0.3905 (k = 8), 0.3905 (k = 9) and 0.3905 (k = 10) (d). a, b: Top-k DDx auto-evaluation of AMIE s and the PCP s differential diagnoses from their own consultations with respect to the ground-truth (a, significant for k > 3) and the list of accepted differentials (b, significant for k > 4). c, d: Top-k DDx auto-evaluation of AMIE s differential diagnoses when provided its own vs. the PCP s consultation transcript with respect to the ground-truth (c, not significant) and the list of accepted differentials (d, not significant). Two-sided bootstrap tests (n = 10,000) with FDR correction were used to assess significance (P < 0.05) on these 159 cases. Centrelines correspond to the average top-k accuracy, with 95% confidence intervals shaded. The FDR-adjusted P values for AMIE vs. the PCP ground-truth comparison: 0.1399 (k = 1), 0.0737 (k = 2), 0.0596 (k = 3), 0.0315 (k = 4), 0.0221 (k = 5), 0.0315 (k = 6), 0.0182 (k = 7), 0.0221 (k = 8), 0.0182 (k = 9) and 0.0182 (k = 10) (a). The FDR-adjusted P values for AMIE vs. the PCP accepted differential comparison: 0.2297 (k = 1), 0.1713 (k = 2), 0.0779 (k = 3), 0.0546 (k = 4), 0.018 (k = 5), 0.0174 (k = 6), 0.006 (k = 7), 0.0033 (k = 8), 0.0033 (k = 9) and 0.0033 (k = 10) (b). The FDR-adjusted P values for AMIE vs. the PCP consultation ground-truth comparison: 0.4929 (k = 1), 0.4929 (k = 2), 0.4929 (k = 3), 0.4929 (k = 4), 0.4929 (k = 5), 0.4929 (k = 6), 0.4929 (k = 7), 0.4929 (k = 8), 0.4929 (k = 9) and 0.4929 (k = 10) (c). The FDR-adjusted P values for AMIE vs. the PCP consultation accepted differential comparison: 0.4461 (k = 1), 0.4461 (k = 2), 0.4461 (k = 3), 0.4461 (k = 4), 0.4461 (k = 5), 0.4461 (k = 6), 0.4461 (k = 7), 0.4461 (k = 8), 0.4461 (k = 9) and 0.4461 (k = 10) (d). a, Total patient actor words elicited by AMIE and the PCPs. b, Total words sent to patient actor from AMIE and the PCPs. c, Total number of turns in AMIE vs. the PCP consultations. For (a-c), Centrelines correspond to the median, with the box indicating 25th and 75th percentiles. The minimum and maximum are presented as the bottom and top whiskers, respectively, excluding the outliers which are defined as data points further than 1.5 times the inter-quartile range from the box. d, e: The top-3 auto-evaluation rated DDx accuracy of AMIE using the first T turns of each consultation, with respect to the ground-truth diagnosis (d) and the accepted differentials (e). Differences on these 159 cases are not significant (P > 0.05) when compared through two-sided bootstrap tests (n = 10,000) with FDR correction. Centrelines correspond to the average top-3 accuracy, with 95% confidence intervals shaded. This file contains Supplementary Figs. 1 3 and Tables 1 14. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. Reprints and permissions Tu, T., Schaekermann, M., Palepu, A. et al. Towards conversational diagnostic artificial intelligence. Nature 642, 442 450 (2025). https://doi.org/10.1038/s41586-025-08866-7 Download citation Received: 18 January 2024 Accepted: 05 March 2025 Published: 09 April 2025 Issue Date: 12 June 2025 DOI: https://doi.org/10.1038/s41586-025-08866-7 Anyone you share the following link with will be able to read this content: Sorry, a shareable link is not currently available for this article. Provided by the Springer Nature SharedIt content-sharing initiative npj Aging (2025) npj Artificial Intelligence (2025) npj Digital Medicine (2025) Current Treatment Options in Psychiatry (2025) Nature (2025) Advertisement Nature (Nature) ISSN 1476-4687 (online) ISSN 0028-0836 (print)   2025 Springer Nature Limited Sign up for the Nature Briefing newsletter   what matters in science, free to your inbox daily.",2
Developing Artificial Intelligence Tools for Health Care - WCM Newsroom,https://news.google.com/rss/articles/CBMiogFBVV95cUxQdlRrd2lKM3pkQXZvMm9IMnZ6cS1rS1gxYmZhLUZkM0ZTN1dqR3NBakc3RjlpbG4ydFJNdmoyUzV3YnlIcG42U0lxVzVwQ1VfOGhqU3JNaDN4MVZ1dVNTZm9yRGZkOTNkS3hndUw0NkVKaklVbDcxY2cwR0c3SEVnMjk0NmotWlROYno4RzNVVHVHUjByUUVZUnJoVVFtN284aHc?oc=5&hl=en-US&gl=US&ceid=US:en,"Research findings in applying cutting-edge AI methods to patient care, including new benchmarking frameworks and more accurate models, brings us incrementally closer to personalized treatment strategies that have the potential to profoundly improve patient health outcomes. Credit: Shutterstock Reinforcement Learning, an artificial intelligence approach, has the potential to guide physicians in designing sequential treatment strategies for better patient outcomes but requires significant improvements before it can be applied in clinical settings, finds a new study by Weill Cornell Medicine and Rockefeller University researchers. Reinforcement Learning (RL) is a class of machine learning algorithms able to make a series of decisions over time. Responsible for recent AI advances, including superhuman performance at chess and Go, RL can use evolving patient conditions, test results and previous treatment responses to suggest the next best step in personalized patient care. This approach is particularly promising for decision making for managing chronic or psychiatric diseases. The research, published in the Proceedings of the Conference on Neural Information Processing Systems (NeurIPS) and presented Dec. 13, introduces  Episodes of Care  (EpiCare), the first RL benchmark for health care. Dr. Logan Grosenick  Benchmarks have driven improvement across machine learning applications including computer vision, natural language processing, speech recognition and self-driving cars. We hope they will now push RL progress in healthcare,  said Dr. Logan Grosenick, assistant professor of neuroscience in psychiatry, who led the research. RL agents refine their actions based on the feedback they receive, gradually learning a policy that enhances their decision-making.  However, our findings show that while current methods are promising, they are exceedingly data hungry,  Dr. Grosenick adds. The researchers first tested the performance of five state-of-the-art online RL models on EpiCare. All five beat a standard-of-care baseline, but only after training on thousands or tens of thousands of realistic simulated treatment episodes. In the real world, RL methods would never be trained directly on patients, so the investigators next evaluated five common  off-policy evaluation  (OPE) methods: popular approaches that aim to use historical data (such as from clinical trials) to circumvent the need for online data collection. Using EpiCare, they found that state-of-the-art OPE methods consistently failed to perform accurately for health care data.  Our findings indicate that current state-of-the-art OPE methods cannot be trusted to accurately predict reinforcement learning performance in longitudinal health care scenarios,  said first author Dr. Mason Hargrave, research fellow at The Rockefeller University. As OPE methods have been increasingly discussed for health care applications, this finding highlights the need for developing more accurate benchmarking tools, like EpiCare, to audit existing RL approaches and provide metrics for measuring improvement.  We hope this work will facilitate more reliable assessment of reinforcement learning in health care settings and help accelerate the development of better RL algorithms and training protocols appropriate for medical applications,  said Dr. Grosenick. In a second NeurIPS publication presented on the same day, Dr. Grosenick shared his research on adapting convolutional neural networks (CNNs), which are widely used to process images, to work for more general graph-structured data such as brain, gene or protein networks. The broad success of CNNs for image recognition tasks during the early 2010s laid the groundwork for  deep learning  with CNNs and the modern era of neural-network-driven AI applications. CNNs are used in many applications, including facial recognition, self-driving cars and medical image analysis.  We are often interested in analyzing neuroimaging data which are more like graphs, with vertices and edges, than like images. But we realized that there wasn't anything available that was truly equivalent to CNNs and deep CNNs for graph-structured data,  said Dr. Grosenick. Brain networks are typically represented as graphs where brain regions (represented as vertices) propagate information to other brain regions (vertices) along  edges  that connect and represent the strength between them. This is also true of gene and protein networks, human and animal behavioral data and of the geometry of chemical compounds like drugs. By analyzing such graphs directly, we can more accurately model dependencies and patterns between both local and more distant connections. Isaac Osafo Nkansah, a research associate who was in the Grosenick lab at the time of the study and first author on the paper, helped develop the Quantized Graph Convolutional Networks (QuantNets) framework that generalizes CNNs to graphs.  We're now using it for modeling EEG (electrical brain activity) data in patients. We can have a net of 256 sensors over the scalp taking readings of neuronal activity that's a graph,  said Dr. Grosenick.  We're taking those large graphs and reducing them down to more interpretable components to better understand how dynamic brain connectivity changes as patients undergo treatment for depression or obsessive-compulsive disorder."" The researchers foresee broad applicability for QuantNets. For instance, they are also looking to model graph-structured pose data to track behavior in mouse models and in human facial expressions extracted using computer vision.  While we re still navigating the safety and complexity of applying cutting-edge AI methods to patient care, every step forward whether it s a new benchmarking framework or a more accurate model brings us incrementally closer to personalized treatment strategies that have the potential to profoundly improve patient health outcomes,  concluded Dr. Grosenick. Back to News Weill Cornell MedicineOffice of External Affairs Phone: (646) 962-9476",2
Good Machine Learning Practice for Medical Device Development: Guiding Principles - fda.gov,https://news.google.com/rss/articles/CBMi2AFBVV95cUxOTlBUVFp0YTlUMThWYjB0UExwYkFWUTl0Z1VsX3BldllyUGNSc2ozNjQtQUl6ZGNOX3BMdldjcjIxX3JlV3V4dUltVWo5VjZqVnVKZWZNMExXbndCRlA2WlBGQnM2bzJIcm9kR2JaRGpzWmxDU2hULVNYaXRjRjZjUWpqSXBjSUtzQTdOTlJXZjBDZHpuV0hyMEZGc1BHVnpWSmxFdUZEOTVkcndPQXJGdWQ0bW1RRmE2Q3ZON1pGV0NLbERBd1hEVlVFTTVxRElVLWxmUUdxQXI?oc=5&hl=en-US&gl=US&ceid=US:en,"The .gov means it s official.Federal government websites often end in .gov or .mil. Before sharing sensitive information, make sure you're on a federal government site. The site is secure. The https:// ensures that you are connecting to the official website and that any information you provide is encrypted and transmitted securely. Download PDF The U.S. Food and Drug Administration (FDA), Health Canada, and the United Kingdom s Medicines and Healthcare products Regulatory Agency (MHRA) have jointly identified 10 guiding principles that can inform the development of Good Machine Learning Practice (GMLP). These guiding principles will help promote safe, effective, and high-quality medical devices that use artificial intelligence and machine learning (AI/ML). Artificial intelligence and machine learning technologies have the potential to transform health care by deriving new and important insights from the vast amount of data generated during the delivery of health care every day. They use software algorithms to learn from real-world use and in some situations may use this information to improve the product s performance. But they also present unique considerations due to their complexity and the iterative and data-driven nature of their development. These 10 guiding principles are intended to lay the foundation for developing Good Machine Learning Practice that addresses the unique nature of these products. They will also help cultivate future growth in this rapidly progressing field. The 10 guiding principles identify areas where the International Medical Device Regulators Forum (IMDRF), international standards organizations, and other collaborative bodies could work to advance GMLP. Areas of collaboration include research, creating educational tools and resources, international harmonization, and consensus standards, which may help inform regulatory policies and regulatory guidelines. We envision these guiding principles may be used to: As the AI/ML medical device field evolves, so too must GMLP best practice and consensus standards. Strong partnerships with our international public health partners will be crucial if we are to empower stakeholders to advance responsible innovations in this area. Thus, we expect this initial collaborative work can inform our broader international engagements, including with the IMDRF. We welcome your continued feedback through the public docket (FDA-2019-N-1185) at Regulations.gov, and we look forward to engaging with you on these efforts. The Digital Health Center of Excellence is spearheading this work for the FDA. Contact us directly at Digitalhealth@fda.hhs.gov, software@mhra.gov.uk, and mddpolicy-politiquesdim@hc-sc.gc.ca. 03/25/2025",2
Doctor explains how artificial intelligence is already being deployed in medical care - CNN,https://news.google.com/rss/articles/CBMilgFBVV95cUxPNzBRZ3FXOGVYeVVzN2paQmpRcG9FekMxTDNuMTFiS2szTndCNzVKenRUaWN5clhJdTB6Z2V2ako5b3hKYmtuZXVyVEtqdXV5ZkpOV0pzYkR5a0UwWm9GX28yYVMtYnI2V2hBOXUxcUxBNmw3bXMycXNHcTdrTEIxSUZ6TTIyeGNxZE1ydU5uMjl5b0w3NHfSAZsBQVVfeXFMTkRhN3dfN3NxRXVmb0ZnbTRjcjQyS3NXaENUVTBpS3JqTjFoY3lKYU9SVjF3X3JqTGhLaE51dFJHeVJuUkdTMkNZWWJOaXhrbjNmR1d6OVdpSFcwNXJ3MjhNQzNPa0ctbkdqdmpxX0YzeWRvVE9fVzl5c2phWHJRX2dXbGJWS2d4X1FCOEpzMGo5OEMxZTJUcXdxbmM?oc=5&hl=en-US&gl=US&ceid=US:en,"Follow: Is my doctor using artificial intelligence to diagnose me during our appointment? Is he recording our conversation to create an AI summary of our visit? The use of AI health care is still new enough that many people may not know what to make of it. Most Americans expressed  significant discomfort  about the idea of their doctors using AI to help manage their care, according to a 2023 survey. But AI is not likely to go away. The use of AI applications in medical care is growing, and it is important for patients to understand the uses that could improve care   and the reasons for continued caution. I wanted to know how AI is already aiding in diagnosis and helping direct treatment, and what clinicians think about the use of AI. And finally, what are areas of concern, and what is being done to address those? To guide us with these questions, I spoke with CNN wellness expert Dr. Leana Wen. Wen is an emergency physician, adjunct associate professor at George Washington University and a nonresident senior fellow at the Brookings Institution, where her research includes the intersection of technology, medicine and public health. She previously was Baltimore s health commissioner. CNN: How should patients think about the different uses of AI in health care? Dr. Leana Wen: First, it helps to know the difference between predictive and generative artificial intelligence, or AI. Predictive AI uses mathematical models and pattern recognition to predict the future. For example, a predictive AI algorithm can identify which patients with pneumonia are most likely to require hospitalization. Let s say you re the patient. Using past experiences with many other patients with a similar condition   such as pneumonia, diabetes or heart disease   an algorithm could come up with a care plan for you based on factors that could impact your illness, such as your age, gender, other medical conditions, laboratory data and racial and ethnic background. The algorithm can help doctors decide, for instance, whether you need to be hospitalized and what treatment is most likely to be effective for your specific set of circumstances. Related article ChatGPT may have better bedside manner than some doctors, but it lacks some expertise Generative AI uses large language models to generate humanlike interactions. Many people may be familiar with ChatGPT, which is a form of generative AI that answers user questions in a conversational manner. Generative AI can summarize huge quantities of information in a very short period of time, far surpassing that of any human. Some studies have suggested that generative AI models can  learn  so much that they can pass medical licensing exams and that they can generate easy-to-understand, well-written patient instructions on a variety of topics. There are, though, concerns that these models could  hallucinate  and come up with responses that are misleading and inaccurate. And with both predictive and generative AI, how well the models work depends on what data they were trained on. When assessing the utility of AI in health care, it s important to look at each AI tool separately and to understand how it was developed and in what circumstances they are meant to be used. CNN: How is AI already being used to diagnose patients? Wen: There are some well-validated examples of predictive AI being used to augment and improve diagnosis. Take a colonoscopy, which is the gold standard for diagnosing colorectal cancer. During the procedure, the physician passes a long tube through the colon and manually identifies and removes polyps that could be cancerous or precancerous. AI can be  trained  to identify polyps and then flag them during a colonoscopy. Multiple randomized controlled studies performed at health systems around the world have shown that using AI to augment colonoscopies substantially reduces the miss rate of potentially cancerous lesions. Related article As more young people get colon cancer, it may be time for a colonoscopy Similarly, AI is being used to assist with reading mammograms, which is a key screening tool to detect breast cancer. Studies have found that AI-supported mammography screening is at least as accurate as having two trained radiologists read the study and may even improve cancer detection while reducing clinician workload. The US Food and Drug Administration has already authorized about two dozen AI products that help with mammogram cancer screening, though their adoption remains limited due to the potential extra cost involved in deploying them in clinical practice. CNN: How is AI being used to direct treatment? Wen: One use case is a predictive AI algorithm developed by researchers at Johns Hopkins University to identify hospitalized patients at high risk of developing sepsis, which is an overwhelming total body infection that could lead to multi-organ failure and death. This early-warning system has been deployed at multiple hospitals and found to reduce the time it takes to detect sepsis and therefore to start antibiotics and other treatment. Kaiser Permanente has also deployed a predictive AI tool that looks for signs of deterioration in hospitalized patients. If data based on a patient s vital signs, laboratory tests, nurse reports and other factors points to worsening clinical status, the system issues an alert so that the patient can be rapidly evaluated by an in-person clinician. This tool was associated with a significantly lower mortality rate. CNN: How can I find out how my provider s office is using AI for diagnosis and other tasks? What can I do to protect my privacy? Wen: A lot of what we now refer to as AI has existed for some time. Predictive algorithms have been used for years to help tailor treatment plans, for instance. Doctors  offices are using AI more and more to help draft responses to emails and to assist with their documentation. I think it s reasonable to assume that your provider s office is using some form of AI in your care. You could ask your provider and also refer to policies that the office asks you to sign, which may include permissions to use certain technologies. Your medical records are secured and protected by the Health Insurance Portability and Accountability Act, or HIPAA, which legally covers protection of sensitive health information. CNN: What do providers think about the use of AI? Does it reduce inefficiency and alleviate the burden of paperwork? Wen: I think clinicians have been especially glad to have AI applications that reduce documentation and paperwork. For instance, generative AI is being used to assist with medical documentation in a technology called ambient AI. Essentially, the tool  listens  to the conversation between doctor and patient and then converts it into a medical note that the doctor can edit. Studies have found that this ambient AI scribe reduces note-taking time, and it s been viewed positively by physicians and patients alike. Doctors are also using generative AI to help with prior authorization letters they have to send to insurance companies to get permission for certain medications and treatments. This can help reduce a medical office s administrative burden and perhaps even combat health care worker burnout. Related article AI chatbots are becoming popular for therapy. Here s what mental health experts say about them CNN: Could AI be used by insurance companies to turn down claims? Wen: It could be. One could also imagine the presence of AI being a substantial barrier to accessing a human being at an insurance company to discuss claims and other issues. This is one of many areas of concern in using AI. Others include issues like continuing to ensure privacy and data security as well as the need to independently validate algorithms and transparently share these results. Technologists, clinicians and regulators would do well to consider the many positive uses of AI in health care while also making sure to rigorously study each tool and to deploy them with caution.   2025 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved. CNN Sans   &   2016 Cable News Network. For privacy options, please see our privacy policy:https://www.cnn.com/privacy.",2
Can artificial intelligence improve the diagnosis and prognosis of disorders of consciousness? A scoping review - Frontiers,https://news.google.com/rss/articles/CBMiogFBVV95cUxQVE1GS0RNNE9nWFJCeDk0ZEs4UTFRTE0zc3JLSlZpbHB4UnJSUmZDQXJDQlJlNVVDeEpKV3JYeW45dlhNay1BTFd0Y1U4Y0FfN3dxV3hpTkJmSm93QWRZNDJ3WW5GaDBZbzFUdzRkZDFlbU1uS0dWMEY2TEpnTlVUMlpoMTY0ald2bW82MGVPRlhqUHFDN2pUZzl2RVd2bmlpUUE?oc=5&hl=en-US&gl=US&ceid=US:en,"Your new experience awaits. Try the new design now and help us make it even better REVIEW article Front. Artif. Intell., 30 May 2025 Sec. Medicine and Public Health Volume 8 - 2025 | https://doi.org/10.3389/frai.2025.1608778 Background: Artificial intelligence (AI), in the form of machine learning (ML) or deep learning (DL) models, can aid clinicians in the diagnostic process and/or in the prognosis of critically medical conditions, as for patients with a disorder of consciousness (DoC), in which both aspects are particularly challenging. DoC is a category of neurological impairments that are mainly caused by severe acquired brain injury, like ischemic or hemorrhagic strokes or traumatic injuries. The aim of this scoping review is to map the literature on the role of ML and DL in the field of diagnosis and prognosis of DoCs. Materials and methods: A scoping search, started from 3rd October 2024, was conducted for all peer-reviewed articles published from 2000 to 2024, using the following databases: PubMed, Embase, Scopus and Cochrane Library. Results: We found a total of 49,417 articles. After duplicate removal and title/abstract screening, 613 articles met the inclusion criteria, but 592 articles were excluded after full-text review. Therefore, only 21 studies involving DoC subjects were included in the review synthesis. Conclusion: Advancing AI in the field of DoC requires standardized data protocols and consideration of demographic variations. AI could enhance diagnosis, prognosis, and differentiation between states like unresponsive wakefulness syndrome (UWS) and minimally conscious state (MCS). Additionally, AI-based applications personalize rehabilitation by identifying key recovery factors, optimizing patient outcomes. Disorder of consciousness (DoC) is a category of neurological impairments that are mainly caused by severe acquired brain injury, like ischemic or hemorrhagic stroke or traumatic injury (Molteni et al., 2023). From a behavioral perspective, DoCs are divided into two main clinical states: the vegetative state that is recently named unresponsive wakefulness syndrome UWS, in which the patient regains a sleep wake cycle although he or she is not aware of him/herself or of the surrounding environment; and the minimally conscious state (MCS) in which patients partially reacquire a certain degree of awareness. MCS patients can further classified into two categories called MCS plus (+) and MCS minus ( ). These two categories are distinct from each other for their clinical characteristics: MCS+ tends to manifest specific pivotal behaviors, such as consistent and reproducible movement to command, while MCS  tends to manifest automatic motor responses (Bodart et al., 2013). One of the challenges in diagnosing and classifying DoCs is related to determine the presence or absence of a residual state of consciousness following severe acquired brain injury. This state, referred to as  covert consciousness,  is observed in DoC bedridden patients who are unable to exhibit overt signs of residual consciousness (Yang et al., 2024). Since this takes the form of a brain activation in the absence of a behavioral response to simple commands, according to Edlow et al. (2021), covert consciousness can be also defined as cognitive-motor dissociation, and it can be detected by specific neuroimaging and neurophysiological techniques (Edlow et al., 2021). Among these techniques, resting-state functional magnetic resonance imaging (rs-fMRI) is notable for being more accessible and objective than task-based fMRI, as it does not depend on auditory or visual stimuli, removing the need for active patient participation (Mwansisya et al., 2017), which is not always possible in this patient population. An increasing number of studies using rs-fMRI have employed traditional machine learning (ML) methods to explore neuroimaging biomarkers for differentiating patients with varying levels of consciousness (Campbell et al., 2020). Although these findings represent promising strides in elucidating differences in brain activity between DoC patients and controls, the precise differentiation of DoC subgroups, such as UWS and MCS, and between MCS+ or MCS  remains a challenge. A second critical aspect of DoC management is the lack of therapeutically opportunities to integrate rehabilitation management. In this sense, understanding the key prognostic factors that influence patients  recovery can provide valuable insights into determining the most effective and personalized treatment for this patient population. To this end, the use of artificial intelligence (AI) and related technologies (e.g., machine learning and deep learning) are increasingly becoming popular. For example, the application of AI within the diagnostic process supporting clinicians could be of great value for the healthcare context and the patients  overall wellbeing (Poalelungi et al., 2023). The use of AI in the rehabilitation field could be useful to monitor patients  progress over time and to adapt the treatment according to the specific patients  needs. Other authors have already investigated the role of AI in the context of dementia (Andargoli et al., 2024). These authors suggested that AI can support clinical decision, providing an opportunity for a better and earlier diagnosis, an improved diagnostic accuracy, and management of people living with dementia. Similarly, Abdelrahim et al. (2025), investigated the role of AI applied to non-invasive neuroimaging techniques for the early diagnosis of patients with autism spectrum disorder. The authors stated that AI was highly beneficial in classifying patients according to the different diagnosis and to distinguish the different severity levels. In the field of DoC, the integration of AI in clinical management could be useful to understand the biomarkers associated with recovery and potential treatments that lead to better outcomes. By applying various algorithms and techniques, machine learning (ML) processes, analyzes, and improves data to enhance the accuracy of its predictions (Sarker, 2021). In the clinical context, supervised and unsupervised ML algorithms are more commonly used. However, they may present various limitations, such as potentially lower accuracy in highly complex, multivariate problems, but they can be applied to smaller sample sizes. This aspect is fundamental since DoCs are neurological conditions with different etiologies, making them extremely heterogeneous. On the other hand, Deep learning (DL), a subset of ML, offers significantly higher accuracy due to the greater number of hyperparameters (Pugliese et al., 2021; Kufel et al., 2023; Zhao et al., 2023). Clinically, DL applications were mostly used to process neuroimaging, predicting neurological disorders through magnetic resonance imaging (MRI) scans (Miotto et al., 2017). In this way, these technologies could aid clinicians in the diagnostic process and in the rehabilitation pathway, especially in the field of DoC, in which this aspect is particularly challenging. The aim of this scoping review is to map the literature on the role of AI, with regards to ML and DL, in the field of diagnosis and prognosis of DoCs, highlighting the current strengths and gaps, suggesting future perspectives. In particular, we tried to answer to the following questions: 1. What is the role of AI in the diagnosis and prognosis of DoCs? 2. What are the most used AI technologies in the field of DoCs? 3. How can AI improve patients  care? To address the research questions, a comprehensive search was conducted across key research databases, including PuMed, Scopus, Embase and Cochrane library, to identify relevant studies. Using predefined inclusion and exclusion criteria, multiple reviewers independently screened the studies at each stage of the review process. Any disagreements were resolved through consensus, after which the relevant data were extracted for analysis to help answer the research question. Following the formulation of the research questions, the review process involved several key steps: identifying relevant studies, selecting studies, extracting and organizing data, and summarizing and reporting the findings. This scoping review followed the Preferred Reporting Items for Scoping Reviews-Systematic Reviews and Meta-Analyses (SR-PRISMA) guidelines (Page et al., 2021) to enhance the transparency, completeness, reliability, and validity of the reported information (SR-PRISMA checklist is available in the Supplementary Material). The protocol was registered in Open Science Framework (OSF): https://doi.org/10.17605/OSF.IO/6D89Z. We defined our combination of search terms using a PICO (Eriksen and Frandsen, 2018) (population, intervention, comparison, outcome) model. The population was limited to patients with DoC including patients with coma, vegetative state, minimally conscious state and post-traumatic confusional state. The intervention included all studies who explored, described or applied AI to diagnosis, prognosis and clinical management of these disorders. The comparison was related to the differences between AI models, ML and DL algorithms. The results included any contribution to the diagnosis, prognosis and clinical management of DoC or AI potential impact on clinical decision. A scoping search, started from 3trd October 2024, was conducted for all peer-reviewed articles published from 2000 to 2024, using the following databases: PubMed, Embase, Scopus and Cochrane Library, which are the most used databases in the context of medicine and bioengineering field. We used a common search query for each consulted database: (((((((artificial intelligence [Title/Abstract]) OR (deep learning [Title/Abstract])) OR (machine learning [Title/Abstract])) AND (disorder of consciousness [Title/Abstract])) OR (coma [Title/Abstract])) OR (vegetative state [Title/Abstract])) OR (minimally conscious state [Title/Abstract])) OR (post-traumatic confusional state [Title/Abstract]). We included all studies on the adult population (>18 years) affected by DoCs, including UWS and MCS, from any etiology (e.g., cerebro-vascular impairments and/or traumatic brain injury TBI). Specifically, the inclusion criteria were: (i) patients with DoCs; (ii) Use of AI and related technologies for the diagnostic and prognostic assessment of patients with DoC; (iii) written in English language; and (iv) published in a peer-reviewed journal. Articles describing theoretical models, methodological approaches, algorithms, and basic technical descriptions were excluded. We excluded also: (i) animal studies; (ii) conference proceedings and review; (iii) studies involving children; (iv) case reports. The list of articles was refined for relevance, revised, and summarized, with key topics identified based on the inclusion and exclusion criteria. Given the limited literature available, various study designs were included in the qualitative synthesis: (i) Randomized Controlled Trials (RCTs); (ii) Observational studies; (iii) Cross-sectional studies; (iv) Case control studies; and (v) Cohort studies. To reduce the risk of bias, the review process was conducted under strict blinding conditions. Two independent reviewers (MB and DC) were blinded to the identities and affiliations of the authors during both the screening and data extraction stages. This blinding aimed to minimize potential biases, such as publication or language bias. All search results were imported into an online database (RYYAN) (Ouzzani et al., 2016), where the reviewers independently evaluated each study s relevance. After an initial screening based on titles and abstracts, the blind was lifted, and any disagreements on article inclusion or exclusion were resolved through discussion between the two reviewers. Following full-text selection, data extraction from the included studies was recorded in a data sheet. The extracted information included: assigned ID number, study title, year of publication or presentation, first author, study aim, sample size, baseline characteristics, type of intervention/evaluation and control, intervention setting, type of neurophysiological and clinical predictors included, type of ML/DL/AI algorithm used, results and performance metrics (e.g., accuracy, sensitivity, specificity, area under the curve AuC), and presence of interpretability techniques. The initial electronic data search yielded a total of 49,417 potentially relevant articles on PubMed, Embase, Web of Science and Cochrane Library. Of these, 17,184 were duplicated and 77 were non-English articles. A total of 31,543 studies were excluded due to title or abstract. Of the resulting 613 articles, 21 fully met the inclusion criteria and were therefore included in this review. The 21 included studies involved a total of 14,683 patients and 180 healthy controls (reported in only 9 studies). Specifically, 6 studies included 11,256 patients with DoC without specifying the type of consciousness clinical state; 9 studies reported on 367 patients with UWS and 329 with MCS; and 6 studies included 2.731 patients in a comatose state. See Supplementary Table 1 for a complete summary of the studies included. The entire search procedure was reported in Figure 1. Figure 1. PRISMA flow chart reporting the study selection process. Venkataramani et al. (2023) extracted EEG and ECG features as well as clinical parameters. Frequency domain metrics such as the mean power spectral density (PSD) for Delta (0.5 4 Hz), Theta (4 8 Hz), Alpha (8 12 Hz), and Beta (12 30 Hz) bands, and the mean Burst Suppression Ratio (BSR) were derived from nine EEG channels. ECG features included mean and standard deviations across channels. The prediction task was framed as a binary classification (good or poor outcomes) with a dataset of 80 features (36 from EEG, 36 from ECG, and 8 from clinical data). EEG recordings were also used by Di Gregorio et al. (2022), four quantitative EEG (qEEG) features were extracted: z-score PSD, dominant frequency peak, permutation entropy, and mean amplitude. These electrophysiological measures were analyzed to determine their ability to discriminate between brain injury etiologies and to predict clinical outcomes at 6 months post-injury, distinguishing between patients who showed improvement and those who did not. Other authors, like Wang et al. (2022), included pain responses, arousal reactions, spontaneous movements, tendon reflexes, light reflexes, and vital signs within the predictors. These assessments were conducted in addition to EEG connectivity analysis. In the study by Tjepkema-Cloostermans et al. (2019), continuous EEG monitoring was used in addition to Cerebral Performance Category (CPC) scale in patients with post-cardiac arrest care. The authors included the combination of neurophysiological and clinical outcomes to compare the accuracy and reliability of automated DNN-based EEG analysis with traditional visual EEG assessments performed by experienced clinical neurophysiologists. This combined approach, between neurophysiological and clinical outcomes, was also implemented by Aellen et al. Both authors assessed the primary outcome of survival and awakening from coma 3 months post-cardiac arrest, CPC scale to classify survivors (CPC 1 3) and non-survivors (CPC 5). In addition, auditory event-related potentials (ERPs) were included in the dataset. ERPs were also used by Armanfard et al. (2019) with the primary outcome measure set to the detection of Mismatch Negativity (MMN), an ERP indicating auditory discrimination. Lee et al. (2022), developed an Explainable Consciousness Indicator (ECI) using TMS-EEG and resting-state EEG, capable of distinguishing various states of arousal and awareness across sleep, anesthesia, and brain injury. ECI outperformed existing metrics like perturbational complexity index in differentiating conditions such as REM vs. deep sleep and ketamine vs. propofol-induced states and was effective in distinguishing MCS from UWS patients. Campbell et al. (2020) analyzed three distinct datasets: one involving participants transitioning from wakefulness to general anesthesia under propofol, monitored with the Ramsay scale; another with sedated subjects classified by the Observer s Assessment of Alertness/Sedation score; and a third comprising DoC patients evaluated with the CRS-R and undergoing fMRI to explore brain connectivity. fMRI scans were also used by Yang et al. (2024), using 3 different Tesla MRI systems (Philips Ingenia, GE Signa, and Siemens), and both functional and high-resolution anatomical images were acquired to aid in the localization of brain activity. Participants were instructed to rest with eyes closed in a wakeful, non-task state. As other authors, Zheng et al. (2017), investigated thalamo-cortical structural connectivity in DoC patients using diffusion tractography. Lastly, Liu et al. (2023), employed CT scans to predict consciousness levels based on structural brain damage. Riganello et al. (2010), investigated heart rate variability (HRV) changes in response to emotional music stimuli. In healthy controls, self-reported emotional responses were used to classify HRV patterns associated with emotional valence. In UWS patients, HRV was used to evaluate whether emotionally charged auditory stimuli could elicit residual autonomic reactivity. Findings indicated that even UWS patients may exhibit autonomic responses to emotional stimuli, suggesting preserved emotional processing. In the study of Wielek et al. (2018), the primary outcome measure was the identification of distinct sleep stages in DoC patients using ML-based classification. The supervised classification model was trained on healthy subjects  polysomnography data, with traditional sleep staging based on the American Academy of Sleep Medicine (AASM) criteria. The classification was validated using video assessments, which identified periods of prolonged eye opening or closure. In the supervised approach, 11 DoC patients (5 MCS and 6 UWS) showed highly accurate sleep classification with an F1-score of 0.87, indicating strong overlap between predicted sleep stages and observed eye closure. The unsupervised clustering approach revealed a more complex pattern of sleep wake states in MCS patients. This suggests that the presence of structured sleep, whereas UWS patients showed no such clustering, indicating a highly fragmented sleep pattern. In the study of Magliacano et al. (2023), the primary outcome measure was the recovery of full consciousness, assessed using CRS-R scores. A novel Consciousness Domain Index (CDI), derived using unsupervised machine learning from CRS-R sub scores, was validated against clinical diagnoses and CRS-R total scores for prognostic accuracy. CDI showed superior sensitivity and specificity in predicting outcomes at 6, 12, and 24 months, outperforming traditional measures. The CDI classified patients into two clusters, revealing that motor, visual, and auditory CRS-R sub scores were the most critical features for outcome prediction. In a similar way, Campagnini et al. (2024), used CSR-R-derived metrics including total and subscale scores, CRS+ (weighted scores), and the CDI for their predictive utility. Narayanan et al. (2023) used two noninvasive neurobehavioral assessment tools: the Music Therapy Assessment Tool for Awareness in Disorders of Consciousness (MATADOC) and the Coma Recovery Scale-Revised (CRS-R) to predict prolonged DoC diagnostic states. In another study, Zheng et al. (2022) assessed neurological outcomes at three-or six-months post-arrest using the CPC scale, categorizing outcomes as good (CPC 1 2) or poor (CPC 3 5). Poor outcomes were observed in 64% of the cohort. Liuzzi et al. (2022) included patients  demographical and medical data in addition to clinical scales to measure level of consciousness (e.g., CRS-R), functional disability (e.g., Disability rating scale, DRS), and level of clinical complexity (e.g., Early Rehabilitation Barthel Index, ERBI), medical comorbidities before the brain injury (e.g., Cumulative Illness Rating Scale, CIRS), and presence of medical devices (e.g., for supporting respiratory functions, feeding). In a study of El-Rashidy et al. (2023) specific patients  data, through three phases: vital sign acquisition, Fog-Assisted Consciousness Management (FACM), and cloud computing integration. Vital sign data were collected through IoT sensor devices, wearable monitors, and medical reports. The FACM phase managed data transmission between local devices and cloud servers, optimizing processing and storage. The cloud infrastructure facilitated large-scale data analysis and clinical service delivery, aiming to transition healthcare data management from conventional storage to a fully digitalized framework. In the study of Molteni et al. (2019), prognostic assessments were conducted at two time points: 3 months (T0) and 6 months (T1) after hospital admission, using the Levels of Cognitive Functioning (LCF) assessment scale. Prognostic outcomes were categorized into four levels 5 years after admission: death (0), UWS (1), MCS (2), and emergence from MCS (exit MCS = 3). The study aimed to develop a predictive model that could classify patients into these categories while identifying the most relevant neurobehavioral assessment domains contributing to prognosis. In another study by Liuzzi et al. (2022), the authors investigated the impact of medical complications (MCs) on the prediction of clinical outcomes in patients with prolonged disorders of consciousness (pDoC) using machine learning models. The primary goal was to determine whether integrating MC at 3 months after the event could enhance the predictive accuracy of functional outcomes at 6 months post-injury, assessed using the Glasgow Outcome Scale-Extended (GOS-E). Unlike other studies, Muller et al. (2019) used serial NSE measurements over 18 days post-ICU admission to predict 1-year neurological outcomes via CPC. Neurological outcomes were assessed at 1 year using the Clinical Performance Category scale, classifying patients into five outcome groups: dead, vegetative state, severe disability, moderate disability, and good recovery. We found that the selected evidence applied various AI-based approaches for both diagnostic and prognostic purposes. In this result s section, we divided into two parts based on the evidence s aim. DL techniques were implemented by Lee et al. (2022), in which the analysis approach relied on a CNN trained on spatiotemporal EEG features extracted from TMS-EEG responses. The model was optimized using a leave-one-subject-out (LOSO) cross-validation strategy and applied domain transfer learning to improve generalization across sleep, anesthesia, and brain injury datasets. Layer-wise relevance propagation (LRP) was employed to interpret the CNN s decision-making process, revealing that EEG activity in the parietal cortex played a key role in distinguishing consciousness states. Similarly, to distinguish between conscious and unconscious states using rs-fMRI-derived features, Campbell et al., compared three ML models: support vector machine (SVM), Extra Trees (ET), and Artificial Neural Networks (ANN). These models were trained on features extracted from fMRI scans of participants under anesthesia and DoC patients, with the goal of identifying biomarkers that could be generalized to clinical populations. SVM, ET, and ANN models all showed varying degrees of success in differentiating levels of consciousness, with ANN exhibiting strong performance in detecting non-linear patterns in brain connectivity data. Di Gregorio et al. (2022) used linear discriminant analysis (LDA) with LOSO cross-validation to classify patients into improved vs. non-improved groups based on EEG features. The classification was performed separately for TBI and non-TBI patients, allowing for a tailored evaluation of EEG biomarkers within each etiology. Given the retrospective nature of their study, the analysis accounted for class imbalance by incorporating balanced accuracy and precision metrics. Only EEG variables that demonstrated significant group differences in the initial analyses were included in the LDA model to maximize classification performance. Wielek et al. (2018) used combined supervised and unsupervised ML methods to classify sleep stages based on EEG-derived permutation entropy features, a measure of signal complexity. The supervised classification used RF and feedforward neural networks trained on healthy sleep data and tested on DoC patients. The unsupervised approach employed hierarchical clustering to analyze group-level sleep patterns, revealing that MCS patients exhibited sleep organization more similar to healthy individuals, while UWS patients lacked structured sleep cycles. In a similar way, Yang et al. (2024) tested the classification performance of DeepDOC in distinguishing DoC patients from healthy controls and, subsequently, in differentiating MCS from UWS patients. DeepDOC was compared to five state-of-the-art ML models: linear SVM, logistic regression (LR), random forest (RF), extreme gradient boosting (XGBoost), and adaptive boosting (AdaBoost). Performance was evaluated using accuracy and area under the curve (AUC) metrics. DeepDOC achieved superior results, with an AUC of 0.927 and an accuracy of 0.861 in distinguishing MCS from UWS patients. Additionally, the framework excelled in identifying covert motor dysfunction, achieving an AUC of 1 and an accuracy of 0.909. The study also utilized gradient-weighted class activation mapping (Grad-CAM) to interpret the model s decision-making, revealing that the posterior cortex, including the visual cortex, played a key role in distinguishing consciousness states. Narayanan et al. (2023) evaluated the accuracy of ML models in classifying prolonged DoC diagnostic states using supervised and unsupervised learning techniques. Supervised learning methods included decision trees as well as ANNs, both trained using computed diagnostic results to achieve optimal classification. Cross-validation using a 10-fold strategy ensured robust model evaluation, and diagnostic accuracy was assessed through the consistency and predictive reliability of the ML models. Network analysis was also applied to explore potential transitions between prolonged DoC states, aiming to provide novel insights into patient progression. The analysis approach implemented by the authors was useful to effectively classify DoC states using only 13 key variables, eliminating the need for extensive clinical or imaging datasets. Furthermore, the authors also applied an unsupervised based approach (i.e., k-means) to identify hidden patterns in patient data, particularly in cases where there was low agreement in computed diagnostic outcomes. Decision trees and ANNs achieved high predictive accuracy, while k-means clustering revealed distinct patient subgroups with shared diagnostic characteristics. Riganello et al. (2010), used data mining procedures were applied to identify significant changes in HRV, which is considered as a biomarker of autonomic correlate of brain activation, in response to complex auditory stimuli associated with emotional value (i.e., music). The data mining techniques were used to derive significant patterns and associations from the dataset. The open-source software WEKA was employed to train decision trees and identify association rules. A 1-R rule-based classification system was developed, where reported emotions were treated as target variables, and HRV parameters served as predictors. This approach successfully modeled emotional states based on HRV data and revealed that autonomic changes with potential emotional valence could be induced in VS patients, offering new insights into their residual reactivity and the broader implications for emotional processing in disorders of consciousness. In the study by Wang et al. (2022), a novel EEG connectivity measure, the PSD difference incorporating a recursive cosine function (CPSDD), was introduced to enhance classification performance. The Ensemble-Of-SVM classifier consisted of 100 individual SVM models, each trained to classify patients as DoC (+) or awake ( ). The final prediction was obtained through a majority-voting scheme across all models. The classifier s performance was benchmarked against 12 alternative connectivity measures. Among these, the EOSVM using the CPSDD feature yielded the best results, with an accuracy of 98.21, 100% sensitivity, and 95.79% specificity. Similarly, Zheng et al. (2017), assessed the classification accuracy of structural connectivity patterns in differentiating UWS, MCS , and MCS+ patients. Probabilistic diffusion tractography was performed to estimate probability distribution functions for fiber directions in each voxel. Two main analyses were conducted: quantifying thalamic connectivity to cortical targets and generating path distribution maps to visualize these projections. These outputs were compared across patient groups to evaluate how thalamo-cortical connectivity varied between UWS, MCS , and MCS+ patients. A multivariate classification approach using cross-validated SVM and reflector mapping identified the most discriminative regions along thalamo-cortical tracts. They used a reflector mapping technique combined with an e-2-subjects-out cross-validated SVM was used to identify the most discriminative regions along the thalamic tracts. Voxel-wise accuracy maps were computed and tested for statistical significance using inverse binomial distribution. Results showed that MCS+ patients exhibited more preserved thalamo-cortical connectivity than MCS  and UWS patients. Different authors implemented DL approaches for prognostic purposes. For example, in the study by Tjepkema-Cloostermans et al. (2019), convolutional neural networks (CNNs) were trained and validated using five-minute EEG epochs recorded 12- and 24-h after a cardiac arrest. EEG signals were preprocessed with filtering, subsampling, and re-referencing via bipolar and Laplacian montages. Binary cross-entropy served as the loss function, and the network s output was the probability of a good neurological outcome, averaging 30 EEG fragments per five-minute epoch. Separate networks were trained for each montage and time point, and the prognostic value of EEG temporal evolution was also analyzed. The study predicted good neurological outcomes with 48% sensitivity at a 5% false positive rate, and poor outcomes with 58% sensitivity and 0% false positives at 12 h post-arrest. The implementation of deep neural networks (DNNs) allowed for more accurate prognostication in a larger proportion of patients, particularly at early post-cardiac arrest time points. This approach demonstrated the potential of temporal EEG and DL to enhance individualized outcome prediction and support ICU decision-making. Aellen et al. (2023) employed EEGNet, a CNN, to predict outcomes in comatose patients. A 10-fold cross-validation method divided patients into training (60%), validation (20%), and test (20%) sets. The model used binary cross-entropy loss, Adam optimizer, and early stopping for training. Confidence scores for survival were derived from averaged predictions across trials. Predictions were based on averaged confidence scores across single-trial EEG classifications. Performance metrics included the AUC mean of 0.70   0.04 on the test set, with a positive predictive value (PPV) of 0.83   0.03 and a negative predictive value (NPV) of 0.57   0.04. Additionally, for patients with indeterminate clinical prognoses ( gray zone ), the CNN achieved a PPV of 0.86 and an AUC of 0.75, underscoring its utility in uncertain cases. Neural synchrony (phase-locking value) and complexity (Lempel-Ziv) correlated with CNN confidence, with higher synchrony and lower complexity indicating survival. This approach demonstrated strong interpretability and robust predictions, even for patients in the clinical  gray zone.  In the study by Zheng et al. (2022), a DL model exploiting bidirectional long-short term memory (Bi-LSTM) networks was developed to analyze EEG dynamics over time. The model utilized clinically interpretable EEG features such as burst suppression ratio, band power metrics, and spike frequency, derived from five-minute epochs. Predictions of poor neurological outcomes were made by analyzing temporal trends, with the model improving accuracy as additional EEG data accumulated over 66 h. Sensitivity and specificity were calculated at various thresholds, and model performance was measured by the area under the receiver operating characteristic curve (AuC-ROC). The Bi-LSTM model significantly outperformed state-of-the-art approaches such as convolutional neural networks and random forests. It achieved a peak AuC-ROC of 0.88 at 66 h, demonstrating the value of incorporating long-term EEG trends. Calibration curves confirmed high concordance between predicted and actual outcomes. In the study of Venkataramani et al. (2023), XGBoost was used for prognostic prediction due to its ability to handle small datasets with missing values, with hyperparameters fine-tuned for optimal performance. A random forest (RF) regressor was employed for CPC prediction. Validation was performed using 5-fold stratified cross-validation to ensure balanced outcome representation, achieving a cross-validation score of 0.34 and an official challenge score of 0.381. While convolutional architectures like AlexNet and ResNet were tested, they were less effective due to high false positive rates despite their high accuracy. Most of the studies employed supervised machine learning techniques for outcome prediction, while some, like Magliacano et al. (2023) combined unsupervised and supervised approaches to derive and validate prognostic indices. In particular, Campagnini et al. (2024), applied supervised ML models, such as LR, SVM, RF, and k-nearest neighbors (KNN) algorithms. Nested cross-validation ensured robust model evaluation, with missing data imputed using KNN-based methods. Performance was validated using F1-scores and AIC/BIC values across folds. These authors suggested that CRS-R subscale scores, particularly when integrated into machine learning frameworks, offer superior prognostic value, enabling personalized rehabilitation plans with minimal reliance on complex diagnostics. Magliacano et al. (2023) employed k-means clustering with cross-validation to derive the CDI from CRS-R subscores and applied LR to evaluate its association with outcomes. Confounding factors such as age, sex, and injury type were included in multivariate analyses. The CDI improved the prognostic accuracy of recovery predictions over conventional measures, particularly for younger patients, those with TBIs, and those assessed early post-injury. The study supports the use of ML-enhanced indices like CDI for personalized prognosis in pDoC, especially in younger or TBI patients assessed early post-injury. In the study by Armanfard et al. (2019), the Localized Feature Selection method was employed to optimize feature extraction from EEG signals. Unlike traditional feature selection methods, this method assigns a unique feature set to each training sample, enabling the model to handle non-stationarity, disjoint class clusters, and non-linear decision boundaries. This method was particularly effective in addressing challenges of small sample sizes and high-dimensional datasets, reducing overfitting risks. The machine learning framework achieved an impressive accuracy of 92.7% on the healthy training set, as evaluated using a Leave-One-Subject-Out (LOSO) cross-validation procedure. Liu et al. (2023) in their study integrate statistical modeling with AI-based image analysis in the analysis approach. Hemorrhage volume, related to brain hemorrhage, was automatically calculated using the DEEPWISE Medical AI software, ensuring accuracy and eliminating human errors in volume estimation. LR modeling confirmed that hemorrhage volume and ventricular involvement were the most reliable indicators of consciousness status, outperforming other CT-derived features such as hematoma shape, density, and midbrain involvement. Liuzzi et al. (2022), trained and tested four ML models: Elastic-Net (EN), Orthogonal-Matching Pursuit (OMP), KNN, and Support Vector Regressor (SVR). A five-fold cross-validation strategy was used, ensuring robust performance evaluation across different patient subsets. To address dataset imbalance, the Synthetic Minority Oversampling Technique (SMOTE) was applied to resample the training sets. Hyperparameter optimization was performed to minimize cross-validation error, with the models  performance evaluated based on prediction accuracy. Initial results showed an 88.6% accuracy in predicting functional outcomes at admission, which improved to 92.6% when MC data were incorporated into the model. The analysis demonstrated that accessible clinical features, combined with relatively simple ML algorithms, could achieve high accuracy in predicting pDoC patient outcomes without the need for specialized instrumental assessments. In this scoping review, we explored the use of AI, including ML and DL, in the context of DoCs. The selected studies highlighted different methodologies and parameters for assessing and predicting outcomes in patients with DoC. Several parameters ranged from advanced EEG analyses, neuroimaging to behavioral assessments, analyzed within AI (in particular ML and DL) applications. For example, Venkataramani et al. (2023) combined EEG and ECG data to predict post-anoxic coma recovery using tree-based ML algorithms, while Aellen et al. (2023) used CNNs to extract EEG features predictive of coma awakening and long-term survival. In addition, they also used CNNs to extract single-trial information from auditory ERPs on the first day of coma, and at predicting survival 3 months later. In line with our results, the literature review by Lee and Laureys (2024) revealed that most of the evidence in this field is focused on AI for diagnostic purposes, differentiating between UWS and MCS. Another aspect is related to the use of clinical scales like GCS or LCF to predict recovery. While these scales are widely accepted, they may not fully capture the complexity of DoC recovery, compared to the multidimensional assessment done via the CRS-R. Furthermore, the possibility to integrate instrumental data, including behavioral, genetic, and neurophysiological, neuroimaging markers has shown the potential to improve outcome prediction. Based on the selected evidence, certain neurophysiological parameters appear to be crucial for extraction, as they provide valuable insights for prognostic assessments. For instance, Venkataramani et al. (2023) used EEG-derivated bands (i.e., delta, theta, alpha, and beta bands) along with the Burst Suppression Ratio (BSR), extracted from EEG, to assess post-anoxic coma recovery. When BRS is high, it indicates a greater proportion of suppression periods, which can be associated with deeper levels of unconsciousness or impaired brain function. These results are in line with those found by Forgacs et al., in terms of corticothalamic and corticocortical activity, measured in resting state EEG (Forgacs et al., 2017). Similarly, ERPs are often derived from EEG data, as in Aellen et al. (2023), to evaluate sensory responses to stimuli and predict survival rates. While Armanfard et al. (2019) found that MMN reflects automatic cortical responses to auditory stimuli and is a reliable indicator of cortical reactivity. The MMN is a negative-going ERP waveform typically observed when there is a deviation or  mismatch  between a regularly occurring stimulus and a rare, unexpected one (the deviant stimulus). Brain s automatic detection of this mismatch or change in the environment seems to be reflected by the MMN response even when the individual is not consciously attending to the stimulus. According to the selected evidence in this review, EEG-based measures, especially qEEG and functional connectivity, are valuable tools for assessing rehabilitation potential and guiding clinical decision-making. Furthermore, the results Di Gregorio et al. (2022), suggested that combining multiple EEG biomarkers using ML techniques enhances predictive accuracy, reinforcing the role of EEG as a critical tool in DoC prognosis and patient management. Crucially, AI was employed to extract information from neuroimaging data, as in Zheng et al. (2017) where the capability of diffusion tractography to assess structural connectivity, at varying levels of consciousness, was assessed. Yang et al. (2024) used rs-fMRI to measure the connectivity between brain regions, offering a deep understanding of altered states of consciousness, identifying subtle neural patterns indicative of awareness in patients with DoC. The findings by Campbell et al. (2020) suggested that ML classifiers trained on rs-fMRI data from anesthetized subjects may serve as a valuable tool for identifying neural signatures of unconsciousness, potentially improving diagnostic accuracy in clinical settings. This study supports the feasibility of using ML to refine DoC assessment and underscores the potential of anesthesia-based training data in developing robust predictive models for impaired consciousness. In addition, parameters such as hemorrhage volume and ventricular involvement were extracted, as seen in Liu et al. (2023), for assessing outcomes in patients with brainstem hemorrhages. These findings suggested that CT data could be predictive of consciousness recovery or progression in brainstem hemorrhage patients (Liu et al., 2023). In this way, clinicians can exploit a rapid method for assessing consciousness status, aiding in early diagnosis, treatment decisions, and surgical planning for DoC patients. In another study, Riganello et al. (2010) found that HRV serves as a marker of autonomic reactivity and emotional processing, even in patients with minimal consciousness. According to some authors, this autonomic parameter is fundamental to monitor potential recovery of consciousness. In addition, it can be considered as a non-invasive, easy, and low-cost way to monitor patients  vital status. Similarly, Wielek et al. (2018), with polysomnography recordings and sleep patterns, demonstrated that ML can improve sleep staging in DoC patients, offering an objective and automated method to assess residual brain functioning and potentially contributing to prognosis and patient management. In this sense, the role of AI can improve the use of autonomic and vital functions in current clinical practice, suggesting their role for diagnostic purposes. Despite the promising results across these studies, a limitation lies in the variability of the parameters selected for analysis through ML and DL techniques. In this review, we reported the key features highlighted by the authors; however, there is still no consensus on which of the reported parameters are the most robust or reliable for diagnosis and/or prognosis in DoC. This lack of standardization may limit comparability across studies and the clinical implementation of these tools. Another limitation concerns the limited availability of advanced neuroimaging technologies, such as fMRI, which are not accessible in all clinical centers. This may restrict the widespread implementation of AI models that rely on such high-resolution data, especially in low-resource settings, and highlights the need for using accessible diagnostic tools. The studies employed various analysis techniques including both ML and DL approaches, as well as traditional statistical methods. It is noteworthy that the choice of technique depends on the data type, volume, and clinical objectives. According to our results, DL techniques seem to be ideal for complex, high-dimensional data (e.g., rs-fMRI), offering superior predictive power but with higher cost of interpretability and computational demands. On the other hand, traditional ML models like RF and SVM are preferred for smaller datasets with clear feature sets, balancing performance and interpretability. Lastly, Hybrid Approaches that combine ML with IoT infrastructure are promising for dynamic, real-time assessments, particularly in clinical environments with continuous monitoring needs (Ala et al., 2024). Supervised ML techniques were used by several authors like Wang et al. (2022) and Molteni et al. (2019). In particular, Wang et al. (2022) used SVM for binary classification tasks (e.g., DoC vs. awake states). They developed an ensemble of multivariate SVMs (EOSVM) to classify patients into DoC (+) or awake ( ) states based on EEG-derived connectivity measures, including their novel CPSDD metric. Notably, the 97% majority voting in EOSVM significantly enhances reliability. Notably, 35% of patients were diagnosed with an accuracy of 98.21%, a sensitivity of 100%, and a specificity of 95.79%. In a similar way, Zheng et al. (2017) applied binary SVM classifiers to thalamo-cortical connectivity features extracted via probabilistic tractography to distinguish between MCS and VS patients. Using SVM in clinical context can be effective to analyze high-dimensional data like EEG features and for binary classification tasks with limited training samples (Zheng et al., 2017; Wang et al., 2022), one of the main struggles of the research in the neurorehabilitation field (Morone et al., 2024). Other supervised learning techniques include RF that was implemented by Molteni et al. (2019). In particular, Molteni et al. (2019) applied RF for multi-class prediction in pediatric DoC patients. The model was used to classify recovery levels based on neurobehavioral data, and feature important scores helped interpret predictions. In this case, the advantage of using RF lies in the possibility to hold missing data and unbalanced datasets effectively. In addition, it provides interpretable results through feature importance rankings. However, RF can show limitations with large datasets with many trees, and it can be sensitive to noisy features. Liuzzi et al. (2022) used another ML technique such as Elastic-Net regularized LR for predicting functional outcomes in DoC patients, combining clinical observations and vital signs. The pros of using this type of analysis can be related to the fact that it can be applied to small datasets, however the main cons of LR are that it assumes linear relationships, which might not capture complex interactions. CNNs were used by Aellen et al. (2023) to extract features from EEG responses to auditory stimuli for survival prediction at 3 months post-cardiac arrest. While Yang et al. (2024) introduced a 3D EfficientNet-B3-based CNN (DeepDOC) to classify rs-fMRI data into MCS and UWS categories. This approach used two sequential networks to identify DoC patients and further differentiate between subgroups. The CNNs have a great advantage in managing high-dimensional, unstructured data like EEG or fMRI images. However, they require large datasets for training, which are often unavailable in clinical research. On the other hand, recurrent neural networks (RNNs) such as Bi-LSTM were used by Zheng et al. (2022) to analyze temporal EEG trends. The model captured temporal dependencies to predict neurological outcomes, with accuracy improving as more data was included. This approach is ideal or temporal and sequential data like EEG trends. In this scoping review, we have also analyzed the advantages and disadvantages of using AI technologies in the field of DoCs, suggesting potential clinical implications that could be applied in future studies. As a strength, AI methods allow multimodal integration, combining neurophysiological, behavioral, and imaging data enhancing diagnostic precision and prognostic accuracy. In addition, this multimodal integration of clinical data of the patient is fundamental to achieve an overall vision of the medical condition and a patient-centered care approach (see Figure 2). Figure 2. Overview of the AI-based pipeline for DoC assessment. The workflow includes five main stages: (1) Data Collection from clinical, neurophysiological, neuroimaging, autonomic, and demographic sources. (2) Preprocessing and Feature Extraction involving signal processing, normalization, imputation, and dimensionality reduction. (3) Application of AI Models, including machine learning and deep learning approaches. (4) Output and Interpretability for diagnostic and prognostic purposes; and (5) Clinical Workflow Integration, supporting decision-making, treatment personalization, and communication with caregivers. Diagnosis of DoCs is a complex challenge, deriving from the fact that DoC is a neurological condition caused by a disease, such as brain hemorrhage, post-anoxic state, tumors and/or TBI. Moreover, there are behavioral differences among various clinical presentations of DoC. This extreme heterogeneity in terms of etiology and clinical presentation makes the diagnostic classification of DoC even more complex. In this context, AI could assist clinicians by providing insights that help them better diagnose patients with DoC. Some authors (Di Gregorio et al., 2022; Lee et al., 2022; Zheng et al., 2022; Campagnini et al., 2024; Yang et al., 2024) have incorporated results related to the interpretability of AI algorithms performance in their studies. This allows clinicians to better understand why the AI made a particular prediction at the patient-level, facilitating its potential integration into clinical practice, extending possibilities for treatment personalization. However, few studies have investigated the aspect related to the interpretability of AI algorithms, thus future studies could include data on this important aspect. In terms of prognosis, the recovery of consciousness depends on various factors related to basic physiology, such as cardiovascular and respiratory functions, as well as secondary aspects like new brain hemorrhages, internal medicine complications, and infections. These factors can influence DoC patients  recovery, which, as mentioned before, is not a single disease but it is a multi-domain condition, since it impacts the whole body s physiology. In this regard Estraneo et al. (2018) found, through separate binary multivariable LR analyses, that patients with male sex and endocrine-metabolic complications in MCS patients were independent risk factors for mortality at all stages. Furthermore, older age, anoxic etiology, lower CRS-R scores, and a VS diagnosis at study entry were linked to a lower chance of clinical or functional improvement in survivors. In survivors, epilepsy was associated with no clinical improvement only at 24 months post-onset. Similarly, Liuzzi et al. (2022) developed and validated an interpretable decision support tool capable of identifying patients who will achieve a sufficient level of independence (GOS-E > 4) within 6 months. Additionally, at 3 months, the model provided an updated prediction, considering the rehabilitation process and newly emerged medical complexities. In this sense, an AI-based approach can be used to tailor patients  treatment and rehabilitation path, thanks to the continuous recording of neurophysiological parameters (e.g., neuroimaging, HRV) and clinical assessment (e.g., CRS-R). This aspect is also important when the patient needs to start the intervention, as early as possible. In fact, prognostic models could identify timely identification of patients with higher recovery potential, ensuring efficient resource allocation. On the other hand, we sought to explore the weaknesses of AI technologies in the field of DoCs. Firstly, many studies rely on limited homogenous samples. However, the etiology of DoCs can vary from traumatic to acquired brain injury and it can affect people of all ages. This is why it is difficult to obtain a homogenous sample of patients to increase generalizability. Another weakness is related to the differences in data acquisition (e.g., EEG configurations, neuroimaging protocols) among the selected studies. Lastly, it is noteworthy that while advanced ML models excel in accuracy, they often lack clinical interpretability, which is essential for decision-making. This is why multidisciplinary collaboration between biomedical engineers, computer scientists and clinicians, is necessary in order to obtain a technologically advanced, easy-to-use and low-cost tool that can aid to solve problems in the clinical field. Furthermore, future developments of AI in DoC must consider ethical aspects, which play a crucial role, particularly regarding the use of neural data for training algorithms. Neuroethics demands careful evaluation of patient consent, data privacy, and the potential biases embedded in AI models (Ienca and Ignatiadis, 2020; Ienca, 2021). In the case of DoC patients, consent for data acquisition cannot be given directly but must be provided by their legal guardian, raising ethical concerns about autonomy and representation. Ensuring transparency, accountability, and fairness in AI-driven DoC diagnostics and rehabilitation will be essential to increase trust and maximize the benefits of these technologies while safeguarding patient rights. It is important to acknowledge the limitations of the reviewed studies, and the scoping review approach employed. First, the generalizability of many findings is constrained by small sample sizes (Riganello et al., 2010; Wielek et al., 2018; Armanfard et al., 2019; Lee et al., 2022). Secondly, many studies (Di Gregorio et al., 2022; Liuzzi et al., 2022; Liu et al., 2023; Yang et al., 2024) are often based on retrospective data and would benefit from prospective validation to support their translation into routine clinical practice. Regarding the limitations of our scoping review, the exclusion of non-English papers may have resulted in the omission of relevant studies, and the lack of statistical analysis limits the quantitative assessment of the evidence. As a result, our scoping review provided a comprehensive qualitative synthesis of the available evidence, offering valuable insights into the role of AI technologies in the field of diagnosis and prognosis of DoC patients, identifying key implications for clinical practice and considerations for future investigation. Advancing AI applications in DoC requires several key developments such as establishing standardized protocols for data acquisition and preprocessing, considering variations in demographic data and underlying etiologies. Finally, the potential integration of AI technologies into clinical management of DoC could be helpful in diagnosis and prognosis. Diagnosing DoC is inherently complex, but exploiting clinical, neurophysiological, laboratory, and neuroimaging data, could provide a more comprehensive understanding of each patient, enhancing the reliability of AI algorithms. Distinguishing among the different alterations of consciousness could be a challenge for the clinicians. AI-based technologies could support this aspect, facilitating the distinction between the different states of DoC, such as UWS and MCS. Furthermore, AI can contribute to improve and personalize rehabilitative treatment by identifying key factors that influence recovery and incorporating them into rehabilitation protocols, ultimately optimizing patient outcomes. MB: Investigation, Writing   review & editing, Writing   original draft, Data curation, Conceptualization. DC: Formal analysis, Methodology, Writing   review & editing, Writing   original draft, Conceptualization. PL: Writing   review & editing, Writing   original draft, Validation, Visualization. AC: Writing   original draft, Visualization, Supervision, Writing   review & editing, Validation. GM: Writing   original draft, Data curation, Writing   review & editing, Investigation. FC: Resources, Validation, Writing   original draft, Writing   review & editing. AQ: Supervision, Validation, Writing   review & editing, Funding acquisition, Writing   original draft. FT: Writing   review & editing, Supervision, Writing   original draft, Formal analysis, Validation. RSC: Writing   original draft, Visualization, Resources, Supervision, Writing   review & editing. The author(s) declare that financial support was received for the research and/or publication of this article. This research was supported by Current Research Funds 2025, Ministry of Health, Italy. MB is a PhD student enrolled in the National PhD in Artificial Intelligence, XL cycle, course on Health and life sciences, organized by University Campus Bio-Medico di Roma. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. The authors declare that no Gen AI was used in the creation of this manuscript. All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher. The Supplementary material for this article can be found online at: https://www.frontiersin.org/articles/10.3389/frai.2025.1608778/full#supplementary-material Abdelrahim, M., Khudri, M., Elnakib, A., Shehata, M., Weafer, K., Khalil, A., et al. (2025). AI-based non-invasive imaging technologies for early autism spectrum disorder diagnosis: a short review and future directions. Artif. Intell. Med. 161:103074. doi: 10.1016/j.artmed.2025.103074 PubMed Abstract | Crossref Full Text | Google Scholar Aellen, F. M., Alnes, S. L., Loosli, F., Rossetti, A. O., Zubler, F., De Lucia, M., et al. (2023). Auditory stimulation and deep learning predict awakening from coma after cardiac arrest. Brain 146, 778 788. doi: 10.1093/brain/awac340 PubMed Abstract | Crossref Full Text | Google Scholar Ala, A., Simic, V., Pamucar, D., and Bacanin, N. (2024). Enhancing patient information performance in internet of things-based smart healthcare system: hybrid artificial intelligence and optimization approaches. Eng. Appl. Artif. Intell. 131:107889. doi: 10.1016/j.engappai.2024.107889 Crossref Full Text | Google Scholar Andargoli, A. E., Ulapane, N., Nguyen, T. A., Shuakat, N., Zelcer, J., and Wickramasinghe, N. (2024). Intelligent decision support systems for dementia care: a scoping review. Artif. Intell. Med. 150:102815. doi: 10.1016/j.artmed.2024.102815 PubMed Abstract | Crossref Full Text | Google Scholar Armanfard, N., Komeili, M., Reilly, J. P., and Connolly, J. F. (2019). A machine learning framework for automatic and continuous MMN detection with preliminary results for coma outcome prediction. IEEE J. Biomed. Health Inform. 23, 1794 1804. doi: 10.1109/JBHI.2018.2877738 PubMed Abstract | Crossref Full Text | Google Scholar Bodart, O., Laureys, S., and Gosseries, O. (2013). Coma and disorders of consciousness: scientific advances and practical considerations for clinicians. Semin. Neurol. 33, 083 090. doi: 10.1055/s-0033-1348965 PubMed Abstract | Crossref Full Text | Google Scholar Campagnini, S., Llorens, R., Navarro, M. D., Colomer, C., Mannini, A., Estraneo, A., et al. (2024). Which information derived from the coma recovery scale-revised provides the most reliable prediction of clinical diagnosis and recovery of consciousness? A comparative study using machine learning techniques. Eur. J. Phys. Rehabil. Med. 60, 190 197. doi: 10.23736/S1973-9087.23.08093-0 PubMed Abstract | Crossref Full Text | Google Scholar Campbell, J. M., Huang, Z., Zhang, J., Wu, X., Qin, P., Northoff, G., et al. (2020). Pharmacologically informed machine learning approach for identifying pathological states of unconsciousness via resting-state fMRI. NeuroImage 206:116316. doi: 10.1016/j.neuroimage.2019.116316 PubMed Abstract | Crossref Full Text | Google Scholar Di Gregorio, F., La Porta, F., Petrone, V., Battaglia, S., Orlandi, S., Ippolito, G., et al. (2022). Accuracy of EEG biomarkers in the detection of clinical outcome in disorders of consciousness after severe acquired brain injury: preliminary results of a pilot study using a machine learning approach. Biomedicines 10:1897. doi: 10.3390/biomedicines10081897 PubMed Abstract | Crossref Full Text | Google Scholar Edlow, B. L., Claassen, J., Schiff, N. D., and Greer, D. M. (2021). Recovery from disorders of consciousness: mechanisms, prognosis and emerging therapies. Nat. Rev. Neurol. 17, 135 156. doi: 10.1038/s41582-020-00428-x PubMed Abstract | Crossref Full Text | Google Scholar El-Rashidy, N., Sedik, A., Siam, A. I., and Ali, Z. H. (2023). An efficient edge/cloud medical system for rapid detection of level of consciousness in emergency medicine based on explainable machine learning models. Neural Comput. & Applic. 35, 10695 10716. doi: 10.1007/s00521-023-08258-w PubMed Abstract | Crossref Full Text | Google Scholar Eriksen, M. B., and Frandsen, T. F. (2018). The impact of patient, intervention, comparison, outcome (PICO) as a search strategy tool on literature search quality: a systematic review. J. Med. Libr. Assoc. 106, 420 431. doi: 10.5195/jmla.2018.345 PubMed Abstract | Crossref Full Text | Google Scholar Estraneo, A., Loreto, V., Masotta, O., Pascarella, A., and Trojano, L. (2018). Do medical complications impact long-term outcomes in prolonged disorders of consciousness? Arch. Phys. Med. Rehabil. 99, 2523 2531.e3. doi: 10.1016/j.apmr.2018.04.024 PubMed Abstract | Crossref Full Text | Google Scholar Forgacs, P. B., Frey, H., Velazquez, A., Thompson, S., Brodie, D., Moitra, V., et al. (2017). Dynamic regimes of neocortical activity linked to corticothalamic integrity correlate with outcomes in acute anoxic brain injury after cardiac arrest. Ann. Clin. Transl. Neurol. 4, 119 129. doi: 10.1002/acn3.385 PubMed Abstract | Crossref Full Text | Google Scholar Ienca, M. (2021). On neurorights. Front. Hum. Neurosci. 15:701258. doi: 10.3389/fnhum.2021.701258 PubMed Abstract | Crossref Full Text | Google Scholar Ienca, M., and Ignatiadis, K. (2020). Artificial intelligence in clinical neuroscience: methodological and ethical challenges. AJOB Neurosci. 11, 77 87. doi: 10.1080/21507740.2020.1740352 PubMed Abstract | Crossref Full Text | Google Scholar Kufel, J., Bargie - czek, K., Kocot, S., Ko lik, M., Bartnikowska, W., Janik, M., et al. (2023). What is machine learning, artificial neural networks and deep learning? examples of practical applications in medicine. Diagnostics (Basel) 13:2582. doi: 10.3390/diagnostics13152582 PubMed Abstract | Crossref Full Text | Google Scholar Lee, M., and Laureys, S. (2024). Artificial intelligence and machine learning in disorders of consciousness. Curr. Opin. Neurol. 37, 614 620. doi: 10.1097/WCO.0000000000001322 PubMed Abstract | Crossref Full Text | Google Scholar Lee, M., Sanz, L. R. D., Barra, A., Wolff, A., Nieminen, J. O., Boly, M., et al. (2022). Quantifying arousal and awareness in altered states of consciousness using interpretable deep learning. Nat. Commun. 13:1064. doi: 10.1038/s41467-022-28451-0 PubMed Abstract | Crossref Full Text | Google Scholar Liu, G., Sun, J., Zuo, S., Zhang, L., Cai, H., Zhang, X., et al. (2023). The signs of computer tomography combined with artificial intelligence can indicate the correlation between status of consciousness and primary brainstem hemorrhage of patients. Front. Neurol. 14:1116382. doi: 10.3389/fneur.2023.1116382 PubMed Abstract | Crossref Full Text | Google Scholar Liuzzi, P., Magliacano, A., De Bellis, F., Mannini, A., and Estraneo, A. (2022). Predicting outcome of patients with prolonged disorders of consciousness using machine learning models based on medical complexity. Sci. Rep. 12:13471. doi: 10.1038/s41598-022-17561-w PubMed Abstract | Crossref Full Text | Google Scholar Magliacano, A., Liuzzi, P., Formisano, R., Grippo, A., Angelakis, E., Thibaut, A., et al. (2023). Predicting long-term recovery of consciousness in prolonged disorders of consciousness based on coma recovery scale-revised subscores: validation of a machine learning-based prognostic index. Brain Sci. 13:51. doi: 10.3390/brainsci13010051 PubMed Abstract | Crossref Full Text | Google Scholar Miotto, R., Wang, F., Wang, S., Jiang, X., and Dudley, J. T. (2017). Deep learning for healthcare: review, opportunities and challenges. Brief. Bioinform. 19, 1236 1246. doi: 10.1093/bib/bbx044 PubMed Abstract | Crossref Full Text | Google Scholar Molteni, E., Canas, L. D. S., Briand, M.-M., Estraneo, A., Font, C. C., Formisano, R., et al. (2023). Scoping review on the diagnosis, prognosis, and treatment of pediatric disorders of consciousness. Neurology 101, e581 e593. doi: 10.1212/WNL.0000000000207473 PubMed Abstract | Crossref Full Text | Google Scholar Molteni, E., Colombo, K., Beretta, E., Galbiati, S., Santos Canas, L. D., Modat, M., et al. (2019). Comparison of multi-class machine learning methods for the identification of factors most predictive of prognosis in neurobehavioral assessment of pediatric severe disorder of consciousness through LOCFAS scale. Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. 2019, 269 272. doi: 10.1109/EMBC.2019.8856880 PubMed Abstract | Crossref Full Text | Google Scholar Morone, G., Claudia, M.-E., Bonanno, M., Ciancarelli, I., Mazzoleni, S., and Calabr , R. S. (2024). Breaking the ice through an effective translationality in neurorehabilitation: are we heading to the right direction? Expert Rev. Med. Devices 21, 999 1006. doi: 10.1080/17434440.2024.2418399 PubMed Abstract | Crossref Full Text | Google Scholar Muller, E., Shock, J. P., Bender, A., Kleeberger, J., H gen, T., Rosenfelder, M., et al. (2019). Outcome prediction with serial neuron-specific enolase and machine learning in anoxic-ischaemic disorders of consciousness. Comput. Biol. Med. 107, 145 152. doi: 10.1016/j.compbiomed.2019.02.006 PubMed Abstract | Crossref Full Text | Google Scholar Mwansisya, T. E., Hu, A., Li, Y., Chen, X., Wu, G., Huang, X., et al. (2017). Task and resting-state fMRI studies in first-episode schizophrenia: a systematic review. Schizophr. Res. 189, 9 18. doi: 10.1016/j.schres.2017.02.026 PubMed Abstract | Crossref Full Text | Google Scholar Narayanan, A., Magee, W. L., and Siegert, R. J. (2023). Machine learning and network analysis for diagnosis and prediction in disorders of consciousness. BMC Med. Inform. Decis. Mak. 23:41. doi: 10.1186/s12911-023-02128-0 PubMed Abstract | Crossref Full Text | Google Scholar Ouzzani, M., Hammady, H., Fedorowicz, Z., and Elmagarmid, A. (2016). Rayyan a web and mobile app for systematic reviews. Syst. Rev. 5:210. doi: 10.1186/s13643-016-0384-4 PubMed Abstract | Crossref Full Text | Google Scholar Page, M. J., McKenzie, J. E., Bossuyt, P. M., Boutron, I., Hoffmann, T. C., Mulrow, C. D., et al. (2021). The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. BMJ 372:n71. doi: 10.1136/bmj.n71 PubMed Abstract | Crossref Full Text | Google Scholar Poalelungi, D. G., Musat, C. L., Fulga, A., Neagu, M., Neagu, A. I., Piraianu, A. I., et al. (2023). Advancing patient care: how artificial intelligence is transforming healthcare. J. Pers. Med. 13:1214. doi: 10.3390/jpm13081214 PubMed Abstract | Crossref Full Text | Google Scholar Pugliese, R., Regondi, S., and Marini, R. (2021). Machine learning-based approach: global trends, research directions, and regulatory standpoints. Data Sci. Manag. 4, 19 29. doi: 10.1016/j.dsm.2021.12.002 Crossref Full Text | Google Scholar Riganello, F., Candelieri, A., Quintieri, M., Conforti, D., and Dolce, G. (2010). Heart rate variability: an index of brain processing in vegetative state? An artificial intelligence, data mining study. Clin. Neurophysiol. 121, 2024 2034. doi: 10.1016/j.clinph.2010.05.010 PubMed Abstract | Crossref Full Text | Google Scholar Sarker, I. H. (2021). Machine learning: algorithms, real-world applications and research directions. SN Comput. Sci. 2:160. doi: 10.1007/s42979-021-00592-x PubMed Abstract | Crossref Full Text | Google Scholar Tjepkema-Cloostermans, M. C., da Silva Louren o, C., Ruijter, B. J., Tromp, S. C., Drost, G., Kornips, F. H. M., et al. (2019). Outcome prediction in Postanoxic coma with deep learning*. Crit. Care Med. 47, 1424 1432. doi: 10.1097/CCM.0000000000003854 PubMed Abstract | Crossref Full Text | Google Scholar Venkataramani, V. V., Garg, A., Maity, M., and Priyakumar, U. D. (2023).  A machine learning approach for outcome prediction in Postanoxic coma patients using frequency domain features  in 2023 Computing in Cardiology (CinC), 1 4. Google Scholar Wang, F., Tian, Y.-C., Zhang, X., and Hu, F. (2022). Detecting disorders of consciousness in brain injuries from EEG connectivity through machine learning. IEEE Trans. Emerg. Top. Comput. Intell. 6, 113 123. doi: 10.1109/TETCI.2020.3032662 Crossref Full Text | Google Scholar Wielek, T., Lechinger, J., Wislowska, M., Blume, C., Ott, P., Wegenkittl, S., et al. (2018). Sleep in patients with disorders of consciousness characterized by means of machine learning. PLoS One 13:e0190458. doi: 10.1371/journal.pone.0190458 PubMed Abstract | Crossref Full Text | Google Scholar Yang, H., Wu, H., Kong, L., Luo, W., Xie, Q., Pan, J., et al. (2024). Precise detection of awareness in disorders of consciousness using deep learning framework. NeuroImage 290:120580. doi: 10.1016/j.neuroimage.2024.120580 PubMed Abstract | Crossref Full Text | Google Scholar Zhao, S., Tang, G., Liu, P., Wang, Q., Li, G., and Ding, Z. (2023). Improving mortality risk prediction with routine clinical data: a practical machine learning model based on eICU patients. Int. J. Gen. Med. 16, 3151 3161. doi: 10.2147/IJGM.S391423 PubMed Abstract | Crossref Full Text | Google Scholar Zheng, W.-L., Amorim, E., Jing, J., Wu, O., Ghassemi, M., Lee, J. W., et al. (2022). Predicting neurological outcome from electroencephalogram dynamics in comatose patients after cardiac arrest with deep learning. I.E.E.E. Trans. Biomed. Eng. 69, 1813 1825. doi: 10.1109/TBME.2021.3139007 PubMed Abstract | Crossref Full Text | Google Scholar Zheng, Z. S., Reggente, N., Lutkenhoff, E., Owen, A. M., and Monti, M. M. (2017). Disentangling disorders of consciousness: insights from diffusion tensor imaging and machine learning. Hum. Brain Mapp. 38, 431 443. doi: 10.1002/hbm.23370 PubMed Abstract | Crossref Full Text | Google Scholar Keywords: artificial intelligence, machine learning, deep learning, interpretability, diagnosis, prognosis, neurorehabilitation, disorder of consciousness Citation: Bonanno M, Cardile D, Liuzzi P, Celesti A, Micali G, Corallo F, Quartarone A, Tomaiuolo F and Calabr  RS (2025) Can artificial intelligence improve the diagnosis and prognosis of disorders of consciousness? A scoping review. Front. Artif. Intell. 8:1608778. doi: 10.3389/frai.2025.1608778 Received: 09 April 2025; Accepted: 09 May 2025; Published: 30 May 2025. Edited by: Reviewed by: Copyright   2025 Bonanno, Cardile, Liuzzi, Celesti, Micali, Corallo, Quartarone, Tomaiuolo and Calabr . This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. *Correspondence: Davide Cardile, davide.cardile@irccsme.it  These authors have contributed equally to this work Disclaimer: All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article or claim that may be made by its manufacturer is not guaranteed or endorsed by the publisher. Frontiers' impact Your research is the real superpower - learn how we maximise its impact through our leading community journals Share on Share on Supplementary Material Table 1.pdf Presentation 1.pdf",2
AI in Life Sciences: Top 5 Use Cases in 2025 - Netguru,https://news.google.com/rss/articles/CBMiakFVX3lxTE92SkxlaGoySXpFb0RFSGl5NS1RYjZHN3U4aW1xRmZncUwtdFVzSUxZNE1hXzVPQUh5THJsMjA3SlFmU2wxM0xHRHcyakVSYUh3QXRzRmh4M3FZaVBrUmpmNGx5XzZJQTVxZUE?oc=5&hl=en-US&gl=US&ceid=US:en,"Ideation Software Development Design Generative AI and Data Maintenance Cooperation Models Industries Finance Commerce Other industries Case studies Speeding Up Merck s Process from 6 Months to 6 Hours with an AI R&D Assistant 60% More Engagement With Hyper-Personalization for a US Proptech Other projects Learn more Join Netguru Contact us Blog A Step by Step Guide to De-Risking Product Globalization Mastering Product Information Management: Your Key to the Digital Age Newsletters and originals Przemys aw Turkowski In this blog post While productivity is one of the most recognized benefits of AI across many industries, for the life science sector, it s just the tip of the iceberg. Underneath it lie groundbreaking capabilities such as safer drug formulation, faster clinical trial recruitment, and advanced restorative medicine. To fully grasp the impact of artificial intelligence on life sciences, it s worth looking at how quickly its value is growing. In 2024, it s expected to reach $2.25 billion; but, by 2032, it s set to be worth over four times more! So, stating that the life science industry is experiencing  a time of change  would be a huge understatement. Each passing month seems to bring a new AI-powered breakthrough for healthcare   up to a point where it might be hard to keep up. To help you navigate this dynamic field, I discuss the top AI in life sciences trends and mention a few examples of successful implementations by researchers and clinicians. I d like to begin by looking at some of the most prominent trends in the life sciences and healthcare sector, starting with how researchers are using AI s generative capabilities. It s hard to find a person that hasn t tested or at least heard of generative artificial intelligence   it s taking the business world by storm, with life sciences and healthcare being no exception. This trend is supposed to continue in 2024 and beyond. According to Deloitte , more than 90% of biopharma and medtech leaders realize the impact of genAI on the industry. In fact, 66% of companies in life sciences are using it to enhance their business operations, including compliance improvement and supply chain transformation. For compliance, genAI automates the monitoring and reporting processes, ensuring that companies adhere to regulatory standards more efficiently and with fewer errors. In supply chain management, it uses predictive analytics to forecast demand and optimize inventory levels, leading to more efficient logistics and distribution strategies. A spike in cross disciplinary partnerships is among the 2024 healthcare trends. Big pharma and digital health companies that in the past were far from becoming allies are now joining forces to make healthcare delivery better. This shift sees pharmaceutical companies increasingly integrating digital solutions into their operations. A prime example is the partnership between Novo Nordisk Pharmaceuticals, a global leader in diabetes treatment and a South-Korean startup, Kakao Healthcare . They aim to offer advanced digital health services that will help chronically ill people better manage their conditions. Additionally, a few pharmaceutical giants, including Gilead, Merck, Novartis, Bayer, and Sanofi launched the Digital Pharma Circle (DPC) , in September 2023. DPC was formed to encourage business leaders from the digital health sector of the pharmaceutical industry to have insightful conversations on the use of digital tech to transform the life sciences sector. I think we re witnessing a positive change, where great minds come together to use their knowledge and expertise to make healthcare more interconnected. This is something the world has been waiting for for a long time. Integrating AI and ML into healthcare research is one of the most exciting trends in life sciences. In fact, it s more than a trend   it s a true revolution that s unraveling in front of our eyes. Data analyses which would normally take weeks or months to complete manually can now be performed almost instantaneously with the help of AI. Since it can peruse millions, or even billions of records at once, it s already shortening the path to drug discovery, medical trial recruitment, and disease detection. One of the lesser publicized, yet very promising areas of AI in life sciences, is the ability to test out potential drug and medical compound interactions. This can not only help spot probable health- or life-threatening blends. It can also indicate how effective each medicine combination could be for disease and symptom treatment. All this can be done in silico , i.e., in a virtual simulation, without putting any humans at risk. It s easy to imagine how this accelerated ability for drug development will optimize the work of researchers and, ultimately, how it could benefit patients in the years to come. All the new research possibilities that artificial intelligence created are a blessing for the life science sector. At the same time though, it s a wake up call for organizations that have been lagging behind in their digital acceleration and still operate on siloed data. Research centers participating in the race for delivering new drugs or medical technologies to the market must have access to verified data from all areas of the business. Or, in the case of cross-organizational initiatives, even from multiple collaborating entities. Companies that have access to complete data and use AI to accelerate product development and manufacturing will have what it takes to win the market. Let s now take a look at the impact of AI in life sciences. This list could be a lot longer, but in this article I will only focus on AI use cases in life sciences. I think one of the areas where AI in healthcare is making the biggest impact is drug development and discovery. Thanks to the use of data science, deep learning and machine learning, AI is able to quickly analyze massive data sets, and as a result, accelerate the discovery of new molecules. Since AI algorithms can simultaneously sift through information from multiple sources like published scientific literature, clinical trial data, public databases and conference summaries it can identify therapies that might work for specific health conditions. The speed with which AI in life sciences can examine large data volumes can cut down the drug discovery and development time to months, rather than years, and I think that is truly revolutionary. The life sciences industry operates under stringent controls and regulations, so another AI use case in life sciences, which I think is worth mentioning is making the manufacturing process more efficient. How does it work? Pharmaceutical and biotechnology companies can use sensors in their manufacturing facilities to gather data about the manufacturing process, including the state of the facility, machine performance, process progress, etc. Once the information is collected it s sent for analysis. Thanks to the use of AI, companies can generate insights and identify patterns that humans would be unable to see. For example, detect potential quality control issues or pinpoint process bottlenecks. Such information can help businesses proactively respond to problems, adapt to changes like demand fluctuations, and ensure a smoother operation of the manufacturing process. Spotting and reporting side effects of newly-tested drugs has always been a challenge in traditional pharmacovigilance processes. Especially, if trials are run across numerous clinics or research centers across the globe. AI now makes this easier by collecting and analyzing patient data from numerous sources. While it s one of the newer AI use cases in life sciences, several studies have already confirmed its efficacy in drug safety research. Depending on the scope of the trial, it can leverage computer vision to analyze CT scans or other visual data, use natural language processing (NLP) to spot potential adverse effects in self-reported notes, or even analyze data from wearable devices. If any potential adverse effects are detected, then an alert can be triggered to the drug development team. They can then act immediately, i.e., investigate the issue and prevent any health- or life-threatening incidents. Not only does this improve drug trial safety, but can also lead to financial savings. How high? Qing Xie, researcher from the Department of Pharmacology at Jinan University, says that  in the United States of America alone, it has been estimated that the cost of adverse drug reactions ranges from $76.6 billion to $152 billion annually. Therefore, pharmacovigilance activities aimed at detecting and preventing ADRs can potentially save healthcare systems significant amounts of money.  Manual, proper recruitment of patients usually takes months, if not years. Or, in some cases, trials are canceled entirely. Before AI in life sciences became a reality, as many as 86% of trials never took off because researchers couldn t put together a sufficient, eligible patient sample. As I ve mentioned earlier, AI can  see ,  listen , and  read  data and handle it by the billion. Drug developers can turn to it to find eligible candidates who meet various criteria   from pure demographics and previous medical treatments to DNA sequencing data. This makes it easier for clinicians and drug developers to shortlist patients for each trial phase. This application of AI in healthcare is already used worldwide. Among others, it has helped select patients for pediatric leukemia treatment trials. While many AI use cases in life sciences operate within the digital realm, artificial intelligence is also reimagining more  tangible  medical devices. Researchers are developing a vast array of hardware, from AI-powered prosthetics to brain and spinal implants. These technologies commonly rely on biological signal analysis. In my opinion, one of the more recent groundbreaking developments was the case of a paralyzed man who had two implants, one in the brain and the other in the abdomen, installed during surgery. One of the implants uses AI to read thoughts in his brain and detect the impulse to move, while the other causes muscular contractions to trigger body movement. Given the pace at which AI is developing, I m certain that we re going to see plenty of more revolutionary developments like this in restorative medicine. Let s now look at some real-word examples of AI in the life science sector, and how they re already benefiting researchers and patients alike. Trial Pathfinder is an open-sourced AI framework developed by a group of scholars at Stanford University. It allows life science organizations to access real-world data, i.e., patient health records, and use them to simulate drug trials. Among others, researchers can use Trial Pathfinder to evaluate drug efficacy and survival ratio, i.e., survival rates for patients who were taking the tested drug vs those who were given a placebo. Some of the more known uses of Trial Pathfinder in recent years include an evaluation of over 61,000 patients against their eligibility for a non-small-cell lung cancer oncology trial. What makes the AI framework stand out in my opinion, though, is that it s used not only for recruiting patients, per se. It also helps researchers improve how clinical trial design is approached by the wider life science sector. This puts this initiative high on my list of projects  to watch . Atomwise is an preclinical pharma research company which uses AI to speed up new drug discovery. According to their website, AtomNet , a neural network-based technology they developed has helped identify more  undruggable targets'' than any other drug discovery platform powered by AI. The company collaborates with over 250 life science organizations across the globe, helping them develop novel solutions for more than 600 illnesses. These include cancer, neurological, and cardiological conditions, as well as infectious diseases. The long-term goal for the company is to help reinvent the entire drug discovery pipeline. And, if we consider the wide interest from investors and the pharma industry, it appears that they might be on the right track. In late-2022, Sanofi signed a $1.2 billion deal, commissioning the development and synthesis of small molecules for the pharma giant s exclusive use. Insilico Medicine is a great example of generative AI in life sciences. The biotech company leverages genAI to find molecules that could be used in new drugs and to forecast their clinical performance. The company has already developed five successful compounds, including a potentially revolutionary Inflammatory Bowel Disease (IBD) drug which has entered Phase I clinical trials in early-2024. The safety of the AI-designed drug, currently known as ISM5411 , is being tested among 76 volunteers. If the first trials are a success, then Insilico intends to set up several centers worldwide and run the study across several groups of IBD sufferers. Assuming that the drug enters the market, the company will make history. They ll be the first to introduce an IBD treatment which doesn t rely on immunosuppression, which in itself comes with significant health risks. The trends in the life sciences field clearly show that AI is going to play a major role in industry   this is undeniable. However, while the potential of AI in healthcare is tremendous, and definitely calls for further exploration, it s not free from challenges, including ethical considerations. As we integrate artificial intelligence into healthcare, numerous questions arise, which relate to: Let s take a closer look at the two most prominent ethical issues of using generative AI in life sciences: algorithmic bias and informed decision-making. Unfortunately, AI algorithms aren t free from bias. In fact, they can magnify certain disparities, which already exist in healthcare including racial and socioeconomic ones. If the data used to train AI is skewed, for example, favoring one race over others, the results will inherently be biased. What are the consequences? This can lead to certain groups receiving less effective or inappropriate treatment, and their overall access to healthcare might be compromised   some groups will be favored while others will be discriminated against. This shows how important it is to train AI on diverse, real-life data sets, which are thoroughly verified before being used. Also, continuous monitoring and ethical guidelines are necessary to guarantee AI s equitable application in healthcare software development. Businesses shouldn t assume that patients know how their data will be used by AI applications, nor should they expect patients to understand the implications of sharing their data. Getting informed consent calls for clear and concise communication with patients, explaining the specifics of how their data will be used and its potential consequences. Additionally, patients must also be given the option to opt out at any time. The often-limited understanding of patients regarding how exactly their data will be used, raises concerns about data privacy. What s more, the rapid development of AI introduces another problem i.e., the expiration of informed consent, an issue which still has to be tackled. I realize that the above statement may seem bold, but this transformation is already underway, and the healthcare trends that we ve discussed above only confirm it. GenAI is revolutionizing the life sciences industry by enhancing disease diagnosis, cutting the time for drug development, and enabling personalized treatments   and this is just the beginning. As AI is rapidly becoming a more integral part of healthcare, it s crucial to find balance between innovation and ethics. Only then will we be able to achieve optimal patient outcomes, and look forward to a future where healthcare is more accessible, efficient and precise. More posts by this author Przemys aw Turkowski Enhance efficiency and patient care. Learn more! At Netguru we specialize in designing, building, shipping and scaling beautiful, usable products with blazing-fast efficiency. Let's talk business I agree to receive marketing communication from Netguru. Netguru is committed to processing the above information in order to subscribe you to the newsletter. Other information is used for statistical purposes and, from time to time, we would like to contact you about our products and services, as well as other content that may be of interest to you. If you consent to contact you for these purposes, please tick the checkbox. You can unsubscribe from these communications at any time. For more information on how to unsubscribe, our privacy practices please view our Privacy Policy.  2025 Netguru S.A. All rights reserved.",2
